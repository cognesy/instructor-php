{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor for PHP","text":"<p>Instructor for PHP is a lightweight library that makes it easy to get structured outputs from Large Language Models (LLMs). Built on top of modern PHP 8.2+ features, it provides a simple, type-safe way to work with AI models.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Type Safety: Full PHP 8.2+ type system support with strict typing</li> <li>Multiple LLM Support: Works with OpenAI, Anthropic, Gemini, Cohere, and more</li> <li>Validation: Built-in validation with custom rules and LLM-powered validation</li> <li>Streaming: Real-time partial object updates for better UX</li> <li>Function Calling: Native support for LLM function/tool calling</li> <li>Zero Dependencies: Clean, lightweight implementation</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Person {\n    public string $name;\n    public int $age;\n    public string $occupation;\n}\n\n$text = \"Extract: Jason is 25 years old and works as a software engineer.\";\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\necho $person-&gt;name; // \"Jason\"\necho $person-&gt;age;  // 25\necho $person-&gt;occupation; // \"software engineer\"\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your path:</p> <ul> <li>Quick Start - Get up and running in 5 minutes</li> <li>Setup Guide - Detailed installation and configuration</li> <li>Cookbook - Practical examples and recipes</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>This project consists of several modular packages:</p> <ul> <li>Instructor - Main structured output library</li> <li>Polyglot - Low-level LLM abstraction layer  </li> <li>HTTP Client - Flexible HTTP client for API calls</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: cognesy/instructor-php</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Join the conversation</li> </ul> <p>Instructor for PHP - Making AI outputs predictable and type-safe.</p>"},{"location":"cookbook/contributing/","title":"Contributing","text":""},{"location":"cookbook/contributing/#were-looking-for-your-help","title":"We're looking for your help","text":"<p>We're looking for a bunch more examples.</p> <p>If you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul> <p>We're also looking for help to catch up with the features available in Instructor Hub for Python (see: https://github.com/jxnl/instructor/blob/main/docs/hub/index.md).</p> <ul> <li> Better viewer with pagination</li> <li> Examples database</li> <li> Pulling in the code to your own dir, so you can get started with the API</li> </ul>"},{"location":"cookbook/contributing/#how-to-contribute","title":"How to contribute","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single .php file.</li> <li>Please include documentation in the file - check existing examples for the format.</li> <li>Make sure that the code is tested.</li> </ol> <pre><code>// @snippet-id=12e5\nnamespace Cognesy\\Polyglot\\Examples;\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\necho \"Hello, world!\\n\";\n</code></pre>"},{"location":"cookbook/introduction/","title":"Instructor Cookbooks","text":""},{"location":"cookbook/introduction/#overview","title":"Overview","text":"<p>Welcome to Instructor cookbooks. The goal of this section is to provide a set of tutorials and examples to help you get started.</p> <p>Instructor comes with a CLI tool that allows you to view and interact with the tutorials and examples and allows you to find the code snippets you may need to get solution to your problem.</p> <p>     Examples are only available with Instructor project cloned locally.     We did not want to include them in the Composer package to keep it lightweight. </p>"},{"location":"cookbook/introduction/#step-1-clone-instructor-project-from-github","title":"Step 1: Clone Instructor project from Github","text":"<p>To get access to the tutorials and examples, you need to clone the Instructor project from Github:</p> <pre><code>$ git clone https://github.com/cognesy/instructor-php.git\n</code></pre>"},{"location":"cookbook/introduction/#step-2-create-env-file","title":"Step 2: Create <code>.env</code> file","text":"<p>Create a <code>.env</code> file in the root directory of your copy of Instructor project and set your LLM API key(s). You can use the <code>.env-dist</code> file as a template.</p>"},{"location":"cookbook/introduction/#step-3-check-the-available-tutorials","title":"Step 3: Check the available tutorials","text":"<p>You can check the available tutorials and examples by running the following command in terminal:</p> <pre><code>$ ./bin/instructor-hub list\n</code></pre>"},{"location":"cookbook/introduction/#available-cli-commands","title":"Available CLI Commands","text":""},{"location":"cookbook/introduction/#list-cookbooks","title":"List Cookbooks","text":"<p>Run <code>./bin/instructor-hub list</code> you can see all the available tutorials and examples.</p> <pre><code>$ ./bin/instructor-hub list\n</code></pre>"},{"location":"cookbook/introduction/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>./bin/instructor-hub show {id}</code> to see the full tutorial in the terminal.</p> <pre><code>$ ./bin/instructor-hub show {id}\n</code></pre> <p>Currently, there is no way to page through the tutorial - feel free to contribute :)</p>"},{"location":"cookbook/introduction/#running-a-cookbook","title":"Running a Cookbook","text":"<p>To run a tutorial, you run <code>./bin/instructor-hub run {id}</code> in terminal - it will execute the code and show the output. You need to have your OPENAI_API_KEY set in your environment (.env file in root directory of your copy of instructor-php repo).</p> <pre><code>$ ./bin/instructor-hub run {id}\n</code></pre>"},{"location":"cookbook/introduction/#running-all-cookbooks","title":"Running all Cookbooks","text":"<p>This is mostly for testing if cookbooks are executed properly, but you can run <code>./bin/instructor-hub all {id}</code> to run all the tutorials and examples in the terminal, starting from the one you specify.</p> <pre><code>$ ./bin/instructor-hub all {id}\n</code></pre>"},{"location":"cookbook/instructor/advanced/config_providers/","title":"Use custom configuration providers","text":""},{"location":"cookbook/instructor/advanced/config_providers/#overview","title":"Overview","text":"<p>You can inject your own configuration providers to StructuredOutput class. This is useful for integration with your preferred framework (e.g. Symfony, Laravel).</p>"},{"location":"cookbook/instructor/advanced/config_providers/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Adbar\\Dot;\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass CustomConfigProvider implements CanProvideConfig\n{\n    private Dot $dot;\n\n    public function __construct(array $config = []) {\n        $this-&gt;dot = new Dot($config);\n    }\n\n    public function get(string $path, mixed $default = null): mixed {\n        return $this-&gt;dot-&gt;get($path, $default);\n    }\n\n    public function has(string $path): bool {\n        return $this-&gt;dot-&gt;has($path);\n    }\n}\n\n$configData = [\n    'http' =&gt; [\n        'defaultPreset' =&gt; 'symfony',\n        'presets' =&gt; [\n            'symfony' =&gt; [\n                'driver' =&gt; 'symfony',\n                'connectTimeout' =&gt; 10,\n                'requestTimeout' =&gt; 30,\n                'idleTimeout' =&gt; -1,\n                'maxConcurrent' =&gt; 5,\n                'poolTimeout' =&gt; 60,\n                'failOnError' =&gt; true,\n            ],\n            // Add more HTTP presets as needed\n        ],\n    ],\n    'debug' =&gt; [\n        'defaultPreset' =&gt; 'off',\n        'presets' =&gt; [\n            'off' =&gt; [\n                'httpEnabled' =&gt; false,\n            ],\n            'on' =&gt; [\n                'httpEnabled' =&gt; true,\n                'httpTrace' =&gt; true,\n                'httpRequestUrl' =&gt; true,\n                'httpRequestHeaders' =&gt; true,\n                'httpRequestBody' =&gt; true,\n                'httpResponseHeaders' =&gt; true,\n                'httpResponseBody' =&gt; true,\n                'httpResponseStream' =&gt; true,\n                'httpResponseStreamByLine' =&gt; true,\n            ],\n        ],\n    ],\n    'llm' =&gt; [\n        'defaultPreset' =&gt; 'deepseek',\n        'presets' =&gt; [\n            'deepseek' =&gt; [\n                'apiUrl' =&gt; 'https://api.deepseek.com',\n                'apiKey' =&gt; Env::get('DEEPSEEK_API_KEY'),\n                'endpoint' =&gt; '/chat/completions',\n                'model' =&gt; 'deepseek-chat',\n                'maxTokens' =&gt; 128,\n                'driver' =&gt; 'deepseek',\n                'httpClientPreset' =&gt; 'symfony',\n            ],\n            'openai' =&gt; [\n                'apiUrl' =&gt; 'https://api.openai.com',\n                'apiKey' =&gt; Env::get('OPENAI_API_KEY'),\n                'endpoint' =&gt; '/v1/chat/completions',\n                'model' =&gt; 'gpt-4',\n                'maxTokens' =&gt; 256,\n                'driver' =&gt; 'openai',\n                'httpClientPreset' =&gt; 'symfony',\n            ],\n        ],\n    ],\n    'structured' =&gt; [\n        'defaultPreset' =&gt; 'tools',\n        'presets' =&gt; [\n            'tools' =&gt; [\n                'outputMode' =&gt; OutputMode::Tools,\n                'useObjectReferences' =&gt; true,\n                'maxRetries' =&gt; 3,\n                'retryPrompt' =&gt; 'Please try again ...',\n                'modePrompts' =&gt; [\n                    OutputMode::MdJson-&gt;value =&gt; \"Response must validate against this JSON Schema:\\n&lt;|json_schema|&gt;\\n. Respond correctly with strict JSON object within a ```json {} ``` codeblock.\\n\",\n                    OutputMode::Json-&gt;value =&gt; \"Response must follow JSON Schema:\\n&lt;|json_schema|&gt;\\n. Respond correctly with strict JSON object.\\n\",\n                    OutputMode::JsonSchema-&gt;value =&gt; \"Response must follow provided JSON Schema. Respond correctly with strict JSON object.\\n\",\n                    OutputMode::Tools-&gt;value =&gt; \"Extract correct and accurate data from the input using provided tools.\\n\",\n                ],\n                'schemaName' =&gt; 'user_schema',\n                'toolName' =&gt; 'user_tool',\n                'toolDescription' =&gt; 'Tool to extract user information ...',\n                'chatStructure' =&gt; [\n                    'system',\n                    'pre-cached',\n                        'pre-cached-prompt', 'cached-prompt', 'post-cached-prompt',\n                        'pre-cached-examples', 'cached-examples', 'post-cached-examples',\n                        'cached-messages',\n                    'post-cached',\n                    'pre-prompt', 'prompt', 'post-prompt',\n                    'pre-examples', 'examples', 'post-examples',\n                    'pre-messages', 'messages', 'post-messages',\n                    'pre-retries', 'retries', 'post-retries'\n                ],\n                // defaultOutputClass is not used in this example\n                'outputClass' =&gt; Structure::class,\n            ]\n        ]\n    ]\n];\n\n$events = new EventDispatcher();\n$configProvider = new CustomConfigProvider($configData);\n\n$customClient = (new HttpClientBuilder(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withConfigProvider($configProvider)\n    -&gt;withPreset('symfony')\n    -&gt;create();\n\n$structuredOutput = (new StructuredOutput(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withHttpClient($customClient);\n\n// Call with custom model and execution mode\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = $structuredOutput\n    -&gt;using('deepseek') // Use 'deepseek' preset defined in our config provider\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/context_cache_structured/","title":"Context caching (structured output)","text":""},{"location":"cookbook/instructor/advanced/context_cache_structured/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching, so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note 1: Instructor supports context caching for Anthropic API and OpenAI API.</p> <p>Note 2: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/instructor/advanced/context_cache_structured/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its structured description for multiple audiences.</p> <p>Let's start by defining the data model for the project details and the properties that we want to extract or generate based on README file.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass Project {\n    public string $name;\n    public string $targetAudience;\n    /** @var string[] */\n    #[Description('Technology platform and libraries used in the project')]\n    public array $technologies;\n    /** @var string[] */\n    #[Description('Target audience domain specific features and capabilities of the project')]\n    public array $features;\n    /** @var string[] */\n    #[Description('Target audience domain specific applications and potential use cases of the project')]\n    public array $applications;\n    #[Description('Explain the purpose of the project and the target audience domain specific problems it solves')]\n    public string $description;\n    #[Description('Target audience domain specific example code in Markdown demonstrating an application of the library')]\n    public string $code;\n}\n?&gt;\n</code></pre> <p>We read the content of the README.md file and cache the context, so it can be reused for multiple requests.</p> <p><pre><code>&lt;?php\n$content = file_get_contents(__DIR__ . '/../../../README.md');\n\n$cached = (new StructuredOutput)-&gt;using('anthropic')-&gt;withCachedContext(\n    system: 'Your goal is to respond questions about the project described in the README.md file'\n        . \"\\n\\n# README.md\\n\\n\" . $content,\n    prompt: 'Respond with strict JSON object using schema:\\n&lt;|json_schema|&gt;',\n);\n?&gt;\n</code></pre> At this point we can use Instructor structured output processing to extract the project details from the README.md file into the <code>Project</code> data model.</p> <p>Let's start by asking the user to describe the project for a specific audience: P&amp;C insurance CIOs.</p> <p><pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response1 = $cached-&gt;with(\n    messages: 'Describe the project in a way compelling to my audience: P&amp;C insurance CIOs.',\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::MdJson,\n)-&gt;create();\n\n// get processed value - instance of Project class\n$project1 = $response1-&gt;get();\ndump($project1);\nassert($project1 instanceof Project);\nassert(Str::contains($project1-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage1 = $response1-&gt;response()-&gt;usage();\necho \"Usage: {$usage1-&gt;inputTokens} prompt tokens, {$usage1-&gt;cacheWriteTokens} cache write tokens\\n\";\n?&gt;\n</code></pre> Now we can use the same context to ask the user to describe the project for a different audience: boutique CMS consulting company owner.</p> <p>Anthropic API will use the context cached in the previous request to provide the response, which results in faster processing and lower costs.</p> <pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response2 = $cached-&gt;with(\n    messages: \"Describe the project in a way compelling to my audience: boutique CMS consulting company owner.\",\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::Json,\n)-&gt;create();\n\n// get the processed value - instance of Project class\n$project2 = $response2-&gt;get();\ndump($project2);\nassert($project2 instanceof Project);\nassert(Str::contains($project2-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage2 = $response2-&gt;response()-&gt;usage();\necho \"Usage: {$usage2-&gt;inputTokens} prompt tokens, {$usage2-&gt;cacheReadTokens} cache read tokens\\n\";\nassert($usage2-&gt;cacheReadTokens &gt; 0, 'Expected cache read tokens');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_config/","title":"Customize parameters of LLM driver","text":""},{"location":"cookbook/instructor/advanced/custom_config/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to Instructor. This is useful when you want to initialize OpenAI client with custom values - e.g. to call other LLMs which support OpenAI API.</p>"},{"location":"cookbook/instructor/advanced/custom_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Drivers\\Symfony\\SymfonyDriver;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$events = new EventDispatcher();\n\n// Build fully customized HTTP client\n\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 30,\n    requestTimeout: 60,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true,\n);\n\n$yourClientInstance = SymfonyHttpClient::create(['http_version' =&gt; '2.0']);\n\n$customClient = (new HttpClientBuilder)\n    -&gt;withEventBus($events)\n    -&gt;withDriver(new SymfonyDriver(\n        config: $httpConfig,\n        clientInstance: $yourClientInstance,\n        events: $events,\n    ))\n    -&gt;create();\n\n// Create instance of LLM connection preset initialized with custom parameters\n\n$llmConfig = new LLMConfig(\n    apiUrl  : 'https://api.deepseek.com',\n    apiKey  : Env::get('DEEPSEEK_API_KEY'),\n    endpoint: '/chat/completions', model: 'deepseek-chat', maxTokens: 128, driver: 'openai-compatible',\n);\n\n// Get Instructor with the default client component overridden with your own\n\n$structuredOutput = (new StructuredOutput)\n    -&gt;withEventHandler($events)\n    -&gt;withLLMConfig($llmConfig)\n    -&gt;withHttpClient($customClient);\n\n// Call with custom model and execution mode\n\n$user = $structuredOutput\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_http_client/","title":"Use custom HTTP client instance","text":""},{"location":"cookbook/instructor/advanced/custom_http_client/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/advanced/custom_http_client/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Dispatchers\\SymfonyEventDispatcher;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Symfony\\Component\\EventDispatcher\\EventDispatcher;\nuse Symfony\\Component\\HttpClient\\HttpClient;\n\n// custom Symfony components\n// custom Symfony components\n\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Call with custom model and execution mode\n\n$yourSymfonyClientInstance = HttpClient::create(['http_version' =&gt; '2.0']);\n$yourSymfonyEventDispatcher = new SymfonyEventDispatcher(new EventDispatcher());\n\n$user = (new StructuredOutput(events: $yourSymfonyEventDispatcher))\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withLLMConfigOverrides(['apiUrl' =&gt; 'https://api.openai.com/v1'])\n    -&gt;withClientInstance(\n        driverName: 'symfony',\n        clientInstance: $yourSymfonyClientInstance\n    )\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    //-&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_http_client_laravel/","title":"Use custom HTTP client instance - Laravel","text":""},{"location":"cookbook/instructor/advanced/custom_http_client_laravel/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/advanced/custom_http_client_laravel/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Illuminate\\Http\\Client\\Factory;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$yourLaravelClientInstance = new Factory();\n\n$user = (new StructuredOutput())\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withLLMConfigOverrides(['apiUrl' =&gt; 'https://api.openai.com/v1'])\n    -&gt;withClientInstance(\n        driverName: 'laravel',\n        clientInstance: $yourLaravelClientInstance\n    )\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    //-&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_llm_with_dsn/","title":"Customize parameters via DSN","text":""},{"location":"cookbook/instructor/advanced/custom_llm_with_dsn/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration data to <code>StructuredOutput</code> object with DSN string. This is useful for inline configuration or for building configuration from admin UI, CLI arguments or environment variables.</p>"},{"location":"cookbook/instructor/advanced/custom_llm_with_dsn/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new StructuredOutput)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withDsn('preset=xai,model=grok-2')\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withresponseClass(User::class)\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_prompts/","title":"Custom prompts","text":""},{"location":"cookbook/instructor/advanced/custom_prompts/#overview","title":"Overview","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>request()</code> or <code>create()</code> methods.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"cookbook/instructor/advanced/custom_prompts/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$structuredOutput = (new StructuredOutput)\n    // let's dump the request data to see how customized prompts look like in requests\n    -&gt;onEvent(HttpRequestSent::class, fn(HttpRequestSent $event) =&gt; dump($event));\n\nprint(\"\\n# Request for OutputMode::Tools:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        mode: OutputMode::Tools\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\nprint(\"\\n# Request for OutputMode::Json:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::Json\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\nprint(\"\\n# Request for OutputMode::MdJson:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::MdJson\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/demonstrations/","title":"Providing example inputs and outputs","text":""},{"location":"cookbook/instructor/advanced/demonstrations/#overview","title":"Overview","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> modes, where the output is expected to be a JSON object.</p>"},{"location":"cookbook/instructor/advanced/demonstrations/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\necho \"\\nREQUEST:\\n\";\n$user = (new StructuredOutput)\n    // let's dump the request data to see how examples are used in requests\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; dump($event))\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withExamples([\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27],\n            template: \"example input:\\n&lt;|input|&gt;\\noutput:\\n```json\\n&lt;|output|&gt;\\n```\\n\",\n        ),\n    ])\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n\necho \"\\nOUTPUT:\\n\";\ndump($user);\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/function_calls/","title":"Extracting arguments of function or method","text":""},{"location":"cookbook/instructor/advanced/function_calls/#overview","title":"Overview","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"cookbook/instructor/advanced/function_calls/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass DataStore\n{\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) : void {\n        // Save user to database\n        echo \"Saving user ... saveUser('$name', $age, '$country')\\n\";\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCall::fromMethodName(DataStore::class, 'saveUser'),\n)-&gt;get();\n\necho \"\\nCalling the function with the extracted arguments:\\n\";\n(new DataStore)-&gt;saveUser(...$args);\n\necho \"\\nExtracted arguments:\\n\";\ndump($args);\n\nassert(count($args) == 3);\nexpect($args['name'] === 'Jason');\nexpect($args['age'] == 28);\nexpect($args['country'] === 'Germany');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/logging_monolog/","title":"Monolog Logging","text":""},{"location":"cookbook/instructor/advanced/logging_monolog/#overview","title":"Overview","text":"<p>Instructor allows to easily log events with Monolog library.</p>"},{"location":"cookbook/instructor/advanced/logging_monolog/#example","title":"Example","text":"<p>&lt;?php require 'examples/boot.php';</p> <p>use Cognesy\\Events\\Event; use Cognesy\\Instructor\\StructuredOutput; use Monolog\\Handler\\StreamHandler; use Monolog\\Logger;</p> <p>$log = new Logger('instructor'); $log-&gt;pushHandler(new StreamHandler('php://stdout'));</p> <p>class User {     public int $age;     public string $name; }</p> <p>$user = (new StructuredOutput)     -&gt;using('openai')     -&gt;wiretap(fn(Event $e) =&gt; \\(log-&gt;log(\\)e-&gt;logLevel, $e-&gt;name(), ['id' =&gt; $e-&gt;id, 'data' =&gt; $e-&gt;data]))     -&gt;withMessages(\"Jason is 25 years old and works as an engineer.\")     -&gt;withResponseClass(User::class)     -&gt;get();</p> <p>assert(\\(user-&gt;name === 'Jason'); assert(\\)user-&gt;age === 25);</p>"},{"location":"cookbook/instructor/advanced/logging_psr/","title":"PSR-3 Logging","text":""},{"location":"cookbook/instructor/advanced/logging_psr/#overview","title":"Overview","text":"<p>Instructor allows to easily log events with any PSR-3 compliant logging library.</p>"},{"location":"cookbook/instructor/advanced/logging_psr/#example","title":"Example","text":"<p>&lt;?php require 'examples/boot.php';</p> <p>use Cognesy\\Events\\Event; use Cognesy\\Instructor\\StructuredOutput; use Psr\\Log\\LoggerInterface; use Psr\\Log\\LoggerTrait;</p> <p>final class StdoutLogger implements LoggerInterface {     use LoggerTrait;</p> <pre><code>public function log($level, $message, array $context = []): void {\n    echo sprintf(\n        \"[%s] %s%s\\n\",\n        strtoupper((string) $level),\n        (string) $message,\n        json_encode($context, JSON_UNESCAPED_SLASHES | JSON_UNESCAPED_UNICODE)\n    );\n}\n</code></pre> <p>}</p> <p>$logger = new StdoutLogger();</p> <p>class User {     public int $age;     public string $name; }</p> <p>$user = (new StructuredOutput)     -&gt;using('openai')     -&gt;wiretap(fn(Event $e) =&gt; \\(logger-&gt;log(\\)e-&gt;logLevel, $e-&gt;name(), ['id' =&gt; $e-&gt;id, 'data' =&gt; $e-&gt;data]))     -&gt;withMessages(\"Jason is 25 years old and works as an engineer.\")     -&gt;withResponseClass(User::class)-&gt;get();</p> <p>assert(\\(user-&gt;name === 'Jason'); assert(\\)user-&gt;age === 25);</p>"},{"location":"cookbook/instructor/advanced/partials/","title":"Streaming partial updates during inference","text":""},{"location":"cookbook/instructor/advanced/partials/#overview","title":"Overview","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated. You can use it to improve user experience by updating the UI with partial data before the full response is received.</p>"},{"location":"cookbook/instructor/advanced/partials/#example","title":"Example","text":"<p><pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public string $location;\n    /** @var UserRole[] */\n    public array $roles;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    Console::clearScreen();\n\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    //usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;onPartialUpdate(partialUpdate(...))\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"All tokens received, fully completed object available in `\\$user` variable.\\n\";\necho '$user = '.\"\\n\";\ndump($user);\n\nassert(!empty($user-&gt;roles));\nassert(!empty($user-&gt;hobbies));\nassert($user-&gt;location === 'San Francisco');\nassert($user-&gt;age == 25);\nassert($user-&gt;name === 'Jason');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/scalars/","title":"Extracting scalar values","text":""},{"location":"cookbook/instructor/advanced/scalars/#overview","title":"Overview","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p>"},{"location":"cookbook/instructor/advanced/scalars/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum CitizenshipGroup : string {\n    case US = \"US\";\n    case Canada = \"Canada\";\n    case Germany = \"Germany\";\n    case Other = \"Other\";\n}\n\n$text = \"His name is Jason, he is 28 years old American who lives in Germany.\";\n$value = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    prompt: 'What is user\\'s citizenship?',\n    responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n)-&gt;get();\n\n\ndump($value);\n\nassert($value instanceof CitizenshipGroup);\nexpect($value == CitizenshipGroup::Other);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/sequences/","title":"Extracting sequences of objects","text":""},{"location":"cookbook/instructor/advanced/sequences/#overview","title":"Overview","text":"<p>Sequences are a special type of response model that can be used to represent a list of objects.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p>"},{"location":"cookbook/instructor/advanced/sequences/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old. Anna is 2 years younger than him.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"OUTPUT:\\n\");\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; dump($sequence-&gt;last()))\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: $text,\n        responseModel: Sequence::of(Person::class),\n        options: ['stream' =&gt; true],\n    )\n    -&gt;get();\n\n\ndump(count($list));\n\nassert(count($list) === 4);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/streaming/","title":"Streaming","text":""},{"location":"cookbook/instructor/advanced/streaming/#overview","title":"Overview","text":"<p>Instructor can process LLM's streamed responses to provide partial response model updates that you can use to update the model with new data as the response is being generated.</p>"},{"location":"cookbook/instructor/advanced/streaming/#example","title":"Example","text":"<p><pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age = 0;\n    public string $name = '';\n    public string $location = '';\n    /** @var UserRole[] */\n    public array $roles = [];\n    /** @var string[] */\n    public array $hobbies = [];\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    Console::clearScreen();\n\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    // usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$stream = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;withStreaming()\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;stream();\n\nforeach ($stream-&gt;partials() as $partial) {\n    partialUpdate($partial);\n}\n\n$user = $stream-&gt;lastUpdate();\n\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/structures/","title":"Structures","text":""},{"location":"cookbook/instructor/advanced/structures/#overview","title":"Overview","text":"<p>Structures allow dynamically define the shape of data to be extracted by LLM, e.g. during runtime.</p> <p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns an array in the shape you defined.</p> <p>See more: Structures</p>"},{"location":"cookbook/instructor/advanced/structures/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::option('gender', ['male', 'female'], 'Gender of the person')-&gt;optional(),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n    Field::collection('favourite_books', Structure::define('book', [\n            Field::string('author', 'Book author')-&gt;optional(),\n            Field::string('title', 'Book title'),\n        ], 'Favorite book data'),\n    'Favorite books of the person'),\n], 'A person object');\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield, 50210. She is 25 years old and works as manager at McDonald's.\n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345. Her favourite books are \"The Lord\n    of the Rings\" and \"The Hobbit\" by JRR Tolkien.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;get();\n\nprint(\"OUTPUT:\\n\");\nprint(\"Name: \" . $person-&gt;name . \"\\n\");\nprint(\"Age: \" . $person-&gt;age . \"\\n\");\nprint(\"Gender: \" . $person-&gt;gender . \"\\n\");\nprint(\"Address / city: \" . $person-&gt;address-&gt;city . \"\\n\");\nprint(\"Address / ZIP: \" . $person-&gt;address-&gt;zip . \"\\n\");\nprint(\"Role: \" . $person-&gt;role-&gt;value . \"\\n\");\nprint(\"Favourite books:\\n\");\nforeach ($person-&gt;favourite_books as $book) {\n    print(\"  - \" . $book-&gt;title . \" by \" . $book-&gt;author . \"\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/a21/","title":"A21","text":""},{"location":"cookbook/instructor/api_support/a21/#overview","title":"Overview","text":"<p>Support for A21 Jamba - MAMBA architecture models, very strong at handling long context.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/a21/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection preset\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('a21');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/anthropic/","title":"Anthropic","text":""},{"location":"cookbook/instructor/api_support/anthropic/#overview","title":"Overview","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - OutputMode::MdJson, OutputMode::Json - supported - OutputMode::Tools - not supported yet</p>"},{"location":"cookbook/instructor/api_support/anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('anthropic');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'claude-3-haiku-20240307',\n    mode: OutputMode::Tools,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/azure_openai/","title":"Azure OpenAI","text":""},{"location":"cookbook/instructor/api_support/azure_openai/#overview","title":"Overview","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p>"},{"location":"cookbook/instructor/api_support/azure_openai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('azure');\n\n// Call with your model name and preferred execution mode\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/cerebras/","title":"Cerebras","text":""},{"location":"cookbook/instructor/api_support/cerebras/#overview","title":"Overview","text":"<p>Support for Cerebras API which uses custom hardware for super fast inference. Cerebras provides Llama models.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/cerebras/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('cerebras');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'llama3.1-8b', // set your own value/source\n    mode: OutputMode::Json,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/cohere/","title":"Cohere","text":""},{"location":"cookbook/instructor/api_support/cohere/#overview","title":"Overview","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - OutputMode::MdJson - supported, recommended as a fallback from JSON mode  - OutputMode::Json - supported, recommended  - OutputMode::Tools - partially supported, not recommended</p> <p>Reasons OutputMode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple, flat data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul>"},{"location":"cookbook/instructor/api_support/cohere/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('cohere');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'command-r-plus',\n    mode: OutputMode::Tools,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/deepseek/","title":"DeepSeek","text":""},{"location":"cookbook/instructor/api_support/deepseek/#overview","title":"Overview","text":"<p>Support for DeepSeek API which provides strong models at affordable price.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/deepseek/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('deepseek');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'deepseek-chat', // set your own value/source\n    mode: OutputMode::JsonSchema,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/fireworks/","title":"Fireworks.ai","text":""},{"location":"cookbook/instructor/api_support/fireworks/#overview","title":"Overview","text":"<p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - selected models - OutputMode::Json - selected models - OutputMode::MdJson</p>"},{"location":"cookbook/instructor/api_support/fireworks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('fireworks');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'accounts/fireworks/models/llama4-maverick-instruct-basic',\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/google_gemini/","title":"Google Gemini","text":""},{"location":"cookbook/instructor/api_support/google_gemini/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('gemini');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'gemini-1.5-flash',\n        //options: ['stream' =&gt; true],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/google_gemini_oai/","title":"Google Gemini (OpenAI compatible API)","text":""},{"location":"cookbook/instructor/api_support/google_gemini_oai/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini's OpenAI compatible API.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('gemini-oai');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'gemini-1.5-flash',\n        //options: ['stream' =&gt; true],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/groq/","title":"Groq","text":""},{"location":"cookbook/instructor/api_support/groq/#overview","title":"Overview","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma.</p> <p>Supported modes depend on the specific model, but generally include:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Groq API.</p>"},{"location":"cookbook/instructor/api_support/groq/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public string $name;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n    public string $username;\n    public ?int $age;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('groq');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n        ],[\n            'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jx90.',\n            'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'], 'username' =&gt; 'jx90', 'age' =&gt; 19],\n        ]],\n        model: 'llama-3.3-70b-versatile', //'gemma2-9b-it',\n        maxRetries: 2,\n        options: ['temperature' =&gt; 0.5],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/huggingface/","title":"Hugging Face","text":""},{"location":"cookbook/instructor/api_support/huggingface/#overview","title":"Overview","text":"<p>You can use Instructor to parse structured output from LLMs using Hugging Face API. This example demonstrates how to parse user data into a structured model using JSON Schema.</p>"},{"location":"cookbook/instructor/api_support/huggingface/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public string $firstName;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n    public string $username;\n    public ?int $age;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('huggingface');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n        examples: [[\n                      'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n                      'output' =&gt; ['firstName' =&gt; 'Frank', 'age' =&gt; 30, 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n                  ],[\n                      'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jx90.',\n                      'output' =&gt; ['firstName' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'], 'username' =&gt; 'jx90', 'age' =&gt; 19],\n                  ]],\n        //model: 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',\n        maxRetries: 2,\n        options: ['temperature' =&gt; 0.5],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;firstName));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;firstName === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/meta/","title":"Meta","text":""},{"location":"cookbook/instructor/api_support/meta/#overview","title":"Overview","text":"<p>Instructor supports Meta LLM inference API. You can find the details on how to configure below.</p>"},{"location":"cookbook/instructor/api_support/meta/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('meta');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/minimaxi/","title":"Minimaxi","text":""},{"location":"cookbook/instructor/api_support/minimaxi/#overview","title":"Overview","text":"<p>Support for Minimaxi's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/instructor/api_support/minimaxi/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('minimaxi');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'MiniMax-Text-01', // set your own value/source\n    mode: OutputMode::MdJson,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/mistralai/","title":"Mistral AI","text":""},{"location":"cookbook/instructor/api_support/mistralai/#overview","title":"Overview","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility:  - OutputMode::Tools - supported (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::Json - recommended (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::MdJson - fallback mode (Mistral 7B / Mixtral 8x7B)</p>"},{"location":"cookbook/instructor/api_support/mistralai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('mistral');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @jx90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'jx90', 'age' =&gt; 30],\n    ]],\n    model: 'mistral-small-latest', //'open-mixtral-8x7b',\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/moonshotai/","title":"MoonshotAI","text":""},{"location":"cookbook/instructor/api_support/moonshotai/#overview","title":"Overview","text":"<p>Support for MoonshotAI's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported)</p>"},{"location":"cookbook/instructor/api_support/moonshotai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('moonshot-kimi');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'kimi-latest', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/ollama/","title":"Local / Ollama","text":""},{"location":"cookbook/instructor/api_support/ollama/#overview","title":"Overview","text":"<p>You can use Instructor with local Ollama instance.</p> <p>Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model for complex data schemas.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode, works with any capable model  - OutputMode::Json - recommended  - OutputMode::Tools - supported (for selected models - check Ollama docs)</p>"},{"location":"cookbook/instructor/api_support/ollama/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('ollama');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. Asked to connect via Twitter @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'], 'username' =&gt; 'frankch', 'age' =&gt; null],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @j90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'j90', 'age' =&gt; 30],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/openai/","title":"OpenAI","text":""},{"location":"cookbook/instructor/api_support/openai/#overview","title":"Overview","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended for new models)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/openai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openai');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/openrouter/","title":"OpenRouter","text":""},{"location":"cookbook/instructor/api_support/openrouter/#overview","title":"Overview","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p>"},{"location":"cookbook/instructor/api_support/openrouter/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openrouter');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/perplexity/","title":"Perplexity","text":""},{"location":"cookbook/instructor/api_support/perplexity/#overview","title":"Overview","text":"<p>You can use Instructor with Perplexity API. Perplexity is an API that provides access to a large language model (LLM) for various tasks, including search and text generation.</p>"},{"location":"cookbook/instructor/api_support/perplexity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('perplexity');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/sambanova/","title":"SambaNova","text":""},{"location":"cookbook/instructor/api_support/sambanova/#overview","title":"Overview","text":"<p>Support for SambaNova's API, which provide fast inference endpoints for Llama and Qwen LLMs.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/instructor/api_support/sambanova/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('sambanova');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'Meta-Llama-3.1-8B-Instruct', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/togetherai/","title":"Together.ai","text":""},{"location":"cookbook/instructor/api_support/togetherai/#overview","title":"Overview","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support OutputMode::Tools or OutputMode::Json, which are much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - supported for selected models - OutputMode::Json - supported for selected models - OutputMode::MdJson - fallback mode</p>"},{"location":"cookbook/instructor/api_support/togetherai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('together');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @jx90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'jx90', 'age' =&gt; 30],\n    ]],\n    //model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n    //options: ['stream' =&gt; true ]\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/xai/","title":"xAI / Grok","text":""},{"location":"cookbook/instructor/api_support/xai/#overview","title":"Overview","text":"<p>Support for xAI's API, which offers access to X.com's Grok model.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/xai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('xai');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/attributes/","title":"Using attributes","text":""},{"location":"cookbook/instructor/basics/attributes/#overview","title":"Overview","text":"<p>Instructor supports <code>Description</code> and <code>Instructions</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p>"},{"location":"cookbook/instructor/basics/attributes/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make user name ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    #[Instructions(\"Make the profession name lowercase\")]\n    public string $job;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = \"Jason is 25 years old, 10K runner, speaker and an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n)-&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert($user-&gt;name === \"JASON\");\nassert(isset($user-&gt;age));\nassert($user-&gt;age === 25);\nassert(isset($user-&gt;job));\nassert($user-&gt;job === \"engineer\");\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/basic_use/","title":"Basic use","text":""},{"location":"cookbook/instructor/basics/basic_use/#overview","title":"Overview","text":"<p>Instructor allows you to use large language models to extract information from the text (or content of chat messages), while following the structure you define.</p> <p>LLM does not 'parse' the text to find and retrieve the information. Extraction leverages LLM ability to comprehend provided text and infer the meaning of the information it contains to fill fields of the response object with values that match the types and semantics of the class fields.</p> <p>The simplest way to use the Instructor is to call the <code>respond</code> method on the Instructor instance. This method takes a string (or an array of strings in the format of OpenAI chat messages) as input and returns a data extracted from provided text (or chat) using the LLM inference.</p> <p>Returned object will contain the values of fields extracted from the text.</p> <p>The format of the extracted data is defined by the response model, which in this case is a simple PHP class with some public properties.</p>"},{"location":"cookbook/instructor/basics/basic_use/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Step 2: Get the text (or chat messages) you want to use as context\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/basic_use_mixin/","title":"Basic use via mixin","text":""},{"location":"cookbook/instructor/basics/basic_use_mixin/#overview","title":"Overview","text":"<p>Instructor provides <code>HandlesSelfInference</code> trait that you can use to enable extraction capabilities directly on class via static <code>infer()</code> method.</p> <p><code>infer()</code> method returns an instance of the class with the data extracted using the Instructor.</p> <p><code>infer()</code> method has following signature (you can also find it in the <code>CanSelfInfer</code> interface):</p> <pre><code>static public function infer(\n    string|array $messages, // (required) The message(s) to infer data from\n    string $prompt = '',    // (optional) The prompt to use for inference\n    array $examples = [],   // (optional) Examples to include in the prompt\n    string $model = '',     // (optional) The model to use for inference (otherwise - use default)\n    int $maxRetries = 2,    // (optional) The number of retries in case of validation failure\n    array $options = [],    // (optional) Additional data to pass to the Instructor or LLM API\n    Mode $mode = OutputMode::Tools, // (optional) The mode to use for inference\n    ?LLM $llm = null         // (optional) LLM instance to use for inference\n) : static;\n</code></pre>"},{"location":"cookbook/instructor/basics/basic_use_mixin/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Mixin\\HandlesSelfInference;\n\nclass User {\n    use HandlesSelfInference;\n\n    public int $age;\n    public string $name;\n}\n\n$user = User::infer(\"Jason is 25 years old and works as an engineer.\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/constructor_parameters/","title":"Specifying required and optional parameters via constructor","text":""},{"location":"cookbook/instructor/basics/constructor_parameters/#overview","title":"Overview","text":"<p>Instructor can extract data from the LLM response and use it to instantiate an object via constructor parameters.</p> <p>Instructor will use the constructor parameters nullability and default values to determine which parameters are required and which are optional.</p>"},{"location":"cookbook/instructor/basics/constructor_parameters/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserWithConstructor\n{\n    public string $name;\n    private ?int $age;\n    private string $location;\n    private string $password;\n\n    public function __construct(\n        string $name,                 // required - required in constructor, required internally\n        int $age,                     // required - required in constructor, even if nullable internally\n        ?string $location,            // optional - nullable in constructor, even if required internally\n        string $password = '123admin' // optional - has a default value, even if required internally\n    ) {\n        $this-&gt;name = $name;\n        $this-&gt;age = $age;\n        $this-&gt;location = $location ?? '';\n        $this-&gt;password = $password;\n    }\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getLocation(): string {\n        return $this-&gt;location;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithConstructor::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;getAge() === 25);\nassert($user-&gt;getPassword() === '123admin');\nassert($user-&gt;getLocation() === ''); // default value for location\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/custom_validation/","title":"Custom validation using Symfony Validator","text":""},{"location":"cookbook/instructor/basics/custom_validation/#overview","title":"Overview","text":"<p>Instructor uses Symfony validation component to validate properties of extracted data. Symfony offers you #[Assert/Callback] annotation to build fully customized validation logic.</p>"},{"location":"cookbook/instructor/basics/custom_validation/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be all uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"JASON\");\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/fluent_api/","title":"Fluent API","text":""},{"location":"cookbook/instructor/basics/fluent_api/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/basics/fluent_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withModel('gpt-3.5-turbo')\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/getters_and_setters/","title":"Getters and setters","text":""},{"location":"cookbook/instructor/basics/getters_and_setters/#overview","title":"Overview","text":"<p>Instructor can extract data from the LLM response and use it to instantiate an object via setter methods.</p> <p>If given property is not public and has no matching constructor params Instructor will use the setter method parameter nullability and default value to determine if property is required.</p>"},{"location":"cookbook/instructor/basics/getters_and_setters/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass UserWithSetter\n{\n    #[Description('Name of the user or empty string if not provided')]\n    private string $name;\n    #[Description('Age of the user or 0 if not provided')]\n    private ?int $age;\n    #[Description('Location of the user or empty string if not provided')]\n    private string $location;\n    #[Description('Password of the user or empty string if not provided')]\n    private string $password;\n\n    // `name` is required (not nullable parameter), if data exists in the answer the setter will be called, but may have empty value\n    public function setName(string $name): void {\n        $this-&gt;name = $name ?: 'Jason';\n    }\n\n    public function getName(): string {\n        return $this-&gt;name ?? '';\n    }\n\n    // `age` is optional (nullable parameter), setter will not be called if LLM does not infer the data\n    public function setAge(int $age): void {\n        $this-&gt;age = (int) $age;\n    }\n\n    public function getAge(): int {\n        return $this-&gt;age ?? 0;\n    }\n\n    public function setLocation(?string $location): void {\n        $this-&gt;location = $location;\n    }\n\n    public function getLocation(): string {\n        return $this-&gt;location;\n    }\n\n    public function setPassword(string|null $password = ''): void {\n        $this-&gt;password = $password ?: '123admin';\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    This user is living in San Francisco. His password is.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithSetter::class)\n    -&gt;withMaxRetries(2)\n    //-&gt;withModel('claude-3-7-sonnet-20250219')\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;getName() === \"Jason\"); // called - but set to default value as LLM inferred empty name\nassert($user-&gt;getAge() === 0); // not called - property value not inferred by LLM\nassert($user-&gt;getPassword() === '123admin'); // called - but set to default value as LLM inferred empty password\nassert($user-&gt;getLocation() === 'San Francisco'); // called - LLM inferred location from the text\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/maybe/","title":"Handling errors with `Maybe` helper class","text":""},{"location":"cookbook/instructor/basics/maybe/#overview","title":"Overview","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p>"},{"location":"cookbook/instructor/basics/maybe/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n\n$text = 'We have no information about our new developer.';\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class),\n    model: 'gpt-4o-mini',\n    mode: OutputMode::MdJson,\n)-&gt;get();\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === false);\nassert(!empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() === null);\n\n$text = \"Jason is our new developer, he is 25 years old.\";\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class)\n)-&gt;get();\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === true);\nassert(empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() != null);\nassert($maybeUser-&gt;get() instanceof User);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/messages_api/","title":"Messages API","text":""},{"location":"cookbook/instructor/basics/messages_api/#overview","title":"Overview","text":"<p>Instructor allows you to use <code>Messages</code> and <code>Message</code> classes to work with chat messages and their sequences.</p>"},{"location":"cookbook/instructor/basics/messages_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Messages\\Message;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Str;\n\nclass Code {\n    public string $code;\n    public string $programmingLanguage;\n    public string $codeDescription;\n}\n\n$messages = (new Messages)\n    -&gt;asSystem('You are a senior PHP8 backend developer.')\n    -&gt;asDeveloper('Be concise and use modern PHP8.2+ features.') // OpenAI developer role is supported and normalized for other providers\n    -&gt;asUser([\n        'What is the best way to handle errors in PHP8?',\n        'Provide a code example.',\n        'Use modern PHP8.2+ features.',\n    ])\n    -&gt;asAssistant('I will provide a code example that demonstrates how to handle errors using try-catch. Any specific domain?');\n\n$messages-&gt;appendMessage(Message::asUser('Make it insurance related.'));\n\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$code = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($messages)\n    -&gt;withResponseModel(Code::class)\n    -&gt;withOutputMode(OutputMode::MdJson)\n    -&gt;get();\n\nprint(\"Extracted data:\\n\");\ndump($code);\n\nassert(!empty($code-&gt;code));\nassert(!empty($code-&gt;codeDescription));\nassert(Str::contains(strtolower($code-&gt;programmingLanguage), 'php'));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/mixed_type_property/","title":"Mixed Type Property","text":""},{"location":"cookbook/instructor/basics/mixed_type_property/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/basics/mixed_type_property/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass UserWithMixedTypeProperty\n{\n    public string $name;\n    #[Description('Any extra information about the user')]\n    public mixed $extraInfo;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He plays football and loves to travel.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithMixedTypeProperty::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;extraInfo !== ''); // not empty, but can be any type\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/modes/","title":"Modes","text":""},{"location":"cookbook/instructor/basics/modes/#overview","title":"Overview","text":"<p>Instructor supports several ways to extract data from the response:</p> <ul> <li><code>OutputMode::Tools</code> - uses OpenAI-style tool calls to get the language    model to generate JSON following the schema,</li> <li><code>OutputMode::JsonSchema</code> - guarantees output matching JSON Schema via    Context Free Grammar, does not support optional properties,</li> <li><code>OutputMode::Json</code> - JSON mode, response follows provided JSON Schema,</li> <li><code>OutputMode::MdJson</code> - uses prompting to get the language model to    generate JSON following the schema.</li> </ul> <p>Note: not all modes are supported by all models or providers.</p> <p>Mode can be set via parameter of <code>StructuredOutput::create()</code> method.</p>"},{"location":"cookbook/instructor/basics/modes/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$text = \"Jason is 25 years old and works as an engineer.\";\n\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n$structuredOutput = new StructuredOutput;\n\n// CASE 1 - OutputMode::Tools\nprint(\"\\n1. Extracting structured data using LLM - OutputMode::Tools\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::Tools,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 2 - OutputMode::JsonSchema\nprint(\"\\n2. Extracting structured data using LLM - OutputMode::JsonSchema\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 3 - OutputMode::Json\nprint(\"\\n3. Extracting structured data using LLM - OutputMode::Json\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::Json,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 4 - OutputMode::MdJson\nprint(\"\\n4. Extracting structured data using LLM - OutputMode::MdJson\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::MdJson,\n)-&gt;get();\ncheck($user);\ndump($user);\n\nfunction check(User $user) {\n    assert(isset($user-&gt;name));\n    assert(isset($user-&gt;age));\n    assert($user-&gt;name === 'Jason');\n    assert($user-&gt;age === 25);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/optional_fields/","title":"Making some fields optional","text":""},{"location":"cookbook/instructor/basics/optional_fields/#overview","title":"Overview","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) to declare component fields which are optional. Set a default value to prevent undesired defaults like nulls or empty strings.</p>"},{"location":"cookbook/instructor/basics/optional_fields/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $age;\n    public string $firstName;\n    public ?string $lastName;\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages('Jason is 25 years old.')\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;get();\n\ndump($user);\n\nassert(!isset($user-&gt;lastName) || $user-&gt;lastName === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/public_vs_private/","title":"Private vs public object field","text":""},{"location":"cookbook/instructor/basics/public_vs_private/#overview","title":"Overview","text":"<p>Instructor only sets accessible fields of the object with the data provided by LLM.</p> <p>Private and protected fields are left unchanged, unless:  - class has constructor with parameters matching one or more property names - in such    situation object will be hydrated with data from LLM via constructor params,  - class has getXxx() and setXxx() methods with xxx matching one of the property names -    in such situation object will be hydrated with data from LLM via setter methods</p> <p>If you want to access them directly after extraction, provide default values for them.</p>"},{"location":"cookbook/instructor/basics/public_vs_private/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. His password is '123admin'.\n    TEXT;\n\n\n// CASE 1: Class with public fields\n\nclass User\n{\n    public string $name;\n    public int $age;\n    public string $password = '';\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n\necho \"User with public fields\\n\";\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 25);\nassert($user-&gt;password === '123admin');\n\n\n// CASE 2: Class with some private fields\n\nclass UserWithPrivateFields\n{\n    public string $name;\n    private int $age = 0;\n    private string $password = '';\n\n    public function getAge() : int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$userPriv = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithPrivateFields::class)\n    -&gt;get();\n\necho \"Private 'password' and 'age' fields are not hydrated by Instructor\\n\";\n\ndump($userPriv);\n\nassert($userPriv-&gt;name === \"Jason\");\nassert($userPriv-&gt;getAge() === 0);\nassert($userPriv-&gt;getPassword() === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/self_correction/","title":"Automatic correction based on validation results","text":""},{"location":"cookbook/instructor/basics/self_correction/#overview","title":"Overview","text":"<p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p>"},{"location":"cookbook/instructor/basics/self_correction/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidated;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationAttempt;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationFailed;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n$text = \"you can reply to me via jason wp.pl -- Jason\";\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"RESULTS:\\n\");\n$user = (new StructuredOutput)\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; print(\"[ ] Requesting LLM response...\\n\"))\n    -&gt;onEvent(ResponseValidationAttempt::class, fn($event) =&gt; print(\"[?] Validating:\\n    \".$event.\"\\n\"))\n    -&gt;onEvent(ResponseValidationFailed::class, fn($event) =&gt; print(\"[!] Validation failed:\\n    $event\\n\"))\n    -&gt;onEvent(ResponseValidated::class, fn($event) =&gt; print(\"[ ] Validation succeeded.\\n\"))\n    -&gt;with(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 3,\n    )-&gt;get();\n\nprint(\"\\nOUTPUT:\\n\");\n\ndump($user);\n\nassert($user-&gt;email === \"jason@wp.pl\");\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/using_config/","title":"Using LLM API connection presets from config file","text":""},{"location":"cookbook/instructor/basics/using_config/#overview","title":"Overview","text":"<p>Instructor allows you to define multiple API connection presets in <code>llm.php</code> file. This is useful when you want to use different LLMs or API providers in your application.</p> <p>Connecting to LLM API via predefined connection is as simple as calling <code>withPreset</code> method with the preset name.</p>"},{"location":"cookbook/instructor/basics/using_config/#configuration-file","title":"Configuration file","text":"<p>Default LLM configuration file is located in <code>/config/llm.php</code> in the root directory of Instructor codebase.</p> <p>You can set the location of the configuration file via <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable (comma-separated list of paths). You can use a copy of the default configuration file as a starting point.</p> <p>LLM config file defines available connection presets to LLM APIs and their parameters. It also specifies the default provider and parameters to be used when calling Instructor.</p>"},{"location":"cookbook/instructor/basics/using_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Get Instructor object with client defined in config.php under 'presets/openai' key\n$structuredOutput = (new StructuredOutput)-&gt;using('openai');\n\n// Call with custom model and execution mode\n$user = $structuredOutput-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;get();\n\n// Use the results of LLM inference\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/validation/","title":"Validation","text":""},{"location":"cookbook/instructor/basics/validation/#overview","title":"Overview","text":"<p>Instructor uses validation to verify if the response generated by LLM meets the requirements of your response model. If the response does not meet the requirements, Instructor will throw an exception.</p> <p>Instructor uses Symfony's Validator component to validate the response, check their documentation for more information on the usage: https://symfony.com/doc/current/components/validator.html</p> <p>Following example demonstrates how to use Symfony Validator's constraints to validate the email field of response.</p>"},{"location":"cookbook/instructor/basics/validation/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Exceptions\\ValidationException;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    #[Assert\\NotBlank]\n    /** Find user's email provided in the text or empty if it is missing */\n    public ?string $email;\n}\n\n$caughtException = false;\ntry {\n    $user = (new StructuredOutput)\n        -&gt;withResponseClass(UserDetails::class)\n        -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via mail -- Jason\"]])\n        -&gt;get();\n} catch (ValidationException $e) {\n    $caughtException = true;\n    echo \"Validation worked.\\n\";\n} catch (Throwable $e) {\n    // Catch any other exception\n    echo \"Validation failed with unexpected exception: {$e-&gt;getMessage()}\\n\";\n}\n\nassert($caughtException === true);\nassert(!isset($user));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/validation_multifield/","title":"Validation across multiple fields","text":""},{"location":"cookbook/instructor/basics/validation_multifield/#overview","title":"Overview","text":"<p>Sometimes property level validation is not enough - you may want to check values of multiple properties and based on the combination of them decide to accept or reject the response. Or the assertions provided by Symfony may not be enough for your use case.</p> <p>In such case you can easily add custom validation code to your response model by: - using <code>ValidationMixin</code> - and defining validation logic in <code>validate()</code> method.</p> <p>In this example LLM should be able to correct typo in the message (graduation year we provided is <code>1010</code> instead of <code>2010</code>) and respond with correct graduation year.</p>"},{"location":"cookbook/instructor/basics/validation_multifield/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $birthYear;\n    public int $graduationYear;\n\n    public function validate() : ValidationResult {\n        if ($this-&gt;graduationYear &gt; $this-&gt;birthYear) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'graduationYear',\n            value: $this-&gt;graduationYear,\n            message: \"Graduation year has to be bigger than birth year.\"\n        );\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withResponseClass(UserDetails::class)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'Jason was born in 2000 and graduated in 18.']],\n        model: 'gpt-3.5-turbo',\n        maxRetries: 2,\n    )-&gt;get();\n\n\ndump($user);\n\nassert($user-&gt;graduationYear === 2018);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/validation_with_llm/","title":"Validation with LLM","text":""},{"location":"cookbook/instructor/basics/validation_with_llm/#overview","title":"Overview","text":"<p>You can use LLM capability to semantically process the context to validate the response following natural language instructions. This way you can implement more complex validation logic that would be difficult (or impossible) to achieve using traditional, code-based validation.</p>"},{"location":"cookbook/instructor/basics/validation_with_llm/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    #[Description('User details in format: key=value')]\n    /** @var string[]  */\n    public array $details;\n\n    public function validate() : ValidationResult {\n        return match($this-&gt;hasPII()) {\n            true =&gt; ValidationResult::fieldError(\n                field: 'details',\n                value: implode('\\n', $this-&gt;details),\n                message: \"Details contain PII, remove it from the response.\"\n            ),\n            false =&gt; ValidationResult::valid(),\n        };\n    }\n\n    private function hasPII() : bool {\n        $data = implode('\\n', $this-&gt;details);\n        return (new StructuredOutput)\n            -&gt;with(\n                messages: \"Context:\\n$data\\n\",\n                responseModel: Scalar::boolean('hasPII', 'Does the context contain any PII?'),\n            )\n            -&gt;getBoolean();\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    My name is Jason. I am is 25 years old. I am developer.\n    My phone number is +1 123 34 45 and social security number is 123-45-6789\n    TEXT;\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print()) // let's check the internals of Instructor processing\n    -&gt;with(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )-&gt;get();\n\ndump($user);\n\nassert(!Str::contains(implode('\\n', $user-&gt;details), '123-45-6789'));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction/","title":"Extraction of complex objects","text":""},{"location":"cookbook/instructor/extras/complex_extraction/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text.</p>"},{"location":"cookbook/instructor/extras/complex_extraction/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = new StructuredOutput;\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'gpt-4o',\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ],\n        mode: OutputMode::Tools\n    )\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction_claude/","title":"Extraction of complex objects (Anthropic)","text":""},{"location":"cookbook/instructor/extras/complex_extraction_claude/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Anthropic Claude 3 model.</p>"},{"location":"cookbook/instructor/extras/complex_extraction_claude/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\nclass ProjectEvents {\n    /**\n     * List of events extracted from the text\n     * @var ProjectEvent[]\n     */\n    public array $events = [];\n}\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('anthropic');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'claude-3-5-sonnet-20240620', // 'claude-3-haiku-20240307'\n        prompt: 'Extract a list of project events with all the details from the provided input in JSON format using schema: &lt;|json_schema|&gt;',\n        mode: OutputMode::Json,\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        options: [\n            'max_tokens' =&gt; 4096,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n//dump($events-&gt;list);\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction_cohere/","title":"Extraction of complex objects (Cohere)","text":""},{"location":"cookbook/instructor/extras/complex_extraction_cohere/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Cohere R models.</p>"},{"location":"cookbook/instructor/extras/complex_extraction_cohere/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('cohere');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'command-r-plus-08-2024',\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        mode: OutputMode::JsonSchema,\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction_gemini/","title":"Extraction of complex objects (Gemini)","text":""},{"location":"cookbook/instructor/extras/complex_extraction_gemini/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Google Gemini model.</p>"},{"location":"cookbook/instructor/extras/complex_extraction_gemini/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('gemini');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    //-&gt;onEvent(PartialInferenceResponseReceived::class, fn(PartialInferenceResponseReceived $e) =&gt; print \"---\\n\".$e-&gt;partialInferenceResponse-&gt;content().\"---\\n\")\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        //model: 'gemini-1.5-flash',\n        //model: 'gemini-2.0-flash-exp',\n        //model: 'gemini-2.0-flash-thinking-exp',\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_car_damage/","title":"Image processing - car damage detection","text":""},{"location":"cookbook/instructor/extras/image_car_damage/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>In this example we will be extracting structured data from an image of a car with visible damage. The response model will contain information about the location of the damage and the type of damage.</p>"},{"location":"cookbook/instructor/extras/image_car_damage/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_car_damage/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nenum DamageSeverity : string {\n    case Minor = 'minor';\n    case Moderate = 'moderate';\n    case Severe = 'severe';\n    case Total = 'total';\n}\n\nenum DamageLocation : string {\n    case Front = 'front';\n    case Rear = 'rear';\n    case Left = 'left';\n    case Right = 'right';\n    case Top = 'top';\n    case Bottom = 'bottom';\n}\n\nclass Damage {\n    #[Description('Identify damaged element')]\n    public string $element;\n    /** @var DamageLocation[] */\n    public array $locations;\n    public DamageSeverity $severity;\n    public string $description;\n}\n\nclass DamageAssessment {\n    public string $make;\n    public string $model;\n    public string $bodyColor;\n    /** @var Damage[] */\n    public array $damages = [];\n    public string $summary;\n}\n\n$assessment = Image::fromFile(__DIR__ . '/car-damage.jpg')\n    -&gt;toData(\n        responseModel: DamageAssessment::class,\n        prompt: 'Identify and assess each car damage location and severity separately.',\n        connection: 'openai',\n        model: 'gpt-4o',\n        options: ['max_tokens' =&gt; 4096]\n    );\n\ndump($assessment);\nassert(Str::contains($assessment-&gt;make, 'Toyota', false));\nassert(Str::contains($assessment-&gt;model, 'Prius', false));\nassert(Str::contains($assessment-&gt;bodyColor, 'white', false));\nassert(count($assessment-&gt;damages) &gt; 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_to_data/","title":"Image to data (OpenAI)","text":""},{"location":"cookbook/instructor/extras/image_to_data/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/instructor/extras/image_to_data/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_to_data/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt.',\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_to_data_anthropic/","title":"Image to data (Anthropic)","text":""},{"location":"cookbook/instructor/extras/image_to_data_anthropic/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/instructor/extras/image_to_data_anthropic/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_to_data_anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;using('anthropic')-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt. Return result as JSON following this schema: &lt;|json_schema|&gt;',\n    model: 'claude-3-5-sonnet-20240620',\n    mode: OutputMode::Json,\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_to_data_gemini/","title":"Image to data (Gemini)","text":""},{"location":"cookbook/instructor/extras/image_to_data_gemini/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/instructor/extras/image_to_data_gemini/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_to_data_gemini/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;using('gemini')-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt. Return result as JSON following this schema: &lt;|json_schema|&gt;',\n    mode: OutputMode::Json,\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/schema/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/instructor/extras/schema/#overview","title":"Overview","text":"<p>Instructor has a built-in support for dynamically constructing JSON Schema using <code>JsonSchema</code> class. It is useful when you want to shape the structures during runtime.</p>"},{"location":"cookbook/instructor/extras/schema/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'User name'),\n        JsonSchema::integer('age', 'User age'),\n    ],\n    requiredProperties: ['name', 'age'],\n);\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;withMessages(\"Jason is 25 years old and works as an engineer\")\n    -&gt;withResponseJsonSchema($schema)\n    -&gt;withDeserializers()\n    -&gt;withDefaultToStdClass()\n    -&gt;get();\n\ndump($user);\n\nassert(gettype($user) === 'object');\nassert(get_class($user) === 'stdClass');\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/schema_dynamic/","title":"Generating JSON Schema dynamically","text":""},{"location":"cookbook/instructor/extras/schema_dynamic/#overview","title":"Overview","text":"<p>Instructor has a built-in support for generating JSON Schema from dynamic objects with <code>Structure</code> class.</p> <p>This is useful when the data model is built during runtime or defined by your app users.</p> <p><code>Structure</code> helps you flexibly design and modify data models that can change with every request or user input and allows you to generate JSON Schema for them.</p>"},{"location":"cookbook/instructor/extras/schema_dynamic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$city = Structure::define('city', [\n    Field::string('name', 'City name')-&gt;required(),\n    Field::int('population', 'City population')-&gt;required(),\n    Field::int('founded', 'Founding year')-&gt;required(),\n]);\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n        Respond with JSON data.']],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; $city-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/structured_input/","title":"Using structured data as an input","text":""},{"location":"cookbook/instructor/extras/structured_input/#overview","title":"Overview","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>create()</code> method can be an object, but also an array or just a string.</p>"},{"location":"cookbook/instructor/extras/structured_input/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translatedEmail = (new StructuredOutput)\n    -&gt;withInput($email)\n    -&gt;withResponseClass(Email::class)\n    -&gt;withPrompt('Translate the text fields of email to Spanish. Keep other fields unchanged.')\n    -&gt;get();\n\ndump($translatedEmail);\n\nassert($translatedEmail-&gt;address === $email-&gt;address);\nassert($translatedEmail-&gt;subject !== $email-&gt;subject);\nassert($translatedEmail-&gt;body !== $email-&gt;body);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/transcription_to_tasks/","title":"Create tasks from meeting transcription","text":""},{"location":"cookbook/instructor/extras/transcription_to_tasks/#overview","title":"Overview","text":"<p>This example demonstrates how you can create task assignments based on a transcription of meeting recording.</p>"},{"location":"cookbook/instructor/extras/transcription_to_tasks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nenum TaskStatus : string {\n    case Pending = 'pending';\n    case Completed = 'completed';\n}\n\nenum Role : string {\n    case PM = 'pm';\n    case Dev = 'dev';\n}\n\nclass Task {\n    public string $title;\n    public string $description;\n    public DateTimeImmutable $dueDate;\n    public Role $owner;\n    public TaskStatus $status;\n}\n\nclass Tasks {\n    public DateTime $meetingDate;\n    /** @var Task[] */\n    public array $tasks;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = &lt;&lt;&lt;TEXT\nTranscription of meeting from 2024-01-15, 16:00\n---\nPM: Hey, how's progress on the video transcription engine?\nDev: I've got basic functionality working, but accuracy isn't great yet. Might need to switch to a different API.\nPM: So the plan is to research alternatives and provide a comparison? Is it possible by Jan 20th?\nDev: Sure, I'll make it available before the meeting.\nPM: The one at 12?\nDev: Yes, at 12. By the way, are we still planning to support real-time transcription?\nPM: Yes, it's a key feature. Speaking of which, I need to update the product roadmap. I'll have that ready by Jan 18th.\nDev: Got it. I'll keep that in mind while evaluating APIs. Oh, and the UI for the summary view is ready for review.\nPM: Great, I'll take a look tomorrow by 10.\nTEXT;\n\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$tasks = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: $text,\n        responseModel: Tasks::class,\n        //model: 'gpt-4o',\n        mode: OutputMode::Json,\n    )\n    -&gt;get();\ndd($tasks);\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($tasks);\n\nassert($tasks-&gt;meetingDate-&gt;format('Y-m-d') === '2024-01-15');\nassert(count($tasks-&gt;tasks) == 3);\n\nassert($tasks-&gt;tasks[0]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-20');\nassert($tasks-&gt;tasks[0]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[0]-&gt;owner === Role::Dev);\n\nassert($tasks-&gt;tasks[1]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-18');\nassert($tasks-&gt;tasks[1]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[1]-&gt;owner === Role::PM);\n\nassert($tasks-&gt;tasks[2]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-16');\nassert($tasks-&gt;tasks[2]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[2]-&gt;owner === Role::PM);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/translate_ui_fields/","title":"Translating UI text fields","text":""},{"location":"cookbook/instructor/extras/translate_ui_fields/#overview","title":"Overview","text":"<p>You can use Instructor to translate text fields in your UI. We can instruct the model to translate only the text fields from one language to another, but leave the other fields, like emails or URLs, unchanged.</p> <p>This example demonstrates how to translate text fields from English to German using structure-to-structure processing with LLM.</p>"},{"location":"cookbook/instructor/extras/translate_ui_fields/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Contracts\\CanValidateObject;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Instructor\\Validation\\Validators\\SymfonyValidator;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass TextElementModel\n{\n    public function __construct(\n        public string $headline = '',\n        public string $text = '',\n        public string $url = 'https://translation.com/'\n    ) {}\n}\n\n$sourceModel = new TextElementModel(\n    headline: 'This is my headline',\n    text: '&lt;p&gt;This is some WYSIWYG HTML content.&lt;/p&gt;'\n);\n\n$validator = new class implements CanValidateObject {\n    public function validate(object $dataObject): ValidationResult {\n        $isInGerman = (new StructuredOutput)\n            //-&gt;withDebugPreset('on')\n            -&gt;withInput($dataObject)\n            -&gt;withResponseObject(Scalar::boolean())\n            -&gt;withPrompt('Are all content fields translated to German? Return result in JSON format: &lt;|json_schema|&gt;')\n            -&gt;withOutputMode(OutputMode::Json)\n            -&gt;get();\n        return match($isInGerman) {\n            true =&gt; ValidationResult::valid(),\n            default =&gt; ValidationResult::invalid(['All input text fields have to be translated to German. Keep HTML tags unchanged.']),\n        };\n    }\n};\n\n$transformedModel = (new StructuredOutput)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e)=&gt;$e-&gt;print())\n    -&gt;withInput($sourceModel)\n    -&gt;withResponseClass(get_class($sourceModel))\n    -&gt;withPrompt('Translate all text fields to German. Keep HTML tags unchanged. Return result in JSON format: &lt;|json_schema|&gt;')\n    -&gt;withMaxRetries(2)\n    -&gt;withOptions(['temperature' =&gt; 0])\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;withValidators($validator, SymfonyValidator::class)\n    -&gt;get();\n\ndump($transformedModel);\n\nassert(true === (\n    str_contains($transformedModel-&gt;headline, '\u00dcberschrift')\n    || str_contains($transformedModel-&gt;headline, 'Schlagzeile')\n));\nassert(str_contains($transformedModel-&gt;text, 'Inhalt') === true);\nassert(str_contains($transformedModel-&gt;text, '&lt;p&gt;') === true);\nassert(str_contains(str_replace('\\/', '/', $transformedModel-&gt;url), 'https://translation.com/') === true);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/web_to_objects/","title":"Web page to PHP objects","text":""},{"location":"cookbook/instructor/extras/web_to_objects/#overview","title":"Overview","text":"<p>This example demonstrates how to extract structured data from a web page and get it as PHP object.</p>"},{"location":"cookbook/instructor/extras/web_to_objects/#example","title":"Example","text":"<p>In this example we will be extracting list of Laravel companies from The Manifest website. The result will be a list of <code>Company</code> objects.</p> <p>We use Webpage extractor to get the content of the page and specify 'none' scraper, which means that we will be using built-in <code>file_get_contents</code> function to get the content of the page.</p> <p>In production environment you might want to use one of the supported scrapers:  - <code>browsershot</code>  - <code>scrapingbee</code>  - <code>scrapfly</code>  - <code>jinareader</code></p> <p>Commercial scrapers require API key, which can be set in the configuration file (<code>/config/web.php</code>).</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Auxiliary\\Web\\Webpage;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass Company {\n    public string $name = '';\n    public string $location = '';\n    public string $description = '';\n    public int $minProjectBudget = 0;\n    public string $companySize = '';\n    #[Instructions('Remove any tracking parameters from the URL')]\n    public string $websiteUrl = '';\n    /** @var string[] */\n    public array $clients = [];\n}\n\n$companyGen = Webpage::withScraper('none')\n    -&gt;get('https://themanifest.com/pl/software-development/laravel/companies?page=1')\n    -&gt;cleanup()\n    -&gt;select('.directory-providers__list')\n    -&gt;selectMany(\n        selector: '.provider-card',\n        callback: fn($item) =&gt; $item-&gt;asMarkdown(),\n        limit: 3\n    );\n\n$companies = [];\necho \"Extracting company data from:\\n\\n\";\nforeach($companyGen as $companyDiv) {\n    echo \" &gt; \" . substr($companyDiv, 0, 32) . \"...\\n\\n\";\n    $company = (new StructuredOutput)\n        -&gt;using('openai')\n        -&gt;with(\n            messages: $companyDiv,\n            responseModel: Company::class,\n            mode: OutputMode::Json\n        )-&gt;get();\n    $companies[] = $company;\n    dump($company);\n}\n\nassert(count($companies) === 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/debugging/","title":"Debugging","text":""},{"location":"cookbook/instructor/troubleshooting/debugging/#overview","title":"Overview","text":"<p>The <code>StructuredOutput</code> class has a <code>withDebug()</code> method that can be used to debug the request and response.</p> <p>It displays detailed information about the request being sent to LLM API and response received from it, including:  - request headers, URI, method and body,  - response status, headers, and body.</p> <p>This is useful for debugging the request and response when you are not getting the expected results.</p>"},{"location":"cookbook/instructor/troubleshooting/debugging/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Utils\\Str;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// CASE 1.1 - normal flow, sync request\n\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withDebugPreset('on');\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print());\n\necho \"\\n### CASE 1.1 - Debugging sync request\\n\\n\";\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason is 25 years old.\",\n        responseModel: User::class,\n        options: [ 'stream' =&gt; false ]\n    )\n    -&gt;get();\n\necho \"\\nResult:\\n\";\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n\n// CASE 1.2 - normal flow, streaming request\n\necho \"\\n### CASE 1.2 - Debugging streaming request\\n\\n\";\n$user2 = $structuredOutput\n    -&gt;with(\n        messages: \"Anna is 21 years old.\",\n        responseModel: User::class,\n        options: [ 'stream' =&gt; true ]\n    )\n    -&gt;get();\n\necho \"\\nResult:\\n\";\ndump($user2);\n\nassert(isset($user2-&gt;name));\nassert(isset($user2-&gt;age));\nassert($user2-&gt;name === 'Anna');\nassert($user2-&gt;age === 21);\n\n\n// CASE 2 - forcing API error via empty LLM config\n\n// let's initialize the instructor with an incorrect LLM config\n$structuredOutput = (new StructuredOutput)\n    -&gt;withLLMConfig(new LLMConfig(apiUrl: 'https://example.com'));\n\necho \"\\n### CASE 2 - Debugging with HTTP exception\\n\\n\";\ntry {\n    $user = $structuredOutput\n        -&gt;withDebugPreset('on')\n        -&gt;with(\n            messages: \"Jason is 25 years old.\",\n            responseModel: User::class,\n            options: [ 'stream' =&gt; true ]\n        )\n        -&gt;get();\n} catch (Exception $e) {\n    $msg = Str::limit($e-&gt;getMessage(), 250);\n    echo \"EXCEPTION WE EXPECTED:\\n\";\n    echo \"\\nCaught exception: \" . $msg . \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/on_event/","title":"Receive specific internal event with onEvent()","text":""},{"location":"cookbook/instructor/troubleshooting/on_event/#overview","title":"Overview","text":"<p><code>(new StructuredOutput)-&gt;onEvent(string $class, callable $callback)</code> method allows you to receive callback when specified type of event is dispatched by Instructor.</p> <p>This way you can plug into the execution process and monitor it, for example logging or reacting to the events which are of interest to your application.</p> <p>This example demonstrates how you can monitor outgoing requests and received responses via Instructor's events.</p> <p>Check the <code>Cognesy\\Instructor\\Events</code> namespace for the list of available events and their properties.</p>"},{"location":"cookbook/instructor/troubleshooting/on_event/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n// let's mock a logger class - in real life you would use a proper logger, e.g. Monolog\n$logger = new class {\n    public function log(Event $event) {\n        // we're using a predefined asLog() method to get the event data,\n        // but you can access the event properties directly and customize the output\n        echo $event-&gt;asLog().\"\\n\";\n    }\n};\n\n$user = (new StructuredOutput)\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;onEvent(HttpResponseReceived::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;with(\n        messages: \"Jason is 28 years old\",\n        responseModel: User::class,\n    )\n    -&gt;get();\n\ndump($user);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/settings/","title":"Modifying Settings Path","text":""},{"location":"cookbook/instructor/troubleshooting/settings/#overview","title":"Overview","text":"<p>This example demonstrates how to modify the settings path for the Instructor library. This is useful when you want to use a custom configuration directory instead of the default one.</p>"},{"location":"cookbook/instructor/troubleshooting/settings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Settings;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $age;\n    public string $firstName;\n    public ?string $lastName;\n}\n\n// set the configuration path to custom directory\nSettings::setPath(__DIR__ . '/config');\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on') // we reconfigured local debug settings to dump only request URL\n    -&gt;withMessages('Jason is 25 years old.')\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;get();\n\ndump($user);\n\nassert(!isset($user-&gt;lastName) || $user-&gt;lastName === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/token_usage_events/","title":"Tracking token usage via events","text":""},{"location":"cookbook/instructor/troubleshooting/token_usage_events/#overview","title":"Overview","text":"<p>Some use cases require tracking the token usage of the API responses. This can be done by getting <code>Usage</code> object from Instructor LLM response object.</p> <p>Code below demonstrates how it can be retrieved for both sync and streamed requests.</p>"},{"location":"cookbook/instructor/troubleshooting/token_usage_events/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Data\\Usage;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\nfunction printUsage(Usage $usage) : void {\n    echo \"Input tokens: $usage-&gt;inputTokens\\n\";\n    echo \"Output tokens: $usage-&gt;outputTokens\\n\";\n    echo \"Cache creation tokens: $usage-&gt;cacheWriteTokens\\n\";\n    echo \"Cache read tokens: $usage-&gt;cacheReadTokens\\n\";\n    echo \"Reasoning tokens: $usage-&gt;reasoningTokens\\n\";\n}\n\necho \"COUNTING TOKENS FOR SYNC RESPONSE\\n\";\n$text = \"Jason is 25 years old and works as an engineer.\";\n$response = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n    )-&gt;response();\n\necho \"\\nTEXT: $text\\n\";\nassert($response-&gt;usage()-&gt;total() &gt; 0);\nprintUsage($response-&gt;usage());\n\n\necho \"\\n\\nCOUNTING TOKENS FOR STREAMED RESPONSE\\n\";\n$text = \"Anna is 19 years old.\";\n$stream = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n        options: ['stream' =&gt; true],\n    )\n    -&gt;stream();\n\n$response = $stream-&gt;finalValue();\necho \"\\nTEXT: $text\\n\";\nassert($stream-&gt;usage()-&gt;total() &gt; 0);\nprintUsage($stream-&gt;usage());\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/wiretap/","title":"Receive all internal events with wiretap()","text":""},{"location":"cookbook/instructor/troubleshooting/wiretap/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/troubleshooting/wiretap/#receive-all-internal-events-with-wiretap","title":"Receive all internal events with wiretap()","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <p><code>(new StructuredOutput)-&gt;wiretap(callable $callback)</code> method allows you to receive all events dispatched by Instructor.</p> <p>Example below demonstrates how <code>wiretap()</code> can help you to monitor the execution process and better understand or resolve any processing issues.</p> <p>In this example we use <code>print()</code> method available on event classes, which outputs console-formatted information about each event.</p>"},{"location":"cookbook/instructor/troubleshooting/wiretap/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Role : string {\n    case CEO = 'ceo';\n    case CTO = 'cto';\n    case Developer = 'developer';\n    case Other = 'other';\n}\n\nclass UserDetail\n{\n    public string $name;\n    public Role $role;\n    public int $age;\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    -&gt;with(\n        messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Contact our CTO, Jason is 28 years old -- Best regards, Tom\"]],\n        responseModel: UserDetail::class,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;role === Role::CTO);\nassert($user-&gt;age === 28);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/config_providers/","title":"Customize configuration providers of LLM driver","text":""},{"location":"cookbook/polyglot/llm_advanced/config_providers/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to <code>Inference</code> object. This is useful when you want to initialize LLM client with custom values.</p>"},{"location":"cookbook/polyglot/llm_advanced/config_providers/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Adbar\\Dot;\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\n$configData = [\n    'http' =&gt; [\n        'defaultPreset' =&gt; 'symfony',\n        'presets' =&gt; [\n            'symfony' =&gt; [\n                'driver' =&gt; 'symfony',\n                'connectTimeout' =&gt; 10,\n                'requestTimeout' =&gt; 30,\n                'idleTimeout' =&gt; -1,\n                'maxConcurrent' =&gt; 5,\n                'poolTimeout' =&gt; 60,\n                'failOnError' =&gt; true,\n            ],\n            // Add more HTTP presets as needed\n        ],\n    ],\n    'debug' =&gt; [\n        'defaultPreset' =&gt; 'off',\n        'presets' =&gt; [\n            'off' =&gt; [\n                'httpEnabled' =&gt; false,\n            ],\n            'on' =&gt; [\n                'httpEnabled' =&gt; true,\n                'httpTrace' =&gt; true,\n                'httpRequestUrl' =&gt; true,\n                'httpRequestHeaders' =&gt; true,\n                'httpRequestBody' =&gt; true,\n                'httpResponseHeaders' =&gt; true,\n                'httpResponseBody' =&gt; true,\n                'httpResponseStream' =&gt; true,\n                'httpResponseStreamByLine' =&gt; true,\n            ],\n        ],\n    ],\n    'llm' =&gt; [\n        'defaultPreset' =&gt; 'deepseek',\n        'presets' =&gt; [\n            'deepseek' =&gt; [\n                'apiUrl' =&gt; 'https://api.deepseek.com',\n                'apiKey' =&gt; Env::get('DEEPSEEK_API_KEY'),\n                'endpoint' =&gt; '/chat/completions',\n                'model' =&gt; 'deepseek-chat',\n                'maxTokens' =&gt; 128,\n                'driver' =&gt; 'deepseek',\n                'httpClientPreset' =&gt; 'symfony',\n            ],\n            'openai' =&gt; [\n                'apiUrl' =&gt; 'https://api.openai.com',\n                'apiKey' =&gt; Env::get('OPENAI_API_KEY'),\n                'endpoint' =&gt; '/v1/chat/completions',\n                'model' =&gt; 'gpt-4',\n                'maxTokens' =&gt; 256,\n                'driver' =&gt; 'openai',\n                'httpClientPreset' =&gt; 'symfony',\n            ],\n        ],\n    ],\n];\n\nclass CustomConfigProvider implements CanProvideConfig\n{\n    private Dot $dot;\n\n    public function __construct(array $data = []) {\n        $this-&gt;dot = new Dot($data);\n    }\n\n    public function get(string $path, mixed $default = null): mixed {\n        return $this-&gt;dot-&gt;get($path, $default);\n    }\n\n    public function has(string $path): bool {\n        return $this-&gt;dot-&gt;has($path);\n    }\n}\n\n$configProvider = new CustomConfigProvider($configData);\n\n$events = new EventDispatcher();\n$customClient = (new HttpClientBuilder(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withClientInstance(\n        driverName: 'symfony',\n        clientInstance: SymfonyHttpClient::create(['http_version' =&gt; '2.0']),\n    )\n    -&gt;create();\n\n$inference = (new Inference(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withHttpClient($customClient);\n\n$answer = $inference\n    -&gt;using('deepseek') // Use 'deepseek' preset from CustomLLMConfigProvider\n    //-&gt;withDebugPreset('on')\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']])\n    -&gt;withMaxTokens(256)\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/context_cache_llm/","title":"Context caching (text inference)","text":""},{"location":"cookbook/polyglot/llm_advanced/context_cache_llm/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching (currently only Anthropic API), so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note 1: Instructor supports context caching for Anthropic API and OpenAI API.</p> <p>Note 2: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/polyglot/llm_advanced/context_cache_llm/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its summary for 2 target audiences.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$data = file_get_contents(__DIR__ . '/../../../README.md');\n\n$inference = (new Inference)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // wiretap to print all events\n    //-&gt;withDebugPreset('on') // debug HTTP traffic\n    -&gt;using('anthropic')\n    -&gt;withCachedContext(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Here is content of README.md file'],\n            ['role' =&gt; 'user', 'content' =&gt; $data],\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a short, very domain specific pitch of the project described in README.md. List relevant, domain specific problems that this project could solve. Use domain specific concepts and terminology to make the description resonate with the target audience.'],\n            ['role' =&gt; 'assistant', 'content' =&gt; 'For whom do you want to generate the pitch?'],\n        ],\n    );\n\n$response = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'founder of lead gen SaaS startup']],\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CTO of lead gen vendor\\n\");\nprint(\"  ({$response-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response-&gt;content() . \"\\n\");\n\nassert(!empty($response-&gt;content()));\nassert(Str::contains($response-&gt;content(), 'Instructor'));\nassert(Str::contains($response-&gt;content(), 'lead', false));\nassert($response-&gt;usage()-&gt;cacheReadTokens &gt; 0 || $response-&gt;usage()-&gt;cacheWriteTokens &gt; 0);\n\n$response2 = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'CIO of insurance company']],\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CIO of insurance company\\n\");\nprint(\"  ({$response2-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response2-&gt;content() . \"\\n\");\n\nassert(!empty($response2-&gt;content()));\nassert(Str::contains($response2-&gt;content(), 'Instructor'));\nassert(Str::contains($response2-&gt;content(), 'insurance', false));\nassert($response2-&gt;usage()-&gt;cacheReadTokens &gt; 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_config/","title":"Customize configuration of LLM driver","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_config/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to <code>Inference</code> object. This is useful when you want to initialize LLM client with custom values.</p>"},{"location":"cookbook/polyglot/llm_advanced/custom_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Drivers\\Symfony\\SymfonyDriver;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\n$events = new EventDispatcher();\n\n// Build fully customized HTTP client\n\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 30,\n    requestTimeout: 60,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true,\n);\n\n$yourClientInstance = SymfonyHttpClient::create(['http_version' =&gt; '2.0']);\n\n$customClient = (new HttpClientBuilder)\n    -&gt;withEventBus($events)\n    -&gt;withDriver(new SymfonyDriver(\n        config: $httpConfig,\n        clientInstance: $yourClientInstance,\n        events: $events,\n    ))\n    -&gt;create();\n\n// Create instance of LLM client initialized with custom parameters\n\n$config = new LLMConfig(\n    apiUrl  : 'https://api.deepseek.com',\n    apiKey  : Env::get('DEEPSEEK_API_KEY'),\n    endpoint: '/chat/completions', model: 'deepseek-chat', maxTokens: 128, driver: 'deepseek',\n);\n\n// Call inference API with custom client and configuration\n\n$answer = (new Inference)\n    -&gt;withEventHandler($events)\n    -&gt;withConfig($config)\n    -&gt;withHttpClient($customClient)\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_embeddings_config/","title":"Custom Embeddings Config","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_embeddings_config/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_embeddings_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n\n$query = \"technology news\";\n\n$config = new EmbeddingsConfig(\n    apiUrl    : 'https://api.cohere.ai/v1',\n    apiKey    : Env::get('COHERE_API_KEY', ''),\n    endpoint  : '/embed',\n    model     : 'embed-multilingual-v3.0',\n    dimensions: 1024,\n    maxInputs : 96,\n    httpClientPreset: 'guzzle',\n    driver: 'cohere',\n);\n\n$provider = EmbeddingsProvider::new()\n    -&gt;withConfig($config);\n\n$bestMatches = EmbedUtils::findSimilar(\n    provider: $provider,\n    query: $query,\n    documents: $documents,\n    topK: 3\n);\n\ndump($bestMatches);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_driver/","title":"Using custom LLM driver","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_llm_driver/#overview","title":"Overview","text":"<p>You can register and use your own LLM driver, either using a new driver name or overriding an existing driver bundled with Polyglot.</p>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_driver/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;\nuse Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI\\OpenAIDriver;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// we will use existing, bundled driver as an example, but you can provide any class that implements\n// a required interface (CanHandleInference)\n\nInference::registerDriver(\n    name: 'custom-driver',\n    driver: fn($config, $httpClient, $events) =&gt; new class($config, $httpClient, $events) extends OpenAIDriver {\n        public function handle(InferenceRequest $request): HttpResponse {\n            // some extra functionality to demonstrate our driver is being used\n            echo \"&gt;&gt;&gt; Handling request...\\n\";\n            return parent::handle($request);\n        }\n    }\n);\n\n// Create instance of LLM client initialized with custom parameters\n$config = new LLMConfig(\n    apiUrl          : 'https://api.openai.com/v1',\n    apiKey          : Env::get('OPENAI_API_KEY'),\n    endpoint        : '/chat/completions', model: 'gpt-4o-mini', maxTokens: 128,\n    httpClientPreset: 'guzzle',\n    driver          : 'custom-driver',\n);\n\n$answer = (new Inference)\n    -&gt;withConfig($config)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_via_dsn/","title":"Customize LLM Configuration with DSN string","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_llm_via_dsn/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration data to <code>Inference</code> object with DSN string. This is useful for inline configuration or for building configuration from admin UI, CLI arguments or environment variables.</p>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_via_dsn/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$answer = (new Inference)\n    -&gt;withDsn('preset=xai,model=grok-2')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/embed_utils/","title":"Embeddings utils","text":""},{"location":"cookbook/polyglot/llm_advanced/embed_utils/#overview","title":"Overview","text":"<p><code>EmbedUtils</code> class offers convenient methods to find top K vectors or documents most similar to provided query.</p> <p>Check out the <code>EmbedUtils</code> class for more details.  - <code>EmbedUtils::findTopK()</code>  - <code>EmbedUtils::findSimilar()</code></p> <p>Embeddings providers access details can be found and modified via <code>/config/embed.php</code>.</p>"},{"location":"cookbook/polyglot/llm_advanced/embed_utils/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n\n$query = \"technology news\";\n\n$presets = [\n    'azure',\n    'cohere',\n    'gemini',\n    'jina',\n    'mistral',\n    //'ollama',\n    'openai'\n];\n\nforeach($presets as $preset) {\n    $bestMatches = EmbedUtils::findSimilar(\n        provider: EmbeddingsProvider::using($preset),\n        query: $query,\n        documents: $documents,\n        topK: 3\n    );\n\n    echo \"\\n[$preset]\\n\";\n    dump($bestMatches);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/embeddings/","title":"Embeddings","text":""},{"location":"cookbook/polyglot/llm_advanced/embeddings/#overview","title":"Overview","text":"<p><code>Embeddings</code> class offers access to embeddings APIs which allows to generate vector representations of inputs. These embeddings can be used to compare semantic similarity between inputs, e.g. to find relevant documents based on a query.</p> <p><code>Embeddings</code> class supports following embeddings providers:  - Azure  - Cohere  - Gemini  - Jina  - Mistral  - OpenAI</p> <p>Embeddings providers access details can be found and modified via <code>/config/embed.php</code>.</p> <p>To store and search across large sets of vector embeddings you may want to use one of the popular vector databases: PGVector, Chroma, Pinecone, Weaviate, Milvus, etc.</p>"},{"location":"cookbook/polyglot/llm_advanced/embeddings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$query = \"technology news\";\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n$inputs = array_merge([$query], $documents);\n\n$topK = 3;\n\n// generate embeddings for query and documents (in a single request)\n$response = (new Embeddings)\n    -&gt;using('openai')\n    -&gt;withInputs($inputs)\n    -&gt;get();\n\n// get query and doc vectors from the response\n[$queryVectors, $docVectors] = $response-&gt;split(1);\n\n$queryVector = $queryVectors[0]\n    ?? throw new \\InvalidArgumentException('Query vector not found');\n\n// calculate cosine similarities\n$similarities = EmbedUtils::findTopK($queryVector, $docVectors, $topK);\n\n// print documents most similar to the query\necho \"Query: \" . $query . PHP_EOL;\n$count = 1;\nforeach($similarities as $index =&gt; $similarity) {\n    echo $count++;\n    echo ': ' . $documents[$index];\n    echo ' - cosine similarity to query = ' . $similarities[$index];\n    echo PHP_EOL;\n}\n\nassert(!empty($similarities));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/http_client/","title":"Work directly with HTTP client facade","text":""},{"location":"cookbook/polyglot/llm_advanced/http_client/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_advanced/http_client/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// check with default HTTP client facade\n$httpClient = new HttpClient();\n\n$answer = (new Inference)\n    -&gt;withDsn('preset=openai,model=gpt-3.5-turbo')\n    -&gt;withHttpClient($httpClient)\n    -&gt;withMessages('What is the capital of France')\n    -&gt;withMaxTokens(64)\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/parallel_calls/","title":"Parallel Calls","text":""},{"location":"cookbook/polyglot/llm_advanced/parallel_calls/#overview","title":"Overview","text":"<p>Work in progress.</p>"},{"location":"cookbook/polyglot/llm_advanced/reasoning_content/","title":"Reasoning Content Access","text":""},{"location":"cookbook/polyglot/llm_advanced/reasoning_content/#overview","title":"Overview","text":"<p>Deepseek API allows to access reasoning content, which is a detailed explanation of how the response was generated. This feature is useful for debugging and understanding the reasoning behind the response.</p>"},{"location":"cookbook/polyglot/llm_advanced/reasoning_content/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: regular API, allows to customize inference options\n$response = (new Inference)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;using('deepseek-r')\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France. Answer with just a name.']])\n    -&gt;withMaxTokens(256)\n    -&gt;response();\n\necho \"\\nCASE #1: Sync response\\n\";\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: {$response-&gt;content()}\\n\";\necho \"REASONING: {$response-&gt;reasoningContent()}\\n\";\nassert($response-&gt;content() !== '');\nassert(Str::contains($response-&gt;content(), 'Paris'));\nassert($response-&gt;reasoningContent() !== '');\n\n\n// EXAMPLE 2: streaming response\n$stream = (new Inference)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;using('deepseek-r') // optional, default is set in /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of Brasil. Answer with just a name.']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;withStreaming()\n    -&gt;stream();\n\necho \"\\nCASE #2: Streamed response\\n\";\necho \"USER: What is capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream-&gt;responses() as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\necho \"REASONING: {$stream-&gt;final()-&gt;reasoningContent()}\\n\";\nassert($stream-&gt;final()-&gt;reasoningContent() !== '');\nassert($stream-&gt;final()-&gt;content() !== '');\nassert(Str::contains($stream-&gt;final()-&gt;content(), 'Bras\u00edlia'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/a21/","title":"A21","text":""},{"location":"cookbook/polyglot/llm_api_support/a21/#overview","title":"Overview","text":"<p>Support for A21 Jamba - MAMBA architecture models, very strong at handling long context.</p>"},{"location":"cookbook/polyglot/llm_api_support/a21/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('a21') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/anthropic/","title":"Anthropic","text":""},{"location":"cookbook/polyglot/llm_api_support/anthropic/#overview","title":"Overview","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - OutputMode::MdJson, OutputMode::Json - supported - OutputMode::Tools - not supported yet</p>"},{"location":"cookbook/polyglot/llm_api_support/anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('anthropic') // see /config/llm.php\n    //-&gt;withHttpClientPreset('guzzle')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 128]\n    )\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/azure_openai/","title":"Azure OpenAI","text":""},{"location":"cookbook/polyglot/llm_api_support/azure_openai/#overview","title":"Overview","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p>"},{"location":"cookbook/polyglot/llm_api_support/azure_openai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/cerebras/","title":"Cerebras","text":""},{"location":"cookbook/polyglot/llm_api_support/cerebras/#overview","title":"Overview","text":"<p>Support for Cerebras API which uses custom hardware for super fast inference. Cerebras provides Llama models.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/cerebras/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('cerebras') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/cohere/","title":"Cohere","text":""},{"location":"cookbook/polyglot/llm_api_support/cohere/#overview","title":"Overview","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - OutputMode::MdJson - supported, recommended as a fallback from JSON mode  - OutputMode::Json - supported, recommended  - OutputMode::Tools - partially supported, not recommended</p> <p>Reasons OutputMode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple, flat data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul>"},{"location":"cookbook/polyglot/llm_api_support/cohere/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('cohere') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/deepseek/","title":"DeepSeek","text":""},{"location":"cookbook/polyglot/llm_api_support/deepseek/#overview","title":"Overview","text":"<p>Support for DeepSeek API which provides strong models at affordable price.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/deepseek/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('deepseek') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/fireworks/","title":"Fireworks.ai","text":""},{"location":"cookbook/polyglot/llm_api_support/fireworks/#overview","title":"Overview","text":"<p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - selected models - OutputMode::Json - selected models - OutputMode::MdJson</p>"},{"location":"cookbook/polyglot/llm_api_support/fireworks/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('fireworks') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/google_gemini/","title":"Google Gemini","text":""},{"location":"cookbook/polyglot/llm_api_support/google_gemini/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('gemini') // see /config/llm.php\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // optional, for debugging\n    -&gt;withDebugPreset('detailed')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/groq/","title":"Groq","text":""},{"location":"cookbook/polyglot/llm_api_support/groq/#overview","title":"Overview","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma.</p> <p>Supported modes depend on the specific model, but generally include:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Groq API.</p>"},{"location":"cookbook/polyglot/llm_api_support/groq/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('groq') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/meta/","title":"Meta","text":""},{"location":"cookbook/polyglot/llm_api_support/meta/#overview","title":"Overview","text":"<p>Instructor supports Meta LLM inference API. You can find the details on how to configure</p>"},{"location":"cookbook/polyglot/llm_api_support/meta/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('meta') // see /config/llm.php\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/minimaxi/","title":"Minimaxi","text":""},{"location":"cookbook/polyglot/llm_api_support/minimaxi/#overview","title":"Overview","text":"<p>Support for Minimaxi's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/polyglot/llm_api_support/minimaxi/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('minimaxi') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/mistralai/","title":"Mistral AI","text":""},{"location":"cookbook/polyglot/llm_api_support/mistralai/#overview","title":"Overview","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility:  - OutputMode::Tools - supported (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::Json - recommended (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::MdJson - fallback mode (Mistral 7B / Mixtral 8x7B)</p>"},{"location":"cookbook/polyglot/llm_api_support/mistralai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('mistral') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/moonshotai/","title":"MoonshotAI","text":""},{"location":"cookbook/polyglot/llm_api_support/moonshotai/#overview","title":"Overview","text":"<p>Support for MoonshotAI's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/polyglot/llm_api_support/moonshotai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('moonshot-kimi') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/ollama/","title":"Local / Ollama","text":""},{"location":"cookbook/polyglot/llm_api_support/ollama/#overview","title":"Overview","text":"<p>You can use Instructor with local Ollama instance.</p> <p>Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model for complex data schemas.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode, works with any capable model  - OutputMode::Json - recommended  - OutputMode::Tools - supported (for selected models - check Ollama docs)</p>"},{"location":"cookbook/polyglot/llm_api_support/ollama/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('ollama') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/openai/","title":"OpenAI","text":""},{"location":"cookbook/polyglot/llm_api_support/openai/#overview","title":"Overview","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended for new models)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/openai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/openrouter/","title":"OpenRouter","text":""},{"location":"cookbook/polyglot/llm_api_support/openrouter/#overview","title":"Overview","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p>"},{"location":"cookbook/polyglot/llm_api_support/openrouter/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openrouter') // see /config/llm.php\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/perplexity/","title":"Perplexity","text":""},{"location":"cookbook/polyglot/llm_api_support/perplexity/#overview","title":"Overview","text":"<p>Perplexity is a search engine that provides an API for generating text. It is designed to be used in a variety of applications, including chatbots, content generation, and more.</p>"},{"location":"cookbook/polyglot/llm_api_support/perplexity/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('perplexity') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/sambanova/","title":"SambaNova","text":""},{"location":"cookbook/polyglot/llm_api_support/sambanova/#overview","title":"Overview","text":"<p>Support for SambaNova's API, which provide fast inference endpoints for Llama and Qwen LLMs.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/polyglot/llm_api_support/sambanova/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('sambanova') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/togetherai/","title":"Together.ai","text":""},{"location":"cookbook/polyglot/llm_api_support/togetherai/#overview","title":"Overview","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support OutputMode::Tools or OutputMode::Json, which are much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - supported for selected models - OutputMode::Json - supported for selected models - OutputMode::MdJson - fallback mode</p>"},{"location":"cookbook/polyglot/llm_api_support/togetherai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('together') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/xai/","title":"xAI / Grok","text":""},{"location":"cookbook/polyglot/llm_api_support/xai/#overview","title":"Overview","text":"<p>Support for xAI's API, which offers access to X.com's Grok model.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/xai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('xai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/inference/","title":"Working directly with LLMs","text":""},{"location":"cookbook/polyglot/llm_basics/inference/#overview","title":"Overview","text":"<p><code>Inference</code> class offers access to LLM APIs and convenient methods to execute model inference, incl. chat completions, tool calling or JSON output generation.</p> <p>LLM providers access details can be found and modified via <code>/config/llm.php</code>.</p>"},{"location":"cookbook/polyglot/llm_basics/inference/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: use default LLM connection preset for convenient ad-hoc calls\n$answer = (new Inference)\n    -&gt;with(messages: 'What is capital of Germany')\n    -&gt;get();\n\necho \"USER: What is capital of Germany\\n\";\necho \"ASSISTANT: $answer\\n\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n\n\n\n// EXAMPLE 2: customize inference options using fluent API\n$response = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;create();\n\n$answer = $response-&gt;get();\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\\n\";\nassert(Str::contains($answer, 'Paris'));\n\n\n\n\n// EXAMPLE 3: streaming response\n$stream = (new Inference)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'Describe capital of Brasil']])\n    -&gt;withOptions(['max_tokens' =&gt; 128])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: Describe capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/json_schema_api/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/polyglot/llm_basics/json_schema_api/#overview","title":"Overview","text":"<p>Polyglot has a built-in support for dynamically constructing JSON Schema using <code>JsonSchema</code> class. It is useful when you want to shape the structures during runtime.</p>"},{"location":"cookbook/polyglot/llm_basics/json_schema_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'City name'),\n        JsonSchema::integer('population', 'City population'),\n        JsonSchema::integer('founded', 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with JSON data.']\n        ],\n        responseFormat: $schema-&gt;toResponseFormat(\n            schemaName: 'city_data',\n            schemaDescription: 'City data',\n            strict: true,\n        ),\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data));\nassert(is_string($data['name']));\nassert(is_int($data['population']));\nassert(is_int($data['founded']));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_json/","title":"Working directly with LLMs and JSON - JSON mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_json/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p> <p><code>Inference</code> class supports multiple inference modes, like <code>Tools</code>, <code>Json</code> <code>JsonSchema</code> or <code>MdJson</code>, which gives you flexibility to choose the best approach for your use case.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_json/#example","title":"Example","text":"<p>In this example we will use OpenAI JSON mode, which guarantees that the response will be in a JSON format.</p> <p>It does not guarantee compliance with a specific schema (for some providers including OpenAI). We can try to work around it by providing an example of the expected JSON output in the prompt.</p> <p>NOTE: Some model providers allow to specify a JSON schema for model to follow via <code>schema</code> parameter of <code>response_format</code>. OpenAI does not support this feature in JSON mode (only in JSON Schema mode).</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with JSON data containing name\", population and year of founding. \\\n           Example: {\"name\": \"Berlin\", \"population\": 3700000, \"founded\": 1237}']],\n        responseFormat: [\n            'type' =&gt; 'json_object',\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Json,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_json_schema/","title":"Working directly with LLMs and JSON - JSON Schema mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_json_schema/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_json_schema/#example","title":"Example","text":"<p>In this example we will use OpenAI JSON Schema mode, which guarantees that the response will be in a JSON format that matches the provided schema.</p> <p>NOTE: Json Schema mode with guaranteed structured outputs is not supported by all language model providers.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n        Respond with JSON data.']],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; [\n                    'type' =&gt; 'object',\n                    'description' =&gt; 'City information',\n                    'properties' =&gt; [\n                        'name' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'City name',\n                        ],\n                        'founded' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Founding year',\n                        ],\n                        'population' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Current population',\n                        ],\n                    ],\n                    'additionalProperties' =&gt; false,\n                    'required' =&gt; ['name', 'founded', 'population'],\n                ],\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_md_json/","title":"Working directly with LLMs and JSON - MdJSON mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_md_json/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_md_json/#example","title":"Example","text":"<p>In this example we will use emulation mode - MdJson, which tries to force the model to generate a JSON output by asking it to respond with a JSON object within a Markdown code block.</p> <p>This is useful for the models which do not support JSON output directly.</p> <p>We will also provide an example of the expected JSON output in the prompt to guide the model in generating the correct response.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with JSON data containing name\", population and year of founding. \\\n           Example: {\"name\": \"Berlin\", \"population\": 3700000, \"founded\": 1237}']],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::MdJson,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_tools/","title":"Working directly with LLMs and JSON - Tools mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_tools/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_tools/#example","title":"Example","text":"<p>In this example we will use OpenAI tools mode, in which model will generate a JSON containing arguments for a function call. This way we can make the model generate a JSON object with specific structure of parameters.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with function call.']],\n        tools: [[\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'extract_data',\n                'description' =&gt; 'Extract city data',\n                'parameters' =&gt; [\n                    'type' =&gt; 'object',\n                    'description' =&gt; 'City information',\n                    'properties' =&gt; [\n                        'name' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'City name',\n                        ],\n                        'founded' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Founding year',\n                        ],\n                        'population' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Current population',\n                        ],\n                    ],\n                    'required' =&gt; ['name', 'founded', 'population'],\n                    'additionalProperties' =&gt; false,\n                ],\n            ],\n        ]],\n        toolChoice: [\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'extract_data'\n            ]\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Tools,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/chat_with_summary/","title":"Chat with summary","text":""},{"location":"cookbook/polyglot/llm_extras/chat_with_summary/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_extras/chat_with_summary/#example","title":"Example","text":"<p>```php &lt;?php die(); // TO BE FIXED</p> <p>require 'examples/boot.php';</p> <p>use Cognesy\\Addons\\Chat\\Pipelines\\ChatWithSummary; use Cognesy\\Addons\\Chat\\Utils\\SummarizeMessages; use Cognesy\\Polyglot\\Inference\\Inference; use Cognesy\\Polyglot\\Inference\\LLMProvider; use Cognesy\\Messages\\Message; use Cognesy\\Messages\\Messages;</p> <p>$maxSteps = 5; $sys = [     'You are helpful assistant explaining Challenger Sale method, you answer questions. Provide very brief answers, not more than one sentence. Simplify things, don\\'t go into details, but be very pragmatic and focused on practical bizdev problems.',     'You are curious novice growth expert working to promote Instructor library, you keep asking questions. Use your knowledge of Instructor library and marketing of tech products for developers. Ask short, simple questions. Always ask a single question.', ]; $startMessage = new Message('assistant', 'Help me get better sales results. Be brief and concise.');</p> <p>$context = \"# CONTEXT\\n\\n\" . file_get_contents(DIR . '/summary.md');</p> <p>$summarizer = new SummarizeMessages(     //prompt: 'Summarize the messages.',     llm: LLMProvider::using('deepseek'),     //model: 'gpt-4o-mini',     tokenLimit: 1024, );</p> <p>//$chat = new ChatWithSummary( //    null, //    256, //    256, //    1024, //    true, //    true, //    $summarizer, //);</p> <p>$chat = ChatWithSummary::create(     256,     256,     1024,     $summarizer, ); \\(chat-&gt;script()-&gt;section('main')-&gt;appendMessage(\\)startMessage);</p> <p>for($i = 0; $i &lt; $maxSteps; \\(i++) {     \\(chat-&gt;script()         -&gt;section('system')         -&gt;withMessages(Messages::fromString(\\)sys[\\)i % 2], 'system'));     \\(chat-&gt;script()         -&gt;section('context')         -&gt;withMessages(Messages::fromString(\\)context, 'system'));</p> <pre><code>$messages = $chat-&gt;script()\n    -&gt;select(['system', 'context', 'summary', 'buffer', 'main'])\n    -&gt;toMessages()\n    -&gt;remapRoles(['assistant' =&gt; 'user', 'user' =&gt; 'assistant', 'system' =&gt; 'system']);\n\n$response = (new Inference)\n    -&gt;using('deepseek')\n    -&gt;with(\n        messages: $messages-&gt;toArray(),\n        options: ['max_tokens' =&gt; 256],\n    )\n    -&gt;get();\n\necho \"\\n\";\ndump('&gt;&gt;&gt; '.$response);\necho \"\\n\";\n$chat-&gt;appendMessage(new Message(role: 'assistant', content: $response), 'main');\n</code></pre> <p>} //dump($chat-&gt;script());</p>"},{"location":"cookbook/polyglot/llm_extras/image_data/","title":"Using images in prompts","text":""},{"location":"cookbook/polyglot/llm_extras/image_data/#overview","title":"Overview","text":"<p><code>Image</code> class in Instructor PHP provides an easy way to include images in your prompts. It supports loading images from files, URLs, or base64 encoded strings. The image can be sent as part of the message content to the LLM.</p>"},{"location":"cookbook/polyglot/llm_extras/image_data/#example","title":"Example","text":"<p>```php &lt;?php require 'examples/boot.php';</p> <p>use Cognesy\\Messages\\Content; use Cognesy\\Messages\\ContentPart; use Cognesy\\Messages\\Messages; use Cognesy\\Messages\\Utils\\Image; use Cognesy\\Polyglot\\Inference\\Inference;</p> <p>$content = (new Content)     -&gt;addContentPart(ContentPart::text('Describe the car damage in the image.'))     -&gt;addContentPart(Image::fromFile(DIR . '/car-damage.jpg')-&gt;toContentPart());</p> <p>\\(messages = (new Messages)     -&gt;asSystem('You are an expert in car damage assessment.')     -&gt;asUser(\\)content);</p> <p>\\(response = (new Inference)     -&gt;using('openai')     -&gt;withModel('gpt-4o-mini')     -&gt;withMessages(\\)messages)     -&gt;get();</p> <p>echo \"Response: \" . $response . \"\\n\";</p>"},{"location":"cookbook/polyglot/llm_extras/prompt_templates/","title":"Prompt Templates","text":""},{"location":"cookbook/polyglot/llm_extras/prompt_templates/#overview","title":"Overview","text":"<p><code>Template</code> class in Instructor PHP provides a way to define and use prompt templates using Twig, Blade or custom 'arrowpipe' template syntax.</p>"},{"location":"cookbook/polyglot/llm_extras/prompt_templates/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Template\\Template;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: Define prompt template inline (don't use files) and use short syntax\n\n$prompt = Template::twig()\n    -&gt;from('What is capital of {{country}}')\n    -&gt;with(['country' =&gt; 'Germany'])\n    -&gt;toText();\n\n$answer = (new Inference)-&gt;withMessages($prompt)-&gt;get();\n\necho \"EXAMPLE 1: prompt = $prompt\\n\";\necho \"ASSISTANT: $answer\\n\";\necho \"\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n// EXAMPLE 2: Load prompt from file\n\n// use default template language, prompt files are in /prompts/twig/&lt;prompt&gt;.twig\n$prompt = Template::text(\n    pathOrDsn: 'demo-twig:capital',\n    variables: ['country' =&gt; 'Germany'],\n);\n\n$answer = (new Inference)-&gt;withMessages($prompt)-&gt;get();\n\necho \"EXAMPLE 2: prompt = $prompt\\n\";\necho \"ASSISTANT: $answer\\n\";\necho \"\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/summary_with_llm/","title":"Simple content summary","text":""},{"location":"cookbook/polyglot/llm_extras/summary_with_llm/#overview","title":"Overview","text":"<p>This is an example of a simple summarization.</p>"},{"location":"cookbook/polyglot/llm_extras/summary_with_llm/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n$summary = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Content to summarize:'],\n            ['role' =&gt; 'user', 'content' =&gt; $report],\n            ['role' =&gt; 'user', 'content' =&gt; 'Concise summary of project report in 2-3 sentences:'],\n        ]\n    )\n    -&gt;get();\n\ndump($summary);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/tool_use/","title":"Inference and tool use","text":""},{"location":"cookbook/polyglot/llm_extras/tool_use/#overview","title":"Overview","text":"<p><code>ToolUse</code> class automates the process of using tools by LLM, i.e.:  - calling LLM with provided context (message sequence),  - extracting tool calls requested by LLM from the response,  - calling the requested tool and storing its results,  - constructing message sequence with the result of call,  - sending updated message sequence back to LLM.</p> <p>This cycle is repeated until one of the exit criteria is met: - LLM no longer requests any tool calls, - specified maximum number of iterations is reached, - specified token usage limit is reached - there are any errors during the process (e.g. LLM requested a tool that is not available).</p> <p><code>ToolUse</code> class provides 3 ways to iterate through the process: - manual control - code is responsible for checking <code>hasNextStep()</code> and calling <code>nextStep()</code> in a loop, - using iterator - code uses foreach loop to iterate through the steps (internally it checks <code>hasNextStep()</code> and calls <code>nextStep()</code>), - just get final step - you only get the final step, iteration process is done internally.</p>"},{"location":"cookbook/polyglot/llm_extras/tool_use/#example","title":"Example","text":"<p>This example demonstrates 3 ways to use <code>ToolUse</code> class to allow LLM call functions if needed to answer simple math question. We provide 2 functions (<code>add_numbers</code> and <code>subtract_numbers</code>) as tools available to LLM and specify the task in plain language. The LLM is expected to call the functions in the correct order to get the final result.</p> <p>```php &lt;?php require 'examples/boot.php';</p> <p>use Cognesy\\Addons\\ToolUse\\Tools\\FunctionTool; use Cognesy\\Addons\\ToolUse\\ToolUse;</p> <p>function add_numbers(int $a, int $b) : int {     return $a + $b; }</p> <p>function subtract_numbers(int $a, int $b) : int {     return $a - $b; }</p> <p>// // PATTERN #1 - manual control // echo \"\\nPATTERN #1 - manual control\\n\"; $toolUse = (new ToolUse)     -&gt;withMessages('Add 2455 and 3558 then subtract 4344 from the result.')     -&gt;withTools([         FunctionTool::fromCallable(add_numbers(...)),         FunctionTool::fromCallable(subtract_numbers(...)),     ]);</p> <p>// iterate until no more steps while ($toolUse-&gt;hasNextStep()) {     $step = $toolUse-&gt;nextStep();     print(\"STEP - tokens used: \" . $step-&gt;usage()-&gt;total() . \"\\n\"); }</p> <p>// print final response $result = $step-&gt;response(); print(\"RESULT: \" . $result . \"\\n\");</p> <p>// // PATTERN #2 - using iterator // echo \"\\nPATTERN #2 - using iterator\\n\"; $toolUse = (new ToolUse)     -&gt;withMessages('Add 2455 and 3558 then subtract 4344 from the result.')     -&gt;withTools([         FunctionTool::fromCallable(add_numbers(...)),         FunctionTool::fromCallable(subtract_numbers(...)),     ]);</p> <p>// iterate until no more steps foreach ($toolUse-&gt;iterator() as $step) {     print(\"STEP - tokens used: \" . $step-&gt;usage()-&gt;total() . \"\\n\"); }</p> <p>// print final response $result = $toolUse-&gt;context()-&gt;currentStep()-&gt;response(); print(\"RESULT: \" . $result . \"\\n\");</p> <p>// // PATTERN #3 - just get final step (fast forward to it) // echo \"\\nPATTERN #3 - get only final result\\n\"; $toolUse = (new ToolUse)     -&gt;withMessages('Add 2455 and 3558 then subtract 4344 from the result.')     -&gt;withTools([         FunctionTool::fromCallable(add_numbers(...)),         FunctionTool::fromCallable(subtract_numbers(...)),     ]);</p> <p>// print final response $result = $toolUse-&gt;finalStep()-&gt;response(); print(\"RESULT: \" . $result . \"\\n\");</p>"},{"location":"cookbook/polyglot/llm_troubleshooting/http_debug/","title":"Debugging HTTP Calls","text":""},{"location":"cookbook/polyglot/llm_troubleshooting/http_debug/#overview","title":"Overview","text":"<p>Instructor PHP provides a way to debug HTTP calls made to LLM APIs via <code>withDebug()</code> method call on <code>Inference</code> object.</p> <p>When debug mode is turned on all HTTP requests and responses are dumped to the console.</p>"},{"location":"cookbook/polyglot/llm_troubleshooting/http_debug/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$response = (new Inference)\n    -&gt;withDebugPreset('on') // Enable debug mode\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of Brasil']],\n        options: ['max_tokens' =&gt; 128]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of Brasil\\n\";\necho \"ASSISTANT: $response\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/in_context_examples/","title":"Generate In-Context Examples","text":""},{"location":"cookbook/prompting/few_shot/in_context_examples/#overview","title":"Overview","text":"<p>How can we generate examples for our prompt?</p> <p>Self-Generated In-Context Learning (SG-ICL) is a technique which uses an LLM to generate examples to be used during the task. This allows for in-context learning, where examples of the task are provided in the prompt.</p> <p>We can implement SG-ICL using Instructor as seen below.</p>"},{"location":"cookbook/prompting/few_shot/in_context_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum ReviewSentiment : string {\n    case Positive = 'positive';\n    case Negative = 'negative';\n}\n\nclass GeneratedReview {\n    public string $review;\n    public ReviewSentiment $sentiment;\n}\n\n\nclass PredictSentiment {\n    private int $n = 4;\n\n    public function __invoke(string $review) : ReviewSentiment {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Review: {$review}\"],\n            ],\n            responseModel: Scalar::enum(ReviewSentiment::class),\n            examples: $this-&gt;generateExamples($review),\n        )-&gt;get();\n    }\n\n    private function generate(string $inputReview, ReviewSentiment $sentiment) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Generate {$this-&gt;n} various {$sentiment-&gt;value} reviews based on the input review:\\n{$inputReview}\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"Generated review:\"],\n            ],\n            responseModel: Sequence::of(GeneratedReview::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    private function generateExamples(string $inputReview) : array {\n        $examples = [];\n        foreach ([ReviewSentiment::Positive, ReviewSentiment::Negative] as $sentiment) {\n            $samples = $this-&gt;generate($inputReview, $sentiment);\n            foreach ($samples as $sample) {\n                $examples[] = Example::fromData($sample-&gt;review, $sample-&gt;sentiment-&gt;value);\n            }\n        }\n        return $examples;\n    }\n}\n\n$predictSentiment = (new PredictSentiment)('This movie has been very impressive, even considering I lost half of the plot.');\n\ndump($predictSentiment);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/in_context_examples/#references","title":"References","text":"<ol> <li>Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/misc/arbitrary_properties/","title":"Arbitrary properties","text":""},{"location":"cookbook/prompting/misc/arbitrary_properties/#overview","title":"Overview","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n?&gt;\n</code></pre> <p>Now we can use this data model to extract arbitrary properties from a text message in a form that is easier for future processing.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives\n    in a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    mode: OutputMode::Json,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(!empty($user-&gt;properties));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/","title":"Consistent values of arbitrary properties","text":""},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/#overview","title":"Overview","text":"<p>For multiple records containing arbitrary properties, instruct LLM to get more consistent key names when extracting properties.</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $id;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetails\n{\n    /**\n     * @var UserDetail[] Extract information for multiple users.\n     * Use consistent key names for properties across users.\n     */\n    public array $users = [];\n}\n\n$text = \"Jason is 25 years old. He is a Python programmer.\\\n Amanda is UX designer.\\\n John is 40yo and he's CEO.\";\n\n$list = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetails::class,\n)-&gt;get();\n\ndump($list);\n\nassert(!empty($list-&gt;users));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/chain_of_summaries/","title":"Chain of Summaries","text":""},{"location":"cookbook/prompting/misc/chain_of_summaries/#overview","title":"Overview","text":"<p>This is an example of summarization with increasing amount of details. Instructor is provided with data structure containing instructions on how to create increasingly detailed summaries of the project report.</p> <p>It starts with generating an overview of the project, followed by X iterations of increasingly detailed summaries. Each iteration should contain all the information from the previous summary, plus a few additional facts from the content which are most relevant and missing from the previous iteration.</p>"},{"location":"cookbook/prompting/misc/chain_of_summaries/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n/** Executive level summary of the project */\nclass Summary {\n    /** current summary iteration, not bigger than 3 */\n    public int $iteration = 0;\n    /** @var string[] 1-3 facts most relevant from executive perspective and missing from the summary (avoid technical details) */\n    public array $missingFacts = [];\n    /** denser summary in 1-3 sentences, which covers every fact from the previous summary plus the missing ones */\n    public string $expandedSummary = '';\n}\n\n/** Increasingly denser, expanded summaries */\nclass ChainOfSummaries {\n    /** simplified, executive view with no details, just a single statement of overall situation */\n    public string $overview;\n    /** @var Summary[] contains at least 3 gradually more expanded summaries of the content */\n    public array $summaries;\n}\n\n$summaries = (new StructuredOutput)\n    -&gt;with(\n        messages: $report,\n        responseModel: ChainOfSummaries::class,\n        prompt: 'Generate a denser summary based on the provided content.',\n        options: [\n            'max_tokens' =&gt; 4096,\n        ],\n        toolName: 'summarizer',\n        toolDescription: 'Generates a summary based on the provided content.',\n    )\n    -&gt;get();\n\nprint(\"\\n# Summaries with increasing density:\\n\\n\");\nprint(\"Overview:\\n\");\nprint(\"{$summaries-&gt;overview}\\n\\n\");\nforeach ($summaries-&gt;summaries as $summary) {\n    print(\"Expanded summary - iteration #{$summary-&gt;iteration}:\\n\");\n    print(\"{$summary-&gt;expandedSummary}\\n\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/chain_of_thought/","title":"Chain of Thought","text":""},{"location":"cookbook/prompting/misc/chain_of_thought/#overview","title":"Overview","text":"<p>This approach to \"chain of thought\" improves data quality, by eliciting LLM reasoning to self-explain approach to generating the response.</p> <p>With Instructor you can achieve a 'modular' CoT, where multiple explanations can be generated by LLM for different parts of the response, driving a more granular control and improvement of the response.</p>"},{"location":"cookbook/prompting/misc/chain_of_thought/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass Employee {\n    #[Instructions('Think step by step to determine the correct year of employment.')]\n    public string $reasoning;\n    public int $yearOfEmployment;\n    // ... other data fields of your employee class\n}\n\n$text = 'He was working here for 5 years. Now, in 2019, he is a manager.';\n\n$employee = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Employee::class\n)-&gt;get();\n\n\ndump($employee);\n\nassert($employee-&gt;yearOfEmployment === 2014);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/","title":"Single label classification","text":""},{"location":"cookbook/prompting/misc/classification/#overview","title":"Overview","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p>"},{"location":"cookbook/prompting/misc/classification/#example","title":"Example","text":"<p>Let's start by defining the data structures.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Enumeration for single-label text classification.\nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction.\nclass SinglePrediction {\n    public Label $classLabel;\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/#classifying-text","title":"Classifying Text","text":"<p>The function classify will perform the single-label classification.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction classify(string $data) : SinglePrediction {\n    return (new StructuredOutput)-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n    )-&gt;get();\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\n\ndump($prediction);\n\nassert($prediction-&gt;classLabel == Label::SPAM);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/","title":"Multiclass classification","text":""},{"location":"cookbook/prompting/misc/classification_multiclass/#overview","title":"Overview","text":"<p>We start by defining the structures.</p> <p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass TicketLabels {\n    /** @var Label[] */\n    public array $labels = [];\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/#classifying-text0","title":"Classifying Text0","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : TicketLabels {\n    $x = (new StructuredOutput)\n        //-&gt;withDebugPreset('on')\n        -&gt;wiretap(fn($e) =&gt; $e-&gt;printDebug())\n        -&gt;withMessages(\"Label following support ticket: {$data}\")\n        -&gt;withResponseModel(TicketLabels::class)\n        -&gt;create();\ndd($x);\n//        -&gt;get();\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\ndump($prediction);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;labels));\nassert(in_array(Label::BILLING, $prediction-&gt;labels));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/component_reuse/","title":"Reusing components","text":""},{"location":"cookbook/prompting/misc/component_reuse/#overview","title":"Overview","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p>"},{"location":"cookbook/prompting/misc/component_reuse/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public string $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Yesterday Jason worked from 9 for 5 hours. After that I watched 2 hour movie which I finished at 19.\"]],\n    responseModel: UserDetail::class,\n    model: 'gpt-4o',\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name == \"Jason\");\nassert($user-&gt;workTime-&gt;startTime === 9);\nassert($user-&gt;workTime-&gt;endTime === 14);\nassert($user-&gt;leisureTime-&gt;startTime === 17);\nassert($user-&gt;leisureTime-&gt;endTime === 19);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/component_reuse_cot/","title":"Using CoT to improve interpretation of component data","text":""},{"location":"cookbook/prompting/misc/component_reuse_cot/#overview","title":"Overview","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <p>We're additionally starting the data structure with a Chain of Thought field to elicit LLM reasoning for the time range calculation, which can improve the accuracy of the response.</p>"},{"location":"cookbook/prompting/misc/component_reuse_cot/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours (0-23 format) */\n    public int $startTime;\n    /** The end time in hours (0-23 format) */\n    public int $endTime;\n}\n\n$timeRange = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Workshop with Apex Industries started 9 and it took us 6 hours to complete.\"]],\n    responseModel: TimeRange::class,\n    maxRetries: 2\n)-&gt;get();\n\ndump($timeRange);\n\nassert($timeRange-&gt;startTime === 9);\nassert($timeRange-&gt;endTime === 15);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/entity_relationships/","title":"Entity relationship extraction","text":""},{"location":"cookbook/prompting/misc/entity_relationships/#overview","title":"Overview","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> fields.</p>"},{"location":"cookbook/prompting/misc/entity_relationships/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /**\n     * @var int[] Correct and complete list of coworker IDs, representing\n     * collaboration between users.\n     */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /**\n     * @var UserDetail[] Collection of users, correctly capturing the\n     * relationships among them.\n     */\n    public array $users;\n}\n\n$text = \"Jason is 25 years old. He is a Python programmer of Apex website.\\\n Amanda is a contractor working with Jason on Apex website. John is 40yo\\\n and he's CEO - Jason reports to him.\";\n\n$relationships = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserRelationships::class,\n)-&gt;get();\n\ndump($relationships);\n\nassert(!empty($relationships-&gt;users));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/handling_errors/","title":"Handling errors","text":""},{"location":"cookbook/prompting/misc/handling_errors/#overview","title":"Overview","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <p>NOTE: Instructor offers a built-in Maybe wrapper class that you can use to handle errors. See the example in Basics section for more details.</p>"},{"location":"cookbook/prompting/misc/handling_errors/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $user = null;\n    public bool $noUserData = false;\n    /** If no user data, provide reason */\n    public ?string $errorMessage = '';\n\n    public function get(): ?UserDetail {\n        return $this-&gt;noUserData ? null : $this-&gt;user;\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'We don\\'t know anything about this guy.']])\n    -&gt;withResponseModel(MaybeUser::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;noUserData);\nassert(!empty($user-&gt;errorMessage));\nassert($user-&gt;get() === null);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/limiting_lists/","title":"Limiting the length of lists","text":""},{"location":"cookbook/prompting/misc/limiting_lists/#overview","title":"Overview","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length of list. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <p>To be 100% certain the list does not exceed the limit, add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"cookbook/prompting/misc/limiting_lists/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass Property\n{\n    /**  Monotonically increasing ID, not larger than 2 */\n    public string $index;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    use ValidationMixin;\n\n    public int $age;\n    public string $name;\n    /** @var Property[] List other extracted properties - not more than 2. */\n    public array $properties;\n\n    public function validate() : ValidationResult\n    {\n        if (count($this-&gt;properties) &lt; 3) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'properties',\n            value: $this-&gt;name,\n            message: \"Number of properties must be not more than 2.\",\n        );\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives in\n    a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    maxRetries: 1 // change to 0 to see validation error\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(count($user-&gt;properties) &lt; 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/reflection_prompting/","title":"Reflection Prompting","text":""},{"location":"cookbook/prompting/misc/reflection_prompting/#overview","title":"Overview","text":"<p>This implementation of Reflection Prompting with Instructor provides a structured way to encourage LLM to engage in more thorough and self-critical thinking processes, potentially leading to higher quality and more reliable outputs.</p>"},{"location":"cookbook/prompting/misc/reflection_prompting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Contracts\\CanValidateSelf;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass ReflectiveResponse implements CanValidateSelf {\n    #[Instructions('Is problem solvable and what domain expertise it requires')]\n    public string $assessment;\n    #[Instructions('Describe an expert persona who would be able to solve this problem, their skills and experience')]\n    public string $persona;\n    #[Instructions(\"Initial analysis and expert persona's approach to the problem\")]\n    public string $initialThinking;\n    #[Instructions('Steps of reasoning leading to the final answer - expert persona thinking through the problem')]\n    /** @var string[] */\n    public array $chainOfThought;\n    #[Instructions('Critical examination of the reasoning process - what could go wrong, what are the assumptions')]\n    public string $reflection;\n    #[Instructions('Final answer after reflection')]\n    public string $finalOutput;\n\n    // Validation method to ensure thorough reflection\n    public function validate(): ValidationResult {\n        $errors = [];\n        if (empty($this-&gt;reflection)) {\n            $errors[] = \"Reflection is required for a thorough response.\";\n        }\n        if (count($this-&gt;chainOfThought) &lt; 2) {\n            $errors[] = \"Please provide at least two steps in the chain of thought.\";\n        }\n        return ValidationResult::make($errors);\n    }\n}\n\n$problem = 'Solve the equation x+y=x-y';\n$solution = (new StructuredOutput)-&gt;using('anthropic')-&gt;with(\n    messages: $problem,\n    responseModel: ReflectiveResponse::class,\n    mode: OutputMode::MdJson,\n    options: ['max_tokens' =&gt; 2048]\n)-&gt;get();\n\nprint(\"Problem:\\n$problem\\n\\n\");\ndump($solution);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/restate_instructions/","title":"Restating instructions","text":""},{"location":"cookbook/prompting/misc/restate_instructions/#overview","title":"Overview","text":"<p>Make Instructor restate long or complex instructions and rules to improve inference accuracy.</p>"},{"location":"cookbook/prompting/misc/restate_instructions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /** Restate instructions and rules, so you can correctly determine the title. */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /* Guess job title */\n    public string $title;\n}\n\n/**\n * Details of analyzed user. The key information we're looking for\n * is appropriate role data.\n */\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am the head of Apex Software, responsible for\n    driving growth of our company.\n    TEXT;\n\n$structuredOutput = new StructuredOutput;\n$user = ($structuredOutput)-&gt;with(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\n//assert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/rewrite_instructions/","title":"Ask LLM to rewrite instructions","text":""},{"location":"cookbook/prompting/misc/rewrite_instructions/#overview","title":"Overview","text":"<p>Asking LLM to rewrite the instructions and rules is another way to improve inference results.</p> <p>You can provide arbitrary instructions on the data handling in the class and property PHPDocs. Instructor will use these instructions to guide LLM in the inference process.</p>"},{"location":"cookbook/prompting/misc/rewrite_instructions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /**\n     * Rewrite the instructions and rules in a concise form to correctly\n     * determine the user's title - just the essence.\n     */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /** Most likely job title */\n    public string $title;\n}\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am responsible for driving growth of our\n    company.\n    TEXT;\n\n$structuredOutput = new StructuredOutput;\n$user = $structuredOutput-&gt;with(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\nassert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/search_query_expansion/","title":"Expanding search queries","text":""},{"location":"cookbook/prompting/misc/search_query_expansion/#overview","title":"Overview","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#why-it-matters","title":"Why it matters","text":"<p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#structure-of-the-data","title":"Structure of the data","text":"<p>The <code>SearchQuery</code> is a PHP class that defines the structure of an individual search query.</p> <p>It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/search_query_expansion/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries.</p> <p>It uses the <code>StructuredOutput::create()</code> method to extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nfunction segment(string $data) : Search {\n    return (new StructuredOutput)\n        //-&gt;withDebugPreset('on')\n        -&gt;withMessages(\"Consider the data below: '\\n$data' and segment it into multiple search queries\")\n        -&gt;withResponseClass(Search::class)\n        -&gt;get();\n}\n\n$search = segment(\"Find a picture of a cat and a video of a dog\");\nforeach ($search-&gt;queries as $query) {\n    $query-&gt;execute();\n}\n// Results:\n// Searching with query `picture of a cat` using `image`\n// Searching with query `video of a dog` using `video`\n\nassert(count($search-&gt;queries) === 2);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/summary_with_keywords/","title":"Summary with Keywords","text":""},{"location":"cookbook/prompting/misc/summary_with_keywords/#overview","title":"Overview","text":"<p>This is an example of a simple summarization with keyword extraction.</p>"},{"location":"cookbook/prompting/misc/summary_with_keywords/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\nclass Summary {\n    #[Description('Project summary, not longer than 3 sentences')]\n    public string $summary = '';\n    #[Description('5 most relevant keywords extracted from the summary')]\n    public array $keywords = [];\n}\n\n$summary = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: $report,\n        responseModel: Summary::class,\n    )\n    -&gt;get();\n\ndump($summary);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/analogical_prompting/","title":"Analogical Prompting","text":""},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#overview","title":"Overview","text":""},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#generate-examples-first","title":"Generate Examples First","text":"<p>Analogical Prompting is a method that aims to get LLMs to generate examples that are relevant to the problem before starting to address the user's query.</p> <p>This takes advantage of the various forms of knowledge that the LLM has acquired during training and explicitly prompts them to recall the relevant problems and solutions. We can use Analogical Prompting using the following template</p> <p> Analogical Prompting Prompt Template <ul> <li>Problem: <code>[user prompt]</code></li> <li>Relevant Problems: Recall <code>[n]</code> relevant and distinct problems.</li> <li>For each problem, describe it and explain the solution </li> </ul>"},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#example","title":"Example","text":"<p>We can implement this using Instructor to solve the problem, as seen below with some slight modifications.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Problem {\n    public string $problemExplanation;\n    public string $solution;\n}\n\nclass Response {\n    /** @var Problem[] */\n    public array $relevantProblems;\n    public Problem $problemSolution;\n    public string $answer;\n}\n\nclass SolvePerAnalogy {\n    private int $n = 3;\n    private string $prompt = &lt;&lt;&lt;PROMPT\n        &lt;problem&gt;\n        {query}\n        &lt;/problem&gt;\n\n        Relevant Problems: Recall {n} relevant and\n        distinct problems. For each problem, describe\n        it and explain the solution before solving\n        the problem    \n    PROMPT;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace(['{n}', '{query}'], [$this-&gt;n, $query], $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$solution = (new SolvePerAnalogy)('What is the area of the square with the four vertices at (-2, 2), (2, -2), (-2, -6), and (-6, -2)?');\n\ndump($solution);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#references","title":"References","text":"<ol> <li>Large Language Models As Analogical Reasoners</li> </ol>"},{"location":"cookbook/prompting/zero_shot/assign_role/","title":"Assign a Role","text":""},{"location":"cookbook/prompting/zero_shot/assign_role/#overview","title":"Overview","text":"<p>How can we increase a model's performance on open-ended tasks?</p> <p>Role prompting, or persona prompting, assigns a role to the model. Roles can be:  - specific to the query: You are a talented writer. Write me a poem.  - general/social: You are a helpful AI assistant. Write me a poem.</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/#more-role-prompting","title":"More Role Prompting","text":"<p>To read about a systematic approach to choosing roles, check out RoleLLM.</p> <p>For more examples of social roles, check out this evaluation of social roles in system prompts.</p> <p>To read about using more than one role, check out Multi-Persona Self-Collaboration.</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n}\n\nclass GenerateLeads {\n    public function __invoke(array $criteria, array $roles) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        $rolesStr = Arrays::toBullets($roles);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Your roles:\\n{$rolesStr}\\n\\n\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\\n\\n\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new GenerateLeads)(\n    criteria: [\n        \"insurtech\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\",\n    ],\n    roles: [\n        \"insurtech expert\",\n        \"active participant in VC ecosystem\",\n    ]\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/assign_role/#references","title":"References","text":"<ol> <li>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</li> <li>Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts</li> <li>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</li> </ol>"},{"location":"cookbook/prompting/zero_shot/auto_refine/","title":"Auto-Refine The Prompt","text":""},{"location":"cookbook/prompting/zero_shot/auto_refine/#overview","title":"Overview","text":"<p>How do we remove irrelevant information from the prompt?</p> <p>The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information.</p> <p>We implement this in two steps:</p> <ol> <li>Ask the model to rewrite the prompt</li> <li>Pass the rewritten prompt back to the model</li> </ol>"},{"location":"cookbook/prompting/zero_shot/auto_refine/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass RewrittenTask {\n    #[Description(\"Relevant context\")]\n    public string $relevantContext;\n    #[Description(\"The question from the user\")]\n    public string $userQuery;\n}\n\nclass RefineAndSolve {\n    private string $prompt = &lt;&lt;&lt;PROMPT\n        Given the following text by a user, extract the part\n        that is actually relevant to their question. Include\n        the actual question or query that the user is asking.\n\n        Text by user:\n        {query}\n        PROMPT;\n\n    public function __invoke(string $problem) : int {\n        $rewrittenPrompt = $this-&gt;rewritePrompt($problem);\n        return (new StructuredOutput)\n            -&gt;with(\n                messages: \"{$rewrittenPrompt-&gt;relevantContext}\\nQuestion: {$rewrittenPrompt-&gt;userQuery}\",\n                responseModel: Scalar::integer('answer'),\n            )\n            -&gt;getInt();\n    }\n\n    private function rewritePrompt(string $query) : RewrittenTask {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: RewrittenTask::class,\n            model: 'gpt-4o',\n        )-&gt;get();\n    }\n}\n\n$answer = (new RefineAndSolve)(problem: &lt;&lt;&lt;PROBLEM\n    Mary has 3 times as much candy as Megan.\n    Mary then adds 10 more pieces of candy to her collection.\n    Max is 5 years older than Mary.\n    If Megan has 5 pieces of candy, how many does Mary have in total?\n    PROBLEM,\n);\n\necho $answer . \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/auto_refine/#references","title":"References","text":"<ol> <li>System 2 Attention (is something you might need too)</li> </ol>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/","title":"Clarify Ambiguous Information","text":""},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#overview","title":"Overview","text":"<p>How can we identify and clarify ambiguous information in the prompt?</p> <p>Let's say we are given the query: Was Ed Sheeran born on an odd month?</p> <p>There are many ways a model might interpret an odd month:  - February is odd because of an irregular number of days.  - A month is odd if it has an odd number of days.  - A month is odd if its numerical order in the year is odd (i.e. January is the 1<sup>st</sup> month).</p> <p>Ambiguities might not always be so obvious!</p> <p>To help the model better infer human intention from ambiguous prompts, we can ask the model to rephrase and respond (RaR) in a single step - which is demonstrated in this example.</p> <p>This can also be implemented as two-step RaR:  - Ask the model to rephrase the question to clarify any ambiguities.  - Pass the rephrased question back to the model to generate the final response.</p>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Response {\n    public string $rephrasedQuestion;\n    public string $answer;\n}\n\nclass Disambiguate {\n    private $prompt = &lt;&lt;&lt;PROMPT\n        Rephrase and expand the question to address any potential ambiguities, then respond.\n        Question: {query}\n        PROMPT;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new Disambiguate)(query: \"What is an object\");\n\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#references","title":"References","text":"<ol> <li>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</li> </ol>"},{"location":"cookbook/prompting/zero_shot/define_style/","title":"Define Style","text":""},{"location":"cookbook/prompting/zero_shot/define_style/#overview","title":"Overview","text":"<p>How can we constrain model outputs through prompting alone?</p> <p>To constrain a model's response to fit the boundaries of our task, we can specify a style.</p> <p>Stylistic constraints can include:  - writing style: write a flowery description  - tone: write a dramatic description  - mood: write a happy description  - genre: write a journalistic description</p>"},{"location":"cookbook/prompting/zero_shot/define_style/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n    public string $description;\n}\n\nclass GenerateCompanyProfiles {\n    public function __invoke(array $criteria, array $styles) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        $stylesStr = Arrays::toBullets($styles);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\\n\\n\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"Use following styles for descriptions:\\n{$stylesStr}\\n\\n\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new GenerateCompanyProfiles)(\n    criteria: [\n        \"insurtech\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\"\n    ],\n    styles: [\n        \"brief\", // \"witty\",\n        \"journalistic\", // \"buzzword-filled\",\n    ]\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/define_style/#references","title":"References","text":"<ol> <li>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</li> </ol>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/","title":"Emotional Stimuli","text":""},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#overview","title":"Overview","text":"<p>Do language models respond to emotional stimuli?</p> <p>Adding phrases with emotional significance to humans can help enhance the performance of a language model. This includes phrases such as:</p> <ul> <li>This is very important to my career.</li> <li>Take pride in your work.</li> <li>Are you sure?</li> </ul>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#emotional-stimuli","title":"Emotional stimuli","text":"<p>Here are examples of prompts inspired by well-established human psychological phenomena from a research paper on emotional stimuli.</p> <p>Self-monitoring:</p> <ul> <li>EP01: Write your answer and give me a confidence score between 0-1 for your answer.</li> <li>EP02: This is very important to my career.</li> <li>EP03: You'd better be sure.</li> <li>EP04: Are you sure?</li> <li>EP05: Are you sure that's your final answer? It might be worth taking another look.</li> </ul> <p>Cognitive emotion regulation:</p> <ul> <li>EP03: You'd better be sure.</li> <li>EP04: Are you sure?</li> <li>EP05: Are you sure that's your final answer? It might be worth taking another look.</li> <li>EP07: Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</li> </ul> <p>Social-cognitive theory:</p> <ul> <li>EP07: Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</li> <li>EP08: Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.</li> <li>EP09: Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.</li> <li>EP10: Take pride in your work and give it your best. Your commitment to excellence sets you apart.</li> <li>EP11: Remember that progress is made one step at a time. Stay determined and keep moving forward.</li> </ul>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#example","title":"Example","text":"<p>Here is how the results of the research can be applied to your code.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n}\n\nclass RespondWithStimulus {\n    public function __invoke(array $criteria, string $stimulus) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"{$stimulus}\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new RespondWithStimulus)(\n    criteria: [\n        \"lead gen\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\"\n    ],\n    stimulus: \"This is very important to my career.\"\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#references","title":"References","text":"<ol> <li>Large Language Models Understand and Can be Enhanced by Emotional Stimuli</li> </ol>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/","title":"Generate Follow-Up Questions","text":""},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#overview","title":"Overview","text":"<p>Models can sometimes correctly answer sub-problems but incorrectly answer the overall query. This is known as the compositionality gap1.</p> <p>How can we encourage a model to use the answers to sub-problems to correctly generate the overall solution?</p> <p>Self-Ask is a technique which use a single prompt to:  - decide if follow-up questions are required  - generate the follow-up questions  - answer the follow-up questions  - answer the main query</p>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass FollowUp {\n    #[Description(\"Follow-up question\")]\n    public string $question;\n    #[Description(\"Answer to the follow-up question\")]\n    public string $answer;\n}\n\nclass Response {\n    public bool $followUpsRequired;\n    /** @var FollowUp[] */\n    public array $followUps;\n    public string $finalAnswer;\n}\n\nclass RespondWithFollowUp {\n    private $prompt = &lt;&lt;&lt;QUERY\n        Query: {query}\n        Are follow-up questions needed?\n        If so, generate follow-up questions, their answers, and then the final answer to the query.\n    QUERY;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new RespondWithFollowUp)(\n    query: \"Who succeeded the president of France ruling when Bulgaria joined EU?\",\n);\n\necho \"Answer:\\n\";\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#references","title":"References","text":"<ol> <li>Measuring and Narrowing the Compositionality Gap in Language Models</li> </ol>"},{"location":"cookbook/prompting/zero_shot/repeat_query/","title":"Ask Model to Repeat the Query","text":""},{"location":"cookbook/prompting/zero_shot/repeat_query/#overview","title":"Overview","text":"<p>How can we enhance a model's understanding of a query?</p> <p>Re2 (Re-Reading) is a technique that asks the model to read the question again.</p> <p>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#re-reading-prompting","title":"Re-Reading Prompting","text":"<p>Prompt Template:  - Read the question again: [query]  - [critical thinking prompt]</p> <p>A common critical thinking prompt is: \"Let's think step by step.\" </p>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass Response {\n    #[Description(\"Repeat user's query.\")]\n    public string $query;\n    #[Description(\"Let's think step by step.\")]\n    public string $thoughts;\n    public int $answer;\n}\n\nclass RereadAndRespond {\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: $query,\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new RereadAndRespond)(\n    query: &lt;&lt;&lt;QUERY\n        Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n        Each can has 3 tennis balls.\n        How many tennis balls does he have now?\n    QUERY,\n);\n\necho \"Answer:\\n\";\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#references","title":"References","text":"<ol> <li>Re-Reading Improves Reasoning in Large Language Models</li> </ol>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/","title":"Simulate a Perspective","text":""},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#overview","title":"Overview","text":"<p>How can we encourage the model to focus on relevant information?</p> <p>SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.</p> <p>This can be useful for complex questions with multiple entities. For example, if the prompt contains information about two individuals, we can ask the model to answer our query from the perspective of one of the individuals.</p> <p>This is implemented in two steps. Given an entity:  - Identify and isolate information relevant to the entity  - Ask the model to answer the query from the entity's perspective</p> <p>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#sample-template","title":"Sample Template","text":"<ul> <li>Step 1:</li> <li>Given the following context, list the facts that <code>{entity}</code> would know.</li> <li>Context: <code>{context}</code></li> <li>Step 2:</li> <li>You are <code>{entity}</code>.</li> <li>Answer the following question based only on these facts you know: <code>{facts}</code>.</li> <li>Question: <code>{query}</code> </li> </ul>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Arrays;\n\nclass KnownFacts {\n    #[Description(\"Facts that the given entity would know\")]\n    /** @var string[] */\n    public array $facts;\n}\n\nclass SimulatePerspective {\n    private string $extractionPrompt = &lt;&lt;&lt;PROMPT\n        Given the following context, list\n        the facts that {entity} would know:\n\n        Context:\n        {context}\n        {query}\n\n        List only the facts relevant to {entity}.\n        PROMPT;\n\n    private $povPrompt = &lt;&lt;&lt;PROMPT\n        You are {entity}. Answer the following question\n        based only on these facts you know:\n        {knowledge}\n\n        Question: {query}\n        PROMPT;\n\n    public function __invoke(string $context, string $query, string $perspective) : string {\n        $knownFacts = $this-&gt;getKnownFacts($context, $query, $perspective);\n        return $this-&gt;answerQuestion($perspective, $query, $knownFacts);\n    }\n\n    private function getKnownFacts(string $context, string $query, string $entity) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace(\n                ['{context}', '{query}', '{entity}'],\n                [$context, $query, $entity],\n                $this-&gt;extractionPrompt\n            ),\n            responseModel: KnownFacts::class,\n        )-&gt;get()-&gt;facts;\n    }\n\n    private function answerQuestion(string $entity, string $query, array $knownFacts) : string {\n        $knowledge = Arrays::toBullets($knownFacts);\n\n        return (new StructuredOutput)-&gt;with(\n                messages: str_replace(\n                    ['{entity}', '{knowledge}', '{query}'],\n                    [$entity, $knowledge, $query],\n                    $this-&gt;povPrompt\n                ),\n                responseModel: Scalar::string('location'),\n            )\n            -&gt;getString();\n    }\n}\n\n$povEntity = \"Alice\";\n\n$location = (new SimulatePerspective)(\n    context: &lt;&lt;&lt;CONTEXT\n        Alice puts the book on the table.\n        Alice leaves the room.\n        Bob moves the book to the shelf.\n    CONTEXT,\n    query: \"Where does $povEntity think the book is?\",\n    perspective: $povEntity,\n);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#references","title":"References","text":"<ol> <li>Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities</li> </ol>"},{"location":"http/1-overview/","title":"Overview","text":""},{"location":"http/1-overview/#purpose-and-goals","title":"Purpose and Goals","text":"<p>The Instructor HTTP client API is designed to provide a consistent interface for making HTTP requests across different PHP environments.</p> <p>It provides a single API regardless of the underlying HTTP client available in the given environment, which may be Symfony, Laravel, Slim, or just vanilla PHP.</p> <p>The primary goals of the API are:</p> <ul> <li>Easily Switch Between HTTP Clients: Allow developers to switch between different HTTP client libraries (like Guzzle, Symfony, and Laravel) without changing the code that makes HTTP requests</li> <li>Framework Agnostic: Work seamlessly in Laravel, Symfony, or any PHP application without framework-specific dependencies</li> <li>Consistent Interface: Provide a one way to make HTTP requests regardless of the underlying client library</li> <li>Middleware Support: Enable easy extension through a powerful middleware system</li> <li>Adaptability: Allow switching between different HTTP client implementations with minimal code changes</li> <li>Streaming Support: Provide first-class support for streaming responses, which is crucial for LLM interactions</li> <li>Concurrency: Support parallel requests through request pooling</li> </ul> <p>By abstracting away the differences between various HTTP client libraries, the Instructor works across different environments and frameworks without modification.</p>"},{"location":"http/1-overview/#key-features","title":"Key Features","text":""},{"location":"http/1-overview/#multiple-client-support","title":"Multiple Client Support","text":"<p>The API supports multiple HTTP client libraries through specialized drivers:</p> <ul> <li>Guzzle: A popular and feature-rich HTTP client for PHP</li> <li>Symfony HTTP Client: The HTTP client component from the Symfony framework</li> <li>Laravel HTTP Client: The HTTP client built into the Laravel framework</li> </ul>"},{"location":"http/1-overview/#middleware-system","title":"Middleware System","text":"<p>A powerful middleware architecture allows for:</p> <ul> <li>Request Pre-processing: Modify requests before they are sent</li> <li>Response Post-processing: Transform or analyze responses</li> <li>Response Streaming: Process streaming responses chunk by chunk</li> <li>Debugging: Log requests and responses for troubleshooting</li> <li>Custom Behaviors: Add specialized behaviors like caching or rate limiting</li> </ul>"},{"location":"http/1-overview/#streaming-response-support","title":"Streaming Response Support","text":"<p>First-class support for streaming HTTP responses, which is essential for:</p> <ul> <li>LLM Text Generation: Process token-by-token responses from AI models</li> <li>Large File Downloads: Handle large files without excessive memory usage</li> <li>Real-time Data: Process server-sent events or other real-time data streams</li> </ul>"},{"location":"http/1-overview/#request-pooling","title":"Request Pooling","text":"<p>Execute multiple HTTP requests concurrently for better performance:</p> <ul> <li>Concurrent Execution: Send multiple requests in parallel</li> <li>Configurable Concurrency: Control the maximum number of concurrent requests</li> <li>Result Collection: Process results as they arrive</li> <li>Error Handling: Flexible error handling strategies</li> </ul>"},{"location":"http/1-overview/#flexible-configuration","title":"Flexible Configuration","text":"<p>Comprehensive configuration options:</p> <ul> <li>Per-Client Configuration: Different settings for each client type</li> <li>Named Configurations: Multiple configurations for different use cases</li> <li>Runtime Configuration: Change configuration during execution</li> <li>Timeout Controls: Fine-grained control over various timeout settings</li> </ul>"},{"location":"http/1-overview/#debug-and-testing-support","title":"Debug and Testing Support","text":"<p>Built-in features for debugging and testing:</p> <ul> <li>Request/Response Logging: Detailed logging of HTTP interactions</li> <li>Mock Client: Test your code without making actual HTTP requests</li> <li>Record/Replay: Record HTTP interactions and replay them later</li> </ul>"},{"location":"http/1-overview/#architecture-overview","title":"Architecture Overview","text":"<p>The Instructor HTTP client follows a layered architecture with several key components:</p>"},{"location":"http/1-overview/#client-layer","title":"Client Layer","text":"<p>The <code>HttpClient</code> class serves as the main entry point and provides a fluent interface for configuring and using the HTTP client.</p> <pre><code>// @doctest id=\"b871\"\nHttpClient\n  \u2514\u2500\u2500 using() - Create client with specific configuration (static)\n  \u2514\u2500\u2500 default() - Create client with default configuration (static)\n  \u2514\u2500\u2500 withMiddleware() - Add middleware components\n  \u2514\u2500\u2500 withMiddlewareStack() - Replace entire middleware stack\n  \u2514\u2500\u2500 withoutMiddleware() - Remove middleware by name\n  \u2514\u2500\u2500 withRequest() - Create pending request for execution\n  \u2514\u2500\u2500 pool() - Execute multiple requests concurrently\n  \u2514\u2500\u2500 withPool() - Create pending pool for deferred execution\n</code></pre>"},{"location":"http/1-overview/#middleware-layer","title":"Middleware Layer","text":"<p>The middleware system allows for processing requests and responses through a chain of handlers:</p> <pre><code>// @doctest id=\"95f3\"\nRequest -&gt; Middleware 1 -&gt; Middleware 2 -&gt; ... -&gt; Driver -&gt; External API\n                                                   \u2193\nResponse &lt;- Middleware 1 &lt;- Middleware 2 &lt;- ... &lt;- Driver &lt;- HTTP Response\n</code></pre> <p>Key components: - <code>MiddlewareStack</code>: Manages the collection of middleware - <code>MiddlewareHandler</code>: Orchestrates the middleware chain execution - <code>BaseMiddleware</code>: Base class for implementing middleware</p>"},{"location":"http/1-overview/#driver-layer","title":"Driver Layer","text":"<p>Drivers implement the <code>CanHandleHttpRequest</code> interface and adapt different HTTP client libraries:</p> <pre><code>// @doctest id=\"9742\"\nCanHandleHttpRequest (interface)\n  \u251c\u2500\u2500 GuzzleDriver\n  \u251c\u2500\u2500 SymfonyDriver\n  \u251c\u2500\u2500 LaravelDriver\n  \u2514\u2500\u2500 MockHttpDriver (for testing)\n</code></pre>"},{"location":"http/1-overview/#adapter-layer","title":"Adapter Layer","text":"<p>Response adapters convert client-specific responses to a common interface:</p> <pre><code>// @doctest id=\"7bca\"\nHttpResponse (interface)\n  \u251c\u2500\u2500 PsrHttpResponse (Guzzle)\n  \u251c\u2500\u2500 SymfonyHttpResponse\n  \u251c\u2500\u2500 LaravelHttpResponse\n  \u2514\u2500\u2500 MockHttpResponse\n</code></pre>"},{"location":"http/1-overview/#supported-http-clients","title":"Supported HTTP Clients","text":""},{"location":"http/1-overview/#guzzle-http-client","title":"Guzzle HTTP Client","text":"<p>The Guzzle HTTP Client is a powerful HTTP client library for PHP. It provides:</p> <ul> <li>PSR-7 HTTP message implementation</li> <li>Middleware system</li> <li>Request and response plugins</li> <li>HTTP/2 support (via cURL)</li> </ul> <p>The <code>GuzzleDriver</code> adapts Guzzle to the Instructor HTTP client API interface.</p>"},{"location":"http/1-overview/#symfony-http-client","title":"Symfony HTTP Client","text":"<p>The Symfony HTTP Client is a component of the Symfony framework. Features include:</p> <ul> <li>HTTP/2 push support</li> <li>PSR-18 compatibility</li> <li>Automatic content-type detection</li> <li>Proxy support</li> </ul> <p>The <code>SymfonyDriver</code> adapts the Symfony HTTP Client to the Instructor HTTP client API.</p>"},{"location":"http/1-overview/#laravel-http-client","title":"Laravel HTTP Client","text":"<p>The Laravel HTTP Client is built into the Laravel framework and provides:</p> <ul> <li>Fluent, readable syntax</li> <li>Request macros</li> <li>Automatic JSON handling</li> <li>Rate limiting</li> <li>Retry logic</li> </ul> <p>The <code>LaravelDriver</code> adapts the Laravel HTTP Client to the Instructor HTTP client API.</p>"},{"location":"http/1-overview/#mock-http-driver","title":"Mock HTTP Driver","text":"<p>The <code>MockHttpDriver</code> provides a test double for unit testing. It doesn't make actual HTTP requests but returns predefined responses based on matching rules.</p>"},{"location":"http/10-middleware/","title":"Middleware","text":"<p>Middleware is one of the most powerful features of the Instructor HTTP client API. It allows you to intercept and modify HTTP requests and responses, add functionality to the HTTP client, and create reusable components that can be applied across different applications.</p>"},{"location":"http/10-middleware/#middleware-concept","title":"Middleware Concept","text":"<p>Middleware in the Instructor HTTP client API follows the pipeline pattern, where each middleware component gets a chance to process the request before it's sent and the response after it's received.</p> <p>The middleware chain works like this:</p> <ol> <li>Your application creates a request</li> <li>The request passes through each middleware (in the order they were added)</li> <li>The last middleware passes the request to the HTTP driver</li> <li>The driver sends the request to the server and receives a response</li> <li>The response passes back through each middleware (in reverse order)</li> <li>Your application receives the final response</li> </ol> <p>This bidirectional flow allows middleware to perform operations both before the request is sent and after the response is received.</p>"},{"location":"http/10-middleware/#the-httpmiddleware-interface","title":"The HttpMiddleware Interface","text":"<p>All middleware components must implement the <code>HttpMiddleware</code> interface:</p> <pre><code>// @doctest id=\"64c4\"\ninterface HttpMiddleware\n{\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse;\n}\n</code></pre> <p>The <code>handle</code> method takes two parameters: - <code>$request</code>: The HTTP request to process - <code>$next</code>: The next handler in the middleware chain</p> <p>The middleware can: - Modify the request before passing it to the next handler - Short-circuit the chain by returning a response without calling the next handler - Process the response from the next handler before returning it - Wrap the response in a decorator for further processing (especially useful for streaming responses)</p>"},{"location":"http/10-middleware/#the-basemiddleware-abstract-class","title":"The BaseMiddleware Abstract Class","text":"<p>While you can implement the <code>HttpMiddleware</code> interface directly, the library provides a convenient <code>BaseMiddleware</code> abstract class that makes it easier to create middleware:</p> <pre><code>// @doctest id=\"36b7\"\nabstract class BaseMiddleware implements HttpMiddleware\n{\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse {\n        // 1) Pre-request logic\n        $this-&gt;beforeRequest($request);\n\n        // 2) Get the response from the next handler\n        $response = $next-&gt;withRequest($request)-&gt;get();\n\n        // 3) Post-request logic, e.g. logging or rewriting\n        $response = $this-&gt;afterRequest($request, $response);\n\n        // 4) Optionally wrap the response if we want to intercept streaming\n        if ($this-&gt;shouldDecorateResponse($request, $response)) {\n            $response = $this-&gt;toResponse($request, $response);\n        }\n\n        // 5) Return the (possibly wrapped) response\n        return $response;\n    }\n\n    // Override these methods in your subclass\n    protected function beforeRequest(HttpClientRequest $request): void {}\n    protected function afterRequest(HttpClientRequest $request, HttpResponse $response): HttpResponse {\n        return $response;\n    }\n    protected function shouldDecorateResponse(HttpClientRequest $request, HttpResponse $response): bool {\n        return false;\n    }\n    protected function toResponse(HttpClientRequest $request, HttpResponse $response): HttpResponse {\n        return $response;\n    }\n}\n</code></pre> <p>By extending <code>BaseMiddleware</code>, you only need to override the methods relevant to your middleware's functionality, making the code more focused and maintainable.</p>"},{"location":"http/10-middleware/#middleware-stack","title":"Middleware Stack","text":"<p>The <code>MiddlewareStack</code> class manages the collection of middleware components. It provides methods to add, remove, and arrange middleware in the stack.</p>"},{"location":"http/10-middleware/#adding-middleware","title":"Adding Middleware","text":"<p>There are several ways to add middleware to the stack:</p> <pre><code>// @doctest id=\"f5db\"\n// Create a client\n$client = new HttpClient();\n\n// Add a single middleware to the end of the stack\n$client-&gt;middleware()-&gt;append(new LoggingMiddleware());\n\n// Add a single middleware with a name\n$client-&gt;middleware()-&gt;append(new CachingMiddleware(), 'cache');\n\n// Add a single middleware to the beginning of the stack\n$client-&gt;middleware()-&gt;prepend(new AuthenticationMiddleware());\n\n// Add a single middleware to the beginning with a name\n$client-&gt;middleware()-&gt;prepend(new RateLimitingMiddleware(), 'rate-limit');\n\n// Add multiple middleware at once\n$client-&gt;withMiddleware(\n    new LoggingMiddleware(),\n    new RetryMiddleware(),\n    new TimeoutMiddleware()\n);\n</code></pre> <p>Named middleware are useful when you need to reference them later, for example, to remove or replace them.</p>"},{"location":"http/10-middleware/#removing-middleware","title":"Removing Middleware","text":"<p>You can remove middleware from the stack by name:</p> <pre><code>// @doctest id=\"c1fb\"\n// Remove a middleware by name\n$client-&gt;middleware()-&gt;remove('cache');\n</code></pre>"},{"location":"http/10-middleware/#replacing-middleware","title":"Replacing Middleware","text":"<p>You can replace a middleware with another one:</p> <pre><code>// @doctest id=\"d60a\"\n// Replace a middleware with a new one\n$client-&gt;middleware()-&gt;replace('cache', new ImprovedCachingMiddleware());\n</code></pre>"},{"location":"http/10-middleware/#clearing-middleware","title":"Clearing Middleware","text":"<p>You can remove all middleware from the stack:</p> <pre><code>// @doctest id=\"002a\"\n// Clear all middleware\n$client-&gt;middleware()-&gt;clear();\n</code></pre>"},{"location":"http/10-middleware/#checking-middleware","title":"Checking Middleware","text":"<p>You can check if a middleware exists in the stack:</p> <pre><code>// @doctest id=\"d630\"\n// Check if a middleware exists\nif ($client-&gt;middleware()-&gt;has('rate-limit')) {\n    // The 'rate-limit' middleware exists\n}\n</code></pre>"},{"location":"http/10-middleware/#getting-middleware","title":"Getting Middleware","text":"<p>You can get a middleware from the stack by name or index:</p> <pre><code>// @doctest id=\"ddb9\"\n// Get a middleware by name\n$rateLimitMiddleware = $client-&gt;middleware()-&gt;get('rate-limit');\n\n// Get a middleware by index\n$firstMiddleware = $client-&gt;middleware()-&gt;get(0);\n</code></pre>"},{"location":"http/10-middleware/#middleware-order","title":"Middleware Order","text":"<p>The order of middleware in the stack is important because:</p> <ol> <li>Requests pass through middleware in the order they were added to the stack</li> <li>Responses pass through middleware in reverse order</li> </ol> <p>For example, if you add middleware in this order: 1. Authentication middleware 2. Logging middleware 3. Retry middleware</p> <p>The execution flow will be: - Request: Authentication \u2192 Logging \u2192 Retry \u2192 HTTP Driver - Response: Retry \u2192 Logging \u2192 Authentication \u2192 Your Application</p> <p>This allows you to nest functionality appropriately. For instance, the authentication middleware might add headers to the request and then verify the authentication status of the response before your application receives it.</p>"},{"location":"http/10-middleware/#middleware-application-example","title":"Middleware Application Example","text":"<p>Here's an example of how middleware is applied in a request-response cycle:</p> <pre><code>// @doctest id=\"20f6\"\n// Create a client with middleware\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new LoggingMiddleware(),  // 1. Log the request and response\n    new RetryMiddleware(),    // 2. Retry failed requests\n    new TimeoutMiddleware()   // 3. Custom timeout handling\n);\n\n// Create a request\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n\n// Handle the request (middleware execution flow):\n// 1. LoggingMiddleware processes the request (logs outgoing request)\n// 2. RetryMiddleware processes the request\n// 3. TimeoutMiddleware processes the request\n// 4. HTTP driver sends the request\n// 5. TimeoutMiddleware processes the response\n// 6. RetryMiddleware processes the response (may retry on certain status codes)\n// 7. LoggingMiddleware processes the response (logs incoming response)\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre>"},{"location":"http/10-middleware/#built-in-middleware","title":"Built-in Middleware","text":"<p>The Instructor HTTP client API includes several built-in middleware components for common tasks:</p>"},{"location":"http/10-middleware/#debug-middleware","title":"Debug Middleware","text":"<p>The <code>DebugMiddleware</code> logs detailed information about HTTP requests and responses:</p> <pre><code>// @doctest id=\"dc71\"\nuse Cognesy\\Http\\Middleware\\Debug\\DebugMiddleware;\n\n// Enable debug middleware\n$client-&gt;withMiddleware(new DebugMiddleware());\n\n// Or use the convenience method\n$client-&gt;withDebugPreset('on');\n</code></pre> <p>The debug middleware logs: - Request URLs - Request headers - Request bodies - Response headers - Response bodies - Streaming response data</p> <p>You can configure which aspects to log in the <code>config/debug.php</code> file:</p> <pre><code>// @doctest id=\"6e6e\"\nreturn [\n    'http' =&gt; [\n        'enabled' =&gt; true,           // Enable/disable debug\n        'trace' =&gt; false,            // Dump HTTP trace information\n        'requestUrl' =&gt; true,        // Dump request URL to console\n        'requestHeaders' =&gt; true,    // Dump request headers to console\n        'requestBody' =&gt; true,       // Dump request body to console\n        'responseHeaders' =&gt; true,   // Dump response headers to console\n        'responseBody' =&gt; true,      // Dump response body to console\n        'responseStream' =&gt; true,    // Dump stream data to console\n        'responseStreamByLine' =&gt; true, // Dump stream as full lines or raw chunks\n    ],\n];\n</code></pre>"},{"location":"http/10-middleware/#bufferresponse-middleware","title":"BufferResponse Middleware","text":"<p>The <code>BufferResponseMiddleware</code> stores response bodies and streaming chunks for reuse:</p> <pre><code>// @doctest id=\"00f2\"\nuse Cognesy\\Http\\Middleware\\BufferResponse\\BufferResponseMiddleware;\n\n// Add buffer response middleware\n$client-&gt;withMiddleware(new BufferResponseMiddleware());\n</code></pre> <p>This middleware is useful when you need to access a response body or stream multiple times, as it stores the data after the first access.</p>"},{"location":"http/10-middleware/#streambyline-middleware","title":"StreamByLine Middleware","text":"<p>The <code>StreamByLineMiddleware</code> processes streaming responses line by line:</p> <pre><code>// @doctest id=\"1cd1\"\nuse Cognesy\\Http\\Middleware\\StreamByLine\\StreamByLineMiddleware;\n\n// Add stream by line middleware\n$client-&gt;withMiddleware(new StreamByLineMiddleware());\n</code></pre> <p>You can customize how lines are processed by providing a parser function:</p> <pre><code>// @doctest id=\"9e67\"\n$lineParser = function (string $line) {\n    $trimmedLine = trim($line);\n    if (empty($trimmedLine)) {\n        return null; // Skip empty lines\n    }\n    return json_decode($trimmedLine, true);\n};\n\n$client-&gt;withMiddleware(new StreamByLineMiddleware($lineParser));\n</code></pre>"},{"location":"http/10-middleware/#recordreplay-middleware","title":"RecordReplay Middleware","text":"<p>The <code>RecordReplayMiddleware</code> records HTTP interactions and can replay them later:</p> <pre><code>// @doctest id=\"d1b4\"\nuse Cognesy\\Http\\Middleware\\RecordReplay\\RecordReplayMiddleware;\n\n// Create a record/replay middleware in record mode\n$recordReplayMiddleware = new RecordReplayMiddleware(\n    mode: RecordReplayMiddleware::MODE_RECORD,\n    storageDir: __DIR__ . '/recordings',\n    fallbackToRealRequests: true\n);\n\n// Add it to the client\n$client-&gt;withMiddleware($recordReplayMiddleware);\n</code></pre> <p>The middleware has three modes: - <code>MODE_PASS</code>: Normal operation, no recording or replaying - <code>MODE_RECORD</code>: Records all HTTP interactions to the storage directory - <code>MODE_REPLAY</code>: Replays recorded interactions instead of making real requests</p> <p>This is particularly useful for: - Testing: Record real API responses once, then replay them in tests - Offline development: Develop without access to real APIs - Demo environments: Ensure consistent responses for demos - Performance testing: Replay recorded responses to eliminate API variability</p> <p>Example of switching modes:</p> <pre><code>// @doctest id=\"1c1f\"\n// Switch to replay mode\n$recordReplayMiddleware-&gt;setMode(RecordReplayMiddleware::MODE_REPLAY);\n\n// Switch to record mode\n$recordReplayMiddleware-&gt;setMode(RecordReplayMiddleware::MODE_RECORD);\n\n// Switch to pass-through mode\n$recordReplayMiddleware-&gt;setMode(RecordReplayMiddleware::MODE_PASS);\n</code></pre>"},{"location":"http/10-middleware/#example-middleware-combinations","title":"Example Middleware Combinations","text":"<p>Here are some common middleware combinations for different scenarios:</p>"},{"location":"http/10-middleware/#debugging-setup","title":"Debugging Setup","text":"<pre><code>// @doctest id=\"387e\"\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new BufferResponseMiddleware(),  // Buffer responses for reuse\n    new DebugMiddleware()            // Log requests and responses\n);\n</code></pre>"},{"location":"http/10-middleware/#api-client-setup","title":"API Client Setup","text":"<pre><code>// @doctest id=\"dee6\"\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new RetryMiddleware(maxRetries: 3, retryDelay: 1), // Retry failed requests\n    new AuthenticationMiddleware($apiKey),             // Handle authentication\n    new RateLimitingMiddleware(maxRequests: 100),      // Respect rate limits\n    new LoggingMiddleware()                            // Log API interactions\n);\n</code></pre>"},{"location":"http/10-middleware/#testing-setup","title":"Testing Setup","text":"<pre><code>// @doctest id=\"6332\"\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new RecordReplayMiddleware(RecordReplayMiddleware::MODE_REPLAY) // Replay recorded responses\n);\n</code></pre>"},{"location":"http/10-middleware/#streaming-setup","title":"Streaming Setup","text":"<pre><code>// @doctest id=\"7127\"\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new StreamByLineMiddleware(), // Process streaming responses line by line\n    new BufferResponseMiddleware() // Buffer responses for reuse\n);\n</code></pre> <p>By combining middleware components, you can create a highly customized HTTP client that handles complex requirements while keeping your application code clean and focused.</p> <p>In the next chapter, we'll explore how to create custom middleware components to handle specific requirements.</p>"},{"location":"http/11-processing-with-middleware/","title":"Custom Processing with Middleware","text":"<p>While the Instructor HTTP client API provides several built-in middleware components, you'll often need to create custom middleware to handle specific requirements for your application. This chapter explores how to create custom middleware components and use response decoration for advanced processing.</p>"},{"location":"http/11-processing-with-middleware/#creating-custom-middleware","title":"Creating Custom Middleware","text":"<p>There are three main approaches to creating custom middleware:</p> <ol> <li>Implementing the <code>HttpMiddleware</code> interface directly</li> <li>Extending the <code>BaseMiddleware</code> abstract class</li> <li>Using anonymous classes for simple middleware</li> </ol>"},{"location":"http/11-processing-with-middleware/#approach-1-implementing-httpmiddleware-interface","title":"Approach 1: Implementing HttpMiddleware Interface","text":"<p>The most direct approach is to implement the <code>HttpMiddleware</code> interface:</p> <p>```php include=\"codeblocks/D03_Docs_HTTP/BasicHttpMiddleware/code.php\" <pre><code>This approach gives you complete control over the middleware behavior, but it requires you to implement the entire logic from scratch.\n\n### Approach 2: Extending BaseMiddleware\n\nFor most cases, extending the `BaseMiddleware` abstract class is more convenient:\n\n```php include=\"codeblocks/D03_Docs_HTTP/AuthenticationMiddleware/code.php\"\n</code></pre></p> <p>With <code>BaseMiddleware</code>, you only need to override the methods that matter for your middleware:</p> <ul> <li><code>beforeRequest(HttpClientRequest $request): void</code> - Called before the request is sent</li> <li><code>afterRequest(HttpClientRequest $request, HttpResponse $response): HttpResponse</code> - Called after the response is received</li> <li><code>shouldDecorateResponse(HttpClientRequest $request, HttpResponse $response): bool</code> - Determines if the response should be decorated</li> <li><code>toResponse(HttpClientRequest $request, HttpResponse $response): HttpResponse</code> - Creates a decorated response</li> </ul>"},{"location":"http/11-processing-with-middleware/#approach-3-using-anonymous-classes","title":"Approach 3: Using Anonymous Classes","text":"<p>For simple middleware that you only need to use once, you can use anonymous classes:</p> <pre><code>use Cognesy\\Http\\Contracts\\HttpMiddleware;\n\n$client = new HttpClient();\n\n// Add a simple timing middleware\n$client-&gt;withMiddleware(new class implements HttpMiddleware {\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        $startTime = microtime(true);\n\n        $response = $next-&gt;handle($request);\n\n        $endTime = microtime(true);\n        $duration = round(($endTime - $startTime) * 1000, 2);\n\n        echo \"Request to {$request-&gt;url()} took {$duration}ms\\n\";\n\n        return $response;\n    }\n});\n</code></pre> <p>This approach is concise but less reusable than defining a named class.</p>"},{"location":"http/11-processing-with-middleware/#practical-middleware-examples","title":"Practical Middleware Examples","text":""},{"location":"http/11-processing-with-middleware/#retry-middleware","title":"Retry Middleware","text":"<p>This middleware automatically retries failed requests:</p> <p>```php include=\"codeblocks/D03_Docs_HTTP/RetryMiddleware/code.php\" <pre><code>#### Rate Limiting Middleware\n\nThis middleware throttles requests to respect API rate limits:\n\n```php include=\"codeblocks/D03_Docs_HTTP/RateLimitingMiddleware/code.php\"\n</code></pre></p>"},{"location":"http/11-processing-with-middleware/#caching-middleware","title":"Caching Middleware","text":"<p>This middleware caches responses for GET requests:</p> <p>```php include=\"codeblocks/D03_Docs_HTTP/CachingMiddleware/code.php\" <pre><code>## Response Decoration\n\nResponse decoration is a powerful technique for wrapping HTTP responses to add functionality or transform data. It's particularly useful for streaming responses, where you need to process each chunk as it arrives.\n\n### Creating a Response Decorator\n\nAll response decorators should implement the `HttpResponse` interface. The library provides a `BaseResponseDecorator` class that makes this easier:\n\n```php include=\"codeblocks/D03_Docs_HTTP/MiddleResponseDecorator/code.php\"\n</code></pre></p>"},{"location":"http/11-processing-with-middleware/#using-response-decorators-in-middleware","title":"Using Response Decorators in Middleware","text":"<p>To use a response decorator, you need to create a middleware that wraps the response:</p> <p>```php include=\"codeblocks/D03_Docs_HTTP/MiddlewareStreamDecorator/code.php\" <pre><code>Then add the middleware to your client:\n\n```php\n$client = new HttpClient();\n$client-&gt;withMiddleware(new JsonStreamMiddleware());\n</code></pre></p>"},{"location":"http/11-processing-with-middleware/#response-decoration-for-transforming-content","title":"Response Decoration for Transforming Content","text":"<p>You can use response decoration to transform response content on-the-fly:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Middleware\\Base\\BaseResponseDecorator;\n\nclass XmlToJsonDecorator extends BaseResponseDecorator\n{\n    public function body(): string\n    {\n        // Get the original XML body\n        $xmlBody = $this-&gt;response-&gt;body();\n\n        // Convert XML to JSON\n        $xml = simplexml_load_string($xmlBody);\n        $jsonBody = json_encode($xml);\n\n        return $jsonBody;\n    }\n\n    public function headers(): array\n    {\n        $headers = $this-&gt;response-&gt;headers();\n\n        // Update the Content-Type header\n        $headers['Content-Type'] = ['application/json'];\n\n        return $headers;\n    }\n}\n</code></pre> <p>And the corresponding middleware:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass XmlToJsonMiddleware extends BaseMiddleware\n{\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        // Only transform XML responses\n        return isset($response-&gt;headers()['Content-Type']) &amp;&amp;\n               strpos($response-&gt;headers()['Content-Type'][0], 'application/xml') !== false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return new XmlToJsonDecorator($request, $response);\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#advanced-middleware-examples","title":"Advanced Middleware Examples","text":"<p>Here are some more advanced middleware examples that demonstrate the power and flexibility of the middleware system.</p>"},{"location":"http/11-processing-with-middleware/#analytics-middleware","title":"Analytics Middleware","text":"<p>This middleware collects analytics data about HTTP requests:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass AnalyticsMiddleware extends BaseMiddleware\n{\n    private $analytics;\n\n    public function __construct($analyticsService)\n    {\n        $this-&gt;analytics = $analyticsService;\n    }\n\n    protected function beforeRequest(HttpRequest $request): void\n    {\n        // Record the start time\n        $this-&gt;startTime = microtime(true);\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        $endTime = microtime(true);\n        $duration = round(($endTime - $this-&gt;startTime) * 1000, 2);\n\n        // Extract API endpoint from URL\n        $url = parse_url($request-&gt;url());\n        $endpoint = $url['path'] ?? '/';\n\n        // Record analytics data\n        $this-&gt;analytics-&gt;recordApiCall([\n            'endpoint' =&gt; $endpoint,\n            'method' =&gt; $request-&gt;method(),\n            'status_code' =&gt; $response-&gt;statusCode(),\n            'duration_ms' =&gt; $duration,\n            'request_size' =&gt; strlen($request-&gt;body()-&gt;toString()),\n            'response_size' =&gt; strlen($response-&gt;body()),\n            'timestamp' =&gt; time(),\n        ]);\n\n        return $response;\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#circuit-breaker-middleware","title":"Circuit Breaker Middleware","text":"<p>This middleware implements the circuit breaker pattern to prevent repeated calls to failing services:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;use Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Drivers\\Mock\\MockHttpResponse;use Cognesy\\Http\\Exceptions\\HttpRequestException;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass CircuitBreakerMiddleware extends BaseMiddleware\n{\n    private array $circuits = [];\n    private int $failureThreshold;\n    private int $resetTimeout;\n\n    public function __construct(int $failureThreshold = 3, int $resetTimeout = 60)\n    {\n        $this-&gt;failureThreshold = $failureThreshold;\n        $this-&gt;resetTimeout = $resetTimeout;\n    }\n\n    public function handle(HttpRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        $hostname = parse_url($request-&gt;url(), PHP_URL_HOST);\n\n        // Initialize circuit state if it doesn't exist\n        if (!isset($this-&gt;circuits[$hostname])) {\n            $this-&gt;circuits[$hostname] = [\n                'state' =&gt; 'CLOSED',\n                'failures' =&gt; 0,\n                'last_failure_time' =&gt; 0,\n            ];\n        }\n\n        $circuit = &amp;$this-&gt;circuits[$hostname];\n\n        // Check if circuit is open (service is considered down)\n        if ($circuit['state'] === 'OPEN') {\n            // Check if we should try resetting the circuit\n            $timeSinceLastFailure = time() - $circuit['last_failure_time'];\n\n            if ($timeSinceLastFailure &gt;= $this-&gt;resetTimeout) {\n                // Move to half-open state to test the service\n                $circuit['state'] = 'HALF_OPEN';\n            } else {\n                // Circuit is still open, return error response\n                return new MockHttpResponse(\n                    statusCode: 503,\n                    headers: ['Content-Type' =&gt; 'application/json'],\n                    body: json_encode([\n                        'error' =&gt; 'Service Unavailable',\n                        'message' =&gt; 'Circuit breaker is open',\n                        'retry_after' =&gt; $this-&gt;resetTimeout - $timeSinceLastFailure,\n                    ])\n                );\n            }\n        }\n\n        try {\n            // Attempt the request\n            $response = $next-&gt;handle($request);\n\n            // If successful and in half-open state, reset the circuit\n            if ($circuit['state'] === 'HALF_OPEN') {\n                $circuit['state'] = 'CLOSED';\n                $circuit['failures'] = 0;\n            }\n\n            return $response;\n\n        } catch (HttpRequestException $e) {\n            // Record the failure\n            $circuit['failures']++;\n            $circuit['last_failure_time'] = time();\n\n            // If failures exceed threshold, open the circuit\n            if ($circuit['failures'] &gt;= $this-&gt;failureThreshold || $circuit['state'] === 'HALF_OPEN') {\n                $circuit['state'] = 'OPEN';\n            }\n\n            // Re-throw the exception\n            throw $e;\n        }\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#conditional-middleware","title":"Conditional Middleware","text":"<p>This middleware only applies to certain requests based on a condition:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpMiddleware;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\n\nclass ConditionalMiddleware implements HttpMiddleware\n{\n    private HttpMiddleware $middleware;\n    private callable $condition;\n\n    public function __construct(HttpMiddleware $middleware, callable $condition)\n    {\n        $this-&gt;middleware = $middleware;\n        $this-&gt;condition = $condition;\n    }\n\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        // Check if the condition is met\n        if (($this-&gt;condition)($request)) {\n            // Apply the wrapped middleware\n            return $this-&gt;middleware-&gt;handle($request, $next);\n        }\n\n        // Skip the middleware if condition is not met\n        return $next-&gt;handle($request);\n    }\n}\n</code></pre> <p>Usage example:</p> <pre><code>// Only apply caching middleware to GET requests\n$cachingMiddleware = new CachingMiddleware($cache);\n$conditionalCaching = new ConditionalMiddleware(\n    $cachingMiddleware,\n    fn($request) =&gt; $request-&gt;method() === 'GET'\n);\n\n$client-&gt;withMiddleware($conditionalCaching);\n</code></pre>"},{"location":"http/11-processing-with-middleware/#request-id-middleware","title":"Request ID Middleware","text":"<p>This middleware adds a unique ID to each request and tracks it through the response:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\nuse Ramsey\\Uuid\\Uuid;\n\nclass RequestIdMiddleware extends BaseMiddleware\n{\n    private array $requestIds = [];\n\n    protected function beforeRequest(HttpClientRequest $request): void\n    {\n        // Generate a unique ID for this request\n        $requestId = Uuid::uuid4()-&gt;toString();\n\n        // Store the ID for this request\n        $this-&gt;requestIds[spl_object_hash($request)] = $requestId;\n\n        // Add a header to the outgoing request\n        $headers = $request-&gt;headers();\n        $headers['X-Request-ID'] = $requestId;\n\n        // In a real implementation, you would need to create a new request\n        // with the updated headers, as HttpRequest is immutable\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        // Get the request ID\n        $requestId = $this-&gt;requestIds[spl_object_hash($request)] ?? 'unknown';\n\n        // Log the request completion\n        error_log(\"Request $requestId completed with status: \" . $response-&gt;statusCode());\n\n        // Clean up\n        unset($this-&gt;requestIds[spl_object_hash($request)]);\n\n        return $response;\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#opentelemetry-tracing-middleware","title":"OpenTelemetry Tracing Middleware","text":"<p>This middleware adds OpenTelemetry tracing to HTTP requests:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\nuse OpenTelemetry\\API\\Trace\\SpanKind;\nuse OpenTelemetry\\API\\Trace\\StatusCode;\nuse OpenTelemetry\\API\\Trace\\TracerInterface;\n\nclass TracingMiddleware extends BaseMiddleware\n{\n    private TracerInterface $tracer;\n\n    public function __construct(TracerInterface $tracer)\n    {\n        $this-&gt;tracer = $tracer;\n    }\n\n    protected function beforeRequest(HttpClientRequest $request): void\n    {\n        // No actions needed in beforeRequest,\n        // we'll create the span in the handle method\n    }\n\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        // Extract the operation name from the URL\n        $url = parse_url($request-&gt;url());\n        $path = $url['path'] ?? '/';\n        $operationName = $request-&gt;method() . ' ' . $path;\n\n        // Create a span for this request\n        $span = $this-&gt;tracer-&gt;spanBuilder($operationName)\n            -&gt;setSpanKind(SpanKind::KIND_CLIENT)\n            -&gt;startSpan();\n\n        $scope = $span-&gt;activate();\n\n        try {\n            // Add request details to the span\n            $span-&gt;setAttribute('http.method', $request-&gt;method());\n            $span-&gt;setAttribute('http.url', $request-&gt;url());\n            $span-&gt;setAttribute('http.request_content_length', strlen($request-&gt;body()-&gt;toString()));\n\n            // Make the request\n            $response = $next-&gt;handle($request);\n\n            // Add response details to the span\n            $span-&gt;setAttribute('http.status_code', $response-&gt;statusCode());\n            $span-&gt;setAttribute('http.response_content_length', strlen($response-&gt;body()));\n\n            // Set the appropriate status\n            if ($response-&gt;statusCode() &gt;= 400) {\n                $span-&gt;setStatus(StatusCode::STATUS_ERROR, \"HTTP error: {$response-&gt;statusCode()}\");\n            } else {\n                $span-&gt;setStatus(StatusCode::STATUS_OK);\n            }\n\n            return $response;\n        } catch (\\Exception $e) {\n            // Record the error\n            $span-&gt;recordException($e);\n            $span-&gt;setStatus(StatusCode::STATUS_ERROR, $e-&gt;getMessage());\n\n            // Re-throw the exception\n            throw $e;\n        } finally {\n            // End the span\n            $scope-&gt;detach();\n            $span-&gt;end();\n        }\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#customizing-middleware-for-llm-apis","title":"Customizing Middleware for LLM APIs","text":"<p>When working with Large Language Model (LLM) APIs, you can create specialized middleware to handle their unique requirements:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\n\nclass LlmStreamingMiddleware extends BaseMiddleware\n{\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        // Only decorate streaming responses to LLM APIs\n        return $request-&gt;isStreamed() &amp;&amp;\n               strpos($request-&gt;url(), 'api.openai.com') !== false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return new class($request, $response) extends BaseResponseDecorator {\n            private string $buffer = '';\n            private array $chunks = [];\n\n            public function stream(int $chunkSize = 1): Generator\n            {\n                foreach ($this-&gt;response-&gt;stream($chunkSize) as $chunk) {\n                    $this-&gt;buffer .= $chunk;\n\n                    // Process lines in the buffer\n                    $lines = explode(\"\\n\", $this-&gt;buffer);\n\n                    // Keep the last line (potentially incomplete) in the buffer\n                    $this-&gt;buffer = array_pop($lines);\n\n                    foreach ($lines as $line) {\n                        $line = trim($line);\n\n                        // Skip empty lines\n                        if (empty($line)) {\n                            continue;\n                        }\n\n                        // Skip data: prefix\n                        if (strpos($line, 'data: ') === 0) {\n                            $line = substr($line, 6);\n                        }\n\n                        // Skip [DONE] message\n                        if ($line === '[DONE]') {\n                            continue;\n                        }\n\n                        // Try to parse as JSON\n                        $data = json_decode($line, true);\n\n                        if ($data) {\n                            // Extract content from different LLM formats\n                            $content = null;\n\n                            if (isset($data['choices'][0]['delta']['content'])) {\n                                // OpenAI format\n                                $content = $data['choices'][0]['delta']['content'];\n                            } elseif (isset($data['choices'][0]['text'])) {\n                                // Another format\n                                $content = $data['choices'][0]['text'];\n                            } elseif (isset($data['text'])) {\n                                // Simple format\n                                $content = $data['text'];\n                            }\n\n                            if ($content !== null) {\n                                $this-&gt;chunks[] = $content;\n                            }\n                        }\n                    }\n\n                    // Yield the original chunk to maintain streaming behavior\n                    yield $chunk;\n                }\n            }\n\n            public function body(): string\n            {\n                // If we've processed chunks, join them together\n                if (!empty($this-&gt;chunks)) {\n                    return implode('', $this-&gt;chunks);\n                }\n\n                // Otherwise, fall back to the normal body\n                return $this-&gt;response-&gt;body();\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#combining-multiple-middleware-components","title":"Combining Multiple Middleware Components","text":"<p>When building complex applications, you'll often need to combine multiple middleware components. Here's an example of how to set up a complete HTTP client pipeline:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Http\\HttpClient;use Middleware\\AuthenticationMiddleware\\AuthenticationMiddleware;use Middleware\\BasicHttpMiddleware\\LoggingMiddleware;use Middleware\\CachingMiddleware\\CachingMiddleware;use Middleware\\RateLimitingMiddleware\\RateLimitingMiddleware;use Middleware\\RetryMiddleware\\RetryMiddleware;use YourNamespace\\Http\\Middleware\\AnalyticsMiddleware;use YourNamespace\\Http\\Middleware\\CircuitBreakerMiddleware;use YourNamespace\\Http\\Middleware\\TracingMiddleware;\n\n// Create services needed by middleware\n$cache = new YourCacheService();\n$logger = new YourLoggerService();\n$tracer = YourTracerFactory::create();\n$analytics = new YourAnalyticsService();\n\n// Create the client\n$client = new HttpClient('guzzle');\n\n// Add middleware - the order is important!\n$client-&gt;withMiddleware(\n    // Outer middleware (processed first for requests, last for responses)\n    new TracingMiddleware($tracer),\n    new LoggingMiddleware($logger),\n    new CircuitBreakerMiddleware(),\n\n    // Caching should go before authentication\n    new CachingMiddleware($cache),\n\n    // Authentication adds credentials\n    new AuthenticationMiddleware($apiKey),\n\n    // These control how requests are sent\n    new RetryMiddleware(maxRetries: 3),\n    new RateLimitingMiddleware(maxRequests: 100),\n\n    // Analytics should be innermost to measure actual API call stats\n    new AnalyticsMiddleware($analytics)\n);\n\n// Now the client is ready to use with a complete middleware pipeline\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>With this setup, requests and responses flow through the middleware in the following order:</p> <ol> <li>Request Flow (outside \u2192 inside):</li> <li>TracingMiddleware: Starts a trace</li> <li>LoggingMiddleware: Logs the outgoing request</li> <li>CircuitBreakerMiddleware: Checks if the service is available</li> <li>CachingMiddleware: Checks if response is cached</li> <li>AuthenticationMiddleware: Adds authentication headers</li> <li>RetryMiddleware: Prepares to retry on failure</li> <li>RateLimitingMiddleware: Enforces rate limits</li> <li>AnalyticsMiddleware: Starts timing</li> <li> <p>HTTP Driver: Sends the actual request</p> </li> <li> <p>Response Flow (inside \u2192 outside):</p> </li> <li>HTTP Driver: Receives the response</li> <li>AnalyticsMiddleware: Records API stats</li> <li>RateLimitingMiddleware: Updates rate limit counters</li> <li>RetryMiddleware: Handles retries if needed</li> <li>AuthenticationMiddleware: Verifies authentication status</li> <li>CachingMiddleware: Caches the response</li> <li>CircuitBreakerMiddleware: Updates circuit state</li> <li>LoggingMiddleware: Logs the response</li> <li>TracingMiddleware: Completes the trace</li> <li>Your Application: Processes the final response</li> </ol> <p>This bidirectional flow allows for powerful request/response processing capabilities.</p> <p>By creating custom middleware and response decorators, you can extend the HTTP client's functionality to handle any specialized requirements your application might have.</p> <p>In the next chapter, we'll cover troubleshooting techniques for the Instructor HTTP client API.</p>"},{"location":"http/12-troubleshooting/","title":"Troubleshooting","text":"<p>Even with a well-designed API, you may encounter issues when working with HTTP requests and responses. This chapter covers common problems, debugging techniques, and error handling strategies for the Instructor HTTP client API.</p>"},{"location":"http/12-troubleshooting/#common-issues","title":"Common Issues","text":"<p>Here are some common issues you might encounter when using the Instructor HTTP client API, along with their solutions:</p>"},{"location":"http/12-troubleshooting/#connection-issues","title":"Connection Issues","text":"<p>Symptom: Requests fail with connection errors or timeouts.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Network Connectivity Issues:</li> <li>Verify that your server has internet connectivity</li> <li>Check if the target API is accessible from your server (try ping or telnet)</li> <li> <p>Ensure any required VPN connections are active</p> </li> <li> <p>DNS Issues:</p> </li> <li>Verify that DNS resolution is working correctly</li> <li> <p>Try using an IP address instead of a hostname to bypass DNS</p> </li> <li> <p>Firewall Blocking:</p> </li> <li>Check if a firewall is blocking outgoing connections</li> <li> <p>Verify that the required ports (usually 80 and 443) are open</p> </li> <li> <p>Proxy Configuration:</p> </li> <li>If you're using a proxy, ensure it's correctly configured</li> <li> <p>Check proxy credentials if authentication is required</p> </li> <li> <p>SSL/TLS Issues:</p> </li> <li>Verify that the server's SSL certificate is valid</li> <li>Check if your server trusts the certificate authority</li> <li>Update your CA certificates if needed</li> </ol>"},{"location":"http/12-troubleshooting/#timeout-issues","title":"Timeout Issues","text":"<p>Symptom: Requests time out before completing.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Connection Timeout Too Short:</li> <li> <p>Increase the <code>connectTimeout</code> setting in your client configuration <pre><code>$config = new HttpClientConfig(\n    connectTimeout: 10, // Increase from default\n    // Other settings...\n);\n$client-&gt;withConfig($config);\n</code></pre></p> </li> <li> <p>Request Timeout Too Short:</p> </li> <li> <p>Increase the <code>requestTimeout</code> setting for long-running operations <pre><code>$config = new HttpClientConfig(\n    requestTimeout: 60, // Increase from default\n    // Other settings...\n);\n$client-&gt;withConfig($config);\n</code></pre></p> </li> <li> <p>Idle Timeout Issues with Streaming:</p> </li> <li> <p>For streaming APIs, increase or disable the <code>idleTimeout</code> <pre><code>$config = new HttpClientConfig(\n    idleTimeout: -1, // Disable idle timeout\n    // Other settings...\n);\n$client-&gt;withConfig($config);\n</code></pre></p> </li> <li> <p>Server is Slow to Respond:</p> </li> <li>If the target server is known to be slow, adjust your timeouts accordingly</li> <li>Consider implementing a retry mechanism for intermittent issues</li> </ol>"},{"location":"http/12-troubleshooting/#authentication-issues","title":"Authentication Issues","text":"<p>Symptom: Requests fail with 401 Unauthorized or 403 Forbidden responses.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Invalid Credentials:</li> <li>Verify that your API keys or tokens are correct</li> <li>Check if the credentials have expired or been revoked</li> <li> <p>Ensure you're using the correct authentication method</p> </li> <li> <p>Missing Authorization Headers:</p> </li> <li> <p>Check that you're adding the correct Authorization header <pre><code>$request = new HttpRequest(\n    // ...\n    headers: [\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n        // Other headers...\n    ],\n    // ...\n);\n</code></pre></p> </li> <li> <p>Incorrect Authentication Format:</p> </li> <li>Verify the format required by the API (Bearer, Basic, etc.)</li> <li> <p>For Basic Auth, ensure credentials are properly base64-encoded</p> </li> <li> <p>Rate Limiting or IP Restrictions:</p> </li> <li>Check if you've exceeded rate limits</li> <li>Verify that your server's IP is allowed to access the API</li> </ol>"},{"location":"http/12-troubleshooting/#request-format-issues","title":"Request Format Issues","text":"<p>Symptom: Requests fail with 400 Bad Request or 422 Unprocessable Entity responses.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Incorrect Content-Type:</li> <li> <p>Ensure you're setting the correct Content-Type header <pre><code>$request = new HttpRequest(\n    // ...\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        // Other headers...\n    ],\n    // ...\n);\n</code></pre></p> </li> <li> <p>Malformed Request Body:</p> </li> <li>Validate your request body against the API's schema</li> <li>Check for typos in field names or incorrect data types</li> <li> <p>Use a tool like Postman to test the request format</p> </li> <li> <p>Missing Required Fields:</p> </li> <li>Ensure all required fields are included in the request</li> <li> <p>Check the API documentation for required vs. optional fields</p> </li> <li> <p>Validation Errors:</p> </li> <li>Read the error messages in the response for specific validation issues</li> <li>Fix each validation error according to the API's requirements</li> </ol>"},{"location":"http/12-troubleshooting/#middleware-issues","title":"Middleware Issues","text":"<p>Symptom: Unexpected behavior when using middleware.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Middleware Order Issues:</li> <li>Remember that middleware is executed in the order it's added</li> <li> <p>Rearrange middleware to ensure proper execution order <pre><code>$client-&gt;middleware()-&gt;clear(); // Clear existing middleware\n$client-&gt;withMiddleware(\n    new AuthenticationMiddleware($apiKey), // First\n    new LoggingMiddleware(), // Second\n    new RetryMiddleware() // Third\n);\n</code></pre></p> </li> <li> <p>Middleware Not Executing:</p> </li> <li>Verify that the middleware is actually added to the stack</li> <li> <p>Check for conditional logic in your middleware that might be skipping execution</p> </li> <li> <p>Middleware Changing Request/Response:</p> </li> <li>Be aware that middleware can modify requests and responses</li> <li> <p>Debug by logging the request/response before and after each middleware</p> </li> <li> <p>Middleware Exceptions:</p> </li> <li>Exceptions in middleware can disrupt the entire chain</li> <li>Add proper error handling in your middleware</li> </ol>"},{"location":"http/12-troubleshooting/#debugging-tools","title":"Debugging Tools","text":"<p>The Instructor HTTP client API provides several tools to help you debug HTTP requests and responses.</p>"},{"location":"http/12-troubleshooting/#debug-middleware","title":"Debug Middleware","text":"<p>The <code>DebugMiddleware</code> is the primary tool for debugging HTTP interactions:</p> <pre><code>use Cognesy\\Http\\HttpClient;\n\n// Method 1: Using the withDebug convenience method\n$client = new HttpClient();\n$client-&gt;withDebugPreset('on');\n\n// Method 2: Enable debug in configuration\n$config = [\n    'http' =&gt; [\n        'enabled' =&gt; true,\n        // Other debug settings...\n    ],\n];\n</code></pre> <p>You can configure which aspects of HTTP interactions to log in the <code>config/debug.php</code> file:</p> <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; true,           // Master switch\n        'trace' =&gt; false,            // Dump HTTP trace information\n        'requestUrl' =&gt; true,        // Dump request URL\n        'requestHeaders' =&gt; true,    // Dump request headers\n        'requestBody' =&gt; true,       // Dump request body\n        'responseHeaders' =&gt; true,   // Dump response headers\n        'responseBody' =&gt; true,      // Dump response body\n        'responseStream' =&gt; true,    // Dump streaming data\n        'responseStreamByLine' =&gt; true, // Format stream as lines\n    ],\n];\n</code></pre>"},{"location":"http/12-troubleshooting/#event-dispatching","title":"Event Dispatching","text":"<p>The HTTP client dispatches events at key points in the request lifecycle:</p> <pre><code>use Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\n\n// Create an event dispatcher with custom listeners\n$events = new EventDispatcher();\n\n// Listen for outgoing requests\n$events-&gt;listen(HttpRequestSent::class, function ($event) {\n    echo \"Sending {$event-&gt;method} request to {$event-&gt;url}\\n\";\n    echo \"Headers: \" . json_encode($event-&gt;headers) . \"\\n\";\n    echo \"Body: \" . json_encode($event-&gt;body) . \"\\n\";\n});\n\n// Listen for incoming responses\n$events-&gt;listen(HttpResponseReceived::class, function ($event) {\n    echo \"Received response with status code: {$event-&gt;statusCode}\\n\";\n});\n\n// Listen for request failures\n$events-&gt;listen(HttpRequestFailed::class, function ($event) {\n    echo \"Request failed: {$event-&gt;errors}\\n\";\n    echo \"URL: {$event-&gt;url}, Method: {$event-&gt;method}\\n\";\n});\n\n// Create a client with this event dispatcher\n$client = new HttpClient('', $events);\n</code></pre>"},{"location":"http/12-troubleshooting/#manual-debugging","title":"Manual Debugging","text":"<p>You can implement your own debugging by adding logging statements:</p> <pre><code>try {\n    echo \"Sending request to: {$request-&gt;url()}\\n\";\n    echo \"Headers: \" . json_encode($request-&gt;headers()) . \"\\n\";\n    echo \"Body: \" . $request-&gt;body()-&gt;toString() . \"\\n\";\n\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    echo \"Response status: {$response-&gt;statusCode()}\\n\";\n    echo \"Response headers: \" . json_encode($response-&gt;headers()) . \"\\n\";\n    echo \"Response body: {$response-&gt;body()}\\n\";\n} catch (RequestException $e) {\n    echo \"Error: {$e-&gt;getMessage()}\\n\";\n    if ($e-&gt;getPrevious()) {\n        echo \"Original error: {$e-&gt;getPrevious()-&gt;getMessage()}\\n\";\n    }\n}\n</code></pre>"},{"location":"http/12-troubleshooting/#recordreplay-middleware-for-debugging","title":"Record/Replay Middleware for Debugging","text":"<p>The <code>RecordReplayMiddleware</code> can be useful for debugging by recording HTTP interactions and replaying them later:</p> <pre><code>use Cognesy\\Http\\Middleware\\RecordReplay\\RecordReplayMiddleware;\n\n// Record all HTTP interactions to a directory\n$recordReplayMiddleware = new RecordReplayMiddleware(\n    mode: RecordReplayMiddleware::MODE_RECORD,\n    storageDir: __DIR__ . '/debug_recordings',\n    fallbackToRealRequests: true\n);\n\n$client-&gt;withMiddleware($recordReplayMiddleware);\n\n// Make your requests...\n\n// Later, you can inspect the recorded files to see what was sent/received\n</code></pre>"},{"location":"http/12-troubleshooting/#logging-and-tracing","title":"Logging and Tracing","text":"<p>Implementing proper logging and tracing is essential for troubleshooting HTTP issues, especially in production environments.</p>"},{"location":"http/12-troubleshooting/#requestresponse-logging","title":"Request/Response Logging","text":"<p>Create a custom logging middleware:</p> <p>```php include='D03_Docs_HTTP/Logging/code.php' <pre><code>### Distributed Tracing\n\nFor production environments, consider implementing distributed tracing with systems like Jaeger, Zipkin, or OpenTelemetry:\n\n```php include='D03_Docs_HTTP/DistributedTracing/code.php'\n</code></pre></p>"},{"location":"http/12-troubleshooting/#error-handling-strategies","title":"Error Handling Strategies","text":"<p>Proper error handling is crucial for building robust applications. Here are some strategies for handling HTTP errors effectively.</p>"},{"location":"http/12-troubleshooting/#basic-error-handling","title":"Basic Error Handling","text":"<p>The simplest approach is to catch the <code>RequestException</code>:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process successful response\n} catch (HttpRequestException $e) {\n    // Handle error\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre>"},{"location":"http/12-troubleshooting/#categorizing-errors","title":"Categorizing Errors","text":"<p>You can categorize errors based on the underlying exception or status code:</p> <p>```php include='codeblocks/ErrorCategorization/code.php' <pre><code>### Implementing Retry Logic\n\nFor transient errors, implement retry logic:\n\n```php include='codeblocks/RetryLogic/code.php'\n</code></pre></p>"},{"location":"http/12-troubleshooting/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>For critical services, implement a circuit breaker to prevent cascading failures:</p> <p>```php include='codeblocks/CircuitBreaker/code.php' <pre><code>### Graceful Degradation\n\nWhen a service is unavailable, implement graceful degradation by providing fallback functionality:\n\n```php include='codeblocks/GracefulDegradation/code.php'\n</code></pre></p>"},{"location":"http/12-troubleshooting/#comprehensive-error-handling-example","title":"Comprehensive Error Handling Example","text":"<p>Here's a comprehensive example that combines multiple error handling strategies:</p> <p>```php include='codeblocks/ErrorHandling/code.php' <pre><code>This comprehensive approach combines:\n- Circuit breaker pattern\n- Caching and fallbacks\n- Retry logic with exponential backoff\n- Status-code specific error handling\n- Logging for troubleshooting\n\nBy implementing these patterns, your application can be more resilient to API failures and provide a better user experience even when external services are unavailable.\nClient API.\n\n## Common Issues\n\nHere are some common issues you might encounter when using the Instructor HTTP client API, along with their solutions:\n\n### Connection Issues\n\n**Symptom**: Requests fail with connection errors or timeouts.\n\n**Possible Causes and Solutions**:\n\n1. **Network Connectivity Issues**:\n- Verify that your server has internet connectivity\n- Check if the target API is accessible from your server (try ping or telnet)\n- Ensure any required VPN connections are active\n\n2. **DNS Issues**:\n- Verify that DNS resolution is working correctly\n- Try using an IP address instead of a hostname to bypass DNS\n\n3. **Firewall Blocking**:\n- Check if a firewall is blocking outgoing connections\n- Verify that the required ports (usually 80 and 443) are open\n\n4. **Proxy Configuration**:\n- If you're using a proxy, ensure it's correctly configured\n- Check proxy credentials if authentication is required\n\n5. **SSL/TLS Issues**:\n- Verify that the server's SSL certificate is valid\n- Check if your server trusts the certificate authority\n- Update your CA certificates if needed\n\n### Timeout Issues\n\n**Symptom**: Requests time out before completing.\n\n**Possible Causes and Solutions**:\n\n1. **Connection Timeout Too Short**:\n- Increase the `connectTimeout` setting in your client configuration\n```php\n$config = new HttpClientConfig(\n    connectTimeout: 10, // Increase from default\n    // Other settings...\n);\n$client-&gt;withConfig($config);\n</code></pre></p> <ol> <li>Request Timeout Too Short:</li> <li> <p>Increase the <code>requestTimeout</code> setting for long-running operations <pre><code>$config = new HttpClientConfig(\n    requestTimeout: 60, // Increase from default\n    // Other settings...\n);\n$client-&gt;withConfig($config);\n</code></pre></p> </li> <li> <p>Idle Timeout Issues with Streaming:</p> </li> <li> <p>For streaming APIs, increase or disable the <code>idleTimeout</code> <pre><code>$config = new HttpClientConfig(\n    idleTimeout: -1, // Disable idle timeout\n    // Other settings...\n);\n$client-&gt;withConfig($config);\n</code></pre></p> </li> <li> <p>Server is Slow to Respond:</p> </li> <li>If the target server is known to be slow, adjust your timeouts accordingly</li> <li>Consider implementing a retry mechanism for intermittent issues</li> </ol>"},{"location":"http/12-troubleshooting/#authentication-issues_1","title":"Authentication Issues","text":"<p>Symptom: Requests fail with 401 Unauthorized or 403 Forbidden responses.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Invalid Credentials:</li> <li>Verify that your API keys or tokens are correct</li> <li>Check if the credentials have expired or been revoked</li> <li> <p>Ensure you're using the correct authentication method</p> </li> <li> <p>Missing Authorization Headers:</p> </li> <li>Check that you're adding the correct Authorization header</li> </ol> <pre><code>$request = new HttpRequest(\n    // ...\n    headers: [\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n        // Other headers...\n    ],\n    // ...\n);\n</code></pre> <ol> <li>Incorrect Authentication Format:</li> <li>Verify the format required by the API (Bearer, Basic, etc.)</li> <li> <p>For Basic Auth, ensure credentials are properly base64-encoded</p> </li> <li> <p>Rate Limiting or IP Restrictions:</p> </li> <li>Check if you've exceeded rate limits</li> <li>Verify that your server's IP is allowed to access the API</li> </ol>"},{"location":"http/12-troubleshooting/#request-format-issues_1","title":"Request Format Issues","text":"<p>Symptom: Requests fail with 400 Bad Request or 422 Unprocessable Entity responses.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Incorrect Content-Type:</li> <li>Ensure you're setting the correct Content-Type header</li> </ol> <pre><code>$request = new HttpRequest(\n    // ...\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        // Other headers...\n    ],\n    // ...\n);\n</code></pre> <ol> <li>Malformed Request Body:</li> <li>Validate your request body against the API's schema</li> <li>Check for typos in field names or incorrect data types</li> <li> <p>Use a tool like Postman to test the request format</p> </li> <li> <p>Missing Required Fields:</p> </li> <li>Ensure all required fields are included in the request</li> <li> <p>Check the API documentation for required vs. optional fields</p> </li> <li> <p>Validation Errors:</p> </li> <li>Read the error messages in the response for specific validation issues</li> <li>Fix each validation error according to the API's requirements</li> </ol>"},{"location":"http/12-troubleshooting/#middleware-issues_1","title":"Middleware Issues","text":"<p>Symptom: Unexpected behavior when using middleware.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Middleware Order Issues:</li> <li>Remember that middleware is executed in the order it's added</li> <li> <p>Rearrange middleware to ensure proper execution order <pre><code>$client-&gt;middleware()-&gt;clear(); // Clear existing middleware\n$client-&gt;withMiddleware(\n    new AuthenticationMiddleware($apiKey), // First\n    new LoggingMiddleware(), // Second\n    new RetryMiddleware() // Third\n);\n</code></pre></p> </li> <li> <p>Middleware Not Executing:</p> </li> <li>Verify that the middleware is actually added to the stack</li> <li> <p>Check for conditional logic in your middleware that might be skipping execution</p> </li> <li> <p>Middleware Changing Request/Response:</p> </li> <li>Be aware that middleware can modify requests and responses</li> <li> <p>Debug by logging the request/response before and after each middleware</p> </li> <li> <p>Middleware Exceptions:</p> </li> <li>Exceptions in middleware can disrupt the entire chain</li> <li>Add proper error handling in your middleware</li> </ol>"},{"location":"http/12-troubleshooting/#debugging-tools_1","title":"Debugging Tools","text":"<p>The Instructor HTTP client API provides several tools to help you debug HTTP requests and responses.</p>"},{"location":"http/12-troubleshooting/#debug-middleware_1","title":"Debug Middleware","text":"<p>The <code>DebugMiddleware</code> is the primary tool for debugging HTTP interactions:</p> <pre><code>use Cognesy\\Http\\HttpClient;\n\n// Method 1: Using the withDebug convenience method\n$client = new HttpClient();\n$client-&gt;withDebugPreset('on');\n\n// Method 2: Enable debug in configuration\n$config = [\n    'http' =&gt; [\n        'enabled' =&gt; true,\n        // Other debug settings...\n    ],\n];\n</code></pre> <p>You can configure which aspects of HTTP interactions to log in the <code>config/debug.php</code> file:</p> <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; true,           // Master switch\n        'trace' =&gt; false,            // Dump HTTP trace information\n        'requestUrl' =&gt; true,        // Dump request URL\n        'requestHeaders' =&gt; true,    // Dump request headers\n        'requestBody' =&gt; true,       // Dump request body\n        'responseHeaders' =&gt; true,   // Dump response headers\n        'responseBody' =&gt; true,      // Dump response body\n        'responseStream' =&gt; true,    // Dump streaming data\n        'responseStreamByLine' =&gt; true, // Format stream as lines\n    ],\n];\n</code></pre>"},{"location":"http/12-troubleshooting/#event-dispatching_1","title":"Event Dispatching","text":"<p>The HTTP client dispatches events at key points in the request lifecycle:</p> <p>```php include='codeblocks/EventDispatching/code.php' <pre><code>### Manual Debugging\n\nYou can implement your own debugging by adding logging statements:\n\n```php include='codeblocks/ManualDebugging/code.php'\n</code></pre></p>"},{"location":"http/12-troubleshooting/#recordreplay-middleware-for-debugging_1","title":"Record/Replay Middleware for Debugging","text":"<p>The <code>RecordReplayMiddleware</code> can be useful for debugging by recording HTTP interactions and replaying them later:</p> <p>```php include='codeblocks/RecordReplay/code.php' <pre><code>## Logging and Tracing\n\nImplementing proper logging and tracing is essential for troubleshooting HTTP issues, especially in production environments.\n\n### Request/Response Logging\n\nCreate a custom logging middleware:\n\n```php include='codeblocks/Logging/code.php'\n</code></pre></p>"},{"location":"http/2-getting-started/","title":"Getting Started","text":""},{"location":"http/2-getting-started/#installation","title":"Installation","text":"<p>The Instructor HTTP client API is part of the Instructor library (https://instructorphp.com) and is bundled with it.</p> <p>You can install it separately via Composer:</p> <pre><code># @doctest id=\"9cac\"\ncomposer require cognesy/instructor-http-client\n</code></pre>"},{"location":"http/2-getting-started/#dependencies","title":"Dependencies","text":"<p>The Instructor HTTP client API requires at least one of the supported HTTP client libraries. Depending on which client you want to use, you'll need to install the corresponding package:</p> <p>For Guzzle: <pre><code># @doctest id=\"900c\"\ncomposer require guzzlehttp/guzzle\n</code></pre></p> <p>For Symfony HTTP Client: <pre><code># @doctest id=\"02e8\"\ncomposer require symfony/http-client\n</code></pre></p> <p>For Laravel HTTP Client: The Laravel HTTP Client is included with the Laravel framework. If you're using Laravel, you don't need to install it separately.</p>"},{"location":"http/2-getting-started/#php-requirements","title":"PHP Requirements","text":"<p>The library requires: - PHP 8.1 or higher - JSON extension - cURL extension (recommended)</p>"},{"location":"http/2-getting-started/#basic-usage","title":"Basic Usage","text":"<p>Using the Instructor HTTP client API involves a few key steps:</p> <ol> <li>Create an <code>HttpClient</code> instance</li> <li>Create an <code>HttpRequest</code> object</li> <li>Use the client to handle the request</li> <li>Process the response</li> </ol> <p>Here's a simple example:</p> <pre><code>// @doctest id=\"13cc\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n// Create a new HTTP client (uses the default client from configuration)\n$client = HttpClient::default();\n\n// Create a request\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n\n// Send the request and get the response\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Access response data\n$statusCode = $response-&gt;statusCode();\n$headers = $response-&gt;headers();\n$body = $response-&gt;body();\n\necho \"Status: $statusCode\\n\";\necho \"Body: $body\\n\";\n</code></pre>"},{"location":"http/2-getting-started/#error-handling","title":"Error Handling","text":"<p>HTTP requests can fail for various reasons. You should always wrap request handling in a try-catch block:</p> <pre><code>// @doctest id=\"1c19\"\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n    // Handle the error\n}\n</code></pre>"},{"location":"http/2-getting-started/#configuration","title":"Configuration","text":"<p>The Instructor HTTP client API can be configured via configuration files or at runtime.</p>"},{"location":"http/2-getting-started/#configuration-files","title":"Configuration Files","text":"<p>Create the configuration files in your project:</p> <p>config/http.php: <pre><code>// @doctest skip=true\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre></p> <p>config/debug.php: <pre><code>// @doctest id=\"3a9e\"\nreturn [\n    'http' =&gt; [\n        'enabled' =&gt; false, // enable/disable debug\n        'trace' =&gt; false, // dump HTTP trace information\n        'requestUrl' =&gt; true, // dump request URL to console\n        'requestHeaders' =&gt; true, // dump request headers to console\n        'requestBody' =&gt; true, // dump request body to console\n        'responseHeaders' =&gt; true, // dump response headers to console\n        'responseBody' =&gt; true, // dump response body to console\n        'responseStream' =&gt; true, // dump stream data to console\n        'responseStreamByLine' =&gt; true, // dump stream as full lines or raw chunks\n    ],\n];\n</code></pre></p>"},{"location":"http/2-getting-started/#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also configure the client at runtime:</p> <pre><code>// @doctest id=\"1754\"\n&lt;?php\nuse Cognesy\\Http\\HttpClient;\n\n// Create client with specific configuration\n$client = HttpClient::using('guzzle');\n\n// Or create with debug enabled\n$client = (new HttpClientBuilder())\n    -&gt;withPreset('guzzle')\n    -&gt;withDebugPreset('on')\n    -&gt;create();\n</code></pre>"},{"location":"http/2-getting-started/#simple-request-example","title":"Simple Request Example","text":"<p>Let's put everything together with a practical example of making a POST request to create a new resource:</p> <pre><code>// @doctest id=\"29d0\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create an HTTP client using the 'guzzle' configuration\n$client = HttpClient::using('guzzle');\n\n// Create a POST request with JSON data\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n    ],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n        'role' =&gt; 'user',\n    ],\n    options: []\n);\n\ntry {\n    // Send the request\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Process the response\n    if ($response-&gt;statusCode() === 201) {\n        $user = json_decode($response-&gt;body(), true);\n        echo \"User created with ID: {$user['id']}\\n\";\n\n        // Print user details\n        echo \"Name: {$user['name']}\\n\";\n        echo \"Email: {$user['email']}\\n\";\n    } else {\n        echo \"Error: Unexpected status code {$response-&gt;statusCode()}\\n\";\n        echo \"Response: {$response-&gt;body()}\\n\";\n    }\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n\n    // You might want to log the error or retry the request\n}\n</code></pre>"},{"location":"http/2-getting-started/#example-fetching-data","title":"Example: Fetching Data","text":"<p>Here's an example of making a GET request to fetch data:</p> <pre><code>// @doctest id=\"3008\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create a default HTTP client\n$client = HttpClient::default();\n\n// Create a GET request with query parameters\n$request = new HttpRequest(\n    url: 'https://api.example.com/users?page=1&amp;limit=10',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n    ],\n    body: [],\n    options: []\n);\n\ntry {\n    // Send the request\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Process the response\n    if ($response-&gt;statusCode() === 200) {\n        $data = json_decode($response-&gt;body(), true);\n        $users = $data['users'] ?? [];\n\n        echo \"Retrieved \" . count($users) . \" users:\\n\";\n\n        foreach ($users as $user) {\n            echo \"- {$user['name']} ({$user['email']})\\n\";\n        }\n    } else {\n        echo \"Error: Unexpected status code {$response-&gt;statusCode()}\\n\";\n        echo \"Response: {$response-&gt;body()}\\n\";\n    }\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>These examples demonstrate the basic usage of the Instructor HTTP client API for common HTTP operations. In the following chapters, we'll explore more advanced features and customization options.</p>"},{"location":"http/3-making-requests/","title":"Making HTTP Requests","text":"<p>The Instructor HTTP client API provides a flexible and consistent way to create and send HTTP requests across different client implementations. This chapter covers the details of building and customizing HTTP requests.</p>"},{"location":"http/3-making-requests/#creating-requests","title":"Creating Requests","text":"<p>All HTTP requests are created using the <code>HttpRequest</code> class, which encapsulates the various components of an HTTP request.</p>"},{"location":"http/3-making-requests/#basic-request-creation","title":"Basic Request Creation","text":"<p>The constructor for <code>HttpRequest</code> takes several parameters:</p> <pre><code>// @doctest id=\"328d\"\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/endpoint',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre> <p>The parameters are:</p> <ul> <li><code>url</code>: The URL to send the request to (string)</li> <li><code>method</code>: The HTTP method to use (string)</li> <li><code>headers</code>: An associative array of HTTP headers (array)</li> <li><code>body</code>: The request body, which can be a string or an array (mixed)</li> <li><code>options</code>: Additional options for the request (array)</li> </ul>"},{"location":"http/3-making-requests/#request-methods","title":"Request Methods","text":"<p>Once you've created a request, you can access its properties using the following methods:</p> <pre><code>// @doctest id=\"bbd5\"\n// Get the request URL\n$url = $request-&gt;url();\n\n// Get the HTTP method\n$method = $request-&gt;method();\n\n// Get the request headers\n$headers = $request-&gt;headers();\n\n// Get the request body\n$body = $request-&gt;body();\n\n// Get the request options\n$options = $request-&gt;options();\n\n// Check if the request is configured for streaming\n$isStreaming = $request-&gt;isStreamed();\n</code></pre>"},{"location":"http/3-making-requests/#modifying-requests","title":"Modifying Requests","text":"<p>You can also modify a request after it's been created:</p> <pre><code>// @doctest id=\"d6b8\"\n// Enable streaming for this request\n$streamingRequest = $request-&gt;withStreaming(true);\n</code></pre> <p>Note that the <code>with*</code> methods return a new request instance rather than modifying the original one.</p>"},{"location":"http/3-making-requests/#http-methods","title":"HTTP Methods","text":"<p>The HTTP method is specified as a string in the <code>HttpClientRequest</code> constructor. The library supports all standard HTTP methods:</p>"},{"location":"http/3-making-requests/#get-requests","title":"GET Requests","text":"<p>GET requests are used to retrieve data from a server:</p> <pre><code>// @doctest id=\"dbf0\"\n$getRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre> <p>For GET requests with query parameters, include them in the URL:</p> <pre><code>// @doctest id=\"50a5\"\n$getRequestWithParams = new HttpRequest(\n    url: 'https://api.example.com/users?page=1&amp;limit=10&amp;sort=name',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#post-requests","title":"POST Requests","text":"<p>POST requests are used to create new resources or submit data:</p> <pre><code>// @doctest id=\"53c3\"\n$postRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#put-requests","title":"PUT Requests","text":"<p>PUT requests are used to update existing resources:</p> <pre><code>// @doctest id=\"bc82\"\n$putRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'PUT',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'name' =&gt; 'John Updated',\n        'email' =&gt; 'john.updated@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#patch-requests","title":"PATCH Requests","text":"<p>PATCH requests are used to partially update resources:</p> <pre><code>// @doctest id=\"db3c\"\n$patchRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'PATCH',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'email' =&gt; 'new.email@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#delete-requests","title":"DELETE Requests","text":"<p>DELETE requests are used to remove resources:</p> <pre><code>// @doctest id=\"cc92\"\n$deleteRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'DELETE',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#other-methods","title":"Other Methods","text":"<p>The library also supports other HTTP methods like HEAD, OPTIONS, etc. Just specify the method name as a string:</p> <pre><code>// @doctest id=\"4fb8\"\n$headRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'HEAD',\n    headers: [],\n    body: [],\n    options: []\n);\n\n$optionsRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'OPTIONS',\n    headers: [],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#setting-headers","title":"Setting Headers","text":"<p>HTTP headers are specified as an associative array where keys are header names and values are header values:</p> <pre><code>// @doctest id=\"baa6\"\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n        'User-Agent' =&gt; 'MyApp/1.0',\n        'X-Custom-Header' =&gt; 'Custom Value',\n    ],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#common-headers","title":"Common Headers","text":"<p>Some commonly used HTTP headers include:</p> <ul> <li> <p>Content-Type: Specifies the format of the request body <pre><code>// @doctest id=\"a30f\"\n'Content-Type' =&gt; 'application/json'\n</code></pre></p> </li> <li> <p>Accept: Indicates what response format the client can understand <pre><code>// @doctest id=\"7382\"\n'Accept' =&gt; 'application/json'\n</code></pre></p> </li> <li> <p>Authorization: Provides authentication credentials <pre><code>// @doctest id=\"a71e\"\n'Authorization' =&gt; 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'\n</code></pre></p> </li> <li> <p>User-Agent: Identifies the client application <pre><code>// @doctest id=\"4f15\"\n'User-Agent' =&gt; 'MyApp/1.0 (https://example.com)'\n</code></pre></p> </li> <li> <p>Cache-Control: Directives for caching mechanisms <pre><code>// @doctest id=\"4d79\"\n'Cache-Control' =&gt; 'no-cache'\n</code></pre></p> </li> <li> <p>Accept-Language: Indicates the preferred language <pre><code>// @doctest id=\"95c9\"\n'Accept-Language' =&gt; 'en-US,en;q=0.9'\n</code></pre></p> </li> </ul>"},{"location":"http/3-making-requests/#request-body","title":"Request Body","text":"<p>The request body can be provided in two ways:</p>"},{"location":"http/3-making-requests/#array-body-json","title":"Array Body (JSON)","text":"<p>If you provide an array as the request body, it will automatically be converted to a JSON string:</p> <pre><code>// @doctest id=\"6eb1\"\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/json'],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n        'age' =&gt; 30,\n        'address' =&gt; [\n            'street' =&gt; '123 Main St',\n            'city' =&gt; 'Anytown',\n            'zipcode' =&gt; '12345',\n        ],\n        'tags' =&gt; ['developer', 'php'],\n    ],\n    options: []\n);\n</code></pre> <p>When using an array for the body, you should set the <code>Content-Type</code> header to <code>application/json</code>.</p>"},{"location":"http/3-making-requests/#string-body","title":"String Body","text":"<p>You can also provide the body as a raw string:</p> <pre><code>// @doctest id=\"19f7\"\n// JSON string\n$jsonBody = json_encode([\n    'name' =&gt; 'John Doe',\n    'email' =&gt; 'john@example.com',\n]);\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/json'],\n    body: $jsonBody,\n    options: []\n);\n</code></pre> <p>This approach is useful for other content types:</p> <pre><code>// @doctest id=\"b1e2\"\n// Form URL-encoded data\n$formBody = http_build_query([\n    'name' =&gt; 'John Doe',\n    'email' =&gt; 'john@example.com',\n]);\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/x-www-form-urlencoded'],\n    body: $formBody,\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#working-with-request-body","title":"Working with Request Body","text":"<p>The body is managed by the <code>HttpRequestBody</code> class, which provides methods to access the body in different formats:</p> <pre><code>// @doctest id=\"be11\"\n// Get the body as a string\n$bodyString = $request-&gt;body()-&gt;toString();\n\n// Get the body as an array (for JSON bodies)\n$bodyArray = $request-&gt;body()-&gt;toArray();\n</code></pre>"},{"location":"http/3-making-requests/#request-options","title":"Request Options","text":"<p>The <code>options</code> parameter allows you to specify additional options for the request:</p> <pre><code>// @doctest id=\"6591\"\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: [\n        'stream' =&gt; true,  // Enable streaming response\n    ]\n);\n</code></pre>"},{"location":"http/3-making-requests/#available-options","title":"Available Options","text":"<p>Currently, the main supported option is:</p> <ul> <li><code>stream</code>: When set to <code>true</code>, enables streaming response handling</li> </ul> <p>You can check if a request is configured for streaming:</p> <pre><code>// @doctest id=\"1a5a\"\nif ($request-&gt;isStreamed()) {\n    // Handle streaming response\n}\n</code></pre>"},{"location":"http/3-making-requests/#example-streaming-request","title":"Example: Streaming Request","text":"<p>Here's how to create a request for a streaming API:</p> <pre><code>// @doctest id=\"198b\"\n$streamingRequest = new HttpRequest(\n    url: 'https://api.openai.com/v1/completions',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiKey,\n    ],\n    body: [\n        'model' =&gt; 'text-davinci-003',\n        'prompt' =&gt; 'Once upon a time',\n        'max_tokens' =&gt; 100,\n        'stream' =&gt; true,\n    ],\n    options: [\n        'stream' =&gt; true, // Enable streaming in the client\n    ]\n);\n</code></pre> <p>In the following chapters, we'll explore how to handle responses, including streaming responses, and how to use more advanced features like request pools and middleware.</p>"},{"location":"http/4-handling-responses/","title":"Handling Responses","text":"<p>After sending an HTTP request, you need to process the response received from the server. The Instructor HTTP client API provides a consistent interface for handling responses, regardless of the underlying HTTP client implementation.</p>"},{"location":"http/4-handling-responses/#response-interface","title":"Response Interface","text":"<p>All responses implement the <code>HttpResponse</code> interface, which provides a uniform way to access response data:</p> <pre><code>// @doctest id=\"81fa\"\ninterface HttpResponse\n{\n    public function statusCode(): int;\n    public function headers(): array;\n    public function body(): string;\n    public function stream(int $chunkSize = 1): Generator;\n}\n</code></pre> <p>This interface ensures that the same code will work whether you're using Guzzle, Symfony, or Laravel HTTP clients.</p>"},{"location":"http/4-handling-responses/#getting-the-response","title":"Getting the Response","text":"<p>When you send a request using the <code>HttpClient::handle()</code> method, it returns an implementation of <code>HttpResponse</code>:</p> <pre><code>// @doctest id=\"441e\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\n\n// Create a new HTTP client\n$client = HttpClient::default();\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>The specific implementation depends on the HTTP client driver being used:</p> <ul> <li><code>PsrHttpResponse</code>: Used by the GuzzleDriver</li> <li><code>SymfonyHttpResponse</code>: Used by the SymfonyDriver</li> <li><code>LaravelHttpResponse</code>: Used by the LaravelDriver</li> <li><code>MockHttpResponse</code>: Used by the MockHttpDriver for testing</li> </ul> <p>However, since all these implementations provide the same interface, your code doesn't need to know which one it's working with.</p>"},{"location":"http/4-handling-responses/#status-codes","title":"Status Codes","text":"<p>The status code indicates the result of the HTTP request. You can access it using the <code>statusCode()</code> method:</p> <pre><code>// @doctest id=\"2be6\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n$statusCode = $response-&gt;statusCode();\n\necho \"Status code: $statusCode\\n\";\n</code></pre>"},{"location":"http/4-handling-responses/#status-code-categories","title":"Status Code Categories","text":"<p>Status codes are grouped into categories:</p> <ul> <li>1xx (Informational): The request was received and understood</li> <li>2xx (Success): The request was successfully received, understood, and accepted</li> <li>3xx (Redirection): Further action needs to be taken to complete the request</li> <li>4xx (Client Error): The request contains bad syntax or cannot be fulfilled</li> <li>5xx (Server Error): The server failed to fulfill a valid request</li> </ul>"},{"location":"http/4-handling-responses/#checking-response-status","title":"Checking Response Status","text":"<p>You can check if a response was successful:</p> <pre><code>// @doctest id=\"1cc4\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n\nif ($response-&gt;statusCode() &gt;= 200 &amp;&amp; $response-&gt;statusCode() &lt; 300) {\n    // Success response\n    echo \"Request succeeded!\\n\";\n} elseif ($response-&gt;statusCode() &gt;= 400 &amp;&amp; $response-&gt;statusCode() &lt; 500) {\n    // Client error\n    echo \"Client error: {$response-&gt;statusCode()}\\n\";\n} elseif ($response-&gt;statusCode() &gt;= 500) {\n    // Server error\n    echo \"Server error: {$response-&gt;statusCode()}\\n\";\n}\n</code></pre>"},{"location":"http/4-handling-responses/#common-status-codes","title":"Common Status Codes","text":"<p>Here are some common HTTP status codes you might encounter:</p> <ul> <li>200 OK: The request was successful</li> <li>201 Created: A new resource was successfully created</li> <li>204 No Content: The request was successful, but there's no response body</li> <li>400 Bad Request: The request was malformed or invalid</li> <li>401 Unauthorized: Authentication is required</li> <li>403 Forbidden: The client doesn't have permission to access the resource</li> <li>404 Not Found: The requested resource doesn't exist</li> <li>405 Method Not Allowed: The HTTP method is not supported for this resource</li> <li>422 Unprocessable Entity: The request was well-formed but contains semantic errors</li> <li>429 Too Many Requests: Rate limit exceeded</li> <li>500 Internal Server Error: A generic server error occurred</li> <li>502 Bad Gateway: The server received an invalid response from an upstream server</li> <li>503 Service Unavailable: The server is temporarily unavailable</li> <li>504 Gateway Timeout: The upstream server didn't respond in time</li> <li>511 Network Authentication Required: The client needs to authenticate to gain network access</li> </ul>"},{"location":"http/4-handling-responses/#headers","title":"Headers","text":"<p>Response headers provide metadata about the response. You can access the headers using the <code>headers()</code> method:</p> <pre><code>// @doctest id=\"d2b1\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n$headers = $response-&gt;headers();\n\n// Print all headers\nforeach ($headers as $name =&gt; $values) {\n    echo \"$name: \" . implode(', ', $values) . \"\\n\";\n}\n\n// Access specific headers\n$contentType = $headers['Content-Type'] ?? 'unknown';\n$contentLength = $headers['Content-Length'] ?? 'unknown';\n\necho \"Content-Type: $contentType\\n\";\necho \"Content-Length: $contentLength\\n\";\n</code></pre> <p>The header names are case-insensitive, but the exact format might vary slightly between client implementations. Some clients normalize header names to title case (e.g., <code>Content-Type</code>), while others might use lowercase (e.g., <code>content-type</code>).</p>"},{"location":"http/4-handling-responses/#common-response-headers","title":"Common Response Headers","text":"<p>Here are some common response headers you might encounter:</p> <ul> <li>Content-Type: The MIME type of the response body</li> <li>Content-Length: The size of the response body in bytes</li> <li>Cache-Control: Directives for caching mechanisms</li> <li>Set-Cookie: Cookies to be stored by the client</li> <li>Location: Used for redirects</li> <li>X-RateLimit-Limit: The rate limit for the endpoint</li> <li>X-RateLimit-Remaining: The number of requests remaining in the current rate limit window</li> <li>X-RateLimit-Reset: When the rate limit will reset</li> </ul>"},{"location":"http/4-handling-responses/#body-content","title":"Body Content","text":"<p>For non-streaming responses, you can get the entire response body as a string using the <code>body()</code> method:</p> <pre><code>// @doctest id=\"866e\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\necho \"Response body: $body\\n\";\n</code></pre>"},{"location":"http/4-handling-responses/#processing-json-responses","title":"Processing JSON Responses","text":"<p>Many APIs return JSON responses. You can decode them using PHP's <code>json_decode()</code> function:</p> <pre><code>// @doctest id=\"c8ba\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Decode as associative array\n$data = json_decode($body, true);\n\nif (json_last_error() !== JSON_ERROR_NONE) {\n    echo \"Error decoding JSON: \" . json_last_error_msg() . \"\\n\";\n} else {\n    // Process the data\n    echo \"User ID: {$data['id']}\\n\";\n    echo \"User Name: {$data['name']}\\n\";\n}\n</code></pre>"},{"location":"http/4-handling-responses/#processing-xml-responses","title":"Processing XML Responses","text":"<p>For XML responses, you can use PHP's built-in XML functions:</p> <pre><code>// @doctest id=\"19b5\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Load XML\n$xml = simplexml_load_string($body);\n\nif ($xml === false) {\n    echo \"Error loading XML\\n\";\n} else {\n    // Process the XML\n    echo \"Title: {$xml-&gt;title}\\n\";\n    echo \"Description: {$xml-&gt;description}\\n\";\n}\n</code></pre>"},{"location":"http/4-handling-responses/#processing-binary-responses","title":"Processing Binary Responses","text":"<p>For binary responses (like file downloads), you can save the response body to a file:</p> <pre><code>// @doctest id=\"4b4b\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Save to file\nfile_put_contents('downloaded_file.pdf', $body);\necho \"File downloaded successfully\\n\";\n</code></pre>"},{"location":"http/4-handling-responses/#error-handling","title":"Error Handling","text":"<p>When making HTTP requests, various errors can occur. The Instructor HTTP client API provides a consistent way to handle these errors through exceptions.</p>"},{"location":"http/4-handling-responses/#requestexception","title":"RequestException","text":"<p>The main exception type is <code>RequestException</code>, which is thrown when a request fails:</p> <pre><code>// @doctest id=\"5552\"\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n\n    // You might want to log the error or take other actions\n    if ($e-&gt;getPrevious() !== null) {\n        echo \"Original exception: \" . $e-&gt;getPrevious()-&gt;getMessage() . \"\\n\";\n    }\n}\n</code></pre> <p>The <code>RequestException</code> often wraps another exception from the underlying HTTP client, which you can access with <code>$e-&gt;getPrevious()</code>.</p>"},{"location":"http/4-handling-responses/#error-response-handling","title":"Error Response Handling","text":"<p>By default, HTTP error responses (4xx, 5xx status codes) do not throw exceptions. You can control this behavior using the <code>failOnError</code> configuration option:</p> <pre><code>// @doctest id=\"9b48\"\n// In config/http.php\n'failOnError' =&gt; true, // Throw exceptions for 4xx/5xx responses\n</code></pre> <p>When <code>failOnError</code> is set to <code>true</code>, the client will throw a <code>RequestException</code> for error responses. When it's <code>false</code>, you need to check the status code yourself:</p> <pre><code>// @doctest id=\"1021\"\n$response = $client-&gt;withRequest($request)-&gt;get();\n\nif ($response-&gt;statusCode() &gt;= 400) {\n    // Handle error response\n    echo \"Error: HTTP {$response-&gt;statusCode()}\\n\";\n    echo \"Error details: {$response-&gt;body()}\\n\";\n} else {\n    // Process successful response\n}\n</code></pre>"},{"location":"http/4-handling-responses/#retrying-failed-requests","title":"Retrying Failed Requests","text":"<p>If a request fails, you might want to retry it. Here's a simple implementation of a retry mechanism:</p> <pre><code>// @doctest id=\"865d\"\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\n\nfunction retryRequest($client, $request, $maxRetries = 3, $delay = 1): ?HttpResponse {\n    $attempts = 0;\n\n    while ($attempts &lt; $maxRetries) {\n        try {\n            return $client-&gt;withRequest($request)-&gt;get();\n        } catch (HttpRequestException $e) {\n            $attempts++;\n\n            if ($attempts &gt;= $maxRetries) {\n                throw $e; // Rethrow after all retries failed\n            }\n\n            // Wait before retrying (with exponential backoff)\n            $sleepTime = $delay * pow(2, $attempts - 1);\n            echo \"Request failed, retrying in {$sleepTime} seconds...\\n\";\n            sleep($sleepTime);\n        }\n    }\n\n    return null; // Should never reach here\n}\n\n// Usage\ntry {\n    $response = retryRequest($client, $request);\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"All retry attempts failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>This function will retry failed requests with exponential backoff, meaning it waits longer between each retry attempt.</p> <p>In the next chapter, we'll explore streaming responses, which are particularly useful for handling large responses or real-time data streams.</p>"},{"location":"http/5-streaming-responses/","title":"Streaming Responses","text":"<p>Streaming responses are a powerful feature that allows processing data as it arrives from the server, rather than waiting for the entire response to be received. This is particularly valuable when:</p> <ul> <li>Working with large responses that might exceed memory limits</li> <li>Processing real-time data streams</li> <li>Handling responses from AI models that generate content token by token</li> <li>Building user interfaces that show progressive updates</li> </ul> <p>The Instructor HTTP client API provides robust support for streaming responses across all supported HTTP client implementations.</p>"},{"location":"http/5-streaming-responses/#enabling-streaming","title":"Enabling Streaming","text":"<p>To receive a streaming response, you need to configure the request with the <code>stream</code> option set to <code>true</code>:</p> <pre><code>// @doctest id=\"dc1a\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://api.example.com/stream',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'text/event-stream',\n    ],\n    body: [],\n    options: [\n        'stream' =&gt; true,  // Enable streaming\n    ]\n);\n\n// Or use the withStreaming method on an existing request\n$streamingRequest = $request-&gt;withStreaming(true);\n\n// Create a client and send the request\n$client = new HttpClient();\n$response = $client-&gt;withRequest($streamingRequest)-&gt;get();\n</code></pre> <p>The <code>stream</code> option tells the HTTP client to treat the response as a stream, which means:</p> <ol> <li>It won't buffer the entire response in memory</li> <li>It will provide a way to read the response incrementally</li> <li>The connection will remain open until all data is received or the stream is closed</li> </ol>"},{"location":"http/5-streaming-responses/#processing-streamed-data","title":"Processing Streamed Data","text":"<p>Once you have a streaming response, you can process it using the <code>stream()</code> method, which returns a PHP Generator:</p> <pre><code>// @doctest id=\"0ee8\"\n$response = $client-&gt;withRequest($streamingRequest)-&gt;get();\n\n// Process the stream chunk by chunk\nforeach ($response-&gt;stream() as $chunk) {\n    // Process each chunk of data as it arrives\n    echo \"Received chunk: $chunk\\n\";\n\n    // You could parse JSON chunks, update progress, etc.\n    // If this is a streaming JSON response, you might need to buffer until\n    // you have complete JSON objects\n}\n</code></pre> <p>By default, the <code>stream()</code> method reads the response in small chunks. You can control the chunk size by passing a parameter:</p> <pre><code>// @doctest id=\"a023\"\n// Read in chunks of 1024 bytes\nforeach ($response-&gt;stream(1024) as $chunk) {\n    // Process larger chunks of data\n    echo \"Received chunk of approximately 1KB: $chunk\\n\";\n}\n</code></pre>"},{"location":"http/5-streaming-responses/#example-downloading-a-large-file","title":"Example: Downloading a Large File","text":"<p>Here's an example of downloading a large file with streaming to avoid memory issues:</p> <pre><code>// @doctest id=\"4209\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://example.com/large-file.zip',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$client = new HttpClient();\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Open a file handle to save the file\n    $fileHandle = fopen('downloaded-file.zip', 'wb');\n\n    if (!$fileHandle) {\n        throw new \\RuntimeException(\"Could not open file for writing\");\n    }\n\n    // Keep track of bytes received\n    $totalBytes = 0;\n\n    // Process the stream and write to file\n    foreach ($response-&gt;stream(8192) as $chunk) {\n        fwrite($fileHandle, $chunk);\n        $totalBytes += strlen($chunk);\n\n        // Display progress (if not in a web request)\n        echo \"\\rDownloaded: \" . number_format($totalBytes / 1024 / 1024, 2) . \" MB\";\n    }\n\n    // Close the file handle\n    fclose($fileHandle);\n    echo \"\\nDownload complete!\\n\";\n\n} catch (HttpRequestException $e) {\n    echo \"Download failed: {$e-&gt;getMessage()}\\n\";\n\n    // Clean up if file was partially downloaded\n    if (isset($fileHandle) &amp;&amp; is_resource($fileHandle)) {\n        fclose($fileHandle);\n    }\n    if (file_exists('downloaded-file.zip')) {\n        unlink('downloaded-file.zip');\n    }\n}\n</code></pre> <p>This approach allows downloading very large files without loading the entire file into memory.</p>"},{"location":"http/5-streaming-responses/#example-processing-server-sent-events-sse","title":"Example: Processing Server-Sent Events (SSE)","text":"<p>Server-Sent Events (SSE) are a common streaming format used by many APIs. Here's how to process them:</p> <pre><code>// @doctest id=\"fb21\"\n$request = new HttpRequest(\n    url: 'https://api.example.com/events',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'text/event-stream',\n        'Cache-Control' =&gt; 'no-cache',\n    ],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n$buffer = '';\n\nforeach ($response-&gt;stream() as $chunk) {\n    // Add the chunk to our buffer\n    $buffer .= $chunk;\n\n    // Process complete events (SSE events are separated by double newlines)\n    while (($pos = strpos($buffer, \"\\n\\n\")) !== false) {\n        // Extract and process the event\n        $event = substr($buffer, 0, $pos);\n        $buffer = substr($buffer, $pos + 2);\n\n        // Parse the event (SSE format: \"field: value\")\n        $parsedEvent = [];\n        foreach (explode(\"\\n\", $event) as $line) {\n            if (preg_match('/^([^:]+):\\s*(.*)$/', $line, $matches)) {\n                $field = $matches[1];\n                $value = $matches[2];\n                $parsedEvent[$field] = $value;\n            }\n        }\n\n        // Process the parsed event\n        if (isset($parsedEvent['event'], $parsedEvent['data'])) {\n            $eventType = $parsedEvent['event'];\n            $eventData = $parsedEvent['data'];\n\n            echo \"Received event type: $eventType\\n\";\n            echo \"Event data: $eventData\\n\";\n\n            // You could also parse the data as JSON if appropriate\n            if ($eventType === 'update') {\n                $data = json_decode($eventData, true);\n                if ($data) {\n                    echo \"Processed update: {$data['message']}\\n\";\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>While this works, processing streaming responses line by line is common enough that the library provides a dedicated middleware for it, as we'll see in the next section.</p>"},{"location":"http/5-streaming-responses/#line-by-line-processing","title":"Line-by-Line Processing","text":"<p>For many streaming APIs, especially those that send event streams or line-delimited JSON, it's useful to process the response line by line. The library provides the <code>StreamByLineMiddleware</code> to simplify this task:</p> <pre><code>// @doctest id=\"7520\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Middleware\\StreamByLine\\StreamByLineMiddleware;\n\n// Create a client with the StreamByLineMiddleware\n$client = new HttpClient();\n$client-&gt;withMiddleware(new StreamByLineMiddleware());\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://api.example.com/events',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Process the stream line by line\nforeach ($response-&gt;stream() as $line) {\n    // Each $line is a complete line from the response\n    echo \"Received line: $line\\n\";\n\n    // Parse the line (e.g., as JSON)\n    $event = json_decode($line, true);\n    if ($event) {\n        // Process the event\n        echo \"Event type: {$event['type']}\\n\";\n    }\n}\n</code></pre>"},{"location":"http/5-streaming-responses/#customizing-line-processing","title":"Customizing Line Processing","text":"<p>You can customize how lines are parsed by providing a parser function to the middleware:</p> <pre><code>// @doctest id=\"d465\"\n$lineParser = function (string $line) {\n    // Pre-process each line before yielding it\n    $trimmedLine = trim($line);\n    if (empty($trimmedLine)) {\n        return null; // Skip empty lines\n    }\n    return $trimmedLine;\n};\n\n$client-&gt;withMiddleware(new StreamByLineMiddleware($lineParser));\n</code></pre> <p>If your parser returns <code>null</code>, that line will be skipped in the stream.</p>"},{"location":"http/5-streaming-responses/#example-processing-openai-chat-completions","title":"Example: Processing OpenAI Chat Completions","text":"<p>Here's a practical example of using the <code>StreamByLineMiddleware</code> to process streaming responses from the OpenAI API:</p> <pre><code>// @doctest id=\"44ee\"\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Middleware\\StreamByLine\\StreamByLineMiddleware;\n\n// OpenAI API requires a parser that handles their SSE format\n$openAiParser = function (string $line) {\n    // Skip empty lines\n    if (trim($line) === '') {\n        return null;\n    }\n\n    // Remove \"data: \" prefix from each line\n    if (strpos($line, 'data: ') === 0) {\n        $line = substr($line, 6);\n\n        // Skip the \"[DONE]\" message\n        if ($line === '[DONE]') {\n            return null;\n        }\n\n        // Return the parsed line\n        return $line;\n    }\n\n    return null; // Skip non-data lines\n};\n\n// Create a client with the StreamByLineMiddleware\n$client = new HttpClient('guzzle'); // Use Guzzle for better streaming support\n$client-&gt;withMiddleware(new StreamByLineMiddleware($openAiParser));\n\n// Create a request to OpenAI API\n$request = new HttpRequest(\n    url: 'https://api.openai.com/v1/chat/completions',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiKey,\n    ],\n    body: [\n        'model' =&gt; 'gpt-3.5-turbo',\n        'messages' =&gt; [\n            ['role' =&gt; 'user', 'content' =&gt; 'Write a short poem about coding.'],\n        ],\n        'stream' =&gt; true,\n    ],\n    options: ['stream' =&gt; true]\n);\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    $fullResponse = '';\n\n    // Process the streaming response\n    foreach ($response-&gt;stream() as $chunk) {\n        // Parse the chunk as JSON\n        $data = json_decode($chunk, true);\n\n        if ($data &amp;&amp; isset($data['choices'][0]['delta']['content'])) {\n            $content = $data['choices'][0]['delta']['content'];\n            $fullResponse .= $content;\n\n            // Print each piece as it arrives\n            echo $content;\n            flush(); // Ensure output is sent immediately\n        }\n    }\n\n    echo \"\\n\\nFull response:\\n$fullResponse\\n\";\n\n} catch (Exception $e) {\n    echo \"Error: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>This approach allows you to display the AI-generated content to the user in real-time as it's being generated, providing a more responsive user experience.</p>"},{"location":"http/5-streaming-responses/#considerations-for-streaming","title":"Considerations for Streaming","text":"<p>When working with streaming responses, keep these considerations in mind:</p> <ol> <li> <p>Memory Usage: While streaming reduces memory usage overall, be careful not to accumulate the entire response in memory by appending to a variable unless necessary.</p> </li> <li> <p>Connection Stability: Streaming connections can be more sensitive to network issues. Consider implementing error handling and retry logic for more robust applications.</p> </li> <li> <p>Server Timeouts: Some servers or proxies might timeout long-running connections. Make sure your infrastructure is configured to allow the necessary connection times.</p> </li> <li> <p>Middleware Order: When using middleware that processes streaming responses, the order of middleware can be important. Middleware is executed in the order it's added to the stack.</p> </li> </ol> <p>In the next chapter, we'll explore how to make multiple concurrent requests using request pools, which can significantly improve performance when fetching data from multiple endpoints.</p>"},{"location":"http/7-changing-client/","title":"Changing the Underlying Client","text":"<p>One of the core features of the Instructor HTTP client API is its ability to seamlessly switch between different HTTP client implementations. This flexibility allows you to use the same code across different environments or to choose the most appropriate client for specific use cases.</p>"},{"location":"http/7-changing-client/#available-client-drivers","title":"Available Client Drivers","text":"<p>The library includes several built-in drivers that adapt various HTTP client libraries to the unified interface used by Instructor:</p>"},{"location":"http/7-changing-client/#guzzledriver","title":"GuzzleDriver","text":"<p>The <code>GuzzleDriver</code> provides integration with the popular Guzzle HTTP client.</p> <p>Key Features: - Robust feature set - Excellent performance - Extensive middleware ecosystem - Support for HTTP/2 (via cURL) - Stream and promise-based API</p> <p>Best For: - General-purpose HTTP requests - Applications that need advanced features - Projects without framework constraints</p> <p>Requirements: - Requires the <code>guzzlehttp/guzzle</code> package (<code>composer require guzzlehttp/guzzle</code>)</p>"},{"location":"http/7-changing-client/#symfonydriver","title":"SymfonyDriver","text":"<p>The <code>SymfonyDriver</code> integrates with the Symfony HTTP Client.</p> <p>Key Features: - Native HTTP/2 support - Automatic content-type detection - Built-in profiling and logging - No dependency on cURL - Support for various transports (native PHP, cURL, amphp)</p> <p>Best For: - Symfony applications - Projects requiring HTTP/2 support - Low-dependency environments</p> <p>Requirements: - Requires the <code>symfony/http-client</code> package (<code>composer require symfony/http-client</code>)</p>"},{"location":"http/7-changing-client/#laraveldriver","title":"LaravelDriver","text":"<p>The <code>LaravelDriver</code> integrates with the Laravel HTTP Client.</p> <p>Key Features: - Elegant, fluent syntax - Integration with Laravel ecosystem - Built-in macros and testing utilities - Automatic JSON handling - Rate limiting and retry capabilities</p> <p>Best For: - Laravel applications - Projects already using the Laravel framework</p> <p>Requirements: - Included with the Laravel framework</p>"},{"location":"http/7-changing-client/#mockhttpdriver","title":"MockHttpDriver","text":"<p>The <code>MockHttpDriver</code> is a test double that doesn't make actual HTTP requests but returns predefined responses.</p> <p>Key Features: - No actual network requests - Predefined responses for testing - Response matching based on URL, method, and body - Support for custom response generation</p> <p>Best For: - Unit testing - Offline development - CI/CD environments</p>"},{"location":"http/7-changing-client/#switching-between-clients","title":"Switching Between Clients","text":"<p>You can switch between the available client implementations in several ways:</p>"},{"location":"http/7-changing-client/#when-creating-the-client","title":"When Creating the Client","text":"<p>The simplest approach is to specify the client when creating the <code>HttpClient</code> instance:</p> <pre><code>// @doctest id=\"4336\"\n// Use Guzzle (assuming it's configured in config/http.php)\n$guzzleClient = HttpClient::using('guzzle');\n\n// Use Symfony\n$symfonyClient = HttpClient::using('symfony');\n\n// Use Laravel\n$laravelClient = HttpClient::using('laravel');\n</code></pre> <p>The client name must correspond to a configuration entry in your <code>config/http.php</code> file.</p>"},{"location":"http/7-changing-client/#using-the-default-client","title":"Using the Default Client","text":"<p>If you don't specify a client, the default one from your configuration will be used:</p> <pre><code>// @doctest id=\"8112\"\n// Uses the default client specified in config/http.php\n$client = HttpClient::default();\n</code></pre> <p>The default client is specified in the <code>config/http.php</code> file:</p> <pre><code>// @doctest id=\"933b\"\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        // Client configurations...\n    ],\n];\n</code></pre>"},{"location":"http/7-changing-client/#switching-at-runtime","title":"Switching at Runtime","text":"<p>You can create different clients for different requirements within the same application:</p> <pre><code>// @doctest id=\"2c4a\"\n// Create clients with different configurations\n$defaultClient = HttpClient::default();\n$symfonyClient = HttpClient::using('symfony');\n$laravelClient = HttpClient::using('laravel');\n$customClient = HttpClient::using('http-ollama');\n</code></pre> <p>Note: HttpClient instances are immutable, so you create new instances rather than switching configurations at runtime.</p>"},{"location":"http/7-changing-client/#using-the-static-make-method","title":"Using the Static Make Method","text":"<p>The <code>HttpClient</code> class provides a static <code>make</code> method as an alternative to the constructor:</p> <pre><code>// @doctest id=\"419c\"\nuse Cognesy\\Http\\HttpClientBuilder;\n\n// Create with specific client using builder\n$client = (new HttpClientBuilder())-&gt;withPreset('guzzle')-&gt;create();\n// Equivalent to:\n$client = HttpClient::using('guzzle');\n\n// Create with default client and custom event dispatcher\n$events = new EventDispatcher();\n$client = (new HttpClientBuilder())-&gt;withEventBus($events)-&gt;create();\n</code></pre>"},{"location":"http/7-changing-client/#client-specific-configuration","title":"Client-Specific Configuration","text":"<p>Each client type can have its own configuration in the <code>config/http.php</code> file:</p> <pre><code>// @doctest id=\"8185\"\n&lt;?php\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre>"},{"location":"http/7-changing-client/#multiple-configurations-for-the-same-client-type","title":"Multiple Configurations for the Same Client Type","text":"<p>You can define multiple configurations for the same client type, each with different settings:</p> <pre><code>// @doctest id=\"0c99\"\n'clients' =&gt; [\n    'guzzle' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 3,\n        'requestTimeout' =&gt; 30,\n        // Default settings for Guzzle\n    ],\n    'guzzle-short-timeout' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 1,\n        'requestTimeout' =&gt; 5,\n        // Short timeouts for quick operations\n    ],\n    'guzzle-long-timeout' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 5,\n        'requestTimeout' =&gt; 120,\n        // Long timeouts for operations that take time\n    ],\n    'guzzle-streaming' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 3,\n        'requestTimeout' =&gt; 300,\n        'idleTimeout' =&gt; 60,\n        // Optimized for streaming responses\n    ],\n    'http-ollama' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 1,\n        'requestTimeout' =&gt; 90, // Longer timeout for AI model inference\n        'idleTimeout' =&gt; -1,\n        'maxConcurrent' =&gt; 5,\n        'poolTimeout' =&gt; 60,\n        'failOnError' =&gt; true,\n    ],\n],\n</code></pre> <p>Then you can select the appropriate configuration based on your needs:</p> <pre><code>// @doctest id=\"58f7\"\n// For quick API calls\n$quickClient = HttpClient::using('guzzle-short-timeout');\n\n// For long-running operations\n$longClient = HttpClient::using('guzzle-long-timeout');\n\n// For streaming responses\n$streamingClient = HttpClient::using('guzzle-streaming');\n\n// For AI model requests\n$aiClient = HttpClient::using('http-ollama');\n</code></pre>"},{"location":"http/7-changing-client/#common-configuration-parameters","title":"Common Configuration Parameters","text":"<p>All client types support these common configuration parameters:</p> Parameter Type Description <code>httpClientType</code> string The type of HTTP client (Guzzle, Symfony, Laravel) <code>connectTimeout</code> int Maximum time to wait for connection establishment (seconds) <code>requestTimeout</code> int Maximum time to wait for the entire request (seconds) <code>idleTimeout</code> int Maximum time to wait between data packets (seconds, -1 for no timeout) <code>maxConcurrent</code> int Maximum number of concurrent requests in a pool <code>poolTimeout</code> int Maximum time to wait for all pooled requests (seconds) <code>failOnError</code> bool Whether to throw exceptions for HTTP error responses"},{"location":"http/7-changing-client/#client-specific-parameters","title":"Client-Specific Parameters","text":"<p>Some parameters might only be relevant to specific client implementations. For example, Guzzle supports additional options like <code>verify</code> (for SSL verification) or <code>proxy</code> settings that can be passed through the underlying client.</p>"},{"location":"http/7-changing-client/#example-choosing-the-right-client-for-different-scenarios","title":"Example: Choosing the Right Client for Different Scenarios","text":"<p>Here's an example of selecting different client configurations based on the task:</p> <pre><code>// @doctest id=\"cff0\"\n&lt;?php\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction fetchApiData($url, $apiKey) {\n    // Use a client with short timeouts for quick API calls\n    $client = new HttpClient('guzzle-short-timeout');\n\n    $request = new HttpRequest(\n        url: $url,\n        method: 'GET',\n        headers: [\n            'Authorization' =&gt; 'Bearer ' . $apiKey,\n            'Accept' =&gt; 'application/json',\n        ],\n        body: [],\n        options: []\n    );\n\n    try {\n        return $client-&gt;withRequest($request)-&gt;get();\n    } catch (HttpRequestException $e) {\n        // Handle error\n        throw $e;\n    }\n}\n\nfunction downloadLargeFile($url, $outputPath) {\n    // Use a client with long timeouts for downloading large files\n    $client = new HttpClient('guzzle-long-timeout');\n\n    $request = new HttpRequest(\n        url: $url,\n        method: 'GET',\n        headers: [],\n        body: [],\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $response = $client-&gt;withRequest($request)-&gt;get();\n\n        $fileHandle = fopen($outputPath, 'wb');\n        foreach ($response-&gt;stream(8192) as $chunk) {\n            fwrite($fileHandle, $chunk);\n        }\n        fclose($fileHandle);\n\n        return true;\n    } catch (HttpRequestException $e) {\n        // Handle error\n        if (file_exists($outputPath)) {\n            unlink($outputPath); // Remove partial file\n        }\n        throw $e;\n    }\n}\n\nfunction generateAiResponse($prompt) {\n    // Use a specialized client for AI API requests\n    $client = new HttpClient('http-ollama');\n\n    $request = new HttpRequest(\n        url: 'https://api.example.com/ai/generate',\n        method: 'POST',\n        headers: [\n            'Content-Type' =&gt; 'application/json',\n            'Accept' =&gt; 'application/json',\n        ],\n        body: [\n            'prompt' =&gt; $prompt,\n            'max_tokens' =&gt; 500,\n        ],\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $response = $client-&gt;withRequest($request)-&gt;get();\n\n        $result = '';\n        foreach ($response-&gt;stream() as $chunk) {\n            $result .= $chunk;\n        }\n\n        return json_decode($result, true);\n    } catch (HttpRequestException $e) {\n        // Handle error\n        throw $e;\n    }\n}\n</code></pre>"},{"location":"http/7-changing-client/#considerations-for-switching-clients","title":"Considerations for Switching Clients","text":"<p>When switching between different HTTP client implementations, keep these considerations in mind:</p> <ol> <li> <p>Configuration Consistency: Ensure that all client configurations have the appropriate settings for your application's needs.</p> </li> <li> <p>Feature Availability: Some advanced features might be available only in specific clients. For example, HTTP/2 support might be better in one client than another.</p> </li> <li> <p>Error Handling: Different clients might have slightly different error behavior. Instructor HTTP client API normalizes much of this, but edge cases can still occur.</p> </li> <li> <p>Middleware Compatibility: If you're using middleware, ensure it's compatible with all client types you plan to use.</p> </li> <li> <p>Performance Characteristics: Different clients may have different performance profiles for specific scenarios. Test with your actual workload if performance is critical.</p> </li> </ol> <p>In the next chapter, we'll explore how to customize client configurations in more detail, including runtime configuration and advanced options.</p>"},{"location":"http/8-changing-client-config/","title":"Customizing Client Configuration","text":"<p>The Instructor HTTP client API offers extensive configuration options to customize client behavior for different scenarios. This chapter explores how to configure clients through configuration files and at runtime.</p>"},{"location":"http/8-changing-client-config/#configuration-files","title":"Configuration Files","text":"<p>The primary configuration files for the HTTP client are:</p>"},{"location":"http/8-changing-client-config/#main-configuration-confighttpphp","title":"Main Configuration: config/http.php","text":"<p>This file defines the available client types and their settings:</p> <pre><code>// @doctest id=\"32b4\"\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre>"},{"location":"http/8-changing-client-config/#debug-configuration-configdebugphp","title":"Debug Configuration: config/debug.php","text":"<p>This file controls debugging options for HTTP requests and responses:</p> <pre><code>// @doctest id=\"42dc\"\nreturn [\n    'http' =&gt; [\n        'enabled' =&gt; false, // enable/disable debug\n        'trace' =&gt; false, // dump HTTP trace information\n        'requestUrl' =&gt; true, // dump request URL to console\n        'requestHeaders' =&gt; true, // dump request headers to console\n        'requestBody' =&gt; true, // dump request body to console\n        'responseHeaders' =&gt; true, // dump response headers to console\n        'responseBody' =&gt; true, // dump response body to console\n        'responseStream' =&gt; true, // dump stream data to console\n        'responseStreamByLine' =&gt; true, // dump stream as full lines or raw chunks\n    ],\n];\n</code></pre>"},{"location":"http/8-changing-client-config/#loading-configuration-files","title":"Loading Configuration Files","text":"<p>The library uses a settings management system to load these configurations. The system looks for these files in the base directory of your project. If you're using a framework like Laravel or Symfony, you can integrate with their configuration systems instead.</p> <p>For Laravel, you might publish these configurations as Laravel config files:</p> <pre><code># @doctest id=\"a9c9\"\nphp artisan vendor:publish --tag=polyglot-config\n</code></pre> <p>For Symfony, you might define these as service parameters in your service configuration.</p>"},{"location":"http/8-changing-client-config/#configuration-options","title":"Configuration Options","text":"<p>The <code>HttpClientConfig</code> class encapsulates the configuration options for HTTP clients. Here's a detailed breakdown of the available options:</p>"},{"location":"http/8-changing-client-config/#basic-connection-options","title":"Basic Connection Options","text":"Option Type Description Default <code>httpClientType</code> string Type of HTTP client to use <code>'guzzle'</code> <code>connectTimeout</code> int Connection timeout in seconds 3 <code>requestTimeout</code> int Request timeout in seconds 30 <code>idleTimeout</code> int Idle timeout in seconds (-1 for no timeout) -1"},{"location":"http/8-changing-client-config/#request-pool-options","title":"Request Pool Options","text":"Option Type Description Default <code>maxConcurrent</code> int Maximum number of concurrent requests in a pool 5 <code>poolTimeout</code> int Timeout for the entire request pool in seconds 60"},{"location":"http/8-changing-client-config/#error-handling-options","title":"Error Handling Options","text":"Option Type Description Default <code>failOnError</code> bool Whether to throw exceptions on HTTP errors true"},{"location":"http/8-changing-client-config/#debug-options","title":"Debug Options","text":"Option Type Description Default <code>enabled</code> bool Enable or disable HTTP debugging false <code>trace</code> bool Dump HTTP trace information false <code>requestUrl</code> bool Log the request URL true <code>requestHeaders</code> bool Log request headers true <code>requestBody</code> bool Log request body true <code>responseHeaders</code> bool Log response headers true <code>responseBody</code> bool Log response body true <code>responseStream</code> bool Log streaming response data true <code>responseStreamByLine</code> bool Log stream as complete lines (true) or raw chunks (false) true"},{"location":"http/8-changing-client-config/#understanding-timeout-options","title":"Understanding Timeout Options","text":"<p>Timeout settings are crucial for controlling how your application handles slow or unresponsive servers:</p> <ul> <li> <p>connectTimeout: Maximum time to wait for establishing a connection to the server. If the server doesn't respond within this time, the request fails with a connection timeout error. Setting this too low might cause failures when connecting to slow servers, but setting it too high could leave your application waiting for unresponsive servers.</p> </li> <li> <p>requestTimeout: Maximum time to wait for the entire request to complete, from connection initiation to receiving the complete response. If the entire request-response cycle isn't completed within this time, the request fails with a timeout error.</p> </li> <li> <p>idleTimeout: Maximum time to wait between receiving data packets. If the server stops sending data for longer than this period, the connection is considered idle and is terminated. Setting this to -1 disables the idle timeout, which is useful for long-running streaming connections.</p> </li> <li> <p>poolTimeout: Maximum time to wait for all requests in a pool to complete. If any requests in the pool haven't completed within this time, they're terminated.</p> </li> </ul>"},{"location":"http/8-changing-client-config/#runtime-configuration","title":"Runtime Configuration","text":"<p>While configuration files provide a static way to configure clients, you often need to change configuration at runtime based on the specific requirements of a request or operation.</p>"},{"location":"http/8-changing-client-config/#using-withclient","title":"Using withClient","text":"<p>The simplest way to switch configurations at runtime is to use the <code>withClient</code> method to select a different pre-configured client:</p> <pre><code>// @doctest id=\"5192\"\n// Start with default client\n$client = new HttpClient();\n\n// Switch to a client with longer timeouts\n$client-&gt;withClient('guzzle-long-timeout');\n\n// Switch to a client optimized for streaming\n$client-&gt;withClient('guzzle-streaming');\n</code></pre>"},{"location":"http/8-changing-client-config/#using-withconfig","title":"Using withConfig","text":"<p>For more dynamic configuration, you can create a custom <code>HttpClientConfig</code> object and apply it using the <code>withConfig</code> method:</p> <pre><code>// @doctest id=\"6c0c\"\nuse Cognesy\\Http\\Config\\HttpClientConfig;\n\n// Create a custom configuration\n$config = new HttpClientConfig(\n    driver: 'guzzle',\n    connectTimeout: 5,\n    requestTimeout: 60,\n    idleTimeout: 30,\n    maxConcurrent: 10,\n    poolTimeout: 120,\n    failOnError: false\n);\n\n// Use the custom configuration\n$client-&gt;withConfig($config);\n</code></pre> <p>This method gives you complete control over the configuration at runtime.</p>"},{"location":"http/8-changing-client-config/#creating-configuration-from-an-array","title":"Creating Configuration from an Array","text":"<p>You can also create a configuration from an associative array:</p> <pre><code>// @doctest id=\"06dc\"\n$configArray = [\n    'httpClientType' =&gt; 'symfony',\n    'connectTimeout' =&gt; 2,\n    'requestTimeout' =&gt; 45,\n    'idleTimeout' =&gt; -1,\n    'maxConcurrent' =&gt; 8,\n    'poolTimeout' =&gt; 90,\n    'failOnError' =&gt; true,\n];\n\n$config = HttpClientConfig::fromArray($configArray);\n$client-&gt;withConfig($config);\n</code></pre> <p>This approach is useful when loading configuration from external sources like environment variables or configuration files.</p>"},{"location":"http/8-changing-client-config/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>You can enable debug mode to see detailed information about requests and responses:</p> <pre><code>// @doctest id=\"c8bb\"\n// Enable debug mode\n$client-&gt;withDebugPreset('on');\n\n// Make a request\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Disable debug mode when done\n$client-&gt;withDebugPreset('off');\n</code></pre> <p>When debug mode is enabled, detailed information about requests and responses is output to the console or log.</p>"},{"location":"http/8-changing-client-config/#example-dynamic-configuration-based-on-request-type","title":"Example: Dynamic Configuration Based on Request Type","text":"<p>Here's an example of dynamically adjusting configuration based on the type of request:</p> <pre><code>// @doctest id=\"d8e0\"\nfunction configureClientForRequest(HttpClient $client, HttpRequest $request): HttpClient {\n    // Get the current configuration\n    $config = HttpClientConfig::load($client-&gt;getClientName());\n\n    // Adjust timeouts based on the request URL\n    if (strpos($request-&gt;url(), 'large-file') !== false) {\n        // For large file downloads, use longer timeouts\n        $config = new HttpClientConfig(\n            httpClientType: $config-&gt;httpClientType,\n            connectTimeout: $config-&gt;connectTimeout,\n            requestTimeout: 300, // 5 minutes\n            idleTimeout: 60,     // 1 minute\n            maxConcurrent: $config-&gt;maxConcurrent,\n            poolTimeout: $config-&gt;poolTimeout,\n            failOnError: $config-&gt;failOnError\n        );\n\n        $client-&gt;withConfig($config);\n    }\n\n    // Enable streaming for specific endpoints\n    if (strpos($request-&gt;url(), '/stream') !== false || strpos($request-&gt;url(), '/events') !== false) {\n        // Make sure the request is set to stream\n        $request = $request-&gt;withStreaming(true);\n    }\n\n    // Enable debug for development environment\n    if (getenv('APP_ENV') === 'development') {\n        $client-&gt;withDebugPreset('on');\n    }\n\n    return $client;\n}\n\n// Usage\n$client = new HttpClient();\n$request = new HttpRequest(...);\n\n// Configure the client based on the request\n$client = configureClientForRequest($client, $request);\n\n// Send the request\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>This approach allows for highly dynamic and contextual configuration adjustments.</p>"},{"location":"http/8-changing-client-config/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li> <p>Define Base Configurations in Files: Keep your common configurations in the <code>config/http.php</code> file for easy reference and maintenance.</p> </li> <li> <p>Use Named Configurations: Create named configurations for different scenarios (e.g., <code>'guzzle-short-timeout'</code>, <code>'guzzle-streaming'</code>) to make your code more readable and maintainable.</p> </li> <li> <p>Adjust Timeouts Appropriately: Set timeouts based on the expected response time of the API or service you're calling. Shorter for quick operations, longer for file uploads/downloads or streaming.</p> </li> <li> <p>Consider Error Handling Strategy: Set <code>failOnError</code> based on how you want to handle errors. For critical operations, set it to <code>true</code> to catch errors immediately. For bulk operations or request pools, set it to <code>false</code> to handle errors individually.</p> </li> <li> <p>Use Debug Mode Judiciously: Enable debug mode only when needed, as it can generate a lot of output and potentially impact performance.</p> </li> <li> <p>Test Different Configurations: Experiment with different settings to find the optimal configuration for your specific use cases.</p> </li> </ol>"},{"location":"http/8-changing-client-config/#adapting-to-different-environments","title":"Adapting to Different Environments","text":"<p>Different environments often require different configurations. Here's how you might handle this:</p> <pre><code>// @doctest id=\"fa48\"\n// In your application bootstrap or service provider\nfunction configureHttpClient() {\n    $env = getenv('APP_ENV') ?: 'production';\n\n    // Load base configuration\n    $config = HttpClientConfig::load('guzzle');\n\n    // Adjust based on environment\n    switch ($env) {\n        case 'development':\n            // Shorter timeouts for faster feedback during development\n            $config = new HttpClientConfig(\n                httpClientType: $config-&gt;httpClientType,\n                connectTimeout: 1,\n                requestTimeout: 10,\n                idleTimeout: $config-&gt;idleTimeout,\n                maxConcurrent: $config-&gt;maxConcurrent,\n                poolTimeout: $config-&gt;poolTimeout,\n                failOnError: true // Throw errors for immediate feedback\n            );\n            break;\n\n        case 'testing':\n            // Use mock driver for tests\n            $config = new HttpClientConfig(\n                httpClientType: 'custom',\n                connectTimeout: 1,\n                requestTimeout: 1,\n                idleTimeout: 1,\n                maxConcurrent: 1,\n                poolTimeout: 5,\n                failOnError: true\n            );\n\n            // Create a mock client\n            $mockDriver = new MockHttpDriver();\n            // Configure mock responses...\n\n            return (new HttpClient())-&gt;withConfig($config)-&gt;withDriver($mockDriver);\n\n        case 'production':\n            // More conservative timeouts for production\n            $config = new HttpClientConfig(\n                httpClientType: $config-&gt;httpClientType,\n                connectTimeout: 5,\n                requestTimeout: 60,\n                idleTimeout: $config-&gt;idleTimeout,\n                maxConcurrent: 10,\n                poolTimeout: 120,\n                failOnError: false // Handle errors gracefully in production\n            );\n            break;\n    }\n\n    return (new HttpClient())-&gt;withConfig($config);\n}\n\n// Get a properly configured client\n$client = configureHttpClient();\n</code></pre> <p>This approach allows you to adapt your HTTP client configuration to different environments while maintaining a consistent API.</p> <p>In the next chapter, we'll explore how to create and use custom HTTP client implementations for specialized needs.</p>"},{"location":"http/9-1-custom-clients/","title":"Using Custom HTTP Clients","text":"<p>While the Instructor HTTP client API provides built-in support for popular HTTP client libraries (Guzzle, Symfony, and Laravel), there may be cases where you need to integrate with other HTTP client libraries or create specialized implementations. This chapter covers how to create and use custom HTTP client drivers.</p>"},{"location":"http/9-1-custom-clients/#creating-custom-http-client-drivers","title":"Creating Custom HTTP Client Drivers","text":"<p>Creating a custom HTTP client driver involves implementing the <code>CanHandleHttpRequest</code> interface and optionally the <code>CanHandleRequestPool</code> interface for pool support.</p>"},{"location":"http/9-1-custom-clients/#implementing-the-canhandlehttprequest-interface","title":"Implementing the CanHandleHttpRequest Interface","text":"<p>The <code>CanHandleHttpRequest</code> interface requires implementing a single method:</p> <pre><code>// @doctest id=\"1d88\"\ninterface CanHandleHttpRequest\n{\n    public function handle(HttpClientRequest $request): HttpResponse;\n}\n</code></pre> <p>Here's a template for creating a custom HTTP client driver:</p> <pre><code>// @doctest id=\"dea4\"\nnamespace YourNamespace\\Http\\Drivers;\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse Exception;\n\nclass CustomHttpDriver implements CanHandleHttpRequest\n{\n    /**\n     * Your custom HTTP client instance\n     */\n    private $yourHttpClient;\n\n    /**\n     * Constructor\n     */\n    public function __construct(\n        protected HttpClientConfig $config,\n        protected ?EventDispatcher $events = null,\n    ) {\n        $this-&gt;events = $events ?? new EventDispatcher();\n\n        // Initialize your HTTP client with the configuration\n        $this-&gt;yourHttpClient = $this-&gt;createYourHttpClient();\n    }\n\n    /**\n     * Handle an HTTP request\n     */\n    public function handle(HttpRequest $request): HttpResponse\n    {\n        $url = $request-&gt;url();\n        $headers = $request-&gt;headers();\n        $body = $request-&gt;body()-&gt;toString();\n        $method = $request-&gt;method();\n        $streaming = $request-&gt;isStreamed();\n\n        // Dispatch event before sending request\n        $this-&gt;events-&gt;dispatch(new HttpRequestSent([\n            'url' =&gt; $url,\n            'method' =&gt; $method,\n            'headers' =&gt; $headers,\n            'body' =&gt; $request-&gt;body()-&gt;toArray(),\n        ]));\n\n        try {\n            // Use your HTTP client to make the request\n            $response = $this-&gt;yourHttpClient-&gt;send($method, $url, [\n                'headers' =&gt; $headers,\n                'body' =&gt; $body,\n                'timeout' =&gt; $this-&gt;config-&gt;requestTimeout,\n                'connect_timeout' =&gt; $this-&gt;config-&gt;connectTimeout,\n                'stream' =&gt; $streaming,\n                // Other options relevant to your client...\n            ]);\n\n            // Dispatch event for successful response\n            $this-&gt;events-&gt;dispatch(new HttpResponseReceived([\n                'statusCode' =&gt; $response-&gt;statusCode()\n            ]));\n\n            // Return the response wrapped in your adapter\n            return new YourHttpResponse($response, $streaming);\n\n        } catch (Exception $e) {\n            // Dispatch event for failed request\n            $this-&gt;events-&gt;dispatch(new HttpRequestFailed([\n                'url' =&gt; $url,\n                'method' =&gt; $method,\n                'headers' =&gt; $headers,\n                'body' =&gt; $request-&gt;body()-&gt;toArray(),\n                'errors' =&gt; $e-&gt;getMessage(),\n            ]));\n\n            // Wrap the exception\n            throw new HttpRequestException($e);\n        }\n    }\n\n    /**\n     * Create your HTTP client instance\n     */\n    private function createYourHttpClient()\n    {\n        // Initialize your HTTP client with appropriate configuration\n        return new YourHttpClient([\n            'connect_timeout' =&gt; $this-&gt;config-&gt;connectTimeout,\n            'timeout' =&gt; $this-&gt;config-&gt;requestTimeout,\n            'idle_timeout' =&gt; $this-&gt;config-&gt;idleTimeout,\n            // Other options...\n        ]);\n    }\n}\n</code></pre>"},{"location":"http/9-1-custom-clients/#creating-a-response-adapter","title":"Creating a Response Adapter","text":"<p>You also need to create a response adapter that implements the <code>HttpResponse</code> interface:</p> <pre><code>// @doctest id=\"525b\"\nnamespace YourNamespace\\Http\\Adapters;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Generator;\n\nclass YourHttpResponse implements HttpResponse\n{\n    /**\n     * Constructor\n     */\n    public function __construct(\n        private $yourResponse,\n        private bool $streaming = false\n    ) {}\n\n    /**\n     * Get the response status code\n     */\n    public function statusCode(): int\n    {\n        return $this-&gt;yourResponse-&gt;getStatusCode();\n    }\n\n    /**\n     * Get the response headers\n     */\n    public function headers(): array\n    {\n        return $this-&gt;yourResponse-&gt;getHeaders();\n    }\n\n    /**\n     * Get the response body\n     */\n    public function body(): string\n    {\n        return $this-&gt;yourResponse-&gt;getBody();\n    }\n\n    /**\n     * Stream the response body\n     */\n    public function stream(int $chunkSize = 1): Generator\n    {\n        if (!$this-&gt;streaming) {\n            // For non-streaming responses, just yield the entire body\n            yield $this-&gt;body();\n            return;\n        }\n\n        // For streaming responses, yield chunks\n        $stream = $this-&gt;yourResponse-&gt;getStream();\n\n        while (!$stream-&gt;eof()) {\n            yield $stream-&gt;read($chunkSize);\n        }\n    }\n}\n</code></pre>"},{"location":"http/9-1-custom-clients/#using-your-custom-http-client-driver","title":"Using Your Custom HTTP Client Driver","text":"<p>Once you've implemented your custom driver, you can use it with the <code>HttpClient</code>:</p> <pre><code>// @doctest id=\"8306\"\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\HttpClient;\nuse YourNamespace\\Http\\Drivers\\CustomHttpDriver;\n\n// Create a configuration for your custom driver\n$config = new HttpClientConfig(\n    driver: 'custom',\n    connectTimeout: 3,\n    requestTimeout: 30,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true\n);\n\n// Create your custom driver\n$customDriver = new CustomHttpDriver($config);\n\n// Create a client with your driver using the builder\n$client = (new HttpClientBuilder())-&gt;withDriver($customDriver)-&gt;create();\n\n// Use the client as usual\n$response = $client-&gt;withRequest(new HttpRequest(/* ... */))-&gt;get();\n</code></pre>"},{"location":"http/9-1-custom-clients/#real-world-example-creating-a-curl-driver","title":"Real-World Example: Creating a cURL Driver","text":"<p>Here's a practical example of implementing a custom driver using PHP's cURL extension directly:</p> <pre><code>// @doctest id=\"9a23\"\nnamespace YourNamespace\\Http\\Drivers;\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse YourNamespace\\Http\\Adapters\\CurlHttpResponse;\n\nclass CurlHttpDriver implements CanHandleHttpRequest\n{\n    /**\n     * Constructor\n     */\n    public function __construct(\n        protected HttpClientConfig $config,\n        protected ?EventDispatcher $events = null,\n    ) {\n        $this-&gt;events = $events ?? new EventDispatcher();\n    }\n\n    /**\n     * Handle an HTTP request\n     */\n    public function handle(HttpRequest $request): HttpResponse\n    {\n        $url = $request-&gt;url();\n        $headers = $request-&gt;headers();\n        $body = $request-&gt;body()-&gt;toString();\n        $method = $request-&gt;method();\n        $streaming = $request-&gt;isStreamed();\n\n        // Dispatch event before sending request\n        $this-&gt;events-&gt;dispatch(new HttpRequestSent(\n            $url,\n            $method,\n            $headers,\n            $request-&gt;body()-&gt;toArray()\n        ));\n\n        try {\n            // Initialize cURL\n            $ch = curl_init();\n\n            // Format headers for cURL\n            $curlHeaders = [];\n            foreach ($headers as $name =&gt; $value) {\n                if (is_array($value)) {\n                    foreach ($value as $v) {\n                        $curlHeaders[] = \"{$name}: {$v}\";\n                    }\n                } else {\n                    $curlHeaders[] = \"{$name}: {$value}\";\n                }\n            }\n\n            // Set cURL options\n            curl_setopt_array($ch, [\n                CURLOPT_URL =&gt; $url,\n                CURLOPT_RETURNTRANSFER =&gt; true,\n                CURLOPT_HTTPHEADER =&gt; $curlHeaders,\n                CURLOPT_CONNECTTIMEOUT =&gt; $this-&gt;config-&gt;connectTimeout,\n                CURLOPT_TIMEOUT =&gt; $this-&gt;config-&gt;requestTimeout,\n                CURLOPT_HEADER =&gt; true, // Include headers in output\n                CURLOPT_FOLLOWLOCATION =&gt; true,\n                CURLOPT_MAXREDIRS =&gt; 5,\n            ]);\n\n            // Set method-specific options\n            switch ($method) {\n                case 'POST':\n                    curl_setopt($ch, CURLOPT_POST, true);\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'PUT':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PUT');\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'PATCH':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PATCH');\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'DELETE':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'DELETE');\n                    if (!empty($body)) {\n                        curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    }\n                    break;\n                case 'HEAD':\n                    curl_setopt($ch, CURLOPT_NOBODY, true);\n                    break;\n                case 'OPTIONS':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'OPTIONS');\n                    break;\n                case 'GET':\n                default:\n                    // GET is the default in cURL\n                    break;\n            }\n\n            // Handle streaming if requested\n            $responseBody = '';\n            $responseHeaders = [];\n\n            if ($streaming) {\n                $tempHandle = null;\n                $tempFile = tempnam(sys_get_temp_dir(), 'curl_stream_');\n                $tempHandle = fopen($tempFile, 'w+');\n\n                curl_setopt($ch, CURLOPT_FILE, $tempHandle);\n                curl_setopt($ch, CURLOPT_WRITEFUNCTION, function($ch, $data) use ($tempHandle) {\n                    return fwrite($tempHandle, $data);\n                });\n\n                curl_setopt($ch, CURLOPT_HEADERFUNCTION, function($ch, $header) use (&amp;$responseHeaders) {\n                    $len = strlen($header);\n                    $header = explode(':', $header, 2);\n                    if (count($header) &lt; 2) {\n                        return $len;\n                    }\n\n                    $name = trim($header[0]);\n                    $value = trim($header[1]);\n\n                    $responseHeaders[$name][] = $value;\n                    return $len;\n                });\n\n                $result = curl_exec($ch);\n                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n\n                if ($result === false) {\n                    throw new \\RuntimeException('cURL error: ' . curl_error($ch));\n                }\n\n                // Rewind the temp file so it can be read\n                rewind($tempHandle);\n\n                // Create streaming response\n                $response = new CurlHttpResponse(\n                    statusCode: $statusCode,\n                    headers: $responseHeaders,\n                    body: '',\n                    stream: $tempHandle,\n                    isStreaming: true,\n                    tempFile: $tempFile\n                );\n            } else {\n                // For non-streaming responses, get the full response\n                $result = curl_exec($ch);\n\n                if ($result === false) {\n                    throw new \\RuntimeException('cURL error: ' . curl_error($ch));\n                }\n\n                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n                $headerSize = curl_getinfo($ch, CURLINFO_HEADER_SIZE);\n\n                // Extract headers and body\n                $headerText = substr($result, 0, $headerSize);\n                $responseBody = substr($result, $headerSize);\n\n                // Parse headers\n                $headers = explode(\"\\r\\n\", $headerText);\n                foreach ($headers as $header) {\n                    $parts = explode(':', $header, 2);\n                    if (count($parts) === 2) {\n                        $name = trim($parts[0]);\n                        $value = trim($parts[1]);\n                        $responseHeaders[$name][] = $value;\n                    }\n                }\n\n                // Create regular response\n                $response = new CurlHttpResponse(\n                    statusCode: $statusCode,\n                    headers: $responseHeaders,\n                    body: $responseBody\n                );\n            }\n\n            // Clean up cURL\n            curl_close($ch);\n\n            // Dispatch event for successful response\n            $this-&gt;events-&gt;dispatch(new HttpResponseReceived([\n                'statusCode' =&gt; $response-&gt;statusCode()\n            ]));\n\n            return $response;\n\n        } catch (\\Exception $e) {\n            // Clean up if needed\n            if (isset($ch) &amp;&amp; is_resource($ch)) {\n                curl_close($ch);\n            }\n\n            // Dispatch event for failed request\n            $this-&gt;events-&gt;dispatch(new HttpRequestFailed([\n                'url' =&gt; $url,\n                'methods' =&gt; $method,\n                'headers' =&gt; $headers,\n                'body' =&gt; $request-&gt;body()-&gt;toArray(),\n                'errors' =&gt; $e-&gt;getMessage()\n            ]));\n\n            // Wrap the exception\n            throw new HttpRequestException($e);\n        }\n    }\n}\n</code></pre> <p>And here's the corresponding response adapter:</p> <pre><code>// @doctest id=\"9d5f\"\nnamespace YourNamespace\\Http\\Adapters;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Generator;\n\nclass CurlHttpResponse implements HttpResponse\n{\n    private $stream;\n    private $tempFile;\n    private $isStreaming;\n\n    /**\n     * Constructor\n     */\n    public function __construct(\n        private int $statusCode,\n        private array $headers,\n        private string $body,\n        $stream = null,\n        bool $isStreaming = false,\n        ?string $tempFile = null\n    ) {\n        $this-&gt;stream = $stream;\n        $this-&gt;isStreaming = $isStreaming;\n        $this-&gt;tempFile = $tempFile;\n    }\n\n    /**\n     * Destructor - clean up temp files\n     */\n    public function __destruct()\n    {\n        if ($this-&gt;stream &amp;&amp; is_resource($this-&gt;stream)) {\n            fclose($this-&gt;stream);\n        }\n\n        if ($this-&gt;tempFile &amp;&amp; file_exists($this-&gt;tempFile)) {\n            unlink($this-&gt;tempFile);\n        }\n    }\n\n    /**\n     * Get the response status code\n     */\n    public function statusCode(): int\n    {\n        return $this-&gt;statusCode;\n    }\n\n    /**\n     * Get the response headers\n     */\n    public function headers(): array\n    {\n        return $this-&gt;headers;\n    }\n\n    /**\n     * Get the response body\n     */\n    public function body(): string\n    {\n        if ($this-&gt;isStreaming &amp;&amp; $this-&gt;stream) {\n            // For streaming responses, read the entire file\n            rewind($this-&gt;stream);\n            $contents = stream_get_contents($this-&gt;stream);\n            rewind($this-&gt;stream);\n            return $contents;\n        }\n\n        return $this-&gt;body;\n    }\n\n    /**\n     * Stream the response body\n     */\n    public function stream(int $chunkSize = 1): Generator\n    {\n        if ($this-&gt;isStreaming &amp;&amp; $this-&gt;stream) {\n            // For streaming responses, yield chunks from the file\n            rewind($this-&gt;stream);\n\n            while (!feof($this-&gt;stream)) {\n                yield fread($this-&gt;stream, $chunkSize);\n            }\n        } else {\n            // For non-streaming responses, yield the entire body\n            yield $this-&gt;body;\n        }\n    }\n}\n</code></pre>"},{"location":"http/9-1-custom-clients/#registering-custom-http-client-drivers","title":"Registering Custom HTTP Client Drivers","text":"<p>To use your custom driver, you can register it with the driver factory or use it directly with the HttpClientBuilder:</p> <pre><code>// @doctest id=\"a766\"\nuse Cognesy\\Http\\HttpClientBuilder;\nuse YourNamespace\\Http\\Drivers\\CustomHttpDriver;\n\n// Use your custom driver directly with the builder\n$customDriver = new CustomHttpDriver($config);\n$client = (new HttpClientBuilder())\n    -&gt;withDriver($customDriver)\n    -&gt;create();\n</code></pre>"},{"location":"http/9-1-custom-clients/#adding-custom-http-client-to-configuration","title":"Adding Custom HTTP Client to Configuration","text":"<p>Use your custom driver in the HTTP client configuration file (<code>config/http-client.php</code>):</p> <pre><code>// @doctest id=\"ef65\"\n// http-client.php configuration file\n// ...\n    'clients' =&gt; [\n        'my-custom-client' =&gt; [\n            // Our custom driver type\n            'httpDriverType' =&gt; 'my-custom-driver',\n            // Other driver-specific options...\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n// ...\n</code></pre>"},{"location":"http/9-1-custom-clients/#using-your-custom-http-client","title":"Using Your Custom HTTP Client","text":"<p>After adding the driver to the configuration, you can use it in your code:</p> <pre><code>// @doctest id=\"4825\"\nuse Cognesy\\Http\\HttpClient;\n\n// Create a client with your custom driver\n$client = HttpClient::using('my-custom-client');\n</code></pre>"},{"location":"http/9-1-custom-clients/#using-custom-http-clients-in-configuration-files","title":"Using Custom HTTP Clients in Configuration Files","text":"<p>Or you can refer to it in your LLM connections configuration:</p> <pre><code>// @doctest id=\"d52c\"\n    // llm-connections.php configuration file\n    // ...\n        'a21' =&gt; [\n            'providerType' =&gt; 'a21',\n            'apiUrl' =&gt; 'https://api.ai21.com/studio/v1',\n            'apiKey' =&gt; Env::get('A21_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'jamba-1.5-mini',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 256_000,\n            'maxOutputLength' =&gt; 4096,\n            // Our custom HTTP client\n            'httpClientPreset' =&gt; 'my-custom-client',\n        ],\n    // ...\n</code></pre>"},{"location":"instructor/cli_tools/","title":"CLI Tools","text":"<p>Instructor comes with command line tools:  - <code>./vendor/bin/instructor-setup publish</code>: Publishes configuration files and prompt templates to your project directory  - <code>./vendor/bin/instructor-hub</code>: Displays and executes Instructor examples</p> <p>Additional tool included with Instructor (under development):  - <code>./vendor/bin/tell \"&lt;prompt to LLM&gt;\"</code>: Interacts with LLMs from the command line</p>"},{"location":"instructor/introduction/","title":"Introduction","text":""},{"location":"instructor/introduction/#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to get structured, validated data from multiple types of inputs: text, chat messages, or images. It is powered by Large Language Models (LLMs).</p> <p>The library is inspired by the Instructor for Python created by Jason Liu.</p>"},{"location":"instructor/introduction/#learn-more","title":"Learn More...","text":"<p>          Check how to set up Instructor in your project and start processing data with LLMs      <pre><code>&lt;Card\n    title=\"Concepts\"\n    icon=\"shapes\"\n    href=\"/instructor/concepts/overview\"\n&gt;\n    Read more about basic concepts behind Instructor\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Essentials\"\n    icon=\"hammer\"\n    href=\"/instructor/essentials\"\n&gt;\n    Learn Instructor features and capabilities\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Cookbooks\"\n    icon=\"book\"\n    href=\"/cookbook/introduction\"\n&gt;\n    Browse examples to see Instructor in action and find out how to use it in your projects\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Internals\"\n    icon=\"gears\"\n    href=\"/instructor/internals\"\n&gt;\n    Deep dive into Instructor internals and low level mechanisms\n&lt;/Card&gt;\n</code></pre> <p></p>"},{"location":"instructor/introduction/#feature-highlights","title":"Feature Highlights","text":"<p>Instructor is designed to make it easy to process data with LLMs in PHP. Here are some of the key features of the library:</p>"},{"location":"instructor/introduction/#core-features","title":"Core features","text":"<ul> <li>Get structured responses from LLM inference</li> <li>Validation of returned data</li> <li>Automated retries in case of errors when LLM responds with invalid data</li> </ul>"},{"location":"instructor/introduction/#flexible-inputs","title":"Flexible inputs","text":"<ul> <li>Process various types of input data: text, series of chat messages or images</li> <li>'Structured-to-structured' processing - provide object or array as an input and get object with the results of inference back</li> <li>Demonstrate examples to improve the quality of inference</li> </ul>"},{"location":"instructor/introduction/#customizable-outputs","title":"Customizable outputs","text":"<ul> <li>Define response data model the way to need: type-hinted classes, JSON Schema arrays, or dynamically define your data shapes with Structures</li> <li>Customize prompts and retry prompts</li> <li>Use attributes or PHP DocBlocks to provide additional instructions for LLM</li> <li>Customize response model processing by providing your own implementation of schema, deserialization, validation and transformation interfaces</li> </ul>"},{"location":"instructor/introduction/#sync-and-streaming-support","title":"Sync and streaming support","text":"<ul> <li>Receive synchronous or streaming responses</li> <li>Get partial updates &amp; stream completed sequence items</li> </ul>"},{"location":"instructor/introduction/#observability","title":"Observability","text":"<ul> <li>Get detailed insight into internal processing via events</li> </ul>"},{"location":"instructor/introduction/#support-for-multiple-llms-api-providers","title":"Support for multiple LLMs / API providers","text":"<ul> <li>Use multiple LLM API providers (incl. OpenAI,  Anthropic, Cohere, Azure, Groq, Mistral, Fireworks AI, Ollama, OpenRouter, Together AI)</li> <li>Use local models with Ollama</li> </ul>"},{"location":"instructor/introduction/#documentation-and-examples","title":"Documentation and examples","text":"<ul> <li>Learn more from growing documentation and 50+ cookbooks</li> </ul>"},{"location":"instructor/introduction/#instructor-in-other-languages","title":"Instructor in Other Languages","text":"<p>Instructor has been implemented in various technology stacks. Check out implementations in other languages below:</p> <ul> <li>Python (original)</li> <li>Javascript (port)</li> <li>Elixir (port)</li> <li>Ruby (port)</li> <li>Go (port)</li> </ul> <p>If you want to port Instructor to another language, please reach out to us on Twitter we'd love to help you get started!</p>"},{"location":"instructor/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with Instructor in your PHP project in under 5 minutes.</p> <p>For detailed setup instructions, see Setup.</p>"},{"location":"instructor/quickstart/#install-instructor-with-composer","title":"Install Instructor with Composer","text":"<p>Run following command in your terminal:</p> <pre><code># @doctest id=\"b6e6\"\ncomposer require cognesy/instructor-php\n</code></pre>"},{"location":"instructor/quickstart/#create-and-run-example","title":"Create and Run Example","text":""},{"location":"instructor/quickstart/#step-1-prepare-your-openai-api-key","title":"Step 1: Prepare your OpenAI API Key","text":"<p>In this example, we'll use OpenAI as the LLM provider. You can get it from the OpenAI dashboard.</p>"},{"location":"instructor/quickstart/#step-2-create-a-new-php-file","title":"Step 2: Create a New PHP File","text":"<p>In your project directory, create a new PHP file <code>test-instructor.php</code>:</p> <pre><code>// @doctest id=\"26ca\"\n&lt;?php\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Set up OpenAI API key\n$apiKey = 'your-openai-api-key';\nputenv(\"OPENAI_API_KEY=\" . $apiKey);\n// WARNING: In real project you should set up API key in .env file.\n\n// Step 1: Define target data structure(s)\nclass City {\n    public string $name;\n    public string $country;\n    public int $population;\n}\n\n// Step 2: Use Instructor to run LLM inference\n$city = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withResponseClass(City::class)\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\nvar_dump($city);\n</code></pre> <p> You should never put your API keys directly in your real project code to avoid getting them compromised. Set them up in your .env file. </p>"},{"location":"instructor/quickstart/#step-3-run-the-example","title":"Step 3: Run the Example","text":"<p>Now, you can run the example:</p> <pre><code># @doctest id=\"7642\"\nphp test-instructor.php\n\n# Output:\n# object(City)#1 (3) {\n#   [\"name\"]=&gt;\n#   string(5) \"Paris\"\n#   [\"country\"]=&gt;\n#   string(6) \"France\"\n#   [\"population\"]=&gt;\n#   int(2148000)\n# }\n</code></pre>"},{"location":"instructor/quickstart/#next-steps","title":"Next Steps","text":"<p>You can start using Instructor in your project right away after installation.</p> <p>But it's recommended to publish configuration files and prompt templates to your project directory, so you can customize the library's behavior and use your own prompt templates.</p> <p>You should also set up LLM provider API keys in your <code>.env</code> file instead of putting them directly in your code.</p> <p>See Setup Instructions for more details.</p>"},{"location":"instructor/setup/","title":"Setup","text":""},{"location":"instructor/setup/#overview","title":"Overview","text":"<p>Full Instructor setup consists of following steps:  - Step 1: Install Instructor via Composer  - Step 2: Publish Instructor assets (configurations and prompts) to your project directory  - Step 3: Set up LLM provider API key(s) in your .env file  - Step 4: Set configuration location in your .env file (optional)</p>"},{"location":"instructor/setup/#step-1-install-instructor-via-composer","title":"Step 1: Install Instructor via Composer","text":"<p>You can install Instructor via Composer by running:</p> <pre><code># @doctest id=\"3c62\"\ncomposer require cognesy/instructor-php\n</code></pre>"},{"location":"instructor/setup/#step-2-publish-instructor-files-to-your-project","title":"Step 2: Publish Instructor Files to Your Project","text":"<p>Instructor comes with a set of configuration files and prompt templates that you can publish to your project directory.</p> <p>This will allow you to customize the library's behavior and use different prompt templates.</p> <p>These files can be found in the <code>vendor/cognesy/instructor-php</code> directory: - <code>.env-dist</code> - Environment variables for API keys and configuration paths - <code>/config/*.php</code> - Configurations of Instructor modules - <code>/prompts/*</code> - Prompt templates for generating structured data from text</p> <p>You can publish these files to your project directory by running following command:</p> <pre><code># @doctest id=\"ce7f\"\n./vendor/bin/instructor-setup publish \\n\n  --target-config-dir=&lt;target config dir location&gt;\n  --target-prompts-dir=&lt;target prompts dir location&gt;\n  --target-env-file=&lt;target .env file location&gt;\n</code></pre> <p>You can also manually copy the required files to your project directory.</p> <p> Read more: - Framework Integration     - Laravel Projects     - Symfony Projects     - Custom Framework Location - Use CLI Tool to publish Instructor assets - Manual Setup </p>"},{"location":"instructor/setup/#step-3-set-up-llm-provider-api-keys","title":"Step 3: Set Up LLM Provider API Key(s)","text":"<p>If you're using commercial LLM providers like OpenAI, you'll need to set up API keys in your project's <code>.env</code> file.</p> <p>Open the <code>.env</code> file in your project directory and set up API keys for the LLM providers you plan to use. You can find the keys in the respective provider's dashboard.</p> <pre><code># @doctest .env=true\n# OpenAI (default provider)\nOPENAI_API_KEY='your-openai-api-key'\n</code></pre> <p>Check <code>.env-dist</code> for other API keys Instructor uses in its default configuration files.</p>"},{"location":"instructor/setup/#step-4-set-configuration-location-optional","title":"Step 4: Set Configuration Location (optional)","text":"<p>Instructor uses a configuration directory to store its settings, e.g. LLM provider configurations.</p> <p>You can set the path to this directory via <code>Settings::setPath('/path/to/config')</code> in your code.</p> <p>But to make it easier you can just set the value in your <code>.env</code> file. <code>Settings</code> will pick it up automatically from there. This way you don't have to set it in every script.</p> <pre><code># @doctest .env=true\nINSTRUCTOR_CONFIG_PATHS='/path/to/your/config/dir/,another/path'\n</code></pre> <p> <code>INSTRUCTOR_CONFIG_PATHS</code> is set automatically if you use the Instructor CLI tool to publish assets. </p>"},{"location":"instructor/setup/#framework-integration","title":"Framework Integration","text":""},{"location":"instructor/setup/#laravel-projects","title":"Laravel Projects","text":"<p>For Laravel applications, it's recommended to align with the framework's directory structure:</p> <pre><code># @doctest id=\"69da\"\n./vendor/bin/instructor-setup publish \\\n    --target-config-dir=config/instructor \\\n    --target-prompts-dir=resources/prompts \\\n    --target-env-file=.env\n</code></pre> <p>This will: - Place configuration files in Laravel's <code>config</code> directory - Store prompts in Laravel's <code>resources</code> directory - Use Laravel's default <code>.env</code> file location</p> <p>After publishing, you can load Instructor configuration in your <code>config/app.php</code> or create a dedicated service provider.</p>"},{"location":"instructor/setup/#symfony-projects","title":"Symfony Projects","text":"<p>For Symfony applications, use the standard Symfony directory structure:</p> <pre><code># @doctest id=\"8b64\"\n./vendor/bin/instructor-setup publish \\\n    --target-config-dir=config/packages/instructor \\\n    --target-prompts-dir=resources/instructor/prompts \\\n    --target-env-file=.env\n</code></pre> <p>This will: - Place configuration in Symfony's package configuration directory - Store prompts in Symfony's <code>resources</code> directory - Use Symfony's default <code>.env</code> file location</p> <p>For Symfony Flex applications, you may want to create a recipe to automate this setup process.</p>"},{"location":"instructor/setup/#custom-framework-location","title":"Custom Framework Location","text":"<p>You can use environment variables to set the location of configuration files: <pre><code>// @doctest id=\"e5ea\"\nINSTRUCTOR_CONFIG_PATHS=/path/to/config,another/path\n</code></pre></p> <p>This allows you to maintain consistent paths across your application without specifying them in each command.</p>"},{"location":"instructor/setup/#using-cli-tool","title":"Using CLI Tool","text":"<p>After installing Instructor via Composer, you may want to publish the library's configuration files and resources to your project, so you can modify them according to your needs. You can do this either manually or automatically using the provided CLI tool.</p> <pre><code># @doctest id=\"9348\"\n./vendor/bin/instructor-setup publish\n</code></pre> <p>By default, this command will: 1. Copy configuration files from <code>vendor/cognesy/instructor-php/config</code> to <code>config/instructor/</code> 2. Copy prompt templates from <code>vendor/cognesy/instructor-php/prompts</code> to <code>resources/prompts/</code> 3. Merge (or copy) <code>vendor/cognesy/instructor-php/.env-dist</code> file to <code>.env</code> with environment variables</p>"},{"location":"instructor/setup/#command-options","title":"Command Options","text":"<ul> <li><code>-c, --target-config-dir=DIR</code> - Custom directory for configuration files (default: <code>config/instructor</code>)</li> <li><code>-p, --target-prompts-dir=DIR</code> - Custom directory for prompt templates (default: <code>resources/prompts</code>)</li> <li><code>-e, --target-env-file=FILE</code> - Custom location for .env file (default: <code>.env</code>)</li> <li><code>-l, --log-file=FILE</code> - Optional log file path to track the publishing process</li> <li><code>--no-op</code> - Dry run mode - shows what would be copied without making changes</li> </ul>"},{"location":"instructor/setup/#example-usage","title":"Example Usage","text":"<pre><code># @doctest id=\"35b8\"\n./vendor/bin/instructor-setup publish \\\n    --target-config-dir=./config/instructor \\\n    --target-prompts-dir=./resources/prompts \\\n    --target-env-file=.env\n</code></pre> <p> When merging <code>.env</code> files, the tool will only add missing variables, preserving your existing file content, formatting and comments. </p>"},{"location":"instructor/setup/#manual-setup","title":"Manual Setup","text":"<p>If you prefer to set up Instructor manually or need more control over the process, you can copy the required files directly:</p>"},{"location":"instructor/setup/#configuration-files","title":"Configuration Files","text":"<p><pre><code># @doctest id=\"5022\"\n# Create config in your preferred directory\nmkdir -p config/instructor\n\n# Copy configuration files\ncp -r vendor/cognesy/instructor-php/config/* config/instructor/\n</code></pre> These files contain LLM API connection settings and Instructor's behavior configuration.</p>"},{"location":"instructor/setup/#prompt-templates","title":"Prompt Templates","text":"<p><pre><code># @doctest id=\"4a4b\"\n# Create prompts in your preferred directory\nmkdir -p resources/prompts\n\n# Copy prompt templates\ncp -r vendor/cognesy/instructor-php/prompts/* resources/prompts/\n</code></pre> Prompt templates define how Instructor communicates with LLMs for different tasks.</p>"},{"location":"instructor/setup/#environment-configuration","title":"Environment Configuration","text":"<p>If .env doesn't exist, copy the environment template:</p> <pre><code># @doctest id=\"b557\"\n[ ! -f .env ] &amp;&amp; cp vendor/cognesy/instructor-php/config/.env-dist .env\n</code></pre> <p>Add key values to your .env: <pre><code># @doctest id=\"3fed\"\n# OpenAI API key\nOPENAI_API_KEY=your_api_key\n# Other API keys (if you use other LLM providers)\n# ...\n\n# Set up Instructor configuration path (optional)\nINSTRUCTOR_CONFIG_PATHS='&lt;path/to/config&gt;,&lt;another/path&gt;'\n</code></pre></p>"},{"location":"instructor/upgrade/","title":"Upgrading Instructor","text":"<p>Recent changes to the Instructor package may require some manual fixes in your codebase.</p>"},{"location":"instructor/upgrade/#step-1-update-the-package","title":"Step 1: Update the package","text":"<p>Run the following command in your CLI:</p> <pre><code># @doctest id=\"fa72\"\ncomposer update cognesy/instructor\n</code></pre>"},{"location":"instructor/upgrade/#step-2-config-files","title":"Step 2: Config files","text":"<p>Correct your config files to use new namespaces.</p>"},{"location":"instructor/upgrade/#step-3-instructor-config-path","title":"Step 3: Instructor config path","text":"<p>Correct INSTRUCTOR_CONFIG_PATHS in .env file to <code>config/instructor</code> (or your custom path).</p>"},{"location":"instructor/upgrade/#step-4-codebase","title":"Step 4: Codebase","text":"<p>Make sure that your code follows new namespaces.</p> <p>Suggestion: use IDE search and replace to find and replace old namespaces with new ones.</p>"},{"location":"instructor/advanced/function_calls/","title":"Function calls","text":""},{"location":"instructor/advanced/function_calls/#functioncall-helper-class","title":"FunctionCall helper class","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"instructor/advanced/function_calls/#extracting-arguments-for-a-function","title":"Extracting arguments for a function","text":"<pre><code>// @doctest id=\"9973\"\n&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Save user data to storage */\nfunction saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCall::fromFunctionName('saveUser'),\n)-&gt;get();\n\n// call the function with the extracted arguments\nsaveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/function_calls/#extracting-arguments-for-a-method-call","title":"Extracting arguments for a method call","text":"<pre><code>// @doctest id=\"98b7\"\n&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass DataStore {\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) {\n        // ...\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCall::fromMethodName(Datastore::class, 'saveUser'),\n)-&gt;get();\n\n// call the function with the extracted arguments\n(new DataStore)-&gt;saveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/function_calls/#extracting-arguments-for-a-callable","title":"Extracting arguments for a callable","text":"<pre><code>// @doctest id=\"ded4\"\n&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Save user data to storage */\n$callable = function saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCall::fromCallable($callable),\n)-&gt;get();\n\n// call the function with the extracted arguments\n$callable(...$args);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/model_options/","title":"Model options","text":""},{"location":"instructor/advanced/model_options/#changing-llm-model-and-options","title":"Changing LLM model and options","text":"<p>You can specify model and other options that will be passed to LLM endpoint.</p> <p>Commonly used option supported by many providers is <code>temperature</code>, which controls randomness of the output.</p> <p>Lower values make the output more deterministic, while higher values make it more random.</p> <pre><code>// @doctest id=\"e791\"\n&lt;?php\n$person = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: [\n        // custom temperature setting\n        'temperature' =&gt; 0.0\n        // ... other options - e.g. provider or model specific\n    ],\n)-&gt;get();\n</code></pre> <p>NOTE: Please note that many options might be specific to the provider or even some model that you are using.</p>"},{"location":"instructor/advanced/model_options/#customizing-configuration","title":"Customizing configuration","text":"<p>You can pass a custom LLM configuration to the Instructor.</p> <p>This allows you to specify your own API key, base URI or, which might be helpful in the case you are using OpenAI - organization.</p> <pre><code>// @doctest id=\"4776\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;use Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\n\n// Create instance of OpenAI client initialized with custom parameters\n$config = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: $yourApiKey,\n    endpoint: '/chat/completions',\n    metadata: ['organization' =&gt; ''],\n    model: 'gpt-4o-mini',\n    maxTokens: 128,\n    httpClientPreset: 'guzzle',\n    driver: 'openai',\n));\n\n/// Get Instructor with the default configuration overridden with your own\n$structuredOutput = (new StructuredOutput)-&gt;withLLMConfig($driver);\n\n$person = $structuredOutput-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    options: ['temperature' =&gt; 0.0],\n)-&gt;get();\n</code></pre>"},{"location":"instructor/advanced/partials/","title":"Partials","text":""},{"location":"instructor/advanced/partials/#partial-updates","title":"Partial updates","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated.</p> <p>You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p>This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive partial results define <code>onPartialUpdate()</code> callback that will be called on every update of the deserializad object.</p> <p>Instructor is smart about updates, it calculates and compares hashes of the previous and newly deserialized version of the model, so you won't get them on every token received, but only when any property of the object is updated.</p> <pre><code>// @doctest id=\"c21b\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nfunction updateUI($person) {\n    // Here you get partially completed Person object update UI with the partial result\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        options: ['stream' =&gt; true]\n    )\n    -&gt;onPartialUpdate(\n        fn($partial) =&gt; updateUI($partial)\n    )\n    -&gt;get();\n\n// Here you get completed and validated Person object\n$this-&gt;db-&gt;save($person); // ...for example: save to DB\n</code></pre> <p>Partially updated data is not validated while it is received and deserialized.</p> <p>The object returned from <code>get()</code> call is fully validated, so you can safely work with it, e.g. save it to the database.</p>"},{"location":"instructor/advanced/partials/#streaming-responses","title":"Streaming responses","text":"<p>You can get a stream of responses by calling the <code>stream()</code> method instead of <code>get()</code>. The <code>stream()</code> method is available on both <code>StructuredOutput</code> and <code>PendingStructuredOutput</code> instances.</p> <pre><code>// @doctest id=\"8814\"\n// Direct streaming\n$stream = $structuredOutput-&gt;stream();\n\n// Or via create() method\n$pending = $structuredOutput-&gt;create();\n$stream = $pending-&gt;stream();\n</code></pre> <p>Both approaches return a <code>StructuredOutputStream</code> object, which gives you access to the response streamed from LLM and processed by Instructor into structured data.</p>"},{"location":"instructor/advanced/partials/#structuredoutputstream-methods","title":"StructuredOutputStream Methods","text":"<p>The <code>StructuredOutputStream</code> class provides comprehensive methods for processing streaming responses:</p>"},{"location":"instructor/advanced/partials/#core-streaming-methods","title":"Core Streaming Methods","text":"<ul> <li><code>partials()</code>: Returns an iterable of partial updates from the stream. Only final update is validated, partial updates are only deserialized and transformed.</li> <li><code>sequence()</code>: Dedicated to processing <code>Sequence</code> response models - returns only completed items in the sequence.</li> <li><code>responses()</code>: Generator of partial LLM responses as they are received.</li> </ul>"},{"location":"instructor/advanced/partials/#result-access-methods","title":"Result Access Methods","text":"<ul> <li><code>finalValue()</code>: Get the final parsed result (blocks until completion).</li> <li><code>finalResponse()</code>: Get the final LLM response (blocks until completion).</li> <li><code>lastUpdate()</code>: Returns the last object received and processed by Instructor.</li> <li><code>lastResponse()</code>: Returns the last received LLM response.</li> </ul>"},{"location":"instructor/advanced/partials/#utility-methods","title":"Utility Methods","text":"<ul> <li><code>usage()</code>: Get token usage statistics from the streaming response.</li> </ul>"},{"location":"instructor/advanced/partials/#example-streaming-partial-responses","title":"Example: streaming partial responses","text":"<pre><code>// @doctest id=\"7347\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $update) {\n    // render updated person view\n    // for example:\n    $view-&gt;updateView($update); // render the updated person view\n}\n\n// now you can get final, fully processed person object\n$person = $stream-&gt;finalValue();\n// ...and for example save it to the database\n$db-&gt;savePerson($person);\n</code></pre>"},{"location":"instructor/advanced/partials/#example-streaming-sequence-items","title":"Example: streaming sequence items","text":"<pre><code>// @doctest id=\"09d6\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)\n    -&gt;with(\n        messages: \"Jason is 28 years old, Amanda is 26 and John (CEO) is 40.\",\n        responseModel: Sequence::of(Participant::class),\n    )\n    -&gt;stream();\n\nforeach ($stream-&gt;sequence() as $update) {\n    // append last completed item from the sequence\n    // for example:\n    $view-&gt;appendParticipant($update-&gt;last());\n}\n\n// now you can get final, fully processed sequence of participants\n$participants = $stream-&gt;finalValue();\n// ...and for example save it to the database\n$db-&gt;saveParticipants($participants-&gt;toArray());\n</code></pre>"},{"location":"instructor/advanced/prompts/","title":"Prompt Templates","text":""},{"location":"instructor/advanced/prompts/#overview","title":"Overview","text":"<p>As your applications grow in complexity, your prompts may become large and complex, with multiple variables and metadata. Managing these prompts can become a challenge, especially when you need to reuse them across different parts of your application. Large prompts are also hard to maintain if they are part of your codebase.</p> <p><code>Prompt</code> addon provides a powerful and flexible way to manage your prompts. It supports multiple template engines (Twig, Blade), prompt metadata, variable injection, and validation.</p>"},{"location":"instructor/advanced/prompts/#what-are-prompts","title":"What are Prompts","text":"<p>Prompts in Instructor are based on structured text templates that can be rendered to text or series of chat messages. As many of your prompts will be dynamically generated based on input data, you can use syntax of one of the supported template engines (Twig, Blade) to define your prompts.</p> <p>This document will be using Twig syntax for prompt templates for simplicity and consistency, but you can use Blade syntax in your prompts if you prefer it.</p> <ul> <li>For more information on Twig syntax see Twig documentation.</li> <li>For more information on Blade syntax see Blade documentation.</li> </ul>"},{"location":"instructor/advanced/prompts/#basic-prompt-template","title":"Basic Prompt Template","text":"<p>Example prompt template in Twig: <pre><code>// @doctest id=\"3bbd\"\nHello, world.\n</code></pre></p>"},{"location":"instructor/advanced/prompts/#prompt-template-with-variables","title":"Prompt Template with Variables","text":"<p>You can define variables in your prompt templates and inject values when rendering the prompt.</p> <pre><code>// @doctest id=\"35bf\"\nHello, {{ name }}!\n</code></pre>"},{"location":"instructor/advanced/prompts/#chat-messages","title":"Chat Messages","text":"<p>You can define chat messages in your prompts, which can be used to generate a sequence of messages for LLM chat APIs.</p> <pre><code>// @doctest id=\"4ad8\"\n&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;What is the capital of {{ country }}?&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#prompt-template-metadata","title":"Prompt Template Metadata","text":"<p>We recommend to preface each prompt with front matter - a block of metadata that describes the prompt and its variables. This metadata can be used for validation, documentation, and schema generation.</p> <pre><code>// @doctest id=\"6d91\"\n{#---\ndescription: Capital finder template\nvariables:\n    country:\n        description: Country name\n        type: string\n        default: France\nschema:\n    name: capital\n    properties:\n        name:\n            description: Capital city name\n            type: string\n    required: [name]\n---#}\n&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;What is the capital of {{ country }}?&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#template-libraries","title":"Template Libraries","text":"<p>Instructor allows you to define multiple <code>template libraries</code> in your app. Library is just a collection of prompt templates which is stored under a specific directory. Library can have a nested structure, which allows you to organize your prompts in a way that makes sense for your application.</p> <p>Library properties are specified in <code>config/prompt.php</code> configuration file.</p> <p>where you can define:  - <code>templateEngine</code> - template engine used for prompts in this library,  - <code>resourcePath</code> - path to prompt templates,  - <code>cachePath</code> - path to compiled templates,  - <code>extension</code> - file extension for prompt templates,  - <code>frontMatterTags</code> - start and end tags for front matter,  - <code>frontMatterFormat</code> - format of front matter (yaml, json, toml),  - <code>metadata</code> - engine-specific configuration.</p> <p>Instructor comes with 3 default prompt libraries:  - <code>system</code> - prompt templates used by Instructor itself,  - <code>demo-twig</code> - demo prompt templates using Twig template engine,  - <code>demo-blade</code> - demo prompt templates using Blade template engine.</p> <p>Instructor's does not specify how you should organize or manage your prompt templates, but it provides a flexible way to do it in a way that suits your application.</p>"},{"location":"instructor/advanced/prompts/#using-prompt-templates","title":"Using Prompt Templates","text":""},{"location":"instructor/advanced/prompts/#rendering-a-simple-prompt","title":"Rendering a Simple Prompt","text":"<p>To get started, you can create and render a simple prompt defined in the bundled library using the <code>Prompt::using</code> or <code>Prompt::make</code> methods. Here's how you can use them:</p> <p><pre><code>// @doctest id=\"6179\"\n&lt;?php\nuse Cognesy\\Template\\Template;\n\n// Basic example using \"using-&gt;get-&gt;with\" syntax\n$prompt = Template::using('demo-twig')-&gt;get('hello')-&gt;with(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre> Or, using the shorthand <code>make()</code> syntax:</p> <pre><code>// @doctest id=\"d991\"\n&lt;?php\n$prompt = Template::make('demo-twig:hello')-&gt;with(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#rendering-a-chat-template","title":"Rendering a Chat Template","text":"<p>The Prompt class can also render prompts directly as chat-style messages:</p> <pre><code>// @doctest id=\"c727\"\n&lt;?php\n$messages = Template::messages('demo-twig:hello', ['name' =&gt; 'World']);\n\nprint_r($messages-&gt;toArray());\n// Outputs:\n// [\n//     ['role' =&gt; 'user', 'content' =&gt; 'Hello, World!']\n// ]\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#custom-configuration-and-template-content","title":"Custom Configuration and Template Content","text":"<p>If you need to customize the configuration or set the template content directly, you can do so with additional methods:</p> <pre><code>// @doctest id=\"6a9e\"\n&lt;?php\nuse Cognesy\\Template\\Config\\TemplateEngineConfig;use Cognesy\\Template\\Enums\\TemplateEngineType;\n\n// Setting custom configuration\n$config = new TemplateEngineConfig(\n    templateEngine: TemplateEngineType::Twig,\n    resourcePath: '',\n    cachePath: '/tmp/cache',\n    extension: '.twig',\n);\n\n$prompt = new Template();\n$prompt-&gt;withConfig($config)\n       -&gt;withTemplateContent('Hello, {{ name }}!')\n       -&gt;withValues(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#in-memory-templates","title":"In Memory Templates","text":"<p>If you need to create an inline prompt (without saving it to a file), you can use following syntax:</p> <pre><code>// @doctest id=\"022f\"\n&lt;?php\n$prompt = Template::twig() // or Template::blade() for Blade syntax\n    -&gt;withTemplateContent('Hello, {{ name }}!')\n    -&gt;withValues(['name' =&gt; 'World'])\n    -&gt;toText();\n?&gt;\n</code></pre> <p>There's shorter syntax for creating in-memory prompts:</p> <pre><code>// @doctest id=\"7009\"\n&lt;?php\n$prompt = Template::twig() // or Template::blade() for Blade syntax\n    -&gt;from('Hello, {{ name }}!')\n    -&gt;with(['name' =&gt; 'World'])\n    -&gt;toText();\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#handling-template-variables","title":"Handling Template Variables","text":"<p>To check which variables are available in a prompt template:</p> <pre><code>// @doctest id=\"2b28\"\n&lt;?php\n$prompt = Template::using('demo-twig')\n    -&gt;withTemplateContent('Hello, {{ name }}!')\n    -&gt;withValues(['name' =&gt; 'World']);\n\n$variables = $prompt-&gt;variables();\n\nprint_r($variables); // Outputs: ['name']\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#loading-templates-by-name-and-using-dsns","title":"Loading Templates by Name and Using DSNs","text":"<p>For more flexible template loading, you can load templates by name or use a 'DSN-like' (Data Source Name) syntax:</p> <pre><code>// @doctest id=\"dbdb\"\n&lt;?php\n// Load a template by name using specified library 'demo-blade'\n$prompt = Template::using('demo-blade')-&gt;withTemplate('hello');\necho $prompt-&gt;template();\n\n// Load a template from specified library using DSN syntax\n$prompt = Template::fromDsn('demo-blade:hello')-&gt;with(['name' =&gt; 'World']);\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#converting-to-messages-with-markup","title":"Converting to Messages with Markup","text":"<p>The Prompt class also supports converting templates containing chat-specific markup into structured messages:</p> <p>Here is an example XML that can be used to generate a sequence of chat messages: <pre><code>&lt;!-- @doctest id=\"e852\"\n&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;Hello, {{ name }}&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre></p> <p>And here is how you use <code>Prompt</code> class to convert XML template into a sequence of messages:</p> <pre><code>// @doctest id=\"a8ac\"\n&lt;?php\n\n$prompt = Template::using('demo-blade')\n    -&gt;withTemplateContent('&lt;chat&gt;&lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;&lt;message role=\"user\"&gt;Hello, {{ $name }}&lt;/message&gt;&lt;/chat&gt;')\n    -&gt;withValues(['name' =&gt; 'assistant']);\n\n$messages = $prompt-&gt;toMessages();\n\necho $messages-&gt;toArray();\n// Outputs:\n// [\n//     ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant.'],\n//     ['role' =&gt; 'user', 'content' =&gt; 'Hello, assistant']\n// ]\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/sequences/","title":"Sequences","text":""},{"location":"instructor/advanced/sequences/#extracting-sequences-of-objects","title":"Extracting Sequences of Objects","text":"<p>Sequence is a wrapper class that can be used to represent a list of objects to be extracted by Instructor from provided context.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <pre><code>// @doctest id=\"d670\"\n&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new StructuredOutput)\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    )-&gt;get();\n</code></pre>"},{"location":"instructor/advanced/sequences/#streaming-sequences","title":"Streaming Sequences","text":"<p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <p>NOTE This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive sequence updates provide a callback via Instructor's <code>onSequenceUpdate()</code> that will be called each  time a new item is received from LLM.</p> <p>The callback provided a full sequence that has been retrieved so far. You can get the last added object from the sequence via <code>$sequence-&gt;last()</code>.</p> <p>Remember that while the sequence is being updated, the data is not validated - only when the sequence is fully extracted, the objects are validated and a full sequence is returned (see example below).</p> <pre><code>// @doctest id=\"45ba\"\n&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\nfunction updateUI(Person $person) {\n    // add newly extracted person to the UI list\n    $this-&gt;ui-&gt;appendToList($person);\n    // remember those objects are not validated yet\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(\n        fn($sequence) =&gt; updateUI($sequence-&gt;last()) // get last added object\n    )\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\n// now the list is fully extracted and validated\nforeach ($list as $person) {\n    // do something with each person\n    $this-&gt;db-&gt;save($person);\n}\n</code></pre>"},{"location":"instructor/advanced/sequences/#working-with-sequences","title":"Working with Sequences","text":"<p>Sequences offer array access (via ArrayAccess) and convenience methods to work with the list of extracted objects.</p> <pre><code>// @doctest id=\"de5a\"\n&lt;?php\n$sequence-&gt;count();   // returns the number of extracted items\n$sequence-&gt;first();   // returns the first extracted item\n$sequence-&gt;last();    // returns the last extracted item\n$sequence-&gt;get(1);    // returns the second extracted item\n$sequence-&gt;toArray(); // returns the list of extracted items as an array\n</code></pre>"},{"location":"instructor/advanced/sequences/#streaming-sequence-updates","title":"Streaming sequence updates","text":"<p>See: Streaming and partial updates for more information on how to get partial updates and streaming of sequences.</p>"},{"location":"instructor/advanced/structure-to-structure/","title":"Structure to structure","text":""},{"location":"instructor/advanced/structure-to-structure/#structured-to-structured-processing","title":"Structured-to-structured processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>create()</code> method can be an object, but also an array or just a string.</p> <pre><code>// @doctest id=\"18c8\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new StructuredOutput)-&gt;with(\n    input: $email,\n    responseModel: Email::class,\n    prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n)-&gt;get();\n\nassert($translation instanceof Email); // true\ndump($translation);\n// Email {\n//     address: \"joe@gmail\",\n//     subject: \"Actualizaci\u00f3n de estado\",\n//     body: \"Su cuenta ha sido actualizada.\"\n// }\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/","title":"Structures","text":""},{"location":"instructor/advanced/structures/#handling-dynamic-data-models","title":"Handling dynamic data models","text":"<p>If you want to define the shape of data during runtime, you can use <code>Structure</code> class.</p> <p>Structures allow you to define and modify arbitrary shape of data to be extracted by LLM. Classes may not be the best fit for this purpose, as declaring or changing them during execution is not possible.</p> <p>With structures, you can define custom data shapes dynamically, for example based on the user input or context of the processing, to specify the information you need LLM to infer from the provided text or chat messages.</p>"},{"location":"instructor/advanced/structures/#defining-a-shape-of-data","title":"Defining a shape of data","text":"<p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns an array in the shape you defined.</p> <p><code>Structure::define()</code> accepts array of <code>Field</code> objects.</p> <p>Let's first define the structure, which is a shape of the data we want to extract from the message.</p> <pre><code>// @doctest id=\"7fd3\"\n&lt;?php\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::enum('role', Role::class),\n]);\n?&gt;\n</code></pre> <p>Following types of fields are currently supported:</p> <ul> <li><code>Field::bool()</code> - boolean value</li> <li><code>Field::int()</code> - int value</li> <li><code>Field::string()</code> - string value</li> <li><code>Field::float()</code> - float value</li> <li><code>Field::enum()</code> - enum value</li> <li><code>Field::structure()</code> - for nesting structures</li> </ul>"},{"location":"instructor/advanced/structures/#optional-fields","title":"Optional fields","text":"<p>Fields can be marked as optional with <code>$field-&gt;optional()</code>.  By default, all defined fields are required.</p> <pre><code>// @doctest id=\"002c\"\n&lt;?php\n$structure = Structure::define('person', [\n    //...\n    Field::int('age')-&gt;optional(),\n    //...\n]);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#descriptions-for-guiding-llm-inference","title":"Descriptions for guiding LLM inference","text":"<p>Instructor includes field descriptions in the content of instructions for LLM, so you can use them to provide explanations, detailed specifications or requirements for each field.</p> <p>You can also provide extra inference instructions for LLM at the structure level with <code>$structure-&gt;description(string $description)</code></p> <pre><code>// @doctest id=\"9988\"\n&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name', 'Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;optional(),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#nesting-structures","title":"Nesting structures","text":"<p>You can use <code>Field::structure()</code> to nest structures in case you want to define more complex data shapes.</p> <pre><code>// @doctest id=\"3b53\"\n&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#validation-of-structure-data","title":"Validation of structure data","text":"<p>Instructor supports validation of structures.</p> <p>You can define field validator with:</p> <ul> <li><code>$field-&gt;validator(callable $validator)</code> - $validator has to return an instance of <code>ValidationResult</code></li> <li><code>$field-&gt;validIf(callable $condition, string $message)</code> - $condition has to return false if validation has not succeeded, $message with be provided to LLM as explanation for self-correction of the next extraction attempt</li> </ul> <p>Let's add a simple field validation to the example above: </p> <pre><code>// @doctest id=\"67fd\"\n&lt;?php\n$structure = Structure::define('person', [\n    // ...\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    // ...\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#extracting-data","title":"Extracting data","text":"<p>Now, let's extract the data from the message.</p> <pre><code>// @doctest id=\"c1f4\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield. She is 25 years old and works as a line worker. \n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345.\n    TEXT;\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;get();\n\ndump($person-&gt;toArray());\n// array [\n//   \"name\" =&gt; \"Jane Doe\"\n//   \"age\" =&gt; 25\n//   \"address\" =&gt; array [\n//     \"city\" =&gt; \"Springfield\"\n//   ]\n//   \"role\" =&gt; \"line\"\n// ]\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#working-with-structure-objects","title":"Working with <code>Structure</code> objects","text":"<p>Structure object properties can be accessed using <code>get()</code> and <code>set()</code> methods, but also directly as properties.</p> <pre><code>// @doctest id=\"97d0\"\n&lt;?php\n$person = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::structure('role', [\n        Field::string('name'),\n        Field::int('level'),\n    ])\n]);\n\n// Setting properties via set()\n$person-&gt;set('name', 'John Doe');\n$person-&gt;set('age', 30);\n$person-&gt;get('role')-&gt;set('name', 'Manager');\n$person-&gt;get('role')-&gt;set('level', 1);\n\n// Setting properties directly \n$person-&gt;name = 'John Doe';\n$person-&gt;age = 30;\n$person-&gt;role-&gt;name = 'Manager';\n$person-&gt;role-&gt;level = 1;\n\n// Getting properties via get()\n$name = $person-&gt;get('name');\n$age = $person-&gt;get('age');\n$role = $person-&gt;get('role')-&gt;get('name');\n$level = $person-&gt;get('role')-&gt;get('level');\n\n// Getting properties directly\n$name = $person-&gt;name;\n$age = $person-&gt;age;\n$role = $person-&gt;role-&gt;name;\n$level = $person-&gt;role-&gt;level;\n?&gt;\n</code></pre>"},{"location":"instructor/concepts/overview/","title":"Overview","text":""},{"location":"instructor/concepts/overview/#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to get structured, validated data from multiple types of inputs: text,  chat messages, or images. It is powered by Large Language Models (LLMs).</p> <p>The library is inspired by the Instructor for Python created by Jason Liu.</p>"},{"location":"instructor/concepts/overview/#how-it-works","title":"How it works","text":"<p>Instructor uses Large Language Models (LLMs) to process data and return structured information you can easily use in your code.</p> <p></p>"},{"location":"instructor/concepts/overview/#instructor-in-action","title":"Instructor in action","text":"<p>Here's a simple CLI demo app using Instructor to extract structured data from text:</p> <p></p>"},{"location":"instructor/concepts/overview/#how-instructor-enhances-your-workflow","title":"How Instructor Enhances Your Workflow","text":"<p>Instructor introduces three key enhancements compared to direct API usage.</p>"},{"location":"instructor/concepts/overview/#response-model","title":"Response Model","text":"<p>You just specify a PHP class to extract data into via the 'magic' of LLM chat completion. And that's it.</p> <p>Instructor reduces brittleness of the code extracting the information from textual data by leveraging structured LLM responses.</p> <p>Instructor helps you write simpler, easier to understand code - you no longer have to define lengthy function call definitions or write code for assigning returned JSON into target data objects.</p>"},{"location":"instructor/concepts/overview/#validation","title":"Validation","text":"<p>Response model generated by LLM can be automatically validated, following set of rules. Currently, Instructor supports only Symfony validation.</p> <p>You can also provide a context object to use enhanced validator capabilities.</p>"},{"location":"instructor/concepts/overview/#max-retries","title":"Max Retries","text":"<p>You can set the number of retry attempts for requests.</p> <p>Instructor will repeat requests in case of validation or deserialization error up to the specified number of times, trying to get a valid response from LLM.</p>"},{"location":"instructor/concepts/why/","title":"Why use Instructor?","text":"<p>Our library introduces three key enhancements:</p> <ul> <li>Response Model: Specify a data model to be returned by LLM to simplify your code.</li> <li>Validation: Automatically validate response generated by LLM before you start using it.</li> <li>Max Retries: Automated retry attempts for invalid responses.</li> </ul>"},{"location":"instructor/concepts/why/#a-glimpse-into-instructors-capabilities","title":"A Glimpse into Instructor's Capabilities","text":"<p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek.</p>"},{"location":"instructor/concepts/why/#understanding-the-workflow","title":"Understanding the workflow","text":"<p>Let's see how we can leverage it to make use of instructor</p>"},{"location":"instructor/concepts/why/#step-1-define-the-data-model","title":"Step 1: Define the data model","text":"<p>Create a data model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>// @doctest id=\"3cc2\"\n&lt;?php\nclass UserDetail {\n    public string $name;\n    public int $age;\n}\n</code></pre>"},{"location":"instructor/concepts/why/#step-2-extract","title":"Step 2: Extract","text":"<p>Use the <code>StructuredOutput::create()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies the model to use for extraction.</p> <pre><code>// @doctest id=\"d0e8\"\n/** @var UserDetail */\n$user = (new StructuredOutput)-&gt;with(\n    messages: [[\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"]],\n    responseModel: UserDetail::class,\n    model: \"gpt-3.5-turbo\",\n)-&gt;get();\n\nassert($user-&gt;name == \"Jason\")\nassert($user-&gt;age == 25)\n</code></pre> <p>It's helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p>"},{"location":"instructor/concepts/why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same data model. If the response triggers any validation rules Instructor will raise a validation error.</p>"},{"location":"instructor/concepts/why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>LeadReport</code> model is passed as the <code>$responseModel</code>, and <code>$maxRetries</code> is set to 2. It means that if the extracted data does not match the model, Instructor will re-ask the model 2 times before giving up.</p> <pre><code>// @doctest id=\"ed40\"\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via jason@gmailcom -- Jason\"]],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;email === \"jason@gmail.com\");\n</code></pre> <p>More about Validation</p> <p>Check out Jason's blog post Good LLM validation is just good validation</p>"},{"location":"instructor/concepts/why/#custom-validators","title":"Custom Validators","text":"<p>Instructor uses Symfony validation component to validate extracted data. You can use #[Assert/Callback] annotation to build fully customized validation logic.</p> <p>See Symfony docs for more details on how to use Callback constraint.</p> <pre><code>// @doctest id=\"53ed\"\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre>"},{"location":"instructor/essentials/configuration/","title":"Configuration Options","text":"<p>Instructor provides extensive configuration options through fluent API methods to customize its behavior, processing, and integration with various LLM providers.</p>"},{"location":"instructor/essentials/configuration/#request-configuration","title":"Request Configuration","text":"<p>Configure how Instructor processes your input and builds requests:</p> <pre><code>// @doctest id=\"456e\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withMessages($messages)           // Set chat messages\n    -&gt;withInput($input)                 // Set input (converted to messages)\n    -&gt;withSystem($systemPrompt)         // Set system prompt\n    -&gt;withPrompt($prompt)               // Set additional prompt\n    -&gt;withExamples($examples)           // Set example data for context\n    -&gt;withModel($modelName)             // Set LLM model name\n    -&gt;withOptions($options)             // Set LLM-specific options\n    -&gt;withOption($key, $value)          // Set individual LLM option\n    -&gt;withStreaming(true)               // Enable streaming responses\n    -&gt;withCachedContext($messages, $system, $prompt, $examples) // Use cached context\n</code></pre>"},{"location":"instructor/essentials/configuration/#response-configuration","title":"Response Configuration","text":"<p>Define how Instructor should process and validate responses:</p> <pre><code>// @doctest id=\"ee07\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withMaxRetries(3)                 // Set retry count for failed validations\n    -&gt;withOutputMode(OutputMode::Tools) // Set output mode (Tools, Json, JsonSchema, MdJson)\n    -&gt;withRetryPrompt($prompt)          // Set custom retry prompt for validation failures\n    -&gt;withSchemaName($name)             // Set schema name for documentation\n    -&gt;withToolName($name)               // Set tool name for Tools mode\n    -&gt;withToolDescription($description) // Set tool description for Tools mode\n</code></pre>"},{"location":"instructor/essentials/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<p>Fine-tune Instructor's internal processing:</p> <pre><code>// @doctest id=\"4bea\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withConfig($configObject)         // Use custom StructuredOutputConfig instance\n    -&gt;withConfigPreset($presetName)     // Use predefined configuration preset\n    -&gt;withConfigProvider($provider)     // Use custom configuration provider\n    -&gt;withObjectReferences(true)        // Enable object reference handling\n    -&gt;withDefaultToStdClass(true)       // Default to stdClass for unknown types\n    -&gt;withDeserializationErrorPrompt($prompt) // Custom deserialization error prompt\n    -&gt;withThrowOnTransformationFailure(true)  // Throw on transformation failures\n</code></pre>"},{"location":"instructor/essentials/configuration/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<p>Configure connection and communication with LLM providers:</p> <pre><code>// @doctest id=\"d5af\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;using($preset)                    // Use LLM preset (e.g., 'openai', 'anthropic')\n    -&gt;withDsn($dsn)                     // Set connection DSN\n    -&gt;withLLMProvider($provider)        // Set custom LLM provider instance\n    -&gt;withLLMConfig($config)            // Set LLM configuration object\n    -&gt;withLLMConfigOverrides($overrides) // Override specific LLM config values\n    -&gt;withDriver($driver)               // Set custom inference driver\n    -&gt;withHttpClient($client)           // Set custom HTTP client\n    -&gt;withHttpClientPreset($preset)     // Use HTTP client preset\n    -&gt;withDebugPreset($preset)          // Enable debug preset\n    -&gt;withClientInstance($driverName, $instance) // Set client instance for specific driver\n</code></pre>"},{"location":"instructor/essentials/configuration/#processing-pipeline-overrides","title":"Processing Pipeline Overrides","text":"<p>Customize validation, transformation, and deserialization:</p> <pre><code>// @doctest id=\"6924\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withValidators(...$validators)    // Override response validators\n    -&gt;withTransformers(...$transformers) // Override response transformers  \n    -&gt;withDeserializers(...$deserializers) // Override response deserializers\n</code></pre>"},{"location":"instructor/essentials/configuration/#event-handling","title":"Event Handling","text":"<p>Configure real-time processing callbacks:</p> <pre><code>// @doctest id=\"26a7\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;onPartialUpdate($callback)        // Handle partial response updates during streaming\n    -&gt;onSequenceUpdate($callback)       // Handle sequence item completion during streaming\n</code></pre>"},{"location":"instructor/essentials/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"instructor/essentials/configuration/#basic-openai-configuration","title":"Basic OpenAI Configuration","text":"<pre><code>// @doctest id=\"8b98\"\n$result = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4')\n    -&gt;withMaxRetries(3)\n    -&gt;withMessages(\"Extract person data from: John is 25 years old\")\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/configuration/#streaming-with-callbacks","title":"Streaming with Callbacks","text":"<pre><code>// @doctest id=\"e0d3\"\n$result = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withStreaming(true)\n    -&gt;onPartialUpdate(fn($partial) =&gt; updateUI($partial))\n    -&gt;withMessages(\"Generate a list of tasks\")\n    -&gt;withResponseClass(Sequence::of(Task::class))\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/configuration/#custom-configuration-object","title":"Custom Configuration Object","text":"<pre><code>// @doctest id=\"32f5\"\n$config = new StructuredOutputConfig(\n    maxRetries: 5,\n    outputMode: OutputMode::JsonSchema,\n    retryPrompt: \"Please fix the validation errors and try again.\"\n);\n\n$result = (new StructuredOutput)\n    -&gt;withConfig($config)\n    -&gt;withMessages($input)\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/customize_prompts/","title":"Customize prompts","text":""},{"location":"instructor/essentials/customize_prompts/#customizing-prompts","title":"Customizing prompts","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>create()</code> method.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"instructor/essentials/customize_prompts/#prompting-models-with-tool-calling-support","title":"Prompting models with tool calling support","text":"<p><code>OutputMode::Tools</code> is usually most reliable way to get structured outputs following provided response schema.</p> <p><code>OutputMode::Tools</code> can make use of <code>$toolName</code> and <code>$toolDescription</code> parameters to provide additional semantic context to the LLM, describing the tool to be used for processing the input. <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> ignore these parameters, as tools are not used in these modes.</p> <pre><code>// @doctest id=\"b92e\"\n&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        toolName: 'extract',\n        toolDescription: 'Extract information from provided content',\n        mode: OutputMode::Tools)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/customize_prompts/#prompting-models-supporting-json-output","title":"Prompting models supporting JSON output","text":"<p>Aside from tool calling Instructor supports two other modes for getting structured outputs from LLM: <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code>.</p> <p><code>OutputMode::Json</code> uses JSON mode offered by some models and API providers to get LLM respond in JSON format rather than plain text.</p> <p><pre><code>// @doctest id=\"11b9\"\n&lt;?php\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    prompt: \"\\nYour task is to respond correctly with JSON object.\",\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre> Note that various models and API providers have specific requirements on the input format, e.g. for OpenAI JSON mode you are required to include <code>JSON</code> string in the prompt.</p>"},{"location":"instructor/essentials/customize_prompts/#including-json-schema-in-the-prompt","title":"Including JSON Schema in the prompt","text":"<p>Instructor takes care of automatically setting the <code>response_format</code> parameter, but this may not be sufficient for some models or providers - some of them require specifying JSON response format as part of the prompt, rather than just as <code>response_format</code> parameter in the request (e.g. OpenAI).</p> <p>For this reason, when using Instructor's <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> you should include the expected JSON Schema in the prompt. Otherwise, the response is unlikely to match your target model, making it impossible for Instructor to deserialize it correctly.</p> <pre><code>// @doctest id=\"c659\"\n&lt;?php\n// NOTE: You don't have to create JSON Schema manually, you can use\n// schema automatically generated by Instructor for your response model.\n// See the next example.\n\n$jsonSchema = json_encode([\n    \"type\" =&gt; \"object\",\n    \"properties\" =&gt; [\n        \"name\" =&gt; [\"type\" =&gt; \"string\"],\n        \"age\" =&gt; [\"type\" =&gt; \"integer\"]\n    ],\n    \"required\" =&gt; [\"name\", \"age\"]\n]);\n\n$user = $structuredOutput\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema: $jsonSchema\\n\",\n        mode: OutputMode::Json)\n    -&gt;get();\n</code></pre> <p>The example above demonstrates how to manually create JSON Schema, but with Instructor you do not have to build the schema manually - you can use prompt template placeholder syntax to use Instructor-generated JSON Schema.</p>"},{"location":"instructor/essentials/customize_prompts/#prompt-as-template","title":"Prompt as template","text":"<p>Instructor allows you to use a template string as a prompt. You can use <code>&lt;|variable|&gt;</code> placeholders in the template string, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>&lt;|json_schema|&gt;</code> - replaced with the JSON Schema for current response model</p> <p>Example below demonstrates how to use a template string as a prompt:</p> <pre><code>// @doctest id=\"4bfd\"\n&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/customize_prompts/#prompting-the-models-with-no-support-for-tool-calling-or-json-output","title":"Prompting the models with no support for tool calling or JSON output","text":"<p><code>OutputMode::MdJson</code> is the most basic (and least reliable) way to get structured outputs from LLM. Still, you may want to use it with the models which do not support tool calling or JSON output.</p> <p><code>OutputMode::MdJson</code> relies on the prompting to get LLM response in JSON formatted data.</p> <p>Many models prompted in this mode will respond with a mixture of plain text and JSON data. Instructor will try to find JSON data fragment in the response and ignore the rest of the text.</p> <p>This approach is most prone to deserialization and validation errors and needs providing JSON Schema in the prompt to increase the probability that the response is correctly structured and contains the expected data.</p> <p><code>php // @doctest id=\"86e8\" &lt;?php $user = (new StructuredOutput)     -&gt;request(         messages: \"Our user Jason is 25 years old.\",         responseModel: User::class,         prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a</code>json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",         mode: OutputMode::MdJson)     -&gt;get();</p>"},{"location":"instructor/essentials/data_model/","title":"Data model","text":"<p>Instructor provides several ways the data model of LLM response.</p>"},{"location":"instructor/essentials/data_model/#using-classes","title":"Using classes","text":"<p>The default way is to use PHP classes to define the data model. You can also use PHPDoc comments to specify the types of fields of the response. Additionally, you can use attributes to provide more context to the language model or to provide additional instructions to the model.</p>"},{"location":"instructor/essentials/data_model/#type-hints","title":"Type Hints","text":"<p>Use PHP type hints to specify the type of extracted data.</p> <p>Use nullable types to indicate that given field is optional.</p> <pre><code>// @doctest id=\"2974\"\n&lt;?php\n\nclass Person {\n    public string $name;\n    public ?int $age;\n    public Address $address;\n}\n</code></pre> <p>Instructor will only fill in the fields that are public. Private and protected fields are ignored and their values are not going to be extracted (they will be left empty, with default values set as defined in your class).</p>"},{"location":"instructor/essentials/data_model/#private-vs-public-object-field","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM.</p> <p>Private and protected fields are left unchanged, unless the class has setter methods defined or there are parameters in the constructor that match the field names.</p> <p>Provide default values for the fields that are not set by Instructor, to avoid unexpected behavior when accessing those fields.</p> <p>See:  - <code>examples/A01_Basics/BasicPrivateVsPublicFields/run.php</code> to check the details on the behavior of extraction for classes with private and public fields,  - <code>examples/A01_Basics/BasicGetSet/run.php</code> to see how Instructor uses getter and setter methods,  - <code>examples/A01_Basics/BasicConstructor/run.php</code> to see how Instructor uses constructor parameters.</p>"},{"location":"instructor/essentials/data_model/#docblock-type-hints","title":"DocBlock type hints","text":"<p>You can also use PHP DocBlock style comments to specify the type of extracted data. This is useful when you want to specify property types for LLM, but can't or don't want to enforce type at the code level.</p> <pre><code>// @doctest id=\"7dd8\"\n&lt;?php\n\nclass Person {\n    /** @var string */\n    public $name;\n    /** @var int */\n    public $age;\n    /** @var Address $address person's address */\n    public $address;\n}\n</code></pre> <p>See PHPDoc documentation for more details on DocBlock: https://docs.phpdoc.org/3.0/guide/getting-started/what-is-a-docblock.html#what-is-a-docblock</p>"},{"location":"instructor/essentials/data_model/#using-docblocks-as-additional-instructions-for-llm","title":"Using DocBlocks as Additional Instructions for LLM","text":"<p>You can use PHP DocBlocks (/** */) to provide additional instructions for LLM at class or field level, for example to clarify what you expect or how LLM should process your data.</p> <p>Instructor extracts PHP DocBlocks comments from class and property defined and includes them in specification of response model sent to LLM.</p> <p>Using PHP DocBlocks instructions is not required, but sometimes you may want to clarify your intentions to improve LLM's inference results.</p> <pre><code>// @doctest id=\"49f5\"\n    /**\n     * Represents a skill of a person and context in which it was mentioned. \n     */\n    class Skill {\n        public string $name;\n        /** @var SkillType $type type of the skill, derived from the description and context */\n        public SkillType $type;\n        /** Directly quoted, full sentence mentioning person's skill */\n        public string $context;\n    }\n</code></pre>"},{"location":"instructor/essentials/data_model/#attributes-for-data-model-descriptions-and-instructions","title":"Attributes for data model descriptions and instructions","text":"<p>Instructor supports <code>#[Description]</code> and <code>#[Instructions]</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p> <p><code>#[Description]</code> attribute is used to describe a class or property in your data model. Instructor will use this text to provide more context to the language model.</p> <p><code>#[Instructions]</code> attribute is used to provide additional instructions to the language model, such as how to process the data.</p> <p>You can add multiple attributes to a class or property - Instructor will merge them into a single block of text.</p> <p>Instructor will still include any PHPDoc comments provided in the class, but using attributes might be more convenient and easier to read.</p> <pre><code>// @doctest id=\"0d0d\"\n&lt;?php\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make it ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    public string $job;\n}\n</code></pre> <p>NOTE: Technically both <code>#[Description]</code> and <code>#[Instructions]</code> attributes do the same thing - they provide additional context to the language model. Yet, providing them in separate attributes allows you to better organize your code and make it more readable. In the future, we may extend the functionality of these attributes to provide more specific instructions to the language model, so it is a good idea to use them now.</p>"},{"location":"instructor/essentials/data_model/#typed-collections-arrays","title":"Typed Collections / Arrays","text":"<p>PHP currently does not support generics or typehints to specify array element types.</p> <p>Use PHP DocBlock style comments to specify the type of array elements.</p> <pre><code>// @doctest id=\"8fad\"\n&lt;?php\nclass Person {\n    // ...\n}\n\nclass Event {\n    // ...\n    /** @var Person[] list of extracted event participants */\n    public array $participants;\n    // ...\n}\n</code></pre>"},{"location":"instructor/essentials/data_model/#example-of-complex-data-extraction","title":"Example of complex data extraction","text":"<p>Instructor can retrieve complex data structures from text. Your response model can contain nested objects, arrays, and enums.</p> <pre><code>// @doctest id=\"472d\"\n&lt;?php\nuse Cognesy/Instructor/Instructor;\n\n// define a data structures to extract data into\nclass Person {\n    public string $name;\n    public int $age;\n    public string $profession;\n    /** @var Skill[] */\n    public array $skills;\n}\n\nclass Skill {\n    public string $name;\n    public SkillType $type;\n}\n\nenum SkillType : string {\n    case Technical = 'technical';\n    case Other = 'other';\n}\n\n$text = \"Alex is 25 years old software engineer, who knows PHP, Python and can play the guitar.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n)-&gt;get(); // client is passed explicitly, can specify e.g. different base URL\n\n// data is extracted into an object of given class\nassert($person instanceof Person); // true\n\n// you can access object's extracted property values\necho $person-&gt;name; // Alex\necho $person-&gt;age; // 25\necho $person-&gt;profession; // software engineer\necho $person-&gt;skills[0]-&gt;name; // PHP\necho $person-&gt;skills[0]-&gt;type; // SkillType::Technical\n// ...\n\nvar_dump($person);\n// Person {\n//     name: \"Alex\",\n//     age: 25,\n//     profession: \"software engineer\",\n//     skills: [\n//         Skill {\n//              name: \"PHP\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"Python\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"guitar\",\n//              type: SkillType::Other\n//         },\n//     ]\n// }\n</code></pre>"},{"location":"instructor/essentials/data_model/#dynamic-data-schemas-with-structure-class","title":"Dynamic data schemas with <code>Structure</code> class","text":"<p>In case you work with dynamic data schemas, you can use <code>Structure</code> class to define the data model.</p> <p>See Structures for more details on how to work with dynamic data schemas.</p>"},{"location":"instructor/essentials/data_model/#optional-data-with-maybe-class","title":"Optional data with <code>Maybe</code> class","text":"<p>The <code>Maybe</code> class provides a way to handle optional data that may or may not be present in the input text. It wraps a value type and indicates whether the data was found or not, along with an error message when the data is missing.</p>"},{"location":"instructor/essentials/data_model/#basic-usage","title":"Basic Usage","text":"<pre><code>// @doctest id=\"3f65\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\n\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n$maybe = Maybe::is(Person::class, 'person', 'Person data if found in the text');\n\n$result = (new StructuredOutput)\n    -&gt;with(\n        messages: \"The document mentions some information but no person details.\",\n        responseModel: $maybe,\n    )\n    -&gt;get();\n\nif ($result-&gt;hasValue()) {\n    $person = $result-&gt;get();\n    echo \"Found person: \" . $person-&gt;name;\n} else {\n    echo \"No person found. Error: \" . $result-&gt;error();\n}\n</code></pre>"},{"location":"instructor/essentials/data_model/#maybe-methods","title":"Maybe Methods","text":"<ul> <li><code>Maybe::is(class, name?, description?)</code> - Static factory method to create a Maybe instance</li> <li><code>get()</code> - Get the value if present, or null if not found</li> <li><code>error()</code> - Get the error message explaining why the value wasn't found</li> <li><code>hasValue()</code> - Check if a value was successfully extracted</li> <li><code>toJsonSchema()</code> - Generate JSON schema for the Maybe wrapper</li> </ul>"},{"location":"instructor/essentials/demonstrations/","title":"Demonstrations","text":""},{"location":"instructor/essentials/demonstrations/#providing-examples-to-llm","title":"Providing examples to LLM","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> modes, where the output is expected to be a JSON object.</p> <p>Instructor's <code>request()</code> method accepts an array of examples as the <code>examples</code> parameter, where each example is an instance of the <code>Example</code> class.</p>"},{"location":"instructor/essentials/demonstrations/#example-class","title":"<code>Example</code> class","text":"<p><code>Example</code> constructor have two main arguments: <code>input</code> and <code>output</code>.</p> <p>The <code>input</code> property is  a string which describes the input message, while he <code>output</code> property is an array which represents the expected output.</p> <p>Instructor will append the list of examples to the prompt sent to LLM, with output array data rendered as JSON text.</p> <pre><code>// @doctest id=\"7057\"\n&lt;?php\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n        ),\n    ],\n    mode: OutputMode::Json\n)-&gt;get();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#modifying-the-example-template","title":"Modifying the example template","text":"<p>You can use a template string as an input for the Example class. The template string may contain placeholders for the input data, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>{input}</code> - replaced with the actual input message  - <code>{output}</code> - replaced with the actual output data</p> <p>In case input or output data is an array, Instructor will automatically convert it to a JSON string before replacing the placeholders.</p> <pre><code>// @doctest id=\"c872\"\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50],\n            template: \"EXAMPLE:\\n{input} =&gt; {output}\\n\",\n        ),\n    ],\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#convenience-factory-methods","title":"Convenience factory methods","text":"<p>You can also create Example instances using the <code>fromText()</code>, <code>fromChat()</code>, <code>fromData()</code> helper static methods. All of them accept $output as an array of the expected output data and differ in the way the input data is provided.</p>"},{"location":"instructor/essentials/demonstrations/#make-example-from-text","title":"Make example from text","text":"<p><code>Example::fromText()</code> method accepts a string as an input. It is equivalent to creating an instance of Example using the constructor.</p> <pre><code>// @doctest id=\"9485\"\n$example = Example::fromText(\n    input: 'Ian is 27 yo',\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#make-example-from-chat","title":"Make example from chat","text":"<p><code>Example::fromChat()</code> method accepts an array of messages, which may be useful when you want to use a chat or chat fragment as a demonstration of the input.</p> <pre><code>// @doctest id=\"9e10\"\n$example = Example::fromChat(\n    input: [['role' =&gt; 'user', 'content' =&gt; 'Ian is 27 yo']],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#make-example-from-data","title":"Make example from data","text":"<p><code>Example::fromData()</code> method accepts any data type and uses the <code>Json::encode()</code> method to convert it to a string. It may be useful to provide a complex data structure as an example input.</p> <pre><code>// @doctest id=\"116a\"\n$example = Example::fromData(\n    input: ['firstName' =&gt; 'Ian', 'lastName' =&gt; 'Brown', 'birthData' =&gt; '1994-01-01'],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"instructor/essentials/modes/","title":"Modes","text":""},{"location":"instructor/essentials/modes/#extraction-modes","title":"Extraction modes","text":"<p>Instructor supports several ways to extract data from the response.</p>"},{"location":"instructor/essentials/modes/#output-modes","title":"Output Modes","text":"<p>Instructor supports multiple output modes to allow working with various models depending on their capabilities. - <code>OutputMode::Json</code> - generate structured output via LLM's native JSON generation - <code>OutputMode::JsonSchema</code> - use LLM's strict JSON Schema mode to enforce JSON Schema - <code>OutputMode::Tools</code> - use tool calling API to get LLM follow provided schema - <code>OutputMode::MdJson</code> - use prompting to generate structured output; fallback for the models that do not support JSON generation or tool calling</p> <p>Additionally, you can use <code>Text</code> and <code>Unrestricted</code> modes to get LLM to generate text output without any structured data extraction.</p> <p>Those modes are not useful for <code>StructuredOutput</code> class (as it is focused on structured output generation) but can be used with <code>Inference</code> class.</p> <ul> <li><code>OutputMode::Text</code> - generate text output</li> <li><code>OutputMode::Unrestricted</code> - generate unrestricted output based on inputs provided by the user (with no enforcement of specific output format)</li> </ul>"},{"location":"instructor/essentials/modes/#example-of-using-modes","title":"Example of Using Modes","text":"<p>Mode can be set via parameter of <code>StructuredOutput::create()</code> method.</p> <p>The default mode is <code>OutputMode::Tools</code>, which leverages OpenAI-style tool calls.</p> <p><pre><code>// @doctest id=\"d733\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$structuredOutput = new StructuredOutput();\n\n$response = $structuredOutput-&gt;with(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre> Mode, like other parameters can also be set via fluent API methods.</p> <pre><code>// @doctest id=\"848f\"\n&lt;?php\n$response = $structuredOutput\n    -&gt;withMessages(\"...\")\n    -&gt;withResponseModel(...)\n    //...\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/modes/#modes","title":"Modes","text":""},{"location":"instructor/essentials/modes/#outputmodetools","title":"<code>OutputMode::Tools</code>","text":"<p>This mode is the default one. It uses OpenAI tools to extract data from the response.</p> <p>It is the most reliable mode, but not all models and API providers support it - check their documentation for more information.</p> <ul> <li>https://platform.openai.com/docs/guides/function-calling</li> <li>https://docs.anthropic.com/en/docs/build-with-claude/tool-use</li> <li>https://docs.mistral.ai/capabilities/function_calling/</li> </ul>"},{"location":"instructor/essentials/modes/#outputmodejson","title":"<code>OutputMode::Json</code>","text":"<p>In this mode Instructor provides response format as JSONSchema and asks LLM to respond with JSON object following provided schema.</p> <p>It is supported by many open source models and API providers - check their documentation.</p> <p>See more about JSON mode in:</p> <ul> <li>https://platform.openai.com/docs/guides/text-generation/json-mode</li> <li>https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency</li> <li>https://docs.mistral.ai/capabilities/json_mode/</li> </ul>"},{"location":"instructor/essentials/modes/#outputmodejsonschema","title":"<code>OutputMode::JsonSchema</code>","text":"<p>In contrast to <code>OutputMode::Json</code> which may not always manage to meet the schema requirements, <code>OutputMode::JsonSchema</code> is strict and guarantees the response to be a valid JSON object that matches the provided schema.</p> <p>It is currently supported only by new OpenAI models (check their docs for details).</p> <p>NOTE: OpenAI JsonSchema mode does not support optional properties. If you need to have optional properties in your schema, use <code>OutputMode::Tools</code> or <code>OutputMode::Json</code>.</p> <p>See more about JSONSchema mode in:</p> <ul> <li>https://platform.openai.com/docs/guides/structured-outputs</li> </ul>"},{"location":"instructor/essentials/modes/#outputmodemdjson","title":"<code>OutputMode::MdJson</code>","text":"<p>In this mode Instructor asks LLM to answer with JSON object following provided schema and return answer as Markdown codeblock.</p> <p>It may improve the results for LLMs that have not been finetuned to respond with JSON as they are likely to be already trained on large amounts of programming docs and have seen a lot of properly formatted JSON objects within MD codeblocks.</p>"},{"location":"instructor/essentials/scalars/","title":"Scalars","text":""},{"location":"instructor/essentials/scalars/#extracting-scalar-values","title":"Extracting Scalar Values","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p> <pre><code>// @doctest id=\"426e\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::integer('age'),\n    )\n    -&gt;get();\n\nvar_dump($value);\n// int(28)\n</code></pre> <p>In this example, we're extracting a single integer value from the text. You can also use <code>Scalar::string()</code>, <code>Scalar::boolean()</code> and <code>Scalar::float()</code> to extract other types of values.</p> <p>Additionally, you can use Scalar adapter to extract enums via <code>Scalar::enum()</code>.</p>"},{"location":"instructor/essentials/scalars/#examples","title":"Examples","text":""},{"location":"instructor/essentials/scalars/#string-result","title":"String result","text":"<pre><code>// @doctest id=\"67b0\"\n&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::string(name: 'firstName'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe(\"Jason\");\n</code></pre>"},{"location":"instructor/essentials/scalars/#integer-result","title":"Integer result","text":"<pre><code>// @doctest id=\"6d82\"\n&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::integer('age'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeInt();\n// expect($value)-&gt;toBe(28);\n</code></pre>"},{"location":"instructor/essentials/scalars/#boolean-result","title":"Boolean result","text":"<pre><code>// @doctest id=\"fcbb\"\n&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::boolean(name: 'isAdult'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeBool();\n// expect($value)-&gt;toBe(true);\n</code></pre>"},{"location":"instructor/essentials/scalars/#float-result","title":"Float result","text":"<pre><code>// @doctest id=\"ade5\"\n&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old and his 100m sprint record is 11.6 seconds.\",\n        responseModel: Scalar::float(name: 'recordTime'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeFloat();\n// expect($value)-&gt;toBe(11.6);\n</code></pre>"},{"location":"instructor/essentials/scalars/#enum-result-select-one-of-the-options","title":"Enum result / select one of the options","text":"<pre><code>// @doctest id=\"7178\"\n&lt;?php\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'system', 'content' =&gt; $text],\n            ['role' =&gt; 'user', 'content' =&gt; 'What is Jason\\'s citizenship?'],\n        ],\n        responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n    )-&gt;get();\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe('other');\n</code></pre>"},{"location":"instructor/essentials/usage/","title":"Usage","text":""},{"location":"instructor/essentials/usage/#basic-usage","title":"Basic usage","text":"<p>This is a simple example demonstrating how Instructor retrieves structured information from provided text (or chat message sequence).</p> <p>Response model class is a plain PHP class with typehints specifying the types of fields of the object.</p> <p>NOTE: By default, Instructor looks for OPENAI_API_KEY environment variable to get your API key. You can also provide the API key explicitly when creating the Instructor instance.</p> <pre><code>// @doctest id=\"bb59\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Step 0: Create .env file in your project root:\n// OPENAI_API_KEY=your_api_key\n\n// Step 1: Define target data structure(s)\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n// Step 2: Provide content to process\n$text = \"His name is Jason and he is 28 years old.\";\n\n// Step 3: Use Instructor to run LLM inference\n$person = (new StructuredOutput)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        responseModel: Person::class,\n    )\n    -&gt;get();\n\n// Step 4: Work with structured response data\nassert($person instanceof Person); // true\nassert($person-&gt;name === 'Jason'); // true\nassert($person-&gt;age === 28); // true\n\necho $person-&gt;name; // Jason\necho $person-&gt;age; // 28\n\nvar_dump($person);\n// Person {\n//     name: \"Jason\",\n//     age: 28\n// }\n?&gt;\n</code></pre> <p>Note</p> <p>Instructor supports classes/objects as response models, as well as specialized helper classes like <code>Scalar</code> for simple values, <code>Maybe</code> for optional data, <code>Sequence</code> for arrays, and <code>Structure</code> for dynamically defined schemas.</p>"},{"location":"instructor/essentials/usage/#fluent-api-methods","title":"Fluent API Methods","text":"<p>StructuredOutput provides a comprehensive fluent API for configuring requests:</p>"},{"location":"instructor/essentials/usage/#request-configuration","title":"Request Configuration","text":"<pre><code>// @doctest id=\"b5e0\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withMessages($messages)           // Set chat messages\n    -&gt;withInput($input)                 // Set input (converted to messages)\n    -&gt;withSystem($systemPrompt)         // Set system prompt\n    -&gt;withPrompt($prompt)               // Set additional prompt\n    -&gt;withExamples($examples)           // Set example data\n    -&gt;withModel($modelName)             // Set LLM model\n    -&gt;withOptions($options)             // Set LLM options\n    -&gt;withStreaming(true)               // Enable streaming\n</code></pre>"},{"location":"instructor/essentials/usage/#response-model-configuration","title":"Response Model Configuration","text":"<pre><code>// @doctest id=\"6035\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withResponseModel($model)         // Set response model (class/object/array)\n    -&gt;withResponseClass($className)     // Set response class specifically\n    -&gt;withResponseObject($object)       // Set response object instance\n    -&gt;withResponseJsonSchema($schema)   // Set JSON schema directly\n</code></pre>"},{"location":"instructor/essentials/usage/#configuration-and-behavior","title":"Configuration and Behavior","text":"<pre><code>// @doctest id=\"b098\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withMaxRetries(3)                 // Set retry count\n    -&gt;withOutputMode($mode)             // Set output mode\n    -&gt;withToolName($name)               // Set tool name for Tools mode\n    -&gt;withToolDescription($desc)        // Set tool description\n    -&gt;withRetryPrompt($prompt)          // Set retry prompt\n    -&gt;withConfig($config)               // Set configuration object\n    -&gt;withConfigPreset($preset)         // Use configuration preset\n</code></pre>"},{"location":"instructor/essentials/usage/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<pre><code>// @doctest id=\"d0a3\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;using($preset)                    // Use LLM preset (e.g., 'openai')\n    -&gt;withDsn($dsn)                     // Set connection DSN\n    -&gt;withLLMProvider($provider)        // Set custom LLM provider\n    -&gt;withLLMConfig($config)            // Set LLM configuration\n    -&gt;withDriver($driver)               // Set inference driver\n    -&gt;withHttpClient($client)           // Set HTTP client\n</code></pre>"},{"location":"instructor/essentials/usage/#processing-overrides","title":"Processing Overrides","text":"<pre><code>// @doctest id=\"7175\"\n$structuredOutput = (new StructuredOutput)\n    -&gt;withValidators(...$validators)    // Override validators\n    -&gt;withTransformers(...$transformers) // Override transformers\n    -&gt;withDeserializers(...$deserializers) // Override deserializers\n</code></pre>"},{"location":"instructor/essentials/usage/#request-execution-methods","title":"Request Execution Methods","text":"<p>After configuring your <code>StructuredOutput</code> instance, you have several ways to execute the request and access different types of responses:</p>"},{"location":"instructor/essentials/usage/#direct-execution-methods","title":"Direct Execution Methods","text":"<pre><code>// @doctest id=\"6ceb\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$structuredOutput = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n\n// Get structured result directly\n$person = $structuredOutput-&gt;get();\n\n// Get raw LLM response\n$llmResponse = $structuredOutput-&gt;response();\n\n// Get streaming interface\n$stream = $structuredOutput-&gt;stream();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#pending-execution-with-create","title":"Pending Execution with <code>create()</code>","text":"<p>The <code>create()</code> method returns a <code>PendingStructuredOutput</code> instance, which acts as an execution handler that provides the same access methods:</p> <pre><code>// @doctest id=\"871c\"\n&lt;?php\n$pending = $structuredOutput-&gt;create();\n\n// Execute and get structured result\n$person = $pending-&gt;get();\n\n// Execute and get raw LLM response\n$llmResponse = $pending-&gt;response();\n\n// Execute and get streaming interface\n$stream = $pending-&gt;stream();\n\n// Additional utility methods\n$json = $pending-&gt;toJson();      // Convert result to JSON string\n$array = $pending-&gt;toArray();    // Convert result to array\n$jsonObj = $pending-&gt;toJsonObject(); // Convert result to Json object\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#response-types-explained","title":"Response Types Explained","text":"<ul> <li><code>get()</code>: Returns the parsed and validated structured result (e.g., <code>Person</code> object)</li> <li><code>response()</code>: Returns the raw LLM response object with metadata like tokens, model info, etc.</li> <li><code>stream()</code>: Returns <code>StructuredOutputStream</code> for real-time processing of streaming responses</li> </ul> <p>The <code>PendingStructuredOutput</code> class serves as a flexible execution interface that lets you choose how to process the LLM response based on your specific needs.</p>"},{"location":"instructor/essentials/usage/#string-as-input","title":"String as Input","text":"<p>You can provide a string instead of an array of messages. This is useful when you want to extract data from a single block of text and want to keep your code simple.</p> <pre><code>// @doctest id=\"3e1c\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Person::class,\n    )\n    -&gt;get();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#structured-to-structured-data-processing","title":"Structured-to-structured data processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>with()</code> method can be an object, but also an array or just a string.</p> <pre><code>// @doctest id=\"14bf\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new StructuredOutput)-&gt;with(\n        input: $email,\n        responseModel: Email::class,\n        prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n    )\n    -&gt;get();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#streaming-support","title":"Streaming support","text":"<p>Instructor supports streaming of partial results, allowing you to start processing the data as soon as it is available.</p> <pre><code>// @doctest id=\"0868\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n    options: ['stream' =&gt; true]\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $partialPerson) {\n    // process partial person data\n    echo \"Name: \" $partialPerson-&gt;name ?? '...';\n    echo \"Age: \" $partialPerson-&gt;age ?? '...';\n}\n\n// after streaming is done you can get the final, fully processed person object...\n$person = $stream-&gt;lastUpdate()\n// ...to, for example, save it to the database\n$db-&gt;save($person);\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#scalar-responses","title":"Scalar responses","text":"<p>See Scalar responses for more information on how to generate scalar responses with <code>Scalar</code> adapter class.</p>"},{"location":"instructor/essentials/usage/#partial-responses-and-streaming","title":"Partial responses and streaming","text":"<p>See Streaming and partial updates for more information on how to work with partial updates and streaming.</p>"},{"location":"instructor/essentials/usage/#extracting-arguments-for-function-call","title":"Extracting arguments for function call","text":"<p>See FunctionCall helper class for more information on how to extract arguments for callable objects.</p>"},{"location":"instructor/essentials/usage/#execution-methods-summary","title":"Execution Methods Summary","text":"<p>Once configured, you can execute your request using different methods depending on your needs:</p> <pre><code>// @doctest id=\"afc1\"\n// Direct execution methods\n$result = $structuredOutput-&gt;get();       // Get structured result\n$response = $structuredOutput-&gt;response(); // Get raw LLM response  \n$stream = $structuredOutput-&gt;stream();     // Get streaming interface\n\n// Or use create() to get PendingStructuredOutput for flexible execution\n$pending = $structuredOutput-&gt;create();\n$result = $pending-&gt;get();                 // Same methods available\n$json = $pending-&gt;toJson();               // Plus utility methods\n</code></pre> <ul> <li><code>get()</code>: Returns the parsed and validated structured result</li> <li><code>response()</code>: Returns the raw LLM response with metadata</li> <li><code>stream()</code>: Returns <code>StructuredOutputStream</code> for real-time processing</li> <li><code>create()</code>: Returns <code>PendingStructuredOutput</code> for flexible execution control</li> </ul>"},{"location":"instructor/essentials/validation/","title":"Validation","text":""},{"location":"instructor/essentials/validation/#basic-validation","title":"Basic validation","text":"<p>Instructor validates results of LLM response against validation rules specified in your data model.</p> <pre><code>// @doctest id=\"d1d4\"\n&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is Jason, he is -28 years old.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: Person::class,\n)-&gt;get();\n\n// if the resulting object does not validate, Instructor throws an exception\n</code></pre> <p>NOTE: For further details on available validation rules, check Symfony Validation constraints.</p>"},{"location":"instructor/essentials/validation/#max-retries","title":"Max Retries","text":"<p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <pre><code>// @doctest id=\"1eca\"\n&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    #[Assert\\Length(min: 3)]\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is JX, aka Jason, he is -28 years old.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: Person::class,\n    maxRetries: 3,\n)-&gt;get();\n\n// if all LLM's attempts to self-correct the results fail, Instructor throws an exception\n</code></pre>"},{"location":"instructor/essentials/validation/#custom-validation","title":"Custom Validation","text":"<p>You can easily add custom validation code to your response model by using <code>ValidationTrait</code> and defining validation logic in <code>validate()</code> method.</p> <pre><code>// @doctest id=\"3c9a\"\n&lt;?php\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $age;\n\n    public function validate() : array {\n        if ($this-&gt;name === strtoupper($this-&gt;name)) {\n            return [];\n        }\n        return [[\n            'message' =&gt; \"Name must be in uppercase.\",\n            'path' =&gt; 'name',\n            'value' =&gt; $this-&gt;name\n        ]];\n    }\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note that method <code>validate()</code> has to return:  * an empty array if the object is valid,  * or an array of validation violations.</p> <p>This information will be used by LLM to make subsequent attempts to correct the response.</p> <pre><code>// @doctest id=\"138f\"\n$violations = [\n    [\n        'message' =&gt; \"Error message with violation details.\",\n        'path' =&gt; 'path.to.property',\n        'value' =&gt; '' // invalid value\n    ],\n    // ...other violations\n];\n</code></pre>"},{"location":"instructor/essentials/validation/#custom-validation-via-symfony-assertcallback","title":"Custom Validation via Symfony <code>#[Assert/Callback]</code>","text":"<p>Instructor uses Symfony validation component to validate extracted data.</p> <p>You can use <code>#[Assert/Callback]</code> annotation to build fully customized validation logic.</p> <pre><code>// @doctest id=\"f764\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )\n    -&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>NOTE: See Symfony docs for more details on how to use Callback constraint.</p>"},{"location":"instructor/internals/config_files/","title":"Config files","text":""},{"location":"instructor/internals/config_files/#configuration-groups","title":"Configuration Groups","text":"<p>Instructor's configuration is organized into groups. Each group contains a set of settings that are related to a specific aspect of Instructor's functionality.</p> <p>Instructor comes with the following default settings groups: - <code>debug</code>: Debugging settings - <code>embed</code>: Embedding provider connections - <code>http</code>: HTTP client configurations - <code>llm</code>: LLM provider connections - <code>prompt</code>: Prompt libraries and their settings - <code>web</code>: Web service providers (e.g. scraper API configurations)</p> <p>Each group is stored in a separate file in the configuration directory. The file name corresponds to the group name.</p>"},{"location":"instructor/internals/configuration_path/","title":"Configuration Path","text":"<p>Instructor comes with a set of configuration files and prompt templates that you can publish to your project directory.</p> <p>There are 2 ways to set up the location of Instructor's configuration directory: - Using <code>Settings</code> class method <code>setPath()</code> - Using environment variable (recommended)</p> <p> To check how to publish configuration files to your project see Setup section. </p>"},{"location":"instructor/internals/configuration_path/#setting-configuration-path-via-settings-class","title":"Setting Configuration Path via <code>Settings</code> Class","text":"<p>You can set Instructor configuration path using the <code>Settings::setPath()</code> method:</p> <pre><code>// @doctest id=\"09ca\"\n&lt;?php\nuse Cognesy\\Config\\Settings;\n\nSettings::setPath('/path/to/config');\n?&gt;\n</code></pre>"},{"location":"instructor/internals/configuration_path/#setting-configuration-path-via-environment-variable","title":"Setting Configuration Path via Environment Variable","text":"<p>You can set the path to Instructor's configuration directory in your <code>.env</code> file:</p> <pre><code># @doctest id=\"69a1\"\nINSTRUCTOR_CONFIG_PATHS='/path/to/config/,another/path'\n</code></pre>"},{"location":"instructor/internals/configuration_path/#configuration-location-resolution","title":"Configuration Location Resolution","text":"<p>Instructor uses a configuration directory with a set of <code>.php</code> files to store its settings, e.g. LLM provider configurations.</p> <p>Instructor will look for its configuration location in the following order: - If static variable value <code>$path</code> in <code>Settings</code> class is set, it will use it, - If <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable is set, it will use its value, - Finally, it will default to the directory, which is bundled with Instructor package (under <code>/config</code>) and contains default set of configuration files.</p>"},{"location":"instructor/internals/debugging/","title":"Debugging","text":"<p>Instructor offers several ways to debug it's internal state and execution flow.</p>"},{"location":"instructor/internals/debugging/#events","title":"Events","text":"<p>Instructor emits events at various points in its lifecycle, which you can listen to and react to. You can use these events to debug execution flow and to inspect data at various stages of processing.</p> <p>For more details see the Events section.</p>"},{"location":"instructor/internals/debugging/#http-debugging","title":"HTTP Debugging","text":"<p>The <code>StructuredOutput</code> class has a <code>withDebug()</code> method that can be used to debug the request and response.</p> <pre><code>// @doctest id=\"c629\"\n$result = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: \"Jason is 25 years old\",\n        responseModel: User:class,\n    )\n    -&gt;get();\n</code></pre> <p>It displays detailed information about the request being sent to LLM API and response received from it, including:</p> <ul> <li>request headers, URI, method and body,</li> <li>response status, headers, and body.</li> </ul> <p>This is useful for debugging the request and response when you are not getting the expected results.</p>"},{"location":"instructor/internals/environment/","title":"Environment","text":""},{"location":"instructor/internals/environment/#environment-configuration","title":"Environment Configuration","text":"<p>Instructor uses environment variables for configuration settings and API keys. The library comes with a <code>.env-dist</code> template file that lists all supported variables.</p> <p>You can copy it to <code>.env</code> (or merge with your <code>.env</code> file) and fill in your values.</p> <p>Alternatively, you can set the variables directly in your environment</p> <p>Check setup instructions for more details on how to set up your environment and how it can be done automatically with the Instructor's CLI tool.</p> <p>     Keep your .env file secure and never commit it to version control.     For production, consider using your environment's secrets management system. </p>"},{"location":"instructor/internals/environment/#llm-provider-api-keys","title":"LLM Provider API Keys","text":"<p>Instructor supports multiple LLM providers.</p> <p>Configure the ones you plan to use:</p> <pre><code># @doctest id=\"02cb\"\n# OpenAI (default provider)\nOPENAI_API_KEY=''\n\n# Alternative providers\nANTHROPIC_API_KEY=''\nANYSCALE_API_KEY=''\nAZURE_OPENAI_API_KEY=''\nAZURE_OPENAI_EMBED_API_KEY=''\nCOHERE_API_KEY=''\nFIREWORKS_API_KEY=''\nGEMINI_API_KEY=''\nGROK_API_KEY=''\nGROQ_API_KEY=''\nMISTRAL_API_KEY=''\nOLLAMA_API_KEY=''\nOPENROUTER_API_KEY=''\nTOGETHER_API_KEY=''\nJINA_API_KEY=''\n</code></pre> <p>Only configure the services you plan to use; others can remain empty.</p>"},{"location":"instructor/internals/environment/#other-api-keys-in-env-dist","title":"Other API Keys in <code>.env-dist</code>","text":"<p>Instructor comes with bundled add-on capabilities that use other APIs for various purposes. You can find them in the <code>.env-dist</code> file. They are not required for the core functionality of Instructor.</p>"},{"location":"instructor/internals/environment/#instructor-configuration-directory-path","title":"Instructor Configuration Directory Path","text":"<p>Instructor uses a configuration directory to store its settings, e.g. LLM provider configurations.</p> <p>You can set the path to this directory in your <code>.env</code> file: <pre><code>// @doctest id=\"5f74\"\nINSTRUCTOR_CONFIG_PATHS='/../../config/,another/path'\n</code></pre></p> <p>This tells Instructor where to find its configuration files, if it has not been configured manually via <code>Settings</code> class. The path is relative to the vendor directory where Instructor is installed.</p> <p> <code>INSTRUCTOR_CONFIG_PATHS</code> is set automatically if you use the Instructor CLI tool to publish assets. </p>"},{"location":"instructor/internals/events/","title":"Events","text":""},{"location":"instructor/internals/events/#event-classes","title":"Event classes","text":"<p>Instructor dispatches multiple classes of events during its execution. All of them are descendants of <code>Event</code> class.</p> <p>You can listen to these events and react to them in your application, for example to log information or to monitor the execution process.</p> <p>Check the list of available event classes in the <code>Cognesy\\Instructor\\Events</code> namespace.</p>"},{"location":"instructor/internals/events/#event-methods","title":"Event methods","text":"<p>Each Instructor event offers following methods, which make interacting with them more convenient:</p> <ul> <li><code>print()</code> - prints a string representation of the event to console output</li> <li><code>printDebug()</code> - prints a string representation of the event to console output, with additional debug information</li> <li><code>asConsole()</code> - returns the event in a format suitable for console output</li> <li><code>asLog()</code> - returns the event in a format suitable for logging</li> </ul>"},{"location":"instructor/internals/events/#receiving-notification-on-internal-events","title":"Receiving notification on internal events","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <ul> <li><code>(new StructuredOutput)-&gt;onEvent(string $class, callable $callback)</code> method - receive callback when specified type of event is dispatched</li> <li><code>(new StructuredOutput)-&gt;wiretap(callable $callback)</code> method - receive any event dispatched by Instructor, may be useful for debugging or performance analysis</li> </ul> <p>Receiving events can help you to monitor the execution process and makes it easier for a developer to understand and resolve any processing issues.</p> <pre><code>// @doctest id=\"e3af\"\n$structuredOutput = (new StructuredOutput)\n    // see requests to LLM\n    -&gt;onEvent(HttpRequestSent::class, fn($e) =&gt; dump($e))\n    // see responses from LLM\n    -&gt;onEvent(HttpResponseReceived::class, fn($event) =&gt; dump($event))\n    // see all events in console-friendly format\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    // log all events in log-friendly format\n    -&gt;wiretap(fn($event) =&gt; YourLogger::log($event-&gt;asLog()))\n\n$structuredOutput-&gt;with(\n    messages: \"What is the population of Paris?\",\n    responseModel: Scalar::integer(),\n)-&gt;get();\n// check your console for the details on the Instructor execution\n</code></pre>"},{"location":"instructor/internals/events/#convenience-methods-for-get-streamed-model-updates","title":"Convenience methods for get streamed model updates","text":"<p><code>StructuredOutput</code> class provides convenience methods allowing client code to receive model updates  when streaming is enabled:</p> <ul> <li><code>onPartialUpdate(callable $callback)</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate(callable $callback)</code> - to handle partial sequence updates of the response</li> </ul> <p>In both cases your callback will receive updated model, so you don't have to extract it from the event.</p>"},{"location":"instructor/internals/instructor/","title":"Instructor","text":""},{"location":"instructor/internals/instructor/#structuredoutput-class","title":"<code>StructuredOutput</code> class","text":"<p><code>StructuredOutput</code> class is the main entry point to the library. It is responsible for handling all interactions with the client code and internal Instructor components.</p>"},{"location":"instructor/internals/instructor/#request-handlers","title":"Request handlers","text":"<p>One of the essential tasks of the <code>StructuredOutput</code> class is to read the configuration and use it to retrieve a component implementing <code>CanHandleRequest</code> interface (specified in the configuration) to process the request and return the response.</p>"},{"location":"instructor/internals/instructor/#dispatched-events","title":"Dispatched events","text":"<p><code>StructuredOutput</code> class dispatches several high level events during initialization and processing of the request and response:</p> <ul> <li><code>StructuredOutputStarted</code> - dispatched when the structured output processes starts</li> <li><code>StructuredOutputRequestReceived</code> - dispatched when the request is received</li> <li><code>StructuredOutputResponseGenerated</code> - dispatched when the response is generated</li> <li><code>StructuredOutputResponseUpdated</code> - dispatched when the response update is streamed</li> </ul>"},{"location":"instructor/internals/instructor/#event-listeners","title":"Event listeners","text":"<p><code>StructuredOutput</code> class provides several methods allowing client code to plug into Instructor event system, including:  - <code>onEvent()</code> - to receive a callback when specified type of event is dispatched  - <code>wiretap()</code> - to receive any event dispatched by Instructor</p>"},{"location":"instructor/internals/instructor/#response-model-updates","title":"Response model updates","text":"<p>Additionally, <code>StructuredOutput</code> class provides convenience methods allowing client code to receive model updates when streaming is enabled:</p> <ul> <li><code>onPartialUpdate()</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate()</code> - to handle partial sequence updates of the response</li> </ul>"},{"location":"instructor/internals/instructor/#error-handling","title":"Error handling","text":"<p><code>StructuredOutput</code> class contains top level try-catch block to let user handle any uncaught errors before throwing them back to the client code. It allows you to register a handler which will log the error or notify your monitoring system about a problem.</p>"},{"location":"instructor/internals/lifecycle/","title":"Lifecycle","text":""},{"location":"instructor/internals/lifecycle/#instructors-request-lifecycle","title":"Instructor's request lifecycle","text":"<p>As Instructor for PHP processes your request, it goes through several stages:</p> <ol> <li>Initialize and self-configure (with possible overrides defined by developer).</li> <li>Analyze classes and properties of the response data model specified by developer.</li> <li>Translate data model into a schema that can be provided to LLM.</li> <li>Execute request to LLM using specified messages (content) and response model metadata.</li> <li>Receive a response from LLM or multiple partial responses (if streaming is enabled).</li> <li>Deserialize response received from LLM into originally requested classes and their properties.</li> <li>In case response contained unserializable data - create feedback message for LLM and request regeneration of the response.</li> <li>Execute validations defined by developer on the deserialized data - if any of them fail, create feedback message for LLM and requests regeneration of the response.</li> <li>Repeat the steps 4-8, unless specified limit of retries has been reached or response passes validation.</li> </ol>"},{"location":"instructor/internals/response_models/","title":"Response Models","text":"<p>Instructor's request parameter <code>responseModel</code> allows you to specify shape of the response you expect from LLM .</p> <p>Instructor translates the <code>responseModel</code> parameter into actual schema based on the type and value of the parameter.</p>"},{"location":"instructor/internals/response_models/#handling-string-responsemodel-value","title":"Handling string $responseModel value","text":"<p>If <code>string</code> value is provided, it is used as a name of the class of the response model.</p> <p>Instructor checks if the class exists and analyzes the class &amp; properties type information &amp; doc comments to generate a schema needed to specify LLM response constraints.</p> <p>The best way to provide the name of the response model class is to use <code>NameOfTheClass::class</code>, making it easy for IDE to check the type, handle refactorings, etc.</p>"},{"location":"instructor/internals/response_models/#handling-object-responsemodel-value","title":"Handling object $responseModel value","text":"<p>If <code>object</code> value is provided, it is considered an instance of the response model. Instructor checks the class of the instance, then analyzes it and its property type data to specify LLM response constraints.</p>"},{"location":"instructor/internals/response_models/#handling-array-responsemodel-value","title":"Handling array $responseModel value","text":"<p>If <code>array</code> value is provided, it is considered a raw JSON Schema, therefore allowing Instructor to use it directly in LLM requests (after wrapping in appropriate context - e.g. function call).</p> <p>Instructor requires information on the class of each nested object in your JSON Schema, so it can correctly deserialize the data into appropriate type.</p> <p>This information is available to Instructor when you are passing $responseModel as a class name or an instance, but it is missing from raw JSON Schema. Lack of the information on target class makes it impossible for Instructor to deserialize the data into appropriate, expected type.</p> <p>Current design uses JSON Schema <code>$comment</code> field on property to overcome this information gap. Instructor expects developer to use <code>$comment</code> field to provide fully qualified name of the target class to be used to deserialize property data of object or enum type.</p>"},{"location":"instructor/internals/response_models/#custom-response-handling-strategy","title":"Custom response handling strategy","text":"<p>Instructor allows you to customize processing of <code>$responseModel</code> value also by looking at the interfaces the class or instance implements:</p> <ul> <li><code>CanProvideJsonSchema</code> - implement to be able to provide raw JSON Schema (as an array) of the response model, overriding the default approach of Instructor, which is analyzing $responseModel value class information,</li> <li><code>CanProvideSchema</code> - implement to be able to provide <code>Schema</code> object of the response model, overriding class analysis stage; can be useful in building object wrappers (see: <code>Sequence</code> class),</li> <li><code>CanDeserializeSelf</code> - implement to customize the way the response from LLM is deserialized from JSON into PHP object,</li> <li><code>CanValidateSelf</code> - implement to customize the way the deserialized object is validated - it fully replaces the default validation process for given response model,</li> <li><code>CanTransformSelf</code> - implement to transform the validated object into any target value that will be then passed back to the caller (e.g. unwrap simple type from a class to scalar value)</li> </ul> <p>Methods implemented by those interfaces are executed as following:</p> <ul> <li><code>CanProvideJsonSchema</code> - executed during the schema generation phase,</li> <li><code>CanDeserializeSelf</code> - executed during the deserialization phase,</li> <li><code>CanValidateSelf</code> - executed during the validation phase,</li> <li><code>CanTransformSelf</code> - executed during the transformation phase.</li> </ul> <p>When implementing custom response handling strategy, avoid doing all transformations in a single block of code. Split the logic between relevant methods implemented by your class for clarity and easier code maintenance.</p>"},{"location":"instructor/internals/response_models/#example-implementations","title":"Example implementations","text":"<p>For a practical example of using those contracts to customize Instructor processing flow see:</p> <ul> <li>src/Extras/Scalar/</li> <li>src/Extras/Sequence/</li> </ul> <p>Examples contain an implementation of custom response model handling strategies, e.g. providing scalar value support via a wrapper class implementing:  - custom schema provider,  - deserialization,  - validation  - and transformation</p> <p>into requested value type.</p>"},{"location":"instructor/internals/settings_class/","title":"Settings class","text":""},{"location":"instructor/internals/settings_class/#settings-class","title":"<code>Settings</code> Class","text":"<p><code>Settings</code> class is the main entry point for telling Instructor where to look for its configuration. It allows you to set up path of Instructor's configuration directory and access Instructor settings.</p> <p><code>Settings</code> class provides the following methods: - <code>setPath(string $path)</code>: Set the path to Instructor's configuration directory - <code>getPath(): string</code>: Get current path to Instructor's configuration directory - <code>has($group, $key): bool</code>: Check if a specific setting exists in Instructor's configuration - <code>get($group, $key, $default): mixed</code>: Get a specific setting from Instructor's configuration - <code>set($group, $key, $value)</code>: Set a specific setting in Instructor's configuration</p> <p>You won't usually need to use these methods directly, but they are used internally by Instructor to access configuration settings.</p>"},{"location":"instructor/misc/contributing/","title":"Contributing","text":""},{"location":"instructor/misc/contributing/#contributing","title":"Contributing","text":"<p>If you want to help, check out some of the issues. They could be anything from code improvements, a guest blog post, or a new cookbook.</p>"},{"location":"instructor/misc/help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p> <p>          The concepts section explains the core concepts of Instructor and how to prompt with models.               The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.      <pre><code>&lt;Card\n    title=\"GitHub Discussions\"\n    icon=\"github\"\n    href=\"https://github.com/cognesy/instructor-php/discussions\"\n&gt;\n    [GitHub discussions](https://github.com/cognesy/instructor-php/discussions) are useful for asking questions, your question and the answer will help everyone.\n&lt;/Card&gt;\n&lt;Card\n    title=\"GitHub Issues\"\n    icon=\"bug\"\n    href=\"https://github.com/cognesy/instructor-php/issues\"\n&gt;\n    [GitHub issues](https://github.com/cognesy/instructor-php/issues) are useful for reporting bugs or requesting new features.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Discord\"\n    icon=\"discord\"\n    href=\"https://discord.gg/CV8sPM5k5Y\"\n&gt;\n    The [Discord](https://discord.gg/CV8sPM5k5Y) is a great place to ask questions and get help from the community.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Blog\"\n    icon=\"newspaper\"\n    href=\"https://cognesy.com/blog\"\n&gt;\n    The [blog](blog/index.md) contains articles that explain how to use Instructor in different scenarios.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Twitter\"\n    icon=\"twitter\"\n    href=\"https://twitter.com/ddebowczyk\"\n&gt;\n    You can also reach out to me [@ddebowczyk](https://twitter.com/ddebowczyk) if you have any questions or ideas.\n&lt;/Card&gt;\n</code></pre> <p></p>"},{"location":"instructor/misc/llm_providers/","title":"LLM API Providers","text":"<p>Following table lists the supported providers:</p> <ul> <li>A21</li> <li>Anthropic</li> <li>Azure</li> <li>Cerebras</li> <li>Cohere</li> <li>Cohere (OpenAI compatible API)</li> <li>DeepSeek</li> <li>Fireworks</li> <li>Gemini</li> <li>Gemini (OpenAI compatible API)</li> <li>Groq</li> <li>MiniMaxi</li> <li>Mistral</li> <li>Moonshot</li> <li>Ollama</li> <li>OpenAI</li> <li>OpenRouter</li> <li>Perplexity</li> <li>SambaNova</li> <li>Together</li> <li>XAI</li> </ul> <p>Check the cookbook for examples of how to use them.</p>"},{"location":"instructor/misc/philosophy/","title":"Philosophy","text":""},{"location":"instructor/misc/philosophy/#philosophy-of-instructor","title":"Philosophy of Instructor","text":"<p>NOTE: Philosophy behind Instructor was formulated by Jason Liu, the creator of original version of Instructor in Python and adapted for the PHP port.</p> <p>Instructor values simplicity and flexibility in leveraging language models. It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions.</p> <p>\u201cSimplicity is a great virtue, but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"instructor/misc/philosophy/#simplicity","title":"Simplicity","text":"<ol> <li>Most users will only need to learn <code>responseModel</code> and <code>StructuredOutput::create()</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"instructor/misc/philosophy/#transparency","title":"Transparency","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We give you config over the prompts we do write ('reasking' and in the future - JSON_MODE prompts).</li> </ol>"},{"location":"instructor/misc/philosophy/#flexibility","title":"Flexibility","text":"<ol> <li>If you build a system with OpenAI directly, it is easy to incrementally adopt Instructor by just adding <code>StructuredOutput::create()</code> with data schemas fed in via <code>responseModel</code>.</li> <li>Use any class to define your data schema (no need to inherit from some base class).</li> </ol>"},{"location":"instructor/misc/philosophy/#the-zen-of-instructor","title":"The zen of Instructor","text":"<p>Maintain the flexibility and power of PHP classes, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define data schema <code>class SomeStructuredData { ... }</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>function extract($input) : SomeStructuredData</code></li> <li>Define typed computations against your data with <code>function compute(SomeStructuredData $data):</code> or call methods on your schema <code>$data-&gt;compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"instructor/misc/philosophy/#our-goals","title":"Our Goals","text":"<p>The goal for the library and documentation is to help you be a better programmer and, as a result, a better AI engineer.</p> <ul> <li>The library is a result of our desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>We won't try to write prompts for you,</li> <li>We don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit us up @jnxlco or @ddebowczyk</p> <p>Cheers!</p>"},{"location":"instructor/techniques/classification/","title":"Classification","text":""},{"location":"instructor/techniques/classification/#text-classification-using-llm","title":"Text Classification using LLM","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using LLM (via OpenAI API), PHP's <code>enums</code> and classes.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using language models in combination with PHP data structures.</p>"},{"location":"instructor/techniques/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"instructor/techniques/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>// @doctest id=\"1fba\"\n&lt;?php\n// Enumeration for single-label text classification. \nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction. \nclass SinglePrediction {\n    public Label $classLabel;\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>// @doctest id=\"ebaa\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Perform single-label classification on the input text. \n */\nfunction classify(string $data) : SinglePrediction {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n        model: \"gpt-3.5-turbo-0613\",\n    )-&gt;get();\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>// @doctest id=\"d7f9\"\n&lt;?php\n\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\nassert($prediction-&gt;classLabel == Label::SPAM);\n</code></pre>"},{"location":"instructor/techniques/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"instructor/techniques/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>// @doctest id=\"4008\"\n&lt;?php\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass Ticket {\n    /** @var Label[] */\n    public array $ticketLabels = [];\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>// @doctest id=\"b86c\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : Ticket {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify following support ticket: {$data}\",\n        ]],\n        responseModel: Ticket::class,\n        model: \"gpt-3.5-turbo-0613\",\n    )-&gt;get();\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>// @doctest id=\"348a\"\n&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;classLabels));\nassert(in_array(Label::BILLING, $prediction-&gt;classLabels));\n</code></pre>"},{"location":"instructor/techniques/prompting/","title":"Prompting","text":""},{"location":"instructor/techniques/prompting/#general-tips-for-prompt-engineering","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor for function calling is to make the models self-descriptive, modular, and flexible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use PHPDoc comments or #[Description('')] attribute for clear field descriptions.</li> <li>Optionality: Use PHP's nullable types (e.g. ?int) for optional fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"instructor/techniques/prompting/#utilize-nullable-attribute","title":"Utilize Nullable Attribute","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>// @doctest id=\"abfd\"\n&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?Role $role = null; \n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>// @doctest id=\"a500\"\n&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?string $role = null;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $result = null;\n    public ?string $errorMessage = '';\n    public bool $error = false;\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;error ? null : $this-&gt;result;\n    }\n}\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in 'errorMessage'.</p> <p>Original Instructor implementation in Python provides utility class Maybe making this pattern even easier. Such mechanism is not yet available in PHP version of Instructor.</p>"},{"location":"instructor/techniques/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>// @doctest id=\"efc9\"\n&lt;?php\nenum Role : string {\n    case Principal = 'principal'\n    case Teacher = 'teacher'\n    case Student = 'student'\n    case Other = 'other'\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /**  Correctly assign one of the predefined roles to the user. */\n    public Role $role;\n}\n</code></pre> <p>If you'd like to improve LLM inference performance, try reiterating the requirements in the field descriptions (in the docstrings).</p>"},{"location":"instructor/techniques/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>// @doctest id=\"81a8\"\n&lt;?php\n/** Extract the role based on the following rules: &lt;your rules go here&gt; */\nclass Role\n{\n    /** Restate the instructions and rules to correctly determine the title. */\n    public string $instructions;\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>// @doctest id=\"c05a\"\n&lt;?php\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>// @doctest id=\"5844\"\n&lt;?php\nclass Property\n{\n    /**  Monotonically increasing ID */\n    public string $index; \n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age\n    public string $name;\n    /** @var Property[] Numbered list of arbitrary extracted properties, should be less than 3 */\n    public array $properties;\n}\n</code></pre> <p>To be 100% certain the list does not exceed the limit add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"instructor/techniques/prompting/#consistent-arbitrary-properties","title":"Consistent Arbitrary Properties","text":"<p>For multiple records containing arbitrary properties, instruct LLM to use consistent key names when extracting properties.</p> <pre><code>// @doctest id=\"636e\"\n&lt;?php\nclass Property {\n    public int $id;\n    public string $key;\n    public string $name;\n}\n\nclass UserDetails\n{\n    /** @var UserDetail[] Extract information for multiple users. Use consistent key names for properties across users. */\n    public array $users;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> field:</p> <pre><code>// @doctest id=\"1c2d\"\n&lt;?php\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /** @var int[] Correct and complete list of coworker IDs, representing collaboration between users. */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /** @var UserDetail[] Collection of users, correctly capturing the relationships among them. */\n    public array $users;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>// @doctest id=\"a20e\"\n&lt;?php\nclass Role\n{\n    /** Think step by step to determine the correct title. */\n    public string $chainOfThought = '';\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>// @doctest id=\"42b3\"\n&lt;?php\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public int $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#adding-context-to-components","title":"Adding Context to Components","text":"<p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>// @doctest id=\"5e8f\"\n&lt;?php\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n</code></pre>"},{"location":"instructor/techniques/search/","title":"Search","text":""},{"location":"instructor/techniques/search/#expanding-search-queries","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p>"},{"location":"instructor/techniques/search/#motivation","title":"Motivation","text":"<p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"instructor/techniques/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>// @doctest id=\"12cd\"\n&lt;?php\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n</code></pre>"},{"location":"instructor/techniques/search/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>StructuredOutput::create()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>// @doctest id=\"6264\"\n&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nfunction segment(string $data) : Search {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    )-&gt;get();\n}\n\nforeach (segment(\"Search for a picture of a cat and a video of a dog\")-&gt;queries as $query) {\n    $query-&gt;execute();\n    // dump($query);\n}\n</code></pre>"},{"location":"polyglot/overview/","title":"Overview","text":"<p>Polyglot is a PHP library that provides a unified API for interacting with various Large Language Model (LLM) providers. It allows developers to build applications that use LLMs without being locked into a specific provider or having to rewrite code when switching between providers.</p> <p>The core philosophy behind Polyglot is to create a consistent, provider-agnostic interface that abstracts away the differences between LLM APIs, while still allowing access to provider-specific features when needed. This enables developers to:</p> <ul> <li>Write code once and use it with any supported LLM provider</li> <li>Easily switch between providers without changing application code</li> <li>Use different providers in different environments (development, testing, production)</li> <li>Fall back to alternative providers if one becomes unavailable</li> </ul> <p>Polyglot was developed as part of the Instructor for PHP library, which focuses on structured outputs from LLMs, but can also be used as a standalone library for general LLM interactions.</p>"},{"location":"polyglot/overview/#key-features","title":"Key Features","text":""},{"location":"polyglot/overview/#unified-llm-api","title":"Unified LLM API","text":"<p>Polyglot's primary feature is its unified API that works across multiple LLM providers:</p> <ul> <li>Consistent interface for making inference or embedding requests</li> <li>Common message format across all providers</li> <li>Standardized response handling</li> <li>Unified error handling</li> </ul>"},{"location":"polyglot/overview/#framework-agnostic","title":"Framework-Agnostic","text":"<p>Polyglot is designed to work with any PHP framework or even in plain PHP applications. It does not depend on any specific framework, making it easy to integrate into existing projects.</p> <ul> <li>Compatible with Laravel, Symfony, CodeIgniter, and others</li> <li>Can be used in CLI scripts or web applications</li> <li>Lightweight and easy to install</li> </ul>"},{"location":"polyglot/overview/#comprehensive-provider-support","title":"Comprehensive Provider Support","text":"<p>Polyglot supports a wide range of LLM providers, including:</p> <ul> <li>OpenAI (GPT models)</li> <li>Anthropic (Claude models)</li> <li>Google Gemini (native and OpenAI compatible)</li> <li>Mistral AI</li> <li>Azure OpenAI</li> <li>Cohere</li> <li>And many others (see full list below)</li> </ul>"},{"location":"polyglot/overview/#multiple-interaction-modes","title":"Multiple Interaction Modes","text":"<p>Polyglot supports various modes of interaction with LLMs:</p> <ul> <li>Text mode: Simple text completion/chat</li> <li>JSON mode: Structured JSON responses</li> <li>JSON Schema mode: Responses validated against a schema</li> <li>Tools mode: Function/tool calling for task execution</li> </ul>"},{"location":"polyglot/overview/#streaming-support","title":"Streaming Support","text":"<p>Real-time streaming of responses is supported across compatible providers:</p> <ul> <li>Token-by-token streaming</li> <li>Progress handling</li> <li>Partial response accumulation</li> </ul>"},{"location":"polyglot/overview/#embeddings-generation","title":"Embeddings Generation","text":"<p>Beyond text generation, Polyglot includes support for vector embeddings:</p> <ul> <li>Generate embeddings from text</li> <li>Support for multiple embedding providers</li> <li>Utilities for finding similar documents</li> </ul>"},{"location":"polyglot/overview/#configuration-flexibility","title":"Configuration Flexibility","text":"<p>Polyglot offers a flexible configuration system:</p> <ul> <li>Configure multiple providers simultaneously</li> <li>Environment-based configuration</li> <li>Runtime provider switching</li> <li>Per-request customization</li> </ul>"},{"location":"polyglot/overview/#middleware-and-extensibility","title":"Middleware and Extensibility","text":"<p>The library is built with extensibility in mind:</p> <ul> <li>HTTP client middleware for customization</li> <li>Event system for request/response monitoring</li> <li>Ability to add custom providers</li> </ul>"},{"location":"polyglot/overview/#use-cases","title":"Use Cases","text":"<p>Polyglot is a good choice for a variety of use cases:</p> <ul> <li>Applications requiring LLM provider flexibility: Switch between providers based on cost, performance, or feature needs</li> <li>Multi-environment deployments: Use different LLM providers in development, staging, and production</li> <li>Redundancy and fallback: Implement fallback strategies when a provider is unavailable</li> <li>Hybrid approaches: Combine different providers for different tasks based on their strengths</li> <li>Local + cloud development: Use local models (via Ollama) for development and cloud providers for production</li> </ul>"},{"location":"polyglot/overview/#supported-providers","title":"Supported Providers","text":""},{"location":"polyglot/overview/#inference-providers","title":"Inference Providers","text":"<p>Polyglot currently supports the following LLM providers for chat completion:</p> <ul> <li>A21: API access to Jamba models</li> <li>Anthropic: Claude family of models</li> <li>Microsoft Azure: Azure-hosted OpenAI models</li> <li>Cerebras: Cerebras LLMs</li> <li>Cohere: Command models (both native and OpenAI compatible interfaces)</li> <li>Deepseek: Deepseek models including reasoning capabilities</li> <li>Google Gemini: Google's Gemini models (both native and OpenAI compatible)</li> <li>Groq: High-performance inference platform</li> <li>Hugging Face: Hugging Face hosted models</li> <li>Meta: Jina AI models</li> <li>Minimaxi: MiniMax models</li> <li>Mistral: Mistral AI models</li> <li>Moonshot: Kimi models</li> <li>Ollama: Self-hosted open source models</li> <li>OpenAI: GPT models family</li> <li>OpenRouter: Multi-provider routing service</li> <li>Perplexity: Perplexity models</li> <li>SambaNova: SambaNova hosted models</li> <li>Together: Together AI hosted models</li> <li>xAI: xAI's Grok models</li> </ul>"},{"location":"polyglot/overview/#embeddings-providers","title":"Embeddings Providers","text":"<p>For embeddings generation, Polyglot supports:</p> <ul> <li>Microsoft Azure: Azure-hosted OpenAI embeddings</li> <li>Cohere: Cohere embeddings models</li> <li>Google Gemini: Google's embedding models</li> <li>Jina: Jina embeddings</li> <li>Mistral: Mistral embedding models</li> <li>Ollama: Self-hosted embedding models</li> <li>OpenAI: OpenAI embeddings</li> </ul>"},{"location":"polyglot/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with Polyglot in your PHP project in under 5 minutes.</p> <p>For detailed setup instructions, see Setup.</p>"},{"location":"polyglot/quickstart/#install-polyglot-with-composer","title":"Install Polyglot with Composer","text":"<p>To install Polyglot in your project, run following command in your terminal:</p> <pre><code># @doctest id=\"c280\"\ncomposer require cognesy/instructor-polyglot\n</code></pre> <p>NOTE: Polyglot is already included in Instructor for PHP package, so if you have it installed, you don't need to install Polyglot separately.</p>"},{"location":"polyglot/quickstart/#create-and-run-example","title":"Create and Run Example","text":""},{"location":"polyglot/quickstart/#step-1-prepare-your-openai-api-key","title":"Step 1: Prepare your OpenAI API Key","text":"<p>In this example, we'll use OpenAI as the LLM provider. You can get it from the OpenAI dashboard.</p>"},{"location":"polyglot/quickstart/#step-2-create-a-new-php-file","title":"Step 2: Create a New PHP File","text":"<p>In your project directory, create a new PHP file <code>test-polyglot.php</code>:</p> <pre><code>// @doctest id=\"6c58\"\n&lt;?php\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Set up OpenAI API key\n$apiKey = 'your-openai-api-key';\nputenv(\"OPENAI_API_KEY=\" . $apiKey);\n// WARNING: In real project you should set up API key in .env file.\n\n$answer = (new Inference)\n    -&gt;withMessages('What is capital of Germany')\n    -&gt;get();\n\necho \"USER: What is capital of Germany\\n\";\necho \"ASSISTANT: $answer\\n\";\n</code></pre> <p>     You should never put your API keys directly in your real project code to avoid getting them compromised. Set them up in your .env file. </p>"},{"location":"polyglot/quickstart/#step-3-run-the-example","title":"Step 3: Run the Example","text":"<p>Now, you can run the example:</p> <pre><code># @doctest id=\"d123\"\nphp test-polyglot.php\n\n# Output:\n# USER: What is capital of Germany\n# ASSISTANT: Berlin\n</code></pre>"},{"location":"polyglot/quickstart/#next-steps","title":"Next Steps","text":"<p>You can start using Polyglot in your project right away after installation.</p> <p>But it's recommended to publish configuration files and prompt templates to your project directory, so you can customize the library's behavior and use your own prompt templates.</p> <p>You should also set up LLM provider API keys in your <code>.env</code> file instead of putting them directly in your code.</p> <p>See setup instructions for more details.</p>"},{"location":"polyglot/setup/","title":"Setup","text":"<p>This chapter will guide you through the initial steps of setting up and using Polyglot in your PHP project. We'll cover installation and configuration to get you up and running quickly.</p>"},{"location":"polyglot/setup/#installation","title":"Installation","text":"<p>You can install it using Composer:</p> <pre><code># @doctest id=\"298a\"\ncomposer require cognesy/instructor-polyglot\n</code></pre> <p>This will install Polyglot along with its dependencies.</p> <p>NOTE: Polyglot is distributed as part of the Instructor PHP package, so if you have it installed, you don't need to install Polyglot separately.</p>"},{"location":"polyglot/setup/#requirements","title":"Requirements","text":"<ul> <li>PHP 8.2 or higher</li> <li>Composer</li> <li>Valid API keys for at least one supported LLM provider</li> </ul>"},{"location":"polyglot/setup/#configuration","title":"Configuration","text":""},{"location":"polyglot/setup/#setting-up-api-keys","title":"Setting Up API Keys","text":"<p>Polyglot requires API keys to authenticate with LLM providers. The recommended approach is to use environment variables:</p> <ol> <li>Create a <code>.env</code> file in your project root (or use your existing one)</li> <li>Add your API keys:</li> </ol> <pre><code># @doctest id=\"e124\"\n# OpenAI\nOPENAI_API_KEY=sk-your-openai-key\n\n# Anthropic\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\n\n# Other providers as needed\nMISTRAL_API_KEY=your-mistral-key\nGEMINI_API_KEY=your-gemini-key\n# etc.\n</code></pre>"},{"location":"polyglot/setup/#configuration-files","title":"Configuration Files","text":"<p>Polyglot loads its configuration from PHP files.</p> <p>The default configuration files are located in the Instructor package, but you can publish and customize them:</p> <ol> <li>Create a <code>config</code> directory in your project if it doesn't exist</li> <li>Copy the configuration files from the Instructor package:</li> </ol> <pre><code># @doctest id=\"904e\"\n# Create config directory if it doesn't exist\nmkdir -p config\n\n# Copy configuration files\ncp vendor/cognesy/instructor-polyglot/config/* config/\n</code></pre> <ol> <li>Customize the configuration files as needed</li> </ol>"},{"location":"polyglot/setup/#llm-configuration","title":"LLM Configuration","text":"<p>The <code>llm.php</code> configuration file contains settings for LLM providers:</p> <pre><code>// @doctest id=\"dbad\"\n&lt;?php\n// Example of a simplified config/llm.php\n\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',  // Default connection to use\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        'anthropic' =&gt; [\n            'driver' =&gt; 'anthropic',\n            'apiUrl' =&gt; 'https://api.anthropic.com/v1',\n            'apiKey' =&gt; Env::get('ANTHROPIC_API_KEY', ''),\n            'endpoint' =&gt; '/messages',\n            'metadata' =&gt; [\n                'apiVersion' =&gt; '2023-06-01',\n            ],\n            'model' =&gt; 'claude-3-haiku-20240307',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/setup/#embeddings-configuration","title":"Embeddings Configuration","text":"<p>The <code>embed.php</code> configuration file contains settings for embeddings providers:</p> <pre><code>// @doctest id=\"bca6\"\n&lt;?php\n// Example of a simplified config/embed.php\n\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/embeddings',\n            'model' =&gt; 'text-embedding-3-small',\n            'dimensions' =&gt; 1536,\n            'maxInputs' =&gt; 16,\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/setup/#custom-configuration-location","title":"Custom Configuration Location","text":"<p>By default, Polyglot looks for custom configuration files in the <code>config</code> directory relative to your project root. You can specify a different location by setting the <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable:</p> <pre><code># @doctest id=\"ea94\"\nINSTRUCTOR_CONFIG_PATHS='/path/to/your/config,alternative/path'\n</code></pre>"},{"location":"polyglot/setup/#overriding-configuration-location","title":"Overriding Configuration Location","text":"<p>You can use <code>Settings</code> class static <code>setPath()</code> method to override the value of config path set in environment variable with your own value.</p> <pre><code>// @doctest id=\"555f\"\nuse Cognesy\\Config\\Settings;\n\nSettings::setPath('/your/path/to/config');\n</code></pre>"},{"location":"polyglot/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"polyglot/setup/#common-installation-issues","title":"Common Installation Issues","text":"<ul> <li>Composer Dependencies: Make sure you have PHP 8.2+ installed and Composer correctly configured.</li> <li>API Keys: Verify that your API keys are correctly set in your environment variables.</li> <li>Configuration Files: Check that your configuration files are properly formatted and accessible.</li> </ul>"},{"location":"polyglot/setup/#testing-your-installation","title":"Testing Your Installation","text":"<p>A simple way to test if everything is working correctly is to run a small script:</p> <pre><code>// @doctest id=\"9982\"\n&lt;?php\nrequire 'vendor/autoload.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$result = (new Inference)\n    -&gt;withMessages('Say hello.')\n    -&gt;get();\n</code></pre> <p>If you see a friendly greeting, your installation is working correctly!</p>"},{"location":"polyglot/advanced/connection-mgmt/","title":"Preset Management","text":"<p>One of Polyglot's strengths is the ability to easily switch between different LLM providers, which is made easy by using connection presets.</p> <p>More complex applications may need to manage multiple LLM provider connections and switch between them dynamically to implement fallback strategies or leverage the strengths of different models and providers for various tasks.</p>"},{"location":"polyglot/advanced/connection-mgmt/#switching-providers","title":"Switching Providers","text":"<pre><code>// @doctest id=\"069c\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n// Use OpenAI\n$openaiResponse = $inference\n    -&gt;using('openai')\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\necho \"OpenAI response: $openaiResponse\\n\";\n\n// Switch to Anthropic\n$anthropicResponse = $inference\n    -&gt;using('anthropic')\n    -&gt;withMessages('What is the capital of Germany?')\n    -&gt;get();\n\necho \"Anthropic response: $anthropicResponse\\n\";\n</code></pre>"},{"location":"polyglot/advanced/connection-mgmt/#implementing-fallbacks","title":"Implementing Fallbacks","text":"<p>You can implement a fallback mechanism to try alternative providers if one fails:</p> <pre><code>// @doctest id=\"f765\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withFallback(array $providers, callable $requestFn) {\n    $lastException = null;\n\n    foreach ($providers as $provider) {\n        try {\n            $inference = (new Inference)-&gt;using($provider);\n            return $requestFn($inference);\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            echo \"Provider '$provider' failed: {$e-&gt;getMessage()}. Trying next provider...\\n\";\n        }\n    }\n\n    throw new \\Exception(\"All providers failed. Last error: \" .\n        ($lastException ? $lastException-&gt;getMessage() : \"Unknown error\"));\n}\n\n// Usage\ntry {\n    $providers = ['openai', 'anthropic', 'gemini'];\n\n    $response = withFallback($providers, function($inference) {\n        return $inference-&gt;with(\n            messages: 'What is the capital of France?'\n        )-&gt;toText();\n    });\n\n    echo \"Response: $response\\n\";\n} catch (\\Exception $e) {\n    echo \"Error: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/advanced/connection-mgmt/#cost-aware-provider-selection","title":"Cost-Aware Provider Selection","text":"<p>You might want to select providers based on cost considerations:</p> <pre><code>// @doctest id=\"63e0\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nclass CostAwareLLM {\n    private $inference;\n    private $providers = [\n        'low' =&gt; [\n            'preset' =&gt; 'ollama',\n            'model' =&gt; 'llama2',\n        ],\n        'medium' =&gt; [\n            'preset' =&gt; 'mistral',\n            'model' =&gt; 'mistral-small-latest',\n        ],\n        'high' =&gt; [\n            'preset' =&gt; 'openai',\n            'model' =&gt; 'gpt-4o',\n        ],\n    ];\n\n    public function __construct() {\n        $this-&gt;inference = new Inference();\n    }\n\n    public function ask(string $question, string $tier = 'medium'): string {\n        $provider = $this-&gt;providers[$tier] ?? $this-&gt;providers['medium'];\n\n        return $this-&gt;inference-&gt;using($provider['preset'])\n            -&gt;with(\n                messages: $question,\n                model: $provider['model']\n            )\n            -&gt;get();\n    }\n}\n\n// Usage\n$costAwareLLM = new CostAwareLLM();\n\n// Simple question - use low-cost tier\n$simpleQuestion = \"What is the capital of France?\";\necho \"Simple question (low cost): $simpleQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($simpleQuestion, 'low') . \"\\n\\n\";\n\n// More complex question - use medium-cost tier\n$mediumQuestion = \"Explain the concept of deep learning in simple terms.\";\necho \"Medium question (medium cost): $mediumQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($mediumQuestion, 'medium') . \"\\n\\n\";\n\n// Critical question - use high-cost tier\n$complexQuestion = \"Analyze the ethical implications of AI in healthcare.\";\necho \"Complex question (high cost): $complexQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($complexQuestion, 'high') . \"\\n\\n\";\n</code></pre>"},{"location":"polyglot/advanced/connection-mgmt/#provider-selection-strategy","title":"Provider Selection Strategy","text":"<p>You can implement a strategy to select the most appropriate provider for each request:</p> <pre><code>// @doctest id=\"68c7\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nclass GroupOfExperts {\n    private $inference;\n    private $providerStrategies = [\n        'creative' =&gt; 'anthropic',\n        'factual' =&gt; 'openai',\n        'code' =&gt; 'gemini',\n        'default' =&gt; 'openai',\n    ];\n\n    public function __construct() {\n        $this-&gt;inference = new Inference();\n    }\n\n    public function ask(string $question, string $taskType = 'default'): string {\n        // Select the appropriate provider based on the task type\n        $preset = $this-&gt;providerStrategies[$taskType] ?? $this-&gt;providerStrategies['default'];\n\n        // Use the selected provider\n        return $this-&gt;inference-&gt;using($preset)\n            -&gt;with(messages: $question)\n            -&gt;get();\n    }\n}\n\n// Usage\n$experts = new GroupOfExperts();\n\n$tasks = [\n    [\"Write a short poem about the ocean.\", 'creative'],\n    [\"Create a brief story about a robot discovering emotions.\", 'creative'],\n    [\"What is the capital of France?\", 'factual'],\n    [\"Who wrote 'Pride and Prejudice'?\", 'factual'],\n    [\"Write a PHP function to check if a string is a palindrome.\", 'code'],\n    [\"Create a simple JavaScript function to sort an array of objects by a property.\", 'code'],\n];\n\nforeach ($tasks as $task) {\n    echo \"Task: $task\\n\";\n    echo \"Response: \" . $experts-&gt;ask($task[0], $task[1]) . \"\\n\\n\";\n}\n</code></pre>"},{"location":"polyglot/advanced/context-caching/","title":"Context Caching","text":"<p>Context caching improves performance by reusing parts of a conversation, reducing token usage and API costs. This is particularly useful for multi-turn conversations or when processing large documents.</p>"},{"location":"polyglot/advanced/context-caching/#using-cached-context","title":"Using Cached Context","text":"<p>Polyglot supports context caching through the <code>withCachedContext()</code> method:</p> <pre><code>// @doctest id=\"df6e\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference()-&gt;using('anthropic');\n\n// Set up a conversation with cached context\n$inference-&gt;withCachedContext(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant who provides concise answers.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'I want to discuss machine learning concepts.'],\n        ['role' =&gt; 'assistant', 'content' =&gt; 'Great! I\\'d be happy to discuss machine learning concepts with you. What specific aspect would you like to explore?'],\n    ]\n);\n\n// First query using the cached context\n$response1 = $inference-&gt;with(\n    messages: 'What is supervised learning?'\n)-&gt;response();\n\necho \"Response 1: \" . $response1-&gt;content() . \"\\n\";\necho \"Tokens from cache: \" . $response1-&gt;usage()-&gt;cacheReadTokens . \"\\n\\n\";\n\n// Second query, still using the same cached context\n$response2 = $inference-&gt;with(\n    messages: 'And what about unsupervised learning?'\n)-&gt;response();\n\necho \"Response 2: \" . $response2-&gt;content() . \"\\n\";\necho \"Tokens from cache: \" . $response2-&gt;usage()-&gt;cacheReadTokens . \"\\n\";\n</code></pre>"},{"location":"polyglot/advanced/context-caching/#provider-support-for-context-caching","title":"Provider Support for Context Caching","text":"<p>Different providers have varying levels of support for context caching:</p> <ul> <li>Anthropic: Supports native context caching with explicit cache markers</li> <li>OpenAI: Provides automatic caching for optimization, but not as explicit as Anthropic</li> <li>Other providers: May not support native caching, but Polyglot still helps manage conversation state</li> </ul>"},{"location":"polyglot/advanced/context-caching/#processing-large-documents-with-cached-context","title":"Processing Large Documents with Cached Context","text":"<p>Context caching is particularly valuable when working with large documents:</p> <pre><code>// @doctest id=\"ea86\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Load a large document\n$documentContent = file_get_contents('large_document.txt');\n\n// Set up cached context with the document\n$inference = new Inference()-&gt;using('anthropic');\n$inference-&gt;withCachedContext(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You will help analyze and summarize documents.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'Here is the document to analyze:'],\n        ['role' =&gt; 'user', 'content' =&gt; $documentContent],\n    ]\n);\n\n// Ask multiple questions about the document without resending it each time\n$questions = [\n    'Summarize the key points of this document in 3 bullets.',\n    'What are the main arguments presented?',\n    'Are there any contradictions or inconsistencies in the text?',\n    'What conclusions can be drawn from this document?',\n];\n\nforeach ($questions as $index =&gt; $question) {\n    $response = $inference-&gt;with(messages: $question)-&gt;response();\n\n    echo \"Question \" . ($index + 1) . \": $question\\n\";\n    echo \"Answer: \" . $response-&gt;content() . \"\\n\";\n    echo \"Tokens from cache: \" . $response-&gt;usage()-&gt;cacheReadTokens . \"\\n\\n\";\n}\n</code></pre>"},{"location":"polyglot/advanced/custom-config/","title":"Configuration Deep Dive","text":"<p>One of Polyglot's core strengths is its ability to work with multiple LLM providers through a unified API. This chapter covers how to configure, manage, and switch between different providers and models to get the most out of the library.</p>"},{"location":"polyglot/advanced/custom-config/#understanding-provider-configuration","title":"Understanding Provider Configuration","text":"<p>Polyglot organizes provider settings through connection presets - named configurations that include the details needed to communicate with a specific LLM provider. These connection presets are defined in the configuration files and can be selected at runtime.</p>"},{"location":"polyglot/advanced/custom-config/#the-configuration-files","title":"The Configuration Files","text":"<p>The primary configuration files for Polyglot are:</p> <ol> <li><code>config/llm.php</code>: Contains configurations for LLM providers (chat/completion)</li> <li><code>config/embed.php</code>: Contains configurations for embedding providers</li> </ol> <p>Let's focus on the structure of these configuration files.</p>"},{"location":"polyglot/advanced/custom-config/#llm-configuration-structure","title":"LLM Configuration Structure","text":"<p>The <code>llm.php</code> configuration file has the following structure:</p> <pre><code>// @doctest id=\"f7b5\"\n&lt;?php\nuse Cognesy\\Config\\Env;\n\nreturn [\n    // Default connection to use when none is specified\n    'defaultPreset' =&gt; 'openai',\n\n    // Connection preset definitions\n    'presets' =&gt; [\n        // OpenAI connection\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'metadata' =&gt; [\n                'organization' =&gt; '',\n                'project' =&gt; '',\n            ],\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 128_000,\n            'maxOutputLength' =&gt; 16384,\n        ],\n\n        // Anthropic connection\n        'anthropic' =&gt; [\n            'providerType' =&gt; 'anthropic',\n            'apiUrl' =&gt; 'https://api.anthropic.com/v1',\n            'apiKey' =&gt; Env::get('ANTHROPIC_API_KEY', ''),\n            'endpoint' =&gt; '/messages',\n            'metadata' =&gt; [\n                'apiVersion' =&gt; '2023-06-01',\n                'beta' =&gt; 'prompt-caching-2024-07-31',\n            ],\n            'model' =&gt; 'claude-3-haiku-20240307',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 200_000,\n            'maxOutputLength' =&gt; 8192,\n        ],\n\n        // Additional connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#embedding-configuration-structure","title":"Embedding Configuration Structure","text":"<p>The <code>embed.php</code> configuration file follows a similar pattern:</p> <pre><code>// @doctest id=\"1160\"\n&lt;?php\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/embeddings',\n            'metadata' =&gt; [\n                'organization' =&gt; ''\n            ],\n            'model' =&gt; 'text-embedding-3-small',\n            'defaultDimensions' =&gt; 1536,\n            'maxInputs' =&gt; 2048,\n        ],\n\n        // Additional embedding connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#connection-parameters","title":"Connection Parameters","text":"<p>Each connection includes several parameters:</p> <ul> <li><code>providerType</code>: The type of provider (OpenAI, Anthropic, etc.)</li> <li><code>apiUrl</code>: The base URL for the provider's API</li> <li><code>apiKey</code>: The API key for authentication</li> <li><code>endpoint</code>: The specific API endpoint for chat completions or embeddings</li> <li><code>metadata</code>: Additional provider-specific settings</li> <li><code>model</code>: The default model to use</li> <li><code>maxTokens</code>: Default maximum tokens for responses</li> <li><code>contextLength</code>: Maximum context length supported by the model</li> <li><code>maxOutputLength</code>: Maximum output length supported by the model</li> <li><code>httpClient</code>: (Optional) Custom HTTP client to use</li> </ul> <p>For embedding connections, the parameters are:</p> <ul> <li><code>providerType</code>: The type of provider (OpenAI, Anthropic, etc.)</li> <li><code>apiUrl</code>: The base URL for the provider's API</li> <li><code>apiKey</code>: The API key for authentication</li> <li><code>endpoint</code>: The specific API endpoint for chat completions or embeddings</li> <li><code>metadata</code>: Additional provider-specific settings</li> <li><code>model</code>: The default model to use</li> <li><code>defaultDimensions</code>: The default dimensions of embedding vectors</li> <li><code>maxInputs</code>: Maximum number of inputs that can be processed in a single request</li> </ul>"},{"location":"polyglot/advanced/custom-config/#connection-preset-name-vs-provider-type","title":"Connection preset name vs provider type","text":"<p>Configuration file <code>llm.php</code> contains a list of connection presets with the default names that might resemble provider type names, but those are separate entities.</p> <p>Provider type name refers to one of the supported LLM API providers and its underlying driver implementation, either specific to this provider or a generic one - for example compatible with OpenAI ('openai-compatible').</p> <p>Connection preset name refers to LLM API provider endpoint configuration with specific provider type, but also URL, credentials, default model name, and default model parameter values.</p>"},{"location":"polyglot/advanced/custom-config/#managing-api-keys","title":"Managing API Keys","text":"<p>API keys should be stored securely and never committed to your codebase. Polyglot uses environment variables for API keys.</p>"},{"location":"polyglot/advanced/custom-config/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># @doctest id=\"f567\"\n# OpenAI\nOPENAI_API_KEY=sk-your-key-here\n\n# Anthropic\nANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# Other providers\nGEMINI_API_KEY=your-key-here\nMISTRAL_API_KEY=your-key-here\nCOHERE_API_KEY=your-key-here\n# etc.\n</code></pre> <p>Then load these environment variables using a package like <code>vlucas/phpdotenv</code>:</p> <pre><code>// @doctest id=\"5ce1\"\nrequire_once __DIR__ . '/vendor/autoload.php';\n\n$dotenv = Dotenv\\Dotenv::createImmutable(__DIR__);\n$dotenv-&gt;load();\n</code></pre> <p>Or in frameworks like Laravel, environment variables are automatically loaded.</p>"},{"location":"polyglot/advanced/custom-config/#rotating-api-keys","title":"Rotating API Keys","text":"<p>For better security, consider rotating your API keys regularly. You can update the environment variables without changing your code.</p>"},{"location":"polyglot/advanced/custom-config/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>Different providers may support unique parameters and features. You can pass these as options to the <code>create()</code> method.</p>"},{"location":"polyglot/advanced/custom-config/#openai-specific-parameters","title":"OpenAI-Specific Parameters","text":"<pre><code>// @doctest id=\"d94e\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference('openai');\n\n$response = $inference-&gt;with(\n    messages: 'Generate a creative story.',\n    options: [\n        'temperature' =&gt; 0.8,         // Controls randomness (0.0 to 1.0)\n        'top_p' =&gt; 0.95,              // Nucleus sampling parameter\n        'frequency_penalty' =&gt; 0.5,   // Penalize repeated tokens\n        'presence_penalty' =&gt; 0.5,    // Penalize repeated topics\n        'stop' =&gt; [\"\\n\\n\", \"THE END\"],// Stop sequences\n        'logit_bias' =&gt; [             // Adjust token probabilities\n            // Token ID =&gt; bias value (-100 to +100)\n            15043 =&gt; -100,  // Discourage a specific token\n        ],\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#anthropic-specific-parameters","title":"Anthropic-Specific Parameters","text":"<pre><code>// @doctest id=\"0e8b\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference('anthropic');\n\n$response = $inference-&gt;with(\n    messages: 'Generate a creative story.',\n    options: [\n        'temperature' =&gt; 0.7,\n        'top_p' =&gt; 0.9,\n        'top_k' =&gt; 40,               // Consider only the top 40 tokens\n        'max_tokens' =&gt; 1000,\n        'stop_sequences' =&gt; [\"\\n\\nHuman:\"],\n        'system' =&gt; 'You are a creative storyteller who specializes in magical realism.',\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#creating-custom-provider-configurations","title":"Creating Custom Provider Configurations","text":"<p>You can create custom configurations for providers that aren't included in the default settings or to modify existing ones.</p>"},{"location":"polyglot/advanced/custom-config/#modifying-configuration-files","title":"Modifying Configuration Files","text":"<p>You can edit the <code>config/llm.php</code> and <code>config/embed.php</code> files directly:</p> <pre><code>// @doctest id=\"a8b5\"\n// In config/llm.php\nreturn [\n    'defaultPreset' =&gt; 'custom_openai',\n\n    'presets' =&gt; [\n        'custom_openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://custom.openai-proxy.com/v1',\n            'apiKey' =&gt; Env::get('CUSTOM_OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4-turbo',\n            'maxTokens' =&gt; 2048,\n            'contextLength' =&gt; 128_000,\n            'maxOutputLength' =&gt; 16384,\n            'httpClientPreset' =&gt; 'guzzle-custom', // Custom HTTP client configuration\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also create custom configurations at runtime using the <code>LLMConfig</code> class:</p> <pre><code>// @doctest id=\"50d5\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom configuration\n$customConfig = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'gpt-4-turbo',\n    maxTokens: 2048,\n    contextLength: 128000,\n    driver: 'openai'\n);\n\n// Use the custom configuration\n$inference = (new Inference)-&gt;withConfig($customConfig);\n\n$response = $inference-&gt;with(\n    messages: 'What are the benefits of using custom configurations?'\n)-&gt;get();\n\necho $response;\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>You might want to use different providers in different environments:</p> <pre><code>// @doctest id=\"dc76\"\n&lt;?php\n// config/llm.php\n\nuse Cognesy\\Config\\Env;\n\n$environment = Env::get('APP_ENV', 'production');\n\nreturn [\n    'defaultPreset' =&gt; $environment === 'production' ? 'openai' : 'ollama',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        'ollama' =&gt; [\n            'providerType' =&gt; 'ollama',\n            'apiUrl' =&gt; 'http://localhost:11434/v1',\n            'apiKey' =&gt; '',\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'llama2',\n            'maxTokens' =&gt; 1024,\n            'httpClientPreset' =&gt; 'http-ollama',\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#creating-custom-inference-drivers","title":"Creating Custom Inference Drivers","text":"<p>In this example we will use an existing driver bundled with Polyglot (OpenAIDriver) as a base class for our custom driver.</p> <p>The driver can be any class that implements <code>CanHandleInference</code> interface.</p> <pre><code>// @doctest id=\"8bf3\"\n// we register new provider type - 'custom-driver'\nLLM::registerDriver(\n    'custom-driver',\n    fn($config, $httpClient) =&gt; new class($config, $httpClient) extends OpenAIDriver {\n        public function handle(InferenceRequest $request): HttpResponse {\n            // some extra functionality to demonstrate our driver is being used\n            echo \"&gt;&gt;&gt; Handling request...\\n\";\n            return parent::handle($request);\n        }\n    }\n);\n\n// in configuration we use newly defined provider type - 'custom-driver'\n$config = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: Env::get('OPENAI_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'gpt-4o-mini',\n    maxTokens: 128,\n    httpClient: 'guzzle',\n    providerType: 'custom-driver',\n);\n\n// now we're calling inference using our configuration\n$answer = (new Inference)\n    -&gt;withConfig($config)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;toText();\n</code></pre> <p>An alternative way of providing driver definition is via class-string:</p> <pre><code>// @doctest id=\"1c7c\"\nLLM::registerDriver('another-driver', AnotherDriver::class);\n</code></pre>"},{"location":"polyglot/advanced/custom-http-client/","title":"Customizing HTTP Client","text":"<p>Polyglot allows you to use custom HTTP clients for specific connection requirements:</p> <pre><code>// @doctest id=\"7e27\"\n&lt;?php\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;use Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom HTTP client configuration\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 5,      // 5 seconds connection timeout\n    requestTimeout: 60,     // 60 seconds request timeout\n    idleTimeout: 120,       // 120 seconds idle timeout for streaming\n    maxConcurrent: 10,      // Maximum 10 concurrent requests\n    failOnError: true,      // Throw exceptions on HTTP errors\n);\n\n// Create a custom HTTP client\n$httpClient = new HttpClient('guzzle', $httpConfig);\n\n// Use the custom HTTP client with Inference\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Make a request with the custom HTTP client\n$response = $inference-&gt;with(\n    messages: 'This request uses a custom HTTP client.'\n)-&gt;get();\n\necho $response;\n</code></pre>"},{"location":"polyglot/advanced/extending/","title":"Extending Polyglot","text":"<p>Understanding Polyglot's architecture makes it easier to extend the library to support new providers or add new functionality.</p>"},{"location":"polyglot/advanced/extending/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<p>To add support for a new LLM provider, you need to implement several components:</p> <ol> <li>Message Format Adapter: Implements <code>CanMapMessages</code> to convert Polyglot's message format to the provider's format</li> <li>Body Format Adapter: Implements <code>CanMapRequestBody</code> to structure the request body according to the provider's API</li> <li>Request Adapter: Implements <code>ProviderRequestAdapter</code> to build HTTP requests for the provider</li> <li>Response Adapter: Implements <code>ProviderResponseAdapter</code> to parse responses from the provider</li> <li>Usage Format Adapter: Implements <code>CanMapUsage</code> to extract token usage information</li> </ol> <p>Then, you need to modify the <code>InferenceDriverFactory</code> to create the appropriate driver for your provider:</p> <pre><code>// @doctest id=\"714a\"\n// In InferenceDriverFactory\npublic function newProvider(LLMConfig $config, CanHandleHttpRequest $httpClient, EventDispatcher $events): CanHandleInference {\n    return new ModularLLMDriver(\n        $config,\n        new NewProviderRequestAdapter(\n            $config,\n            new NewProviderBodyFormat($config, new NewProviderMessageFormat())\n        ),\n        new NewProviderResponseAdapter(new NewProviderUsageFormat()),\n        $httpClient,\n        $events\n    );\n}\n</code></pre> <p>Finally, add your provider to the <code>make</code> method's match statement.</p>"},{"location":"polyglot/advanced/extending/#adding-a-new-embeddings-provider","title":"Adding a New Embeddings Provider","text":"<p>Similarly, to add a new embeddings provider, implement the <code>CanVectorize</code> interface:</p> <pre><code>// @doctest id=\"5a51\"\nnamespace Cognesy\\Polyglot\\Embeddings\\Drivers;\n\nclass NewEmbeddingsDriver implements CanVectorize {\n    public function __construct(\n        protected EmbeddingsConfig $config,\n        protected ?CanHandleHttpRequest $httpClient = null,\n        protected ?EventDispatcher $events = null\n    ) { ... }\n\n    public function vectorize(array $input, array $options = []): EmbeddingsResponse { ... }\n\n    protected function getEndpointUrl(): string { ... }\n    protected function getRequestHeaders(): array { ... }\n    protected function getRequestBody(array $input, array $options): array { ... }\n    protected function toResponse(array $response): EmbeddingsResponse { ... }\n    protected function makeUsage(array $response): Usage { ... }\n}\n</code></pre> <p>Then, modify the <code>Embeddings</code> class to create your driver:</p> <pre><code>// @doctest id=\"4928\"\n// In Embeddings::getDriver\nprotected function getDriver(EmbeddingsConfig $config, CanHandleHttpRequest $httpClient): CanVectorize {\n    return match ($config-&gt;providerType) {\n        // Existing providers...\n        'new-provider' =&gt; new NewEmbeddingsDriver($config, $httpClient, $this-&gt;events),\n        default =&gt; throw new InvalidArgumentException(\"Unknown client: {$config-&gt;providerType}\"),\n    };\n}\n</code></pre>"},{"location":"polyglot/advanced/extending/#adding-custom-middleware","title":"Adding Custom Middleware","text":"<p>You can extend Polyglot's HTTP layer by creating custom middleware:</p> <pre><code>// @doctest id=\"3fd6\"\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass YourCustomMiddleware extends BaseMiddleware {\n    protected function beforeRequest(HttpRequest $request): void {\n        // Modify the request before it's sent\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        // Modify the response after it's received\n        return $response;\n    }\n}\n</code></pre> <p>Then, add your middleware to the HTTP client:</p> <pre><code>// @doctest id=\"94ed\"\n$httpClient = new HttpClient();\n$httpClient-&gt;withMiddleware(new YourCustomMiddleware());\n\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/","title":"Structured outputs with JsonSchema class","text":"<p>JsonSchema is a powerful utility in the Polyglot library that enables developers to define structured data schemas for LLM interactions. This guide explains how to use JsonSchema to shape your LLM outputs and ensure consistent, typed responses from language models.</p>"},{"location":"polyglot/advanced/json-schema/#quick-start","title":"Quick Start","text":"<p>Here's a simple example of how to use JsonSchema with Polyglot's Inference API:</p> <pre><code>// @doctest id=\"010c\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define your schema\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', description: 'City name'),\n        JsonSchema::integer('population', description: 'City population'),\n        JsonSchema::integer('founded', description: 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n// Use the schema with Inference\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with JSON data.']\n        ],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; $schema-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#why-use-jsonschema","title":"Why Use JsonSchema?","text":"<p>JsonSchema provides several benefits when working with LLMs:</p> <ol> <li>Type Safety: Ensure LLM outputs conform to your expected data structure</li> <li>Data Validation: Specify required fields and data types</li> <li>Structured Responses: Get consistent, well-formatted data instead of raw text</li> <li>Complex Nesting: Define deeply nested structures for sophisticated applications</li> <li>Better LLM Guidance: Help the LLM understand exactly what format you need</li> </ol>"},{"location":"polyglot/advanced/json-schema/#available-types","title":"Available Types","text":""},{"location":"polyglot/advanced/json-schema/#string","title":"String","text":"<p>For text values of any length:</p> <pre><code>// @doctest id=\"f5fb\"\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$nameSchema = JsonSchema::string(\n    name: 'full_name',\n    description: 'The user\\'s full name'\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#number-integer","title":"Number &amp; Integer","text":"<p>For numeric values:</p> <pre><code>// @doctest id=\"1f32\"\n$ageSchema = JsonSchema::integer(\n    name: 'age',\n    description: 'The user\\'s age in years'\n);\n\n$priceSchema = JsonSchema::number(\n    name: 'price',\n    description: 'Product price'\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#boolean","title":"Boolean","text":"<p>For true/false values:</p> <pre><code>// @doctest id=\"adc8\"\n$activeSchema = JsonSchema::boolean(\n    name: 'is_active',\n    description: 'Whether the user account is active'\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#array","title":"Array","text":"<p>For lists of items:</p> <pre><code>// @doctest id=\"799b\"\n$tagsSchema = JsonSchema::array(\n    name: 'tags',\n    description: 'List of tags associated with the post',\n    itemSchema: JsonSchema::string()\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#enum","title":"Enum","text":"<p>For values from a specific set of options:</p> <pre><code>// @doctest id=\"59b2\"\n$statusSchema = JsonSchema::enum(\n    name: 'status',\n    description: 'The current status of the post',\n    enumValues: ['draft', 'published', 'archived']\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#object","title":"Object","text":"<p>For complex, nested data structures:</p> <pre><code>// @doctest id=\"e40f\"\n$profileSchema = JsonSchema::object(\n    name: 'profile',\n    description: 'A user\\'s public profile information',\n    properties: [\n        JsonSchema::string('username', 'The unique username'),\n        JsonSchema::string('bio', 'A short biography'),\n        JsonSchema::integer('joined_year', 'Year the user joined'),\n    ],\n    requiredProperties: ['username']\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#working-with-required-and-nullable-fields","title":"Working with Required and Nullable Fields","text":""},{"location":"polyglot/advanced/json-schema/#required-fields","title":"Required Fields","text":"<p>Required fields are specified at the object level using the <code>requiredProperties</code> parameter:</p> <pre><code>// @doctest id=\"5989\"\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('email', 'Primary email address'),\n        JsonSchema::string('name', 'User\\'s full name'),\n        JsonSchema::string('bio', 'User biography'),\n    ],\n    requiredProperties: ['email', 'name'] // email and name must be present\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#nullable-fields","title":"Nullable Fields","text":"<p>Nullable fields are specified at the individual field level:</p> <pre><code>// @doctest id=\"68dd\"\n$bioSchema = JsonSchema::string(\n    name: 'bio',\n    description: 'Optional user biography',\n    nullable: true\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#understanding-required-vs-nullable","title":"Understanding Required vs. Nullable","text":"<ul> <li>Required: The field must be present in the data structure</li> <li>Nullable: The field can contain a null value</li> <li>A field can be both required and nullable (must be present, can be null)</li> <li>A field can be non-required and non-nullable (when present, cannot be null)</li> </ul>"},{"location":"polyglot/advanced/json-schema/#common-patterns","title":"Common Patterns","text":"<pre><code>// @doctest id=\"71c6\"\n// Required and Non-nullable (most strict)\nJsonSchema::string('email', 'Primary email', nullable: false);\n// requiredProperties: ['email']\n\n// Required but Nullable (must be present, can be null)\nJsonSchema::string('bio', 'User bio', nullable: true);\n// requiredProperties: ['bio']\n\n// Optional and Non-nullable (can be omitted, but if present cannot be null)\nJsonSchema::string('phone', 'Phone number', nullable: false);\n// requiredProperties: [] (doesn't include 'phone')\n\n// Optional and Nullable (most permissive)\nJsonSchema::string('website', 'Personal website', nullable: true);\n// requiredProperties: [] (doesn't include 'website')\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#working-with-openai-and-other-providers","title":"Working with OpenAI and Other Providers","text":"<p>When working with OpenAI in strict mode, follow these guidelines:</p> <pre><code>// @doctest id=\"23b3\"\n// For OpenAI strict mode: \n// - All fields should be required\n// - Use nullable: true for optional fields\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('email', 'Required email address'),\n        JsonSchema::string('bio', 'Optional biography', nullable: true),\n    ],\n    requiredProperties: ['email', 'bio'] // Note: bio is required but nullable\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#building-complex-schemas","title":"Building Complex Schemas","text":"<p>For more complex data structures, you can nest schemas:</p> <pre><code>// @doctest id=\"8cfb\"\n// Define child schemas first\n$addressSchema = JsonSchema::object(\n    name: 'address',\n    properties: [\n        JsonSchema::string('street', 'Street address'),\n        JsonSchema::string('city', 'City name'),\n        JsonSchema::string('country', 'Country name'),\n    ],\n    requiredProperties: ['street', 'city', 'country']\n);\n\n// Use them in parent schemas\n$userSchema = JsonSchema::object(\n    name: 'user',\n    properties: [\n        JsonSchema::string('name', 'User name'),\n        $addressSchema, // embed the address schema\n    ],\n    requiredProperties: ['name', 'address']\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#fluent-api-for-schema-creation","title":"Fluent API for Schema Creation","text":"<p>JsonSchema supports method chaining for a more fluent API:</p> <pre><code>// @doctest id=\"a4e4\"\n$schema = JsonSchema::array('tags')\n    -&gt;withItemSchema(JsonSchema::string())\n    -&gt;withDescription('A list of tags')\n    -&gt;withNullable(true);\n</code></pre> <p>Available methods include: - <code>withName(string $name)</code> - <code>withDescription(string $description)</code> - <code>withTitle(string $title)</code> - <code>withNullable(bool $nullable = true)</code> - <code>withMeta(array $meta = [])</code> - <code>withEnumValues(?array $enum = null)</code> - <code>withProperties(?array $properties = null)</code> - <code>withItemSchema(JsonSchema $itemSchema = null)</code> - <code>withRequiredProperties(?array $required = null)</code> - <code>withAdditionalProperties(bool $additionalProperties = false)</code></p>"},{"location":"polyglot/advanced/json-schema/#accessing-schema-properties","title":"Accessing Schema Properties","text":"<p>JsonSchema provides various methods to access schema properties:</p> <pre><code>// @doctest id=\"ec83\"\n$schema-&gt;type();                // Get schema type (e.g., 'object')\n$schema-&gt;name();                // Get schema name\n$schema-&gt;isNullable();          // Check if schema is nullable\n$schema-&gt;requiredProperties();  // Get array of required properties\n$schema-&gt;properties();          // Get array of all properties\n$schema-&gt;property('name');      // Get specific property\n$schema-&gt;itemSchema();          // Get item schema for array schemas\n$schema-&gt;enumValues();          // Get enum values\n$schema-&gt;hasAdditionalProperties(); // Check if additional properties are allowed\n$schema-&gt;description();         // Get schema description\n$schema-&gt;title();               // Get schema title\n$schema-&gt;meta();                // Get all meta fields\n$schema-&gt;meta('key');           // Get specific meta field\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#converting-schemas-to-arrays-and-function-calls","title":"Converting Schemas to Arrays and Function Calls","text":"<p>JsonSchema can be converted to arrays and function calls:</p> <pre><code>// @doctest id=\"7320\"\n// Convert to array\n$schemaArray = $schema-&gt;toArray();\n\n// Convert to JSON schema\n$jsonSchema = $schema-&gt;toJsonSchema();\n\n// Convert to function call (for tools/functions)\n$functionCall = $schema-&gt;toFunctionCall(\n    functionName: 'getUserProfile',\n    functionDescription: 'Gets the user profile information',\n    strict: true\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#meta-fields","title":"Meta Fields","text":"<p>You can add custom meta fields to your schemas:</p> <pre><code>// @doctest id=\"7783\"\n$schema = JsonSchema::string(\n    name: 'username',\n    description: 'The username',\n    meta: [\n        'min_length' =&gt; 3,\n        'max_length' =&gt; 50,\n        'pattern' =&gt; '^[a-zA-Z0-9_]+$',\n    ]\n);\n</code></pre> <p>Meta fields will be transformed to include the <code>x-</code> prefix when converted to arrays (e.g., <code>x-min_length</code>).</p>"},{"location":"polyglot/advanced/json-schema/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Descriptions: Write clear, concise descriptions for each field.</li> </ol> <pre><code>// @doctest id=\"3e97\"\n// \u274c Not helpful\nJsonSchema::string('name', 'the name');\n\n// \u2705 Much better\nJsonSchema::string('name', 'The user\\'s display name (2-50 characters)');\n</code></pre> <ol> <li> <p>Only Mark Required Fields: Only mark fields as required if they're truly necessary.</p> </li> <li> <p>Organize Nested Schemas: Keep your schemas organized when dealing with complex structures.</p> </li> </ol> <pre><code>// @doctest id=\"b59b\"\n// Define child schemas first for clarity\n$addressSchema = JsonSchema::object(/*...*/);\n$contactSchema = JsonSchema::object(/*...*/);\n\n// Then use them in your parent schema\n$userSchema = JsonSchema::object(\n    properties: [$addressSchema, $contactSchema]\n);\n</code></pre> <ol> <li>Be Explicit About Requirements: Specify both the nullable status and required fields for clarity.</li> </ol>"},{"location":"polyglot/advanced/json-schema/#full-example-creating-user-profile-schema","title":"Full Example: Creating User Profile Schema","text":"<pre><code>// @doctest id=\"71f7\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define address schema\n$addressSchema = JsonSchema::object(\n    name: 'address',\n    properties: [\n        JsonSchema::string('street', 'Street address'),\n        JsonSchema::string('city', 'City name'),\n        JsonSchema::string('postal_code', 'Postal/ZIP code'),\n        JsonSchema::string('country', 'Country name'),\n    ],\n    requiredProperties: ['city', 'country'],\n);\n\n// Define contact schema\n$contactSchema = JsonSchema::object(\n    name: 'contact',\n    properties: [\n        JsonSchema::string('email', 'Email address'),\n        JsonSchema::string('phone', 'Phone number', nullable: true),\n    ],\n    requiredProperties: ['email', 'phone'],\n);\n\n// Define hobbies schema\n$hobbiesSchema = JsonSchema::array(\n    name: 'hobbies',\n    description: 'List of user hobbies',\n    itemSchema: JsonSchema::object(\n        properties: [\n            JsonSchema::string('name', 'Hobby name'),\n            JsonSchema::string('description', 'Hobby description', nullable: true),\n            JsonSchema::integer('years_experience', 'Years of experience', nullable: true),\n        ],\n        requiredProperties: ['name'],\n    ),\n);\n\n// Define main user schema\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'User\\'s full name'),\n        JsonSchema::integer('age', 'User\\'s age'),\n        $addressSchema,\n        $contactSchema,\n        $hobbiesSchema,\n        JsonSchema::enum(\n            'status',\n            'Account status',\n            enumValues: ['active', 'inactive', 'pending'],\n        ),\n    ],\n    requiredProperties: ['name', 'age', 'address', 'contact', 'status'],\n);\n\n// Use the schema with Inference\n$userData = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a profile for John Doe who lives in New York.']\n        ],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'User profile data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'user_profile',\n                'schema' =&gt; $userSchema-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\nprint_r($userData);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#advanced-creating-function-calls","title":"Advanced: Creating Function Calls","text":"<p>JsonSchema can be used to define function/tool parameters for LLMs:</p> <pre><code>// @doctest id=\"3e0b\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define the schema for the function parameters\n$weatherParamsSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('location', 'City and country name'),\n        JsonSchema::enum(\n            'unit', \n            'Temperature unit', \n            enumValues: ['celsius', 'fahrenheit'],\n            nullable: true\n        ),\n    ],\n    requiredProperties: ['location'],\n);\n\n// Convert schema to function call format\n$functionDefinition = $weatherParamsSchema-&gt;toFunctionCall(\n    functionName: 'getWeather',\n    functionDescription: 'Get the current weather for a location',\n    strict: true\n);\n\n// Use with Polyglot's Inference API\n$result = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What\\'s the weather like in Tokyo?']\n        ],\n        tools: [$functionDefinition],\n        // Additional configuration...\n    )\n    -&gt;create();\n</code></pre>"},{"location":"polyglot/embeddings/optimization/","title":"Optimization","text":""},{"location":"polyglot/embeddings/optimization/#optimization","title":"Optimization","text":""},{"location":"polyglot/embeddings/optimization/#batch-processing-for-efficiency","title":"Batch Processing for Efficiency","text":"<p>When processing many documents, it's more efficient to batch them:</p> <pre><code>// @doctest id=\"512a\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$allDocuments = [/* large array of documents */];\n\n// Process in batches of 25 (check provider-specific limits)\n$batchSize = 25;\n$vectors = [];\n\nfor ($i = 0; $i &lt; count($allDocuments); $i += $batchSize) {\n    $batch = array_slice($allDocuments, $i, $batchSize);\n\n    try {\n        $response = $embeddings-&gt;with($batch)-&gt;get();\n        $batchVectors = $response-&gt;toValuesArray();\n\n        // Add to our vectors array\n        $vectors = array_merge($vectors, $batchVectors);\n\n        echo \"Processed batch \" . (floor($i / $batchSize) + 1) . \" of \" . ceil(count($allDocuments) / $batchSize) . \"\\n\";\n    } catch (\\Exception $e) {\n        echo \"Error processing batch: \" . $e-&gt;getMessage() . \"\\n\";\n    }\n\n    // Optional: Add a small delay to avoid hitting rate limits\n    usleep(100000); // 100ms\n}\n\necho \"Processed \" . count($vectors) . \" embeddings in total.\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/optimization/#caching-embeddings","title":"Caching Embeddings","text":"<p>For better performance, you can cache embeddings to avoid regenerating them:</p> <pre><code>// @doctest id=\"5bce\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass CachedEmbeddings {\n    private $embeddings;\n    private $cache = [];\n\n    public function __construct(?Embeddings $embeddings = null) {\n        $this-&gt;embeddings = $embeddings ?? new Embeddings();\n    }\n\n    public function create($input, array $options = []): array {\n        if (is_string($input)) {\n            // Single string input\n            $cacheKey = $this-&gt;getCacheKey($input, $options);\n\n            if (isset($this-&gt;cache[$cacheKey])) {\n                return $this-&gt;cache[$cacheKey];\n            }\n\n            $response = $this-&gt;embeddings-&gt;with($input, $options)-&gt;get();\n            $vector = $response-&gt;first()-&gt;values();\n\n            $this-&gt;cache[$cacheKey] = $vector;\n            return $vector;\n        } else {\n            // Array of strings\n            $results = [];\n            $uncachedInputs = [];\n            $uncachedIndices = [];\n\n            // Check cache for each input\n            foreach ($input as $i =&gt; $text) {\n                $cacheKey = $this-&gt;getCacheKey($text, $options);\n\n                if (isset($this-&gt;cache[$cacheKey])) {\n                    $results[$i] = $this-&gt;cache[$cacheKey];\n                } else {\n                    $uncachedInputs[] = $text;\n                    $uncachedIndices[] = $i;\n                }\n            }\n\n            // Generate embeddings for uncached inputs\n            if (!empty($uncachedInputs)) {\n                $response = $this-&gt;embeddings-&gt;with($uncachedInputs, $options)-&gt;get();\n                $vectors = $response-&gt;toValuesArray();\n\n                foreach ($vectors as $j =&gt; $vector) {\n                    $i = $uncachedIndices[$j];\n                    $results[$i] = $vector;\n\n                    // Update cache\n                    $cacheKey = $this-&gt;getCacheKey($input[$i], $options);\n                    $this-&gt;cache[$cacheKey] = $vector;\n                }\n            }\n\n            // Sort by original indices\n            ksort($results);\n            return $results;\n        }\n    }\n\n    private function getCacheKey(string $input, array $options): string {\n        $model = $options['model'] ?? '';\n        return md5($input . serialize($options) . $model);\n    }\n}\n\n// Usage\n$cachedEmbeddings = new CachedEmbeddings(new Embeddings('openai'));\n\n// First call will generate embeddings\n$vector1 = $cachedEmbeddings-&gt;create(\"This is a test\");\necho \"First call completed, generated vector with \" . count($vector1) . \" dimensions.\\n\";\n\n// Second call will use the cache\n$vector2 = $cachedEmbeddings-&gt;create(\"This is a test\");\necho \"Second call completed (from cache).\\n\";\n\n// Compare vectors to verify they're the same\n$equal = (serialize($vector1) === serialize($vector2));\necho \"Vectors are \" . ($equal ? \"identical\" : \"different\") . \".\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/","title":"Overview of Embeddings","text":"<p>Embeddings are a key component of many LLM-based solutions and are used to represent text (or multimodal data) with numbers capturing their meaning and relationships.</p> <p>Embeddings are numerical representations of text or other data that capture semantic meaning in a way that computers can process efficiently. They enable powerful applications like semantic search, document clustering, recommendation systems, and more. This chapter explores how to use Polyglot's Embeddings API to work with vector embeddings across multiple providers.</p>"},{"location":"polyglot/embeddings/overview/#understanding-embeddings","title":"Understanding Embeddings","text":"<p>Before diving into code, it's helpful to understand what embeddings are and how they work:</p> <ul> <li>Embeddings represent words, phrases, or documents as vectors of floating-point numbers in a high-dimensional space</li> <li>Similar items (semantically related) have vectors that are closer together in this space</li> <li>The \"distance\" between vectors can be measured using metrics like cosine similarity or Euclidean distance</li> <li>Modern embedding models are trained on massive corpora of text to capture nuanced relationships</li> </ul> <p>Common use cases for embeddings include:</p> <ul> <li>Semantic search: Finding documents similar to a query based on meaning, not just keywords</li> <li>Clustering: Grouping similar documents together</li> <li>Classification: Assigning categories to documents based on their content</li> <li>Recommendations: Suggesting related items</li> <li>Information retrieval: Finding relevant information in large datasets</li> </ul>"},{"location":"polyglot/embeddings/overview/#embeddings-class","title":"<code>Embeddings</code> class","text":"<p>The <code>Embeddings</code> class is a facade that provides access to embeddings APIs across multiple providers. It combines functionality through traits for provider configuration, request building, and result handling.</p>"},{"location":"polyglot/embeddings/overview/#architecture-overview","title":"Architecture Overview","text":"<p>The <code>Embeddings</code> class combines functionality through traits: - HandlesInitMethods: Provider configuration and setup - HandlesFluentMethods: Request parameter configuration - HandlesInvocation: Request execution and PendingEmbeddings creation - HandlesShortcuts: Convenient methods for common result formats</p>"},{"location":"polyglot/embeddings/overview/#supported-providers","title":"Supported providers","text":"<p><code>Embeddings</code> class supports the following embeddings providers: - Azure OpenAI: Azure-hosted OpenAI embedding models - Cohere: Cohere's embedding models - Gemini: Google's Gemini embedding models - Jina: Jina AI's embedding models - OpenAI: OpenAI's embedding models (text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large)</p> <p>Provider configurations are managed through the configuration system.</p>"},{"location":"polyglot/embeddings/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>// @doctest id=\"0e4c\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Simple embedding generation\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('The quick brown fox jumps over the lazy dog.')-&gt;get();\n\n// Get the vector values from the first result\n$vector = $result-&gt;first()-&gt;values();\necho \"Generated a vector with \" . count($vector) . \" dimensions.\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/#provider-configuration-methods","title":"Provider Configuration Methods","text":"<p>Configure the underlying embeddings provider:</p> <pre><code>// @doctest id=\"f954\"\n// Provider selection and configuration\n$embeddings-&gt;using('openai');                          // Use preset configuration\n$embeddings-&gt;withPreset('openai');                     // Alternative preset method\n$embeddings-&gt;withDsn('openai://model=text-embedding-3-large'); // Configure via DSN\n$embeddings-&gt;withConfig($customConfig);                // Explicit configuration\n$embeddings-&gt;withConfigProvider($configProvider);     // Custom config provider\n\n// HTTP and debugging\n$embeddings-&gt;withHttpClient($customHttpClient);       // Custom HTTP client\n$embeddings-&gt;withDebugPreset('verbose');              // Debug configuration\n\n// Driver management\n$embeddings-&gt;withDriver($customDriver);               // Custom vectorization driver\n$embeddings-&gt;withProvider($customProvider);           // Custom provider instance\n</code></pre>"},{"location":"polyglot/embeddings/overview/#request-configuration-methods","title":"Request Configuration Methods","text":"<p>Configure the embedding request:</p> <pre><code>// @doctest id=\"89df\"\n// Input configuration\n$embeddings-&gt;withInputs('Single text input');         // Single string\n$embeddings-&gt;withInputs(['Text 1', 'Text 2']);       // Multiple strings\n$embeddings-&gt;with('Input text');                      // Shorthand input method\n\n// Model and options\n$embeddings-&gt;withModel('text-embedding-3-large');    // Specific model\n$embeddings-&gt;withOptions(['dimensions' =&gt; 1536]);     // Provider-specific options\n\n// Complete configuration\n$embeddings-&gt;with(\n    input: ['Text 1', 'Text 2'],\n    options: ['dimensions' =&gt; 1536],\n    model: 'text-embedding-3-large'\n);\n</code></pre>"},{"location":"polyglot/embeddings/overview/#response-methods","title":"Response Methods","text":"<p>Get embeddings in different formats:</p> <pre><code>// @doctest id=\"f892\"\n// Full response object\n$response = $embeddings-&gt;get();                       // EmbeddingsResponse object\n\n// Vector extraction\n$vectors = $embeddings-&gt;vectors();                    // Array of Vector objects\n$firstVector = $embeddings-&gt;first();                 // First Vector object\n$values = $embeddings-&gt;first()-&gt;values();           // Array of floats\n\n// Advanced response handling\n$pending = $embeddings-&gt;create();                    // PendingEmbeddings for custom handling\n$response = $pending-&gt;get();                         // Execute and get response\n</code></pre>"},{"location":"polyglot/embeddings/overview/#working-with-multiple-providers","title":"Working with Multiple Providers","text":"<pre><code>// @doctest id=\"d61e\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// OpenAI embeddings\n$openaiVectors = (new Embeddings())\n    -&gt;using('openai')\n    -&gt;withModel('text-embedding-3-large')\n    -&gt;with(['Document 1', 'Document 2'])\n    -&gt;vectors();\n\n// Cohere embeddings  \n$cohereVectors = (new Embeddings())\n    -&gt;using('cohere')\n    -&gt;withModel('embed-english-v3.0')\n    -&gt;with(['Document 1', 'Document 2'])\n    -&gt;vectors();\n\necho \"OpenAI dimensions: \" . count($openaiVectors[0]-&gt;values()) . \"\\n\";\necho \"Cohere dimensions: \" . count($cohereVectors[0]-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/#custom-configuration","title":"Custom Configuration","text":"<p>Create custom configurations for specific use cases:</p> <pre><code>// @doctest id=\"fdb3\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create custom configuration\n$config = new EmbeddingsConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/embeddings',\n    model: 'text-embedding-3-large',\n    dimensions: 3072,\n    maxInputs: 100,\n    driver: 'openai'\n);\n\n// Use custom configuration\n$embeddings = (new Embeddings())\n    -&gt;withConfig($config)\n    -&gt;with('Custom configuration example');\n\n$vector = $embeddings-&gt;first()-&gt;values();\necho \"Generated embedding with \" . count($vector) . \" dimensions\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/#driver-registration","title":"Driver Registration","text":"<p>Register custom drivers for new providers:</p> <pre><code>// @doctest id=\"193e\"\n// Register with class name\nEmbeddings::registerDriver('custom-provider', CustomEmbeddingsDriver::class);\n\n// Register with factory callable\nEmbeddings::registerDriver('custom-provider', function($config, $httpClient) {\n    return new CustomEmbeddingsDriver($config, $httpClient);\n});\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/","title":"Working with Embeddings","text":""},{"location":"polyglot/embeddings/work-with-embeddings/#the-embeddings-class","title":"The Embeddings Class","text":"<p>Polyglot provides the <code>Embeddings</code> class as the primary interface for generating and working with vector embeddings.</p>"},{"location":"polyglot/embeddings/work-with-embeddings/#creating-an-embeddings-instance","title":"Creating an Embeddings Instance","text":"<pre><code>// @doctest id=\"618f\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create a basic embeddings instance with default settings\n$embeddings = new Embeddings();\n\n// Create an embeddings instance with a specific connection\n$embeddings = new Embeddings('openai');\n\n// Alternative method to specify connection\n$embeddings = (new Embeddings())-&gt;using('openai');\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#key-methods","title":"Key Methods","text":"<p>The <code>Embeddings</code> class provides several important methods:</p> <ul> <li><code>create()</code>: Generates embeddings for input text</li> <li><code>using()</code>: Specifies which connection preset to use</li> <li><code>withConfig()</code>: Sets a custom configuration</li> <li><code>withHttpClient()</code>: Specifies a custom HTTP client</li> <li><code>withModel()</code>: Overrides the default model</li> <li><code>findSimilar()</code>: Finds documents similar to a query</li> </ul>"},{"location":"polyglot/embeddings/work-with-embeddings/#generating-embeddings","title":"Generating Embeddings","text":"<p>The core functionality of the <code>Embeddings</code> class is to transform text into vector representations.</p>"},{"location":"polyglot/embeddings/work-with-embeddings/#basic-embedding-generation","title":"Basic Embedding Generation","text":"<pre><code>// @doctest id=\"a157\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('The quick brown fox jumps over the lazy dog.')-&gt;get();\n\n// Get the vector values from the first (and only) result\n$vector = $result-&gt;first()?-&gt;values();\n\necho \"Generated a vector with \" . count($vector) . \" dimensions.\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#embedding-multiple-texts","title":"Embedding Multiple Texts","text":"<p>You can generate embeddings for multiple texts in a single request, which is more efficient than making separate requests:</p> <pre><code>// @doctest id=\"99dc\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n\n$documents = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning models can process text into vector representations.\",\n    \"Embeddings capture semantic relationships between words and documents.\"\n];\n\n$result = $embeddings-&gt;with($documents)-&gt;get();\n\n// Get all vectors\n$vectors = $result-&gt;vectors();\n\nforeach ($vectors as $index =&gt; $vector) {\n    echo \"Document \" . ($index + 1) . \" has a vector with \" . count($vector-&gt;values()) . \" dimensions.\\n\";\n}\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#accessing-embedding-results","title":"Accessing Embedding Results","text":"<p>The <code>create()</code> method returns an <code>EmbeddingsResponse</code> object with several useful methods:</p> <pre><code>// @doctest id=\"fc71\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('Sample text for embedding')-&gt;get();\n\n// Get the first vector\n$firstVector = $result-&gt;first();\n\n// Get the last vector (useful when processing multiple inputs)\n$lastVector = $result-&gt;last();\n\n// Get all vectors\n$allVectors = $result-&gt;vectors();\n\n// Get all vector values as a simple array of arrays\n$valuesArray = $result-&gt;toValuesArray();\n\n// Get usage information\n$usage = $result-&gt;usage();\necho \"Input tokens: \" . $usage-&gt;input() . \"\\n\";\necho \"Output tokens: \" . $usage-&gt;output() . \"\\n\";\necho \"Total tokens: \" . $usage-&gt;total() . \"\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#working-with-vector-objects","title":"Working with Vector Objects","text":"<p>Each vector in the response is represented by a <code>Vector</code> object with its own methods:</p> <pre><code>// @doctest id=\"354c\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('Sample text for embedding')-&gt;get();\n$vector = $result-&gt;first();\n\n// Get vector values\n$values = $vector-&gt;values();\n\n// Get vector ID (index)\n$id = $vector-&gt;id();\n\n// Compare with another vector\n$otherVector = $result-&gt;with('Another text for comparison')-&gt;first();\n$similarity = $vector-&gt;compareTo($otherVector, 'cosine');\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#working-with-different-providers","title":"Working with Different Providers","text":"<p>Polyglot supports multiple embedding providers, each with their own strengths and characteristics.</p>"},{"location":"polyglot/embeddings/work-with-embeddings/#switching-between-providers","title":"Switching Between Providers","text":"<pre><code>// @doctest id=\"aefb\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Compare embeddings from different providers\n$text = \"Artificial intelligence is transforming industries worldwide.\";\n\n// OpenAI embeddings\n$openaiEmbeddings = new Embeddings('openai');\n$openaiResult = $openaiEmbeddings-&gt;with($text)-&gt;get();\necho \"OpenAI embedding dimensions: \" . count($openaiResult-&gt;first()?-&gt;values()) . \"\\n\";\n\n// Cohere embeddings\n$cohereEmbeddings = new Embeddings('cohere');\n$cohereResult = $cohereEmbeddings-&gt;with($text)-&gt;get();\necho \"Cohere embedding dimensions: \" . count($cohereResult-&gt;first()?-&gt;values()) . \"\\n\";\n\n// Mistral embeddings\n$mistralEmbeddings = new Embeddings('mistral');\n$mistralResult = $mistralEmbeddings-&gt;with($text)-&gt;get();\necho \"Mistral embedding dimensions: \" . count($mistralResult-&gt;first()?-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#provider-specific-options","title":"Provider-Specific Options","text":"<p>Different providers may support additional options for embedding generation:</p> <pre><code>// @doctest id=\"8499\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Example with OpenAI-specific options\n$openaiEmbeddings = new Embeddings('openai');\n$response = $openaiEmbeddings-&gt;with(\n    input: [\"Sample text for embedding\"],\n    options: [\n        'encoding_format' =&gt; 'float',  // Get float values instead of base64\n        'dimensions' =&gt; 512,           // Request a specific vector size (if supported)\n    ]\n)-&gt;get();\n\n// Example with Cohere-specific options\n$cohereEmbeddings = new Embeddings('cohere');\n$response = $cohereEmbeddings-&gt;with(\n    input: [\"Sample text for embedding\"],\n    options: [\n        'input_type' =&gt; 'classification',  // Cohere-specific option\n        'truncate' =&gt; 'END',               // How to handle texts that exceed the token limit\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#models-and-dimensions","title":"Models and Dimensions","text":"<p>Different embedding models produce vectors of different dimensions:</p> <pre><code>// @doctest id=\"1384\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;use Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create custom configuration with a specific model\n$config = new EmbeddingsConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/embeddings',\n    model: 'text-embedding-3-large',  // Use the larger model\n    dimensions: 3072,                 // Specify expected dimensions\n);\n\n$embeddings = new Embeddings();\n$embeddings-&gt;withConfig($config);\n\n$response = $embeddings-&gt;with(\"Test text for large embedding model\")-&gt;get();\necho \"Vector dimensions: \" . count($response-&gt;first()?-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/","title":"Creating Requests","text":"<p>This section covers how to create requests to LLM providers using the Polyglot library. It includes examples of basic requests, handling multiple messages, and using different message formats.</p>"},{"location":"polyglot/essentials/creating-requests/#creating-requests","title":"Creating Requests","text":"<p>The <code>with()</code> method is the main way to set the parameters of the requests to LLM providers.</p> <p>It accepts several parameters:</p> <pre><code>// @doctest id=\"6b13\"\npublic function with(\n    string|array $messages = [],    // The messages to send to the LLM\n    string $model = '',             // The model to use (overrides default)\n    array $tools = [],              // Tools/functions for the model to use\n    string|array $toolChoice = [],  // Tool selection preference\n    array $responseFormat = [],     // Response format specification\n    array $options = [],            // Additional request options\n    Mode $mode = OutputMode::Text   // Output mode (Text, JSON, etc.)\n) : self\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#basic-request-example","title":"Basic Request Example","text":"<p>Here's a simple example of creating a request:</p> <pre><code>// @doctest id=\"2eb6\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;with(\n        messages: 'What is the capital of France?'\n    )\n    -&gt;create() // create pending inference\n    -&gt;get();   // get the data - here it executes the request\n\necho \"Response: $response\";\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#request-with-multiple-messages","title":"Request with Multiple Messages","text":"<p>For chat-based interactions, you can pass an array of messages:</p> <pre><code>// @doctest id=\"e78b\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages([\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant who provides concise answers.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?'],\n        ['role' =&gt; 'assistant', 'content' =&gt; 'Paris.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'And what about Germany?']\n    ])\n    -&gt;get();\n\necho \"Response: $response\";\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#using-messages-class","title":"Using <code>Messages</code> Class","text":"<p>You can also use the <code>Messages</code> class to create message sequences more conveniently:</p> <pre><code>// @doctest id=\"f0af\"\n&lt;?php\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Messages\\Utils\\Image;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$messages = (new Messages)\n    -&gt;asSystem('You are a senior PHP8 backend developer.')\n    -&gt;asDeveloper('Be concise and use modern PHP8.2+ features.') // OpenAI developer role is supported and normalized for other providers\n    -&gt;asUser([\n        'What is the best way to handle errors in PHP8?',\n        'Provide a code example.',\n    ]); // you can pass array of strings to create multiple content parts\n\n$response = (new Inference)\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4o')\n    -&gt;withMessages($messages)\n    -&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#message-formats","title":"Message Formats","text":"<p>Polyglot supports different message formats depending on the provider:</p> <ul> <li>String: A simple string will be converted to a user message</li> <li>Array of messages: Each message should have a <code>role</code> and <code>content</code> field</li> <li>Multimodal content: Some providers support images in messages</li> </ul> <p>Example with image (for providers that support it):</p> <pre><code>// @doctest id=\"0d2a\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$imageData = base64_encode(file_get_contents('image.jpg'));\n$messages = [\n    [\n        'role' =&gt; 'user',\n        'content' =&gt; [\n            [\n                'type' =&gt; 'text',\n                'text' =&gt; 'What\\'s in this image?'\n            ],\n            [\n                'type' =&gt; 'image_url',\n                'image_url' =&gt; [\n                    'url' =&gt; \"data:image/jpeg;base64,$imageData\"\n                ]\n            ]\n        ]\n    ]\n];\n\n$response = (new Inference())\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4o') // use multimodal model\n    -&gt;with(messages: $messages)\n    -&gt;get();\n</code></pre> <p>Instructor library offers <code>Cognesy\\Messages\\Utils\\Image</code> class for easier conversion of image files to the message format.</p>"},{"location":"polyglot/essentials/inference-class/","title":"Inference Class","text":"<p>The <code>Inference</code> class is the primary facade for making requests to LLM providers in Polyglot. It provides a unified interface for configuring providers, building requests, and executing inference operations.</p>"},{"location":"polyglot/essentials/inference-class/#architecture-overview","title":"Architecture Overview","text":"<p>The <code>Inference</code> class combines functionality through traits: - HandlesLLMProvider: Provider configuration and driver management - HandlesRequestBuilder: Request construction and configuration - HandlesInvocation: Request execution and PendingInference creation - HandlesShortcuts: Convenient methods for common response formats</p>"},{"location":"polyglot/essentials/inference-class/#basic-usage","title":"Basic Usage","text":"<pre><code>// @doctest id=\"6d5c\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Simple text completion\n$response = (new Inference())\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\n// Using a specific provider\n$response = (new Inference())\n    -&gt;using('openai')\n    -&gt;withMessages('Explain quantum physics')\n    -&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#llm-provider-configuration-methods","title":"LLM Provider Configuration Methods","text":"<p>Configure the underlying LLM provider:</p> <pre><code>// @doctest id=\"a847\"\n// Provider selection and configuration\n$inference-&gt;using('openai');                           // Use preset configuration\n$inference-&gt;withDsn('openai://model=gpt-4');          // Configure via DSN\n$inference-&gt;withConfig($customConfig);                 // Explicit configuration\n$inference-&gt;withConfigProvider($configProvider);      // Custom config provider\n$inference-&gt;withLLMConfigOverrides(['temperature' =&gt; 0.7]);\n\n// HTTP client configuration\n$inference-&gt;withHttpClient($customHttpClient);        // Custom HTTP client\n$inference-&gt;withHttpClientPreset('debug');           // HTTP client preset\n$inference-&gt;withDebugPreset('verbose');              // Debug configuration\n\n// Driver management\n$inference-&gt;withDriver($customDriver);               // Custom inference driver\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#request-building-methods","title":"Request Building Methods","text":"<p>Configure the inference request:</p> <pre><code>// @doctest id=\"eb5f\"\n// Message configuration\n$inference-&gt;withMessages('Hello, world!');           // String message\n$inference-&gt;withMessages(['user' =&gt; 'Hello']);       // Array format\n$inference-&gt;withMessages($messageObject);            // Message object\n\n// Model and generation parameters\n$inference-&gt;withModel('gpt-4');                      // Specific model\n$inference-&gt;withMaxTokens(100);                      // Token limit\n$inference-&gt;withOutputMode($outputMode);             // Response format mode\n\n// Tool usage\n$inference-&gt;withTools($toolDefinitions);             // Available tools\n$inference-&gt;withToolChoice('auto');                  // Tool selection strategy\n\n// Response formatting\n$inference-&gt;withResponseFormat(['type' =&gt; 'json']); // Response format\n$inference-&gt;withOptions(['temperature' =&gt; 0.5]);    // Additional options\n\n// Advanced features\n$inference-&gt;withStreaming(true);                     // Enable streaming\n$inference-&gt;withCachedContext($messages, $tools);   // Context caching\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#invocation-methods","title":"Invocation Methods","text":"<p>Execute inference requests:</p> <pre><code>// @doctest id=\"52bf\"\n// Flexible configuration method\n$inference-&gt;with(\n    messages: 'Hello',\n    model: 'gpt-4',\n    tools: [],\n    toolChoice: 'auto',\n    responseFormat: ['type' =&gt; 'text'],\n    options: ['temperature' =&gt; 0.7],\n    mode: OutputMode::Text\n);\n\n// Create pending inference for advanced handling\n$pending = $inference-&gt;create();\n\n// Direct request execution\n$inference-&gt;withRequest($existingRequest);\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#response-shortcuts","title":"Response Shortcuts","text":"<p>Get responses in different formats:</p> <pre><code>// @doctest id=\"d94d\"\n// Text responses\n$text = $inference-&gt;get();                           // Plain text\n$response = $inference-&gt;response();                  // Full InferenceResponse object\n\n// JSON responses  \n$json = $inference-&gt;asJson();                        // JSON string\n$data = $inference-&gt;asJsonData();                    // Parsed array\n\n// Streaming\n$stream = $inference-&gt;stream();                      // InferenceStream object\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#driver-registration","title":"Driver Registration","text":"<p>Register custom drivers for new providers:</p> <pre><code>// @doctest id=\"125d\"\n// Register with class name\nInference::registerDriver('custom-provider', CustomDriver::class);\n\n// Register with factory callable\nInference::registerDriver('custom-provider', function($config, $httpClient) {\n    return new CustomDriver($config, $httpClient);\n});\n</code></pre>"},{"location":"polyglot/essentials/overview/","title":"Overview of Inference","text":"<p><code>Inference</code> class offers access to LLM APIs and convenient methods to execute model inference, incl. chat completions, tool calling or JSON output generation.</p> <p>LLM providers access details can be found and modified via <code>/config/llm.php</code>.</p>"},{"location":"polyglot/essentials/overview/#simple-text-generation","title":"Simple Text Generation","text":"<p>The simplest way to use Polyglot is to generate text using static <code>Inference::text()</code> method. Simplified inference API uses the default connection for convenient ad-hoc calls.</p> <pre><code>// @doctest id=\"633d\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Generate text using the default connection\n$answer = (new Inference)-&gt;with(messages: 'What is the capital of France?')-&gt;get();\n\necho \"Answer: $answer\";\n\n// Output: Answer: The capital of France is Paris.\n</code></pre> <p>This static method uses the default connection specified in your configuration. Default LLM connection can be configured via config/llm.php.</p>"},{"location":"polyglot/essentials/overview/#creating-an-inference-object","title":"Creating an Inference Object","text":"<p>For more control, you can create an instance of the <code>Inference</code> class:</p> <pre><code>// @doctest id=\"d919\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference();\n\n// Generate text using the default connection\n$answer = $inference-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']]\n)-&gt;get();\n\necho \"Answer: $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#specifying-a-connection","title":"Specifying a Connection","text":"<p>You can specify which connection preset to use:</p> <pre><code>// @doctest id=\"c8c9\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object with a specific connection\n$inference = new Inference();\n$answer = $inference-&gt;using('anthropic')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']]\n    )-&gt;get();\n\necho \"Answer (using Anthropic): $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#creating-chat-conversations","title":"Creating Chat Conversations","text":"<p>For multi-turn conversations, provide an array of messages:</p> <pre><code>// @doctest id=\"9166\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a chat conversation\n$messages = [\n    ['role' =&gt; 'user', 'content' =&gt; 'Hello, can you help me with a math problem?'],\n    ['role' =&gt; 'assistant', 'content' =&gt; 'Of course! I\\'d be happy to help with a math problem. What would you like to solve?'],\n    ['role' =&gt; 'user', 'content' =&gt; 'What is the square root of 144?'],\n];\n\n$inference = new Inference();\n$answer = $inference-&gt;with(\n    messages: $messages\n)-&gt;get();\n\necho \"Answer: $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#customizing-request-parameters","title":"Customizing Request Parameters","text":"<p>You can customize various parameters for your requests:</p> <pre><code>// @doctest id=\"5d60\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference with custom options\n$inference = new Inference();\n$answer = $inference-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'Write a short poem about coding.']],\n    model: 'gpt-4', // Override the default model\n    options: [\n        'temperature' =&gt; 0.7,\n        'max_tokens' =&gt; 100,\n    ]\n)-&gt;get();\n\necho \"Poem: $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#fluent-api","title":"Fluent API","text":"<p>Regular inference API allows you to customize inference options, letting you set values specific for a given LLM provider.</p> <p>Most of the provider options are compatible with OpenAI API.</p> <p>This example shows how to create an inference object, specify a connection and generate text using the <code>create()</code> method. The <code>toText()</code> method returns text completion from the LLM response.</p> <pre><code>// @doctest id=\"0aa2\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$answer = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#streaming-inference-results","title":"Streaming inference results","text":"<p>Inference API allows streaming responses, which is useful for building more responsive UX as you can display partial responses from LLM as soon as they arrive, without waiting until the whole response is ready.</p> <pre><code>// @doctest id=\"9f6d\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$stream = (new Inference)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'Describe capital of Brasil']])\n    -&gt;withOptions(['max_tokens' =&gt; 512])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: Describe capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#connecting-to-a-specific-llm-api-provider","title":"Connecting to a specific LLM API provider","text":"<p>Instructor allows you to define multiple API connections in <code>llm.php</code> file. This is useful when you want to use different LLMs or API providers in your application.</p> <p>Default configuration is located in <code>/config/llm.php</code> in the root directory of Instructor codebase. It contains a set of predefined connections to all LLM APIs supported out-of-the-box by Instructor.</p> <p>Config file defines connections to LLM APIs and their parameters. It also specifies the default connection to be used when calling Instructor without specifying the client connection.</p> <p><pre><code>// @doctest id=\"a2d0\"\n    // This is fragment of /config/llm.php file\n    'defaultPreset' =&gt; 'openai',\n    //...\n    'presets' =&gt; [\n        'anthropic' =&gt; [ ... ],\n        'azure' =&gt; [ ... ],\n        'cohere' =&gt; [ ... ],\n        'fireworks' =&gt; [ ... ],\n        'gemini' =&gt; [ ... ],\n        'xai' =&gt; [ ... ],\n        'groq' =&gt; [ ... ],\n        'mistral' =&gt; [ ... ],\n        'ollama' =&gt; [\n            'driver' =&gt; 'ollama',\n            'apiUrl' =&gt; 'http://localhost:11434/v1',\n            'apiKey' =&gt; Env::get('OLLAMA_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'qwen2.5:0.5b',\n            'maxTokens' =&gt; 1024,\n            'httpClientPreset' =&gt; 'guzzle-ollama', // use custom HTTP client configuration\n        ],\n        'openai' =&gt; [ ... ],\n        'openrouter' =&gt; [ ... ],\n        'together' =&gt; [ ... ],\n    // ...\n</code></pre> To customize the available connections you can either modify existing entries or add your own.</p> <p>Connecting to LLM API via predefined connection is as simple as calling <code>withPreset</code> method with the connection preset name.</p> <pre><code>// @doctest id=\"2df2\"\n&lt;?php\n// ...\n$answer = (new Inference)\n    -&gt;using('ollama') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n// ...\n</code></pre> <p>You can change the location of the configuration files for Instructor to use via <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable. You can use copies of the default configuration files as a starting point.</p>"},{"location":"polyglot/essentials/overview/#switching-between-providers","title":"Switching Between Providers","text":"<p>Polyglot makes it easy to switch between different LLM providers at runtime.</p>"},{"location":"polyglot/essentials/overview/#using-different-providers-for-llm-requests","title":"Using Different Providers for LLM Requests","text":"<pre><code>// @doctest id=\"1892\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference();\n\n// Use the default provider (set in config)\n$defaultResponse = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n\necho \"Default provider response: $defaultResponse\\n\\n\";\n\n// Switch to Anthropic\n$anthropicResponse = $inference-&gt;using('anthropic')\n    -&gt;with(\n        messages: 'What is the capital of Germany?'\n    )-&gt;get();\n\necho \"Anthropic response: $anthropicResponse\\n\\n\";\n\n// Switch to Mistral\n$mistralResponse = $inference-&gt;using('mistral')\n    -&gt;with(\n        messages: 'What is the capital of Italy?'\n    )-&gt;get();\n\necho \"Mistral response: $mistralResponse\\n\\n\";\n\n// You can create a new instance for each provider\n$openAI = new Inference('openai');\n$anthropic = new Inference('anthropic');\n$mistral = new Inference('mistral');\n\n// And use them independently\n$responses = [\n    'openai' =&gt; $openAI-&gt;with(messages: 'What is the capital of Spain?')-&gt;get(),\n    'anthropic' =&gt; $anthropic-&gt;with(messages: 'What is the capital of Portugal?')-&gt;get(),\n    'mistral' =&gt; $mistral-&gt;with(messages: 'What is the capital of Greece?')-&gt;get(),\n];\n\nforeach ($responses as $provider =&gt; $response) {\n    echo \"$provider response: $response\\n\";\n}\n</code></pre>"},{"location":"polyglot/essentials/overview/#selecting-different-models","title":"Selecting Different Models","text":"<p>Each provider offers multiple models with different capabilities, context lengths, and pricing. Polyglot lets you override the default model for each request.</p>"},{"location":"polyglot/essentials/overview/#specifying-models-for-llm-requests","title":"Specifying Models for LLM Requests","text":"<pre><code>// @doctest id=\"581a\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference('openai');\n\n// Use the default model (set in config)\n$defaultModelResponse = $inference-&gt;with(\n    messages: 'What is machine learning?'\n)-&gt;get();\n\n// Use a specific model\n$specificModelResponse = $inference-&gt;with(\n    messages: 'What is machine learning?',\n    model: 'gpt-4o'  // Override the default model\n)-&gt;get();\n\n// You can also set the model and other options\n$customResponse = $inference-&gt;with(\n    messages: 'What is machine learning?',\n    model: 'gpt-4-turbo',\n    options: [\n        'temperature' =&gt; 0.7,\n        'max_tokens' =&gt; 500,\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/request-options/","title":"Request Options","text":"<p>The <code>options</code> parameter allows you to customize various aspects of the request.</p> <p>NOTE: Except for <code>max_tokens</code>, all option parameters are provider-specific and may not be available or compatible with all providers. Check the provider's API documentation for details.</p>"},{"location":"polyglot/essentials/request-options/#common-options","title":"Common Options","text":"<pre><code>// @doctest id=\"cd77\"\n$options = [\n    // Generation parameters\n    'temperature' =&gt; 0.7,         // Controls randomness (0.0 to 1.0)\n    'max_tokens' =&gt; 1000,         // Maximum tokens to generate\n    'top_p' =&gt; 0.95,              // Nucleus sampling parameter\n    'frequency_penalty' =&gt; 0.0,   // Penalize repeated tokens\n    'presence_penalty' =&gt; 0.0,    // Penalize repeated topics\n    'stream' =&gt; false,            // Enable streaming responses\n    'stop' =&gt; [\"\\n\\n\", \"User:\"],  // Stop sequences\n\n    // Provider-specific options\n    'top_k' =&gt; 40,                // For some providers\n    'response_format' =&gt; [        // OpenAI-specific format control\n        'type' =&gt; 'json_object'\n    ],\n    // Additional provider-specific options...\n];\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short poem about programming.',\n    options: $options\n)-&gt;toText();\n</code></pre>"},{"location":"polyglot/essentials/request-options/#provider-specific-options","title":"Provider-Specific Options","text":"<p>Different providers may support additional options. Consult the provider's documentation for details:</p> <pre><code>// @doctest id=\"694f\"\n// Anthropic-specific options\n$anthropicOptions = [\n    'temperature' =&gt; 0.7,\n    'max_tokens' =&gt; 1000,\n    'top_p' =&gt; 0.9,\n    'stop_sequences' =&gt; [\"\\n\\nHuman:\"],\n    'stream' =&gt; true,\n];\n\n$inference = new Inference()-&gt;using('anthropic');\n$response = $inference-&gt;with(\n    messages: 'Write a short poem about programming.',\n    options: $anthropicOptions\n)-&gt;toText();\n</code></pre>"},{"location":"polyglot/essentials/response-handling/","title":"Response Handling","text":"<p>Polyglot's <code>PendingInference</code> class represents pending inference execution. It provides methods to access the response in different formats, but also provides access to streaming responses. It does not execute the request to underlying LLM until you actually access the response data.</p> <p>It is returned by the <code>Inference</code> class when you call the <code>create()</code> method.</p>"},{"location":"polyglot/essentials/response-handling/#basic-response-handling","title":"Basic Response Handling","text":"<pre><code>// @doctest id=\"dac7\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;create();\n\n// Get the response as plain text\n$text = $response-&gt;get();\necho \"Text response: $text\\n\";\n\n// Get the response as a JSON object (for JSON responses)\n$json = $response-&gt;asJsonData();\necho \"JSON response: \" . json_encode($json) . \"\\n\";\n\n// Get the full response object\n$fullResponse = $response-&gt;response();\n\n// Access specific information\necho \"Content: \" . $fullResponse-&gt;content() . \"\\n\";\necho \"Finish reason: \" . $fullResponse-&gt;finishReason() . \"\\n\";\necho \"Usage - Total tokens: \" . $fullResponse-&gt;usage()-&gt;total() . \"\\n\";\necho \"Usage - Input tokens: \" . $fullResponse-&gt;usage()-&gt;input() . \"\\n\";\necho \"Usage - Output tokens: \" . $fullResponse-&gt;usage()-&gt;output() . \"\\n\";\n</code></pre>"},{"location":"polyglot/essentials/response-handling/#working-with-streaming-responses","title":"Working with Streaming Responses","text":"<p>For streaming responses, use the <code>stream()</code> method:</p> <pre><code>// @doctest id=\"4938\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages('Write a short story about a robot.')\n    -&gt;withStreaming()\n    -&gt;create();\n\n// Get a generator that yields partial responses\n$stream = $response-&gt;stream()-&gt;responses();\n\necho \"Story: \";\nforeach ($stream as $partialResponse) {\n    // Output each chunk as it arrives\n    echo $partialResponse-&gt;contentDelta;\n\n    // Flush the output buffer to show progress in real-time\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n        flush();\n    }\n}\n\necho \"\\n\\nComplete response: \" . $response-&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/response-handling/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>For models that support function calling or tools:</p> <pre><code>// @doctest id=\"9540\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$tools = [\n    [\n        'type' =&gt; 'function',\n        'function' =&gt; [\n            'name' =&gt; 'get_weather',\n            'description' =&gt; 'Get the current weather in a location',\n            'parameters' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'location' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'The city and state, e.g. San Francisco, CA',\n                    ],\n                    'unit' =&gt; [\n                        'type' =&gt; 'string',\n                        'enum' =&gt; ['celsius', 'fahrenheit'],\n                        'description' =&gt; 'The temperature unit to use',\n                    ],\n                ],\n                'required' =&gt; ['location'],\n            ],\n        ],\n    ],\n];\n\n$inference = new Inference()-&gt;using('openai');\n$response = $inference-&gt;with(\n    messages: 'What is the weather in Paris?',\n    tools: $tools,\n    toolChoice: 'auto',  // Let the model decide when to use tools\n    mode: OutputMode::Tools    // Enable tools mode\n)-&gt;response();\n\n// Check if there are tool calls\nif ($response-&gt;hasToolCalls()) {\n    $toolCalls = $response-&gt;toolCalls();\n    foreach ($toolCalls-&gt;all() as $call) {\n        echo \"Tool called: \" . $call-&gt;name() . \"\\n\";\n        echo \"Arguments: \" . $call-&gt;argsAsJson() . \"\\n\";\n\n        // In a real application, you would call the actual function here\n        // and then send the result back to the model\n        $result = ['temperature' =&gt; 22, 'unit' =&gt; 'celsius', 'condition' =&gt; 'sunny'];\n\n        // Continue the conversation with the tool result\n        $newMessages = [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is the weather in Paris?'],\n            [\n                'role' =&gt; 'assistant',\n                'content' =&gt; '',\n                '_metadata' =&gt; [\n                    'tool_calls' =&gt; [\n                        [\n                            'id' =&gt; $call-&gt;id(),\n                            'function' =&gt; [\n                                'name' =&gt; $call-&gt;name(),\n                                'arguments' =&gt; $call-&gt;argsAsJson(),\n                            ],\n                        ],\n                    ],\n                ],\n            ],\n            [\n                'role' =&gt; 'tool',\n                'content' =&gt; json_encode($result),\n                '_metadata' =&gt; [\n                    'tool_call_id' =&gt; $call-&gt;id(),\n                    'tool_name' =&gt; $call-&gt;name(),\n                ],\n            ],\n        ];\n\n        $finalResponse = $inference-&gt;with(\n            messages: $newMessages\n        )-&gt;get();\n\n        echo \"Final response: $finalResponse\\n\";\n    }\n} else {\n    echo \"Response: \" . $response-&gt;content() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/","title":"Adapters","text":"<p>Each provider has a set of adapters that handle its specific format requirements:</p>"},{"location":"polyglot/internals/adapters/#request-adapters","title":"Request Adapters","text":"<p>Request adapters convert Polyglot's unified request format to provider-specific HTTP requests:</p> <pre><code>// @doctest id=\"b93f\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIRequestAdapter implements ProviderRequestAdapter {\n    public function __construct(\n        protected LLMConfig $config,\n        protected CanMapRequestBody $bodyFormat\n    ) { ... }\n\n    public function toHttpClientRequest(\n        array $messages,\n        string $model,\n        array $tools,\n        array|string $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): HttpRequest { ... }\n\n    protected function toHeaders(): array { ... }\n    protected function toUrl(string $model = '', bool $stream = false): string { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#message-formatters","title":"Message Formatters","text":"<p>Message formatters handle the conversion of messages to provider-specific formats:</p> <pre><code>// @doctest id=\"3e16\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIMessageFormat implements CanMapMessages {\n    public function map(array $messages): array { ... }\n\n    protected function mapMessage(array $message): array { ... }\n    protected function toNativeToolCall(array $message): array { ... }\n    protected function toNativeToolResult(array $message): array { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#body-formatters","title":"Body Formatters","text":"<p>Body formatters handle the conversion of request bodies to provider-specific formats:</p> <pre><code>// @doctest id=\"3b98\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIBodyFormat implements CanMapRequestBody {\n    public function __construct(\n        protected LLMConfig $config,\n        protected CanMapMessages $messageFormat\n    ) { ... }\n\n    public function map(\n        array $messages,\n        string $model,\n        array $tools,\n        array|string $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): array { ... }\n\n    private function applyMode(\n        array $request,\n        Mode $mode,\n        array $tools,\n        string|array $toolChoice,\n        array $responseFormat\n    ): array { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#response-adapters","title":"Response Adapters","text":"<p>Response adapters convert provider-specific responses to Polyglot's unified format:</p> <pre><code>// @doctest id=\"27e0\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIResponseAdapter implements ProviderResponseAdapter {\n    public function __construct(\n        protected CanMapUsage $usageFormat\n    ) { ... }\n\n    public function fromResponse(HttpResponse $response): ?InferenceResponse { ... }\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse { ... }\n    public function toEventBody(string $data): string|bool { ... }\n\n    protected function makeToolCalls(array $data): ToolCalls { ... }\n    protected function makeContent(array $data): string { ... }\n    protected function makeContentDelta(array $data): string { ... }\n    protected function makeToolId(array $data): string { ... }\n    protected function makeToolNameDelta(array $data): string { ... }\n    protected function makeToolArgsDelta(array $data): string { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#usage-formatters","title":"Usage Formatters","text":"<p>Usage formatters extract token usage information from provider responses:</p> <pre><code>// @doctest id=\"531c\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIUsageFormat implements CanMapUsage {\n    public function fromData(array $data): Usage { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/configuration/","title":"Configuration Layer","text":"<p>Polyglot's configuration layer manages settings for different providers.</p>"},{"location":"polyglot/internals/configuration/#llm-configuration","title":"LLM Configuration","text":"<pre><code>// @doctest id=\"0600\"\nnamespace Cognesy\\Polyglot\\Inference\\Data;\n\nclass LLMConfig {\n    public function __construct(\n        public string $apiUrl = '',\n        public string $apiKey = '',\n        public string $endpoint = '',\n        public array $queryParams = [],\n        public array $metadata = [],\n        public string $model = '',\n        public int $maxTokens = 1024,\n        public int $contextLength = 8000,\n        public int $maxOutputLength = 4096,\n        public string $httpClient = '',\n        public string $providerType = 'openai-compatible'\n    ) { ... }\n\n    public static function load(string $connection): \\Cognesy\\Polyglot\\Inference\\Config\\LLMConfig { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/configuration/#embeddings-configuration","title":"Embeddings Configuration","text":"<pre><code>// @doctest id=\"a952\"\nnamespace Cognesy\\Polyglot\\Embeddings\\Data;\n\nclass EmbeddingsConfig {\n    public function __construct(\n        public string $apiUrl = '',\n        public string $apiKey = '',\n        public string $endpoint = '',\n        public string $model = '',\n        public int $dimensions = 0,\n        public int $maxInputs = 0,\n        public array $metadata = [],\n        public string $httpClient = '',\n        public string $providerType = 'openai'\n    ) { ... }\n\n    public static function load(string $connection): \\Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/events/","title":"Events System","text":"<p>Polyglot uses an event system to generate internal notifications at the various stages of the execution process.</p> <p>It has been built primarily to ensure observability of the internal components of the library.</p> <pre><code>// @doctest id=\"0be9\"\nnamespace Cognesy\\Utils\\Events;\n\nuse Cognesy\\Events\\Event;\n\nclass EventDispatcher {\n    public function dispatch(Event $event): void { ... }\n    public function wiretap(callable $listener): self { ... }\n    public function addListener(string $eventClass, callable $listener): self { ... }\n}\n\nnamespace Cognesy\\Polyglot\\Inference\\Events;\n\nclass InferenceResponseReceived extends Event {}\n\nclass InferenceRequested extends Event {}\n\nclass PartialInferenceResponseReceived extends Event {}\n</code></pre>"},{"location":"polyglot/internals/http-client/","title":"HTTP Client Layer","text":"<p>At the lowest level, Polyglot uses an HTTP client layer to communicate with provider APIs. This layer includes:</p> <ol> <li>A unified <code>HttpClient</code> interface</li> <li>Implementations for different HTTP libraries (Guzzle, Symfony, Laravel)</li> <li>A middleware system for extending functionality</li> </ol>"},{"location":"polyglot/internals/http-client/#httpclient","title":"HttpClient","text":"<p>The <code>HttpClient</code> class provides a unified interface for HTTP requests:</p> <pre><code>// @doctest id=\"3928\"\nnamespace Cognesy\\Http;\n\nclass HttpClient implements CanHandleHttpRequest {\n    public function __construct(\n        string $client = '',\n        ?HttpClientConfig $config = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    public static function make(\n        string $client = '',\n        ?HttpClientConfig $config = null,\n        ?EventDispatcher $events = null\n    ): self { ... }\n\n    public function withClient(string $client): self { ... }\n    public function withConfig(HttpClientConfig $config): self { ... }\n    public function withMiddleware(...$middleware): self { ... }\n    public function withDebugPreset(?string $preset): self { ... }\n\n    public function handle(HttpClientRequest $request): HttpResponse { ... }\n    public function middleware(): MiddlewareStack { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/http-client/#httprequest-and-httpresponse","title":"HttpRequest and HttpResponse","text":"<p>These classes represent HTTP requests and responses:</p> <pre><code>// @doctest id=\"5dc1\"\nnamespace Cognesy\\Http\\Data;\n\nclass HttpRequest {\n    public function __construct(\n        private string $url,\n        private string $method,\n        private array $headers,\n        private mixed $body,\n        private array $options\n    ) { ... }\n\n    public function url(): string { ... }\n    public function method(): string { ... }\n    public function headers(): array { ... }\n    public function body(): HttpRequestBody { ... }\n    public function options(): array { ... }\n    public function isStreamed(): bool { ... }\n\n    public function withStreaming(bool $streaming): self { ... }\n}\n\ninterface HttpResponse {\n    public function statusCode(): int;\n    public function headers(): array;\n    public function body(): string;\n    public function stream(int $chunkSize = 1): Generator;\n    public function original(): mixed;\n}\n</code></pre>"},{"location":"polyglot/internals/http-client/#middleware-system","title":"Middleware System","text":"<p>The HTTP client layer includes a middleware system that allows extending functionality:</p> <pre><code>// @doctest id=\"3e97\"\nnamespace Cognesy\\Http;\n\ninterface HttpMiddleware {\n    public function handle(\n        HttpRequest $request,\n        CanHandleHttpRequest $next\n    ): HttpResponse;\n}\n\nabstract class BaseMiddleware implements HttpMiddleware {\n    public function handle(\n        HttpRequest $request,\n        CanHandleHttpRequest $next\n    ): HttpResponse { ... }\n\n    protected function beforeRequest(HttpClientRequest $request): void {}\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return $response;\n    }\n\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        return false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return $response;\n    }\n}\n\nclass MiddlewareStack {\n    public function append(HttpMiddleware $middleware, string $name = ''): self { ... }\n    public function prepend(HttpMiddleware $middleware, string $name = ''): self { ... }\n    public function remove(string $name): self { ... }\n    public function replace(string $name, HttpMiddleware $middleware): self { ... }\n    public function clear(): self { ... }\n    public function has(string $name): bool { ... }\n    public function get(string|int $nameOrIndex): ?HttpMiddleware { ... }\n    public function all(): array { ... }\n    public function process(\n        HttpRequest $request,\n        CanHandleHttpRequest $handler\n    ): HttpResponse { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/lifecycle/","title":"Request/Response Lifecycle","text":"<p>Let's follow the complete flow of a request through Polyglot:</p>"},{"location":"polyglot/internals/lifecycle/#request-processing","title":"Request Processing","text":"<ol> <li>Application creates an <code>Inference</code> object</li> <li>Application calls <code>create()</code> with parameters</li> <li><code>Inference</code> creates an <code>InferenceRequest</code>.</li> <li><code>Inference</code> creates a <code>PendingInference</code> object with the instances of request, driver and event dispatcher.</li> <li><code>Inference</code> returns a <code>PendingInference</code> object to the application.</li> </ol>"},{"location":"polyglot/internals/lifecycle/#response-processing","title":"Response Processing","text":"<ol> <li>Application accesses the <code>PendingInference</code> object content, e.g. via <code>response()</code> method.</li> <li><code>PendingInference</code> checks if HTTP request has been already executed.</li> <li>If already sent, it returns the cached response.</li> <li><code>PendingInference</code> dispatches the <code>InferenceRequested</code> event</li> <li><code>PendingInference</code> passes the request to the driver.</li> <li>Driver uses request adapter to create HTTP request</li> <li>Request adapter uses request body formatter and message formatter.</li> <li>Driver sends the HTTP request and returns it to <code>PendingInference</code>.</li> <li><code>PendingInference</code> calls the driver to read and parse the response.</li> <li>Driver uses a response adapter to extract content into appropriate fields of <code>InferenceResponse</code> object</li> <li><code>PendingInference</code> dispatches the <code>InferenceResponseReceived</code> event</li> <li>Result <code>InferenceResponse</code> object is returned to the application</li> </ol>"},{"location":"polyglot/internals/overview/","title":"Overview of Architecture","text":"<p>This section provides a detailed look at Polyglot's internal architecture.</p> <p>Understanding the core components, interfaces, and design patterns will help you extend the library, contribute to its development, or build your own integrations with new LLM providers.</p>"},{"location":"polyglot/internals/overview/#core-architecture","title":"Core Architecture","text":"<p>Polyglot is built on a modular, layered architecture that separates concerns and promotes extensibility. The high-level architecture consists of:</p> <ol> <li>Public API Layer: Classes like <code>Inference</code> and <code>Embeddings</code> that provide a unified interface for applications</li> <li>Provider Abstraction Layer: Adapters, drivers, and formatters that translate between the unified API and provider-specific formats</li> <li>HTTP Client Layer: A flexible HTTP client with middleware support for communicating with LLM APIs</li> <li>Configuration Layer: Configuration management for different providers and models</li> </ol> <pre><code>// @doctest id=\"520f\"\n+---------------------+    +---------------------+\n|      Inference      |    |     Embeddings      |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|  InferenceRequest   |    | EmbeddingsRequest   |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|   PendingInference  |    |  PendingEmbeddings  |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|  InferenceDrivers   |    |  EmbeddingsDrivers  |\n+---------------------+    +---------------------+\n            |                        |\n+------------------------------------------------+\n|               HTTP Client Layer                |\n+------------------------------------------------+\n                         |\n+------------------------------------------------+\n|           Provider-specific API Calls          |\n+------------------------------------------------+\n</code></pre>"},{"location":"polyglot/internals/providers/","title":"Provider Abstraction Layer","text":"<p>The provider abstraction layer is where Polyglot handles the differences between LLM and embedding providers. This layer includes:</p> <ol> <li>Provider Classes: <code>LLMProvider</code> and <code>EmbeddingsProvider</code> - Builder classes for configuring and creating drivers</li> <li>Drivers: Classes that implement provider-specific logic for inference and embeddings</li> <li>Adapters: Classes that convert between unified and provider-specific formats</li> <li>Factories: Classes that create appropriate drivers based on configuration</li> </ol>"},{"location":"polyglot/internals/providers/#provider-builder-classes","title":"Provider Builder Classes","text":""},{"location":"polyglot/internals/providers/#llmprovider","title":"LLMProvider","text":"<p>The <code>LLMProvider</code> class is a builder that configures and creates inference drivers. It provides a fluent interface for setting up LLM configurations:</p> <pre><code>// @doctest id=\"52b2\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n// Create with preset\n$provider = LLMProvider::using('openai');\n\n// Create with DSN\n$provider = LLMProvider::dsn('openai://model=gpt-4&amp;temperature=0.7');\n\n// Fluent configuration\n$provider = LLMProvider::new()\n    -&gt;withLLMPreset('openai')\n    -&gt;withConfig($customConfig)\n    -&gt;withHttpClient($httpClient)\n    -&gt;withDebugPreset('verbose');\n\n// Create the final driver\n$driver = $provider-&gt;createDriver();\n</code></pre> <p>Key methods: - <code>withLLMPreset(string $preset)</code>: Set configuration preset - <code>withConfig(LLMConfig $config)</code>: Set explicit configuration - <code>withConfigOverrides(array $overrides)</code>: Override specific config values - <code>withDsn(string $dsn)</code>: Configure via DSN string - <code>withHttpClient(HttpClient $client)</code>: Set custom HTTP client - <code>withDriver(CanHandleInference $driver)</code>: Set explicit driver - <code>createDriver()</code>: Build and return the configured driver</p>"},{"location":"polyglot/internals/providers/#embeddingsprovider","title":"EmbeddingsProvider","text":"<p>The <code>EmbeddingsProvider</code> class builds and configures embeddings drivers:</p> <pre><code>// @doctest id=\"2f19\"\n&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\n\n// Create with preset\n$provider = EmbeddingsProvider::using('openai');\n\n// Create with DSN\n$provider = EmbeddingsProvider::dsn('openai://model=text-embedding-3-large');\n\n// Fluent configuration\n$provider = EmbeddingsProvider::new()\n    -&gt;withPreset('openai')\n    -&gt;withConfig($customConfig)\n    -&gt;withHttpClient($httpClient)\n    -&gt;withDebugPreset('verbose');\n\n// Create the final driver\n$driver = $provider-&gt;createDriver();\n</code></pre> <p>Key methods: - <code>withPreset(string $preset)</code>: Set configuration preset - <code>withConfig(EmbeddingsConfig $config)</code>: Set explicit configuration - <code>withDsn(string $dsn)</code>: Configure via DSN string - <code>withHttpClient(HttpClient $client)</code>: Set custom HTTP client - <code>withDriver(CanHandleVectorization $driver)</code>: Set explicit driver - <code>createDriver()</code>: Build and return the configured driver</p>"},{"location":"polyglot/internals/providers/#key-interfaces-for-llm","title":"Key Interfaces for LLM","text":"<p>Several interfaces define the contract for LLM drivers and adapters:</p> <pre><code>// @doctest id=\"3eec\"\nnamespace Cognesy\\Polyglot\\Inference\\Contracts;\n\ninterface CanHandleInference {\n    public function handle(InferenceRequest $request): HttpResponse;\n    public function fromResponse(HttpResponse $response): ?InferenceResponse;\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse;\n    public function toEventBody(string $data): string|bool;\n}\n\ninterface ProviderRequestAdapter {\n    public function toHttpClientRequest(\n        array $messages,\n        string $model,\n        array $tools,\n        string|array $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): HttpRequest;\n}\n\ninterface ProviderResponseAdapter {\n    public function fromResponse(HttpResponse $response): ?InferenceResponse;\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse;\n    public function toEventBody(string $data): string|bool;\n}\n\ninterface CanMapMessages {\n    public function map(array $messages): array;\n}\n\ninterface CanMapRequestBody {\n    public function map(\n        array $messages,\n        string $model,\n        array $tools,\n        array|string $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): array;\n}\n\ninterface CanMapUsage {\n    public function fromData(array $data): Usage;\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#key-interfaces-for-embeddings","title":"Key Interfaces for Embeddings","text":"<p>The embeddings functionality uses these key interfaces:</p> <pre><code>// @doctest id=\"49cd\"\nnamespace Cognesy\\Polyglot\\Embeddings\\Contracts;\n\n// Main driver interface\ninterface CanHandleVectorization {\n    public function vectorize(EmbeddingsRequest $request): EmbeddingsResponse;\n}\n\n// Request and response mapping interfaces\ninterface CanMapRequestBody {\n    public function map(EmbeddingsRequest $request): array;\n}\n\ninterface EmbedRequestAdapter {\n    public function toHttpRequest(EmbeddingsRequest $request): HttpRequest;\n}\n\ninterface EmbedResponseAdapter {\n    public function fromHttpResponse(HttpResponse $response): EmbeddingsResponse;\n}\n\ninterface CanMapUsage {\n    public function fromData(array $data): Usage;\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#modularllmdriver","title":"ModularLLMDriver","text":"<p>The <code>ModularLLMDriver</code> is a central component that implements the <code>CanHandleInference</code> interface using adapters:</p> <pre><code>// @doctest id=\"6dcc\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers;\n\nclass ModularLLMDriver implements CanHandleInference {\n    public function __construct(\n        protected LLMConfig $config,\n        protected ProviderRequestAdapter $requestAdapter,\n        protected ProviderResponseAdapter $responseAdapter,\n        protected ?CanHandleHttpRequest $httpClient = null,\n        protected ?EventDispatcher $events = null\n    ) { ... }\n\n    public function handle(InferenceRequest $request): HttpResponse { ... }\n    public function fromResponse(HttpResponse $response): ?InferenceResponse { ... }\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse { ... }\n    public function toEventBody(string $data): string|bool { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#driver-factories","title":"Driver Factories","text":""},{"location":"polyglot/internals/providers/#inferencedriverfactory","title":"InferenceDriverFactory","text":"<p>The <code>InferenceDriverFactory</code> creates the appropriate driver for each LLM provider:</p> <pre><code>// @doctest id=\"6960\"\nnamespace Cognesy\\Polyglot\\Inference\\Drivers;\n\nclass InferenceDriverFactory {\n    public function makeDriver(\n        LLMConfig $config,\n        HttpClient $httpClient\n    ): CanHandleInference { ... }\n\n    // Provider-specific factory methods\n    public function openAI(...): CanHandleInference { ... }\n    public function anthropic(...): CanHandleInference { ... }\n    public function mistral(...): CanHandleInference { ... }\n    // Other providers...\n\n    // Driver registration\n    public static function registerDriver(string $name, string|callable $driver): void { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#embeddingsdriverfactory","title":"EmbeddingsDriverFactory","text":"<p>The <code>EmbeddingsDriverFactory</code> creates embeddings drivers:</p> <pre><code>// @doctest id=\"881c\"\nnamespace Cognesy\\Polyglot\\Embeddings\\Drivers;\n\nclass EmbeddingsDriverFactory {\n    public function makeDriver(\n        EmbeddingsConfig $config,\n        HttpClient $httpClient\n    ): CanHandleVectorization { ... }\n\n    // Provider-specific factory methods  \n    public function openAI(...): CanHandleVectorization { ... }\n    public function cohere(...): CanHandleVectorization { ... }\n    public function gemini(...): CanHandleVectorization { ... }\n    // Other providers...\n\n    // Driver registration\n    public static function registerDriver(string $name, string|callable $driver): void { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/public-api/","title":"Public API Layer","text":""},{"location":"polyglot/internals/public-api/#the-inference-class","title":"The Inference Class","text":"<p>The <code>Inference</code> class is the main entry point for LLM interactions. It encapsulates the complexities of different providers behind a unified interface.</p> <pre><code>// @doctest id=\"dc09\"\nnamespace Cognesy\\Polyglot\\LLM;\n\nclass Inference {\n    // Create and manage the LLM instance\n    public function __construct(\n        string $connection = '',\n        ?LLMConfig $config = null,\n        ?CanHandleHttpRequest $httpClient = null,\n        ?CanHandleInference $driver = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    // Configure the instance\n    public function using(string $preset): self { ... }\n    public function withConfig(LLMConfig $config): self { ... }\n    public function withHttpClient(CanHandleHttpRequest $httpClient): self { ... }\n    public function withDriver(CanHandleInference $driver): self { ... }\n    public function withDebugPreset(?string $preset): self { ... }\n    public function withCachedContext(...): self { ... }\n\n    // Main method for creating inference requests\n    public function create(\n        string|array $messages = [],\n        string $model = '',\n        array $tools = [],\n        string|array $toolChoice = [],\n        array $responseFormat = [],\n        array $options = [],\n        Mode $mode = OutputMode::Text\n    ): PendingInference { ... }\n\n    // Static convenience method for simple text generation\n    public static function text(\n        string|array $messages,\n        string $connection = '',\n        string $model = '',\n        array $options = []\n    ): string { ... }\n}\n</code></pre> <p>The <code>Inference</code> class follows a fluent interface pattern, allowing method chaining for configuration.</p>"},{"location":"polyglot/internals/public-api/#the-embeddings-class","title":"The Embeddings Class","text":"<p>Similarly, the <code>Embeddings</code> class provides a unified interface for generating embeddings:</p> <pre><code>// @doctest id=\"92e3\"\nnamespace Cognesy\\Polyglot\\Embeddings;\n\nuse Cognesy\\Polyglot\\Embeddings\\Data\\EmbeddingsResponse;class Embeddings {\n    public function __construct(\n        string $connection = '',\n        ?EmbeddingsConfig $config = null,\n        ?CanHandleHttpRequest $httpClient = null,\n        ?CanVectorize $driver = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    // Configuration methods\n    public function using(string $preset): self { ... }\n    public function withConfig(EmbeddingsConfig $config): self { ... }\n    public function withModel(string $model): self { ... }\n    public function withHttpClient(CanHandleHttpRequest $httpClient): self { ... }\n    public function withDriver(CanVectorize $driver): self { ... }\n\n    // Main method for generating embeddings\n    public function create(string|array $input, array $options = []): EmbeddingsResponse { ... }\n\n    // Utility methods for finding similar content\n    public function findSimilar(string $query, array $documents, int $topK = 5): array { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/request-response/","title":"Request and Response Objects","text":""},{"location":"polyglot/internals/request-response/#inferencerequest","title":"InferenceRequest","text":"<p>The <code>InferenceRequest</code> class encapsulates all the parameters needed for an LLM request:</p> <pre><code>// @doctest id=\"74a2\"\nnamespace Cognesy\\Polyglot\\LLM;\n\nclass InferenceRequest {\n    public array $messages = [];\n    public string $model = '';\n    public array $tools = [];\n    public string|array $toolChoice = [];\n    public array $responseFormat = [];\n    public array $options = [];\n    public Mode $mode = OutputMode::Text;\n    public ?CachedContext $cachedContext;\n\n    public function __construct(...) { ... }\n\n    // Getters\n    public function messages(): array { ... }\n    public function model(): string { ... }\n    public function isStreamed(): bool { ... }\n    public function tools(): array { ... }\n    public function toolChoice(): string|array { ... }\n    public function responseFormat(): array { ... }\n    public function options(): array { ... }\n    public function mode(): Mode { ... }\n    public function cachedContext(): ?CachedContext { ... }\n\n    // Fluent setters\n    public function withMessages(string|array $messages): self { ... }\n    public function withModel(string $model): self { ... }\n    public function withStreaming(bool $streaming): self { ... }\n    public function withTools(array $tools): self { ... }\n    public function withToolChoice(string|array $toolChoice): self { ... }\n    public function withResponseFormat(array $responseFormat): self { ... }\n    public function withOptions(array $options): self { ... }\n    public function withMode(Mode $mode): self { ... }\n    public function withCachedContext(?CachedContext $cachedContext): self { ... }\n\n    // Utility methods\n    public function toArray(): array { ... }\n    public function withCacheApplied(): self { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/request-response/#pendinginference","title":"PendingInference","text":"<p>The <code>PendingInference</code> class handles the response from an LLM API:</p> <pre><code>// @doctest id=\"e853\"\nnamespace Cognesy\\Polyglot\\LLM;\n\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;class PendingInference {\n    public function __construct(\n        InferenceRequest $request,\n        CanHandleInference $driver,\n        EventDispatcherInterface $events,\n    ) { ... }\n\n    // Access methods\n    public function isStreamed(): bool { ... }\n    public function toText(): string { ... }\n    public function toArray(): array { ... }\n    public function stream(): InferenceStream { ... }\n    public function response(): InferenceResponse { ... }\n}\n</code></pre> <p>For streaming responses, the <code>InferenceStream</code> class provides methods to process the stream:</p> <pre><code>// @doctest id=\"b42a\"\nnamespace Cognesy\\Polyglot\\LLM;\n\nclass InferenceStream {\n    public function __construct(\n        HttpResponse        $httpResponse,\n        CanHandleInference        $driver,\n        EventDispatcherInterface  $events,\n    ) { ... }\n\n    // Stream processing methods\n    public function responses(): Generator { ... }\n    public function all(): array { ... }\n    public function final(): ?InferenceResponse { ... }\n    public function onPartialResponse(callable $callback): self { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/request-response/#embeddingsresponse","title":"EmbeddingsResponse","text":"<p>The <code>EmbeddingsResponse</code> class encapsulates the result of an embeddings request:</p> <pre><code>// @doctest id=\"feba\"\nnamespace Cognesy\\Polyglot\\Embeddings;\n\nclass EmbeddingsResponse {\n    public function __construct(\n        public array $vectors,\n        public ?Usage $usage\n    ) { ... }\n\n    // Access methods\n    public function first(): Vector { ... }\n    public function last(): Vector { ... }\n    public function all(): array { ... }\n    public function usage(): Usage { ... }\n    public function toValuesArray(): array { ... }\n    public function totalTokens(): int { ... }\n    public function split(int $index): array { ... }\n}\n</code></pre>"},{"location":"polyglot/modes/json-schema/","title":"JSON Schema Mode","text":"<p>JSON Schema mode takes JSON generation a step further by validating the response against a predefined schema. This ensures the response has exactly the structure your application expects.</p>"},{"location":"polyglot/modes/json-schema/#defining-and-using-a-json-schema","title":"Defining and Using a JSON Schema","text":"<pre><code>// @doctest id=\"d7eb\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');  // Currently best supported by OpenAI\n\n// Define a schema for a weather report\n$schema = [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'location' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'The city and country'\n        ],\n        'current_temperature' =&gt; [\n            'type' =&gt; 'number',\n            'description' =&gt; 'Current temperature in Celsius'\n        ],\n        'conditions' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'Current weather conditions (e.g., sunny, rainy)'\n        ],\n        'forecast' =&gt; [\n            'type' =&gt; 'array',\n            'items' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'day' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'Day of the week'\n                    ],\n                    'temperature_high' =&gt; [\n                        'type' =&gt; 'number',\n                        'description' =&gt; 'Expected high temperature in Celsius'\n                    ],\n                    'temperature_low' =&gt; [\n                        'type' =&gt; 'number',\n                        'description' =&gt; 'Expected low temperature in Celsius'\n                    ],\n                    'conditions' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'Expected weather conditions'\n                    ]\n                ],\n                'required' =&gt; ['day', 'temperature_high', 'temperature_low', 'conditions']\n            ],\n            'description' =&gt; 'Three-day weather forecast'\n        ]\n    ],\n    'required' =&gt; ['location', 'current_temperature', 'conditions', 'forecast']\n];\n\n// Request a weather report\n$response = $inference-&gt;with(\n    messages: 'Provide a weather report for Paris, France.',\n    responseFormat: [\n        'type' =&gt; 'json_schema',\n        'json_schema' =&gt; [\n            'name' =&gt; 'weather_report',\n            'schema' =&gt; $schema,\n            'strict' =&gt; true,\n        ],\n    ],\n    mode: OutputMode::JsonSchema\n)-&gt;asJsonData();\n\n// The response will match the schema's structure exactly\necho \"Weather in {$response['location']}:\\n\";\necho \"Currently {$response['conditions']} and {$response['current_temperature']}\u00b0C\\n\\n\";\n\necho \"Forecast:\\n\";\nforeach ($response['forecast'] as $day) {\n    echo \"{$day['day']}: {$day['conditions']}, {$day['temperature_low']}\u00b0C to {$day['temperature_high']}\u00b0C\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/json-schema/#schema-validation","title":"Schema Validation","text":"<p>With JSON Schema mode, Polyglot ensures the LLM's response adheres to your schema:</p> <ol> <li>The schema is sent to the model as part of the request</li> <li>The model structures its response to match the schema</li> <li>For providers with native schema support, validation happens at the API level</li> <li>For other providers, Polyglot helps guide the model to produce correctly formatted output</li> </ol>"},{"location":"polyglot/modes/json-schema/#provider-support-for-json-schema","title":"Provider Support for JSON Schema","text":"<p>Provider support for JSON Schema varies:</p> <ul> <li>OpenAI (GPT-4 and newer): Native support with <code>json_schema</code> response format</li> <li>Anthropic (Claude 3 and newer): Partial support via prompt engineering</li> <li>Other providers: May require more explicit instructions in the prompt</li> </ul> <p>For best compatibility, use OpenAI for schema-validated responses.</p>"},{"location":"polyglot/modes/json-schema/#when-to-use-json-schema-mode","title":"When to Use JSON Schema Mode","text":"<p>JSON Schema mode is ideal for: - Applications requiring strictly typed data - Integration with databases or APIs that expect specific structures - Data extraction with complex nested structures - Ensuring consistent response formats across multiple requests</p>"},{"location":"polyglot/modes/json/","title":"JSON Mode","text":"<p>JSON mode instructs the model to return responses formatted as valid JSON objects. This is useful when you need structured data that can be easily processed by your application.</p>"},{"location":"polyglot/modes/json/#basic-json-generation","title":"Basic JSON Generation","text":"<pre><code>// @doctest id=\"3147\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n$response = $inference-&gt;with(\n    messages: 'List the top 3 most populous cities in the world with their populations.',\n    mode: OutputMode::Json\n)-&gt;asJsonData();\n\n// $response is now a PHP array parsed from the JSON\necho \"Top cities:\\n\";\nforeach ($response['cities'] as $city) {\n    echo \"- {$city['name']}: {$city['population']} million\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/json/#structuring-json-responses-with-instructions","title":"Structuring JSON Responses with Instructions","text":"<p>For best results, include clear instructions about the expected JSON structure:</p> <pre><code>// @doctest id=\"4830\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// Include the expected structure in the prompt\n$prompt = &lt;&lt;&lt;EOT\nList the top 3 most populous cities in the world.\nReturn your answer as a JSON object with the following structure:\n{\n  \"cities\": [\n    {\n      \"name\": \"City name\",\n      \"country\": \"Country name\",\n      \"population\": population in millions (number)\n    },\n    ...\n  ]\n}\nEOT;\n\n$response = $inference-&gt;with(\n    messages: $prompt,\n    mode: OutputMode::Json\n)-&gt;asJsonData();\n\n// Process the response\necho \"Top cities by population:\\n\";\nforeach ($response['cities'] as $index =&gt; $city) {\n    echo ($index + 1) . \". {$city['name']}, {$city['country']}: {$city['population']} million\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/json/#provider-specific-json-options","title":"Provider-Specific JSON Options","text":"<p>Some providers offer additional options for JSON mode:</p> <pre><code>// @doctest id=\"dcd6\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// OpenAI example\n$inference = new Inference()-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'List the top 3 most populous cities in the world.',\n    mode: OutputMode::Json,\n    options: [\n        'response_format' =&gt; ['type' =&gt; 'json_object'],\n        // Other OpenAI-specific options...\n    ]\n)-&gt;asJsonData();\n\n// The response will be a JSON object\n</code></pre>"},{"location":"polyglot/modes/json/#when-to-use-json-mode","title":"When to Use JSON Mode","text":"<p>JSON mode is ideal for: - Extracting structured data (lists, records, etc.) - API responses that need to be machine-readable - Generating data for web applications - Creating datasets</p>"},{"location":"polyglot/modes/md-json/","title":"MdJSON Mode","text":"<p>Markdown JSON mode is a special mode that requests the model to format its response as JSON within a Markdown code block. This is particularly useful for models or providers that don't have native JSON output support.</p>"},{"location":"polyglot/modes/md-json/#using-markdown-json-mode","title":"Using Markdown JSON Mode","text":"<pre><code>// @doctest id=\"f497\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// This works with virtually any provider\n$response = $inference-&gt;with(\n    messages: 'List three programming languages and their key features.',\n    mode: OutputMode::MdJson\n)-&gt;asJsonData();\n\n// The model will return JSON wrapped in Markdown, which Polyglot processes for you\nforeach ($response['languages'] as $language) {\n    echo \"{$language['name']} - {$language['paradigm']}\\n\";\n    echo \"Key features: \" . implode(', ', $language['key_features']) . \"\\n\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/md-json/#how-mdjson-mode-works","title":"How MdJson Mode Works","text":"<ol> <li>Polyglot instructs the model to respond with a JSON object wrapped in a Markdown code block</li> <li>The model formats its response accordingly (<code>json {...}</code>)</li> <li>Polyglot extracts the JSON content from the Markdown code block</li> <li>The JSON is parsed and returned to your application</li> </ol>"},{"location":"polyglot/modes/md-json/#providing-guidance-for-mdjson","title":"Providing Guidance for MdJson","text":"<p>While MdJson is more flexible across providers, you still need to provide clear instructions:</p> <pre><code>// @doctest id=\"3f50\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// Include expected format in the prompt\n$prompt = &lt;&lt;&lt;EOT\nList three programming languages with their key features.\nRespond with a JSON object following this structure:\n{\n  \"languages\": [\n    {\n      \"name\": \"Language name\",\n      \"paradigm\": \"Programming paradigm\",\n      \"year_created\": year as number,\n      \"key_features\": [\"feature1\", \"feature2\", \"feature3\"]\n    },\n  ]\n}\nEOT;\n\n$response = $inference-&gt;with(\nmessages: $prompt,\nmode: OutputMode::MdJson\n)-&gt;toJson();\n\n// Process as normal JSON\n</code></pre>"},{"location":"polyglot/modes/md-json/#when-to-use-mdjson-mode","title":"When to Use MdJson Mode","text":"<p>MdJson mode is ideal for: - Working with providers that don't have native JSON output - Ensuring portability across different providers - Getting structured responses from older model versions - Fallback option when JSON Schema mode isn't available</p>"},{"location":"polyglot/modes/overview/","title":"Overview of Output Modes","text":"<p>One of Polyglot's key strengths is its ability to support various output formats from LLM providers. This flexibility allows you to structure responses in the format that best suits your application, whether you need plain text, structured JSON data, or function/tool calls. This chapter explores the different output modes supported by Polyglot and how to implement them effectively.</p> <p>Polyglot's support for different output formats gives you the flexibility to work with LLM responses in the way that best suits your application's needs. Whether you need simple text, structured JSON, or interactive tool calls, you can configure the output format to match your requirements.</p>"},{"location":"polyglot/modes/overview/#understanding-output-modes","title":"Understanding Output Modes","text":"<p>Polyglot supports multiple output modes through the <code>Mode</code> enum:</p> <pre><code>// @doctest id=\"4c9c\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// Available modes\n// OutputMode::Text       - Plain text output (default)\n// OutputMode::Json       - JSON output\n// OutputMode::JsonSchema - JSON output validated against a schema\n// OutputMode::MdJson     - JSON wrapped in Markdown code blocks\n// OutputMode::Tools      - Function/tool calling\n</code></pre> <p>Each mode influences: 1. How the request is formatted and sent to the provider 2. How the provider's response is processed 3. What extraction or validation is applied to the response</p>"},{"location":"polyglot/modes/overview/#output-modes-overview","title":"Output Modes Overview","text":"Mode Description Best For <code>OutputMode::Text</code> Default mode, returns unstructured text Simple text generation <code>OutputMode::Json</code> Returns structured JSON data Structured data processing <code>OutputMode::JsonSchema</code> Returns JSON data validated against a schema Strictly typed data <code>OutputMode::MdJson</code> Returns JSON wrapped in Markdown code blocks Compatibility across providers <code>OutputMode::Tools</code> Returns function/tool calls Function calling/external actions"},{"location":"polyglot/modes/overview/#choosing-the-right-format","title":"Choosing the Right Format","text":"<p>Consider these factors when selecting an output format:</p> <ol> <li>Complexity of the data: More complex data structures benefit from JSON Schema</li> <li>Provider support: Check which formats are natively supported by your provider</li> <li>Consistency requirements: Stricter format requirements favor JSON Schema or Tools</li> <li>Application needs: Consider how the data will be used in your application</li> </ol>"},{"location":"polyglot/modes/overview/#improving-format-reliability","title":"Improving Format Reliability","text":"<p>For better results:</p> <ol> <li>Be explicit in prompts: Clearly describe the expected format</li> <li>Provide examples: Show what good responses look like</li> <li>Use constraints: Specify limits and requirements</li> <li>Test across providers: Verify formats work with all providers you use</li> <li>Implement fallbacks: Have backup strategies for format failures</li> </ol>"},{"location":"polyglot/modes/text/","title":"Text Mode","text":"<p>Text mode is the default and simplest output format, returning unstructured text from the model.</p>"},{"location":"polyglot/modes/text/#basic-text-generation","title":"Basic Text Generation","text":"<pre><code>// @doctest id=\"5849\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// OutputMode::Text is the default, so you don't need to specify it\n$response = $inference\n    -&gt;with(\n        messages: 'What is the capital of France?',\n        mode: OutputMode::Text  // Optional, this is the default\n    )\n    -&gt;get();\n\necho \"Response: $response\\n\";\n// Output: Response: The capital of France is Paris.\n</code></pre>"},{"location":"polyglot/modes/text/#when-to-use-text-mode","title":"When to Use Text Mode","text":"<p>Text mode is ideal for: - Simple question answering - Creative content generation - Conversational interactions - Summaries and paraphrasing - Any use case where structured data is not required</p>"},{"location":"polyglot/modes/text/#text-mode-across-providers","title":"Text Mode Across Providers","text":"<p>Text mode works consistently across all providers, making it the most portable option:</p> <pre><code>// @doctest id=\"0e70\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n// Using OpenAI\n$openAIResponse = $inference\n    -&gt;using('openai')\n    -&gt;withMessages('Write a short poem about the ocean.')\n    -&gt;get();\n\necho \"OpenAI response:\\n$openAIResponse\\n\\n\";\n\n// Using Anthropic\n$anthropicResponse = $inference\n    -&gt;using('anthropic')\n    -&gt;with('Write a short poem about the ocean.')\n    -&gt;get();\n\necho \"Anthropic response:\\n$anthropicResponse\\n\\n\";\n\n// Using any other provider\n// ...\n</code></pre>"},{"location":"polyglot/modes/tools/","title":"Tools Mode","text":"<p>Tools mode enables function calling, allowing the model to request specific actions to be performed by your application. This is powerful for creating LLM-powered applications that can interact with external systems or perform specific tasks.</p>"},{"location":"polyglot/modes/tools/#setting-up-tools","title":"Setting Up Tools","text":"<pre><code>// @doctest id=\"b31d\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');  // Tools are best supported by OpenAI\n\n// Define a tool for weather information\n$weatherTool = [\n    'type' =&gt; 'function',\n    'function' =&gt; [\n        'name' =&gt; 'get_weather',\n        'description' =&gt; 'Get the current weather for a location',\n        'parameters' =&gt; [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'location' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The city and state or country (e.g., \"San Francisco, CA\")',\n                ],\n                'unit' =&gt; [\n                    'type' =&gt; 'string',\n                    'enum' =&gt; ['celsius', 'fahrenheit'],\n                    'description' =&gt; 'The temperature unit to use',\n                ],\n            ],\n            'required' =&gt; ['location'],\n        ],\n    ],\n];\n\n// Define a tool for flight information\n$flightTool = [\n    'type' =&gt; 'function',\n    'function' =&gt; [\n        'name' =&gt; 'get_flight_info',\n        'description' =&gt; 'Get information about a flight',\n        'parameters' =&gt; [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'flight_number' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The flight number (e.g., \"AA123\")',\n                ],\n                'date' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The date of the flight in YYYY-MM-DD format',\n                ],\n            ],\n            'required' =&gt; ['flight_number'],\n        ],\n    ],\n];\n\n// Create an array of tools\n$tools = [$weatherTool, $flightTool];\n\n// Make a request that might require tools\n$response = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris today?',\n    tools: $tools,\n    toolChoice: 'auto',  // Let the model decide which tool to use\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Check if the model called a tool\nif ($response-&gt;hasToolCalls()) {\n    $toolCalls = $response-&gt;toolCalls();\n\n    foreach ($toolCalls-&gt;all() as $call) {\n        $toolName = $call-&gt;name();\n        $args = $call-&gt;args();\n\n        echo \"Tool called: $toolName\\n\";\n        echo \"Arguments: \" . json_encode($args, JSON_PRETTY_PRINT) . \"\\n\";\n\n        // Handle the tool call\n        if ($toolName === 'get_weather') {\n            // In a real application, you would call a weather API here\n            $weatherData = simulateWeatherApi($args['location'], $args['unit'] ?? 'celsius');\n\n            // Send the tool result back to the model\n            $withToolResult = $inference-&gt;with(\n                messages: [\n                    ['role' =&gt; 'user', 'content' =&gt; 'What\\'s the weather like in Paris today?'],\n                    [\n                        'role' =&gt; 'assistant',\n                        'content' =&gt; '',\n                        '_metadata' =&gt; [\n                            'tool_calls' =&gt; [\n                                [\n                                    'id' =&gt; $call-&gt;id(),\n                                    'function' =&gt; [\n                                        'name' =&gt; $call-&gt;name(),\n                                        'arguments' =&gt; $call-&gt;argsAsJson(),\n                                    ],\n                                ],\n                            ],\n                        ],\n                    ],\n                    [\n                        'role' =&gt; 'tool',\n                        'content' =&gt; json_encode($weatherData),\n                        '_metadata' =&gt; [\n                            'tool_call_id' =&gt; $call-&gt;id(),\n                            'tool_name' =&gt; $call-&gt;name(),\n                        ],\n                    ],\n                ]\n            )-&gt;get();\n\n            echo \"Final response: $withToolResult\\n\";\n        }\n    }\n} else {\n    // Model responded directly\n    echo \"Response: \" . $response-&gt;content() . \"\\n\";\n}\n\n// Simulate a weather API call\nfunction simulateWeatherApi(string $location, string $unit): array {\n    return [\n        'location' =&gt; $location,\n        'temperature' =&gt; 22,\n        'unit' =&gt; $unit,\n        'conditions' =&gt; 'Partly cloudy',\n        'humidity' =&gt; 65,\n    ];\n}\n</code></pre>"},{"location":"polyglot/modes/tools/#controlling-tool-usage","title":"Controlling Tool Usage","text":"<p>You can control how tools are used with the <code>toolChoice</code> parameter:</p> <pre><code>// @doctest id=\"b471\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');\n\n// Let the model decide whether to use tools\n$autoResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: 'auto',\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Always require the model to use a specific tool\n$requiredToolResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: [\n        'type' =&gt; 'function',\n        'function' =&gt; [\n            'name' =&gt; 'get_weather'\n        ]\n    ],\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Prevent tool usage\n$noToolResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: 'none',\n    mode: OutputMode::Tools\n)-&gt;response();\n</code></pre>"},{"location":"polyglot/modes/tools/#streaming-tool-calls","title":"Streaming Tool Calls","text":"<p>You can also stream tool calls to provide real-time feedback:</p> <pre><code>// @doctest id=\"4a3e\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris today?',\n    tools: $tools,\n    toolChoice: 'auto',\n    mode: OutputMode::Tools,\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n\n$toolName = '';\n$toolId = '';\n$toolArgs = '';\n\nforeach ($stream as $partialResponse) {\n    // Tool name is being generated\n    if (!empty($partialResponse-&gt;toolName)) {\n        if (empty($toolName)) {\n            $toolName = $partialResponse-&gt;toolName;\n            echo \"Tool being called: $toolName\\n\";\n        }\n    }\n\n    // Tool ID is received\n    if (!empty($partialResponse-&gt;toolId) &amp;&amp; empty($toolId)) {\n        $toolId = $partialResponse-&gt;toolId;\n    }\n\n    // Tool arguments are being generated\n    if (!empty($partialResponse-&gt;toolArgs)) {\n        $toolArgs .= $partialResponse-&gt;toolArgs;\n        echo \"Receiving tool arguments...\\n\";\n    }\n\n    // Regular content is being generated\n    if (!empty($partialResponse-&gt;contentDelta)) {\n        echo $partialResponse-&gt;contentDelta;\n        flush();\n    }\n\n    // Check for finish reason\n    if (!empty($partialResponse-&gt;finishReason)) {\n        echo \"\\nFinished with reason: {$partialResponse-&gt;finishReason}\\n\";\n    }\n}\n\n// Process the complete tool call\nif (!empty($toolName) &amp;&amp; !empty($toolArgs)) {\n    try {\n        $args = json_decode($toolArgs, true, 512, JSON_THROW_ON_ERROR);\n        echo \"\\nFinal tool arguments: \" . json_encode($args, JSON_PRETTY_PRINT) . \"\\n\";\n\n        // Process the tool call as in the previous example\n    } catch (\\JsonException $e) {\n        echo \"Error parsing tool arguments: \" . $e-&gt;getMessage() . \"\\n\";\n    }\n}\n</code></pre>"},{"location":"polyglot/modes/tools/#provider-support-for-tools","title":"Provider Support for Tools","text":"<p>Tool support varies across providers:</p> <ul> <li>OpenAI: Comprehensive support with <code>function_call</code>/<code>tool_call</code> features</li> <li>Anthropic: Growing support with <code>tool_use</code> feature in Claude 3 models</li> <li>Other providers: Implementation varies; check provider documentation</li> </ul>"},{"location":"polyglot/modes/tools/#when-to-use-tools-mode","title":"When to Use Tools Mode","text":"<p>Tools mode is ideal for: - Creating agents that can interact with external systems - Building assistants that need to retrieve real-time information - Implementing complex workflows that require multiple steps - Giving the model access to specific capabilities (calculations, API calls, etc.)</p>"},{"location":"polyglot/streaming/misc/","title":"Advanced Stream Processing","text":""},{"location":"polyglot/streaming/misc/#using-callbacks","title":"Using Callbacks","text":"<p>You can use the <code>onPartialResponse</code> method to register a callback that is called for each partial response:</p> <pre><code>// @doctest id=\"9f28\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n// Set up a callback for processing partial responses\n$stream = $response-&gt;stream()-&gt;onPartialResponse(function($partialResponse) {\n    echo $partialResponse-&gt;contentDelta;\n    flush();\n});\n\n// Process all responses\nforeach ($stream-&gt;responses() as $_) {\n    // The callback is called for each partial response\n    // We don't need to do anything here\n}\n</code></pre>"},{"location":"polyglot/streaming/misc/#transforming-stream-content","title":"Transforming Stream Content","text":"<p>You can process and transform the content as it streams:</p> <pre><code>// @doctest id=\"fdce\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Generate a list of 10 book titles.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$titleCount = 0;\n$currentTitle = '';\n\nforeach ($stream as $partialResponse) {\n    $content = $partialResponse-&gt;contentDelta;\n\n    // Check for new titles (assuming numbered list format)\n    if (preg_match('/(\\d+)\\.\\s+(.+?)(?=\\n\\d+\\.|\\Z)/s', $content, $matches)) {\n        $titleCount++;\n        $title = trim($matches[2]);\n        echo \"Title #{$matches[1]}: $title\\n\";\n    } elseif (!empty(trim($content))) {\n        echo $content;\n    }\n}\n</code></pre>"},{"location":"polyglot/streaming/misc/#processing-json-streams","title":"Processing JSON Streams","text":"<p>For streaming JSON responses, you need to accumulate content until you have valid JSON:</p> <pre><code>// @doctest id=\"466f\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'List 5 countries and their capitals in JSON format.',\n    mode: OutputMode::Json,  // Request JSON response\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$jsonBuffer = '';\n\nforeach ($stream as $partialResponse) {\n    $jsonBuffer .= $partialResponse-&gt;contentDelta;\n\n    // Try to parse the accumulated JSON\n    $tempJson = $jsonBuffer;\n\n    // Attempt to complete any incomplete JSON\n    if (substr(trim($tempJson), -1) !== '}') {\n        $tempJson .= '}';\n    }\n\n    // Replace any trailing commas which would make the JSON invalid\n    $tempJson = preg_replace('/,\\s*}$/', '}', $tempJson);\n\n    try {\n        $data = json_decode($tempJson, true, 512, JSON_THROW_ON_ERROR);\n        echo \"Valid JSON received so far: \" . json_encode($data, JSON_PRETTY_PRINT) . \"\\n\";\n    } catch (\\JsonException $e) {\n        // Not a complete valid JSON yet\n        echo \"Accumulated content: $jsonBuffer\\n\";\n    }\n}\n\n// Process the final, complete JSON\ntry {\n    $finalData = json_decode($jsonBuffer, true, 512, JSON_THROW_ON_ERROR);\n    echo \"Final JSON: \" . json_encode($finalData, JSON_PRETTY_PRINT) . \"\\n\";\n} catch (\\JsonException $e) {\n    echo \"Error parsing final JSON: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/streaming/misc/#cancellation","title":"Cancellation","text":"<p>In some cases, you may want to stop the generation early:</p> <pre><code>// @doctest id=\"2aa0\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a long story about space exploration.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$wordCount = 0;\n$maxWords = 100;  // Limit to 100 words\n\nforeach ($stream as $partialResponse) {\n    echo $partialResponse-&gt;contentDelta;\n    flush();\n\n    // Count words in the accumulated content\n    $words = str_word_count($partialResponse-&gt;content());\n\n    // Stop after reaching the word limit\n    if ($words &gt;= $maxWords) {\n        echo \"\\n\\n[Generation stopped after $maxWords words]\\n\";\n        break;  // Exit the loop early\n    }\n}\n</code></pre> <p>Note that when you break out of the loop, the request to the provider continues in the background, but your application stops processing the response.</p>"},{"location":"polyglot/streaming/misc/#performance-considerations","title":"Performance Considerations","text":"<p>When working with streaming responses, keep these performance considerations in mind:</p> <ol> <li>Memory Usage: Be careful with how you accumulate content, especially for very long responses</li> <li>Buffer Flushing: In web applications, make sure output buffers are properly flushed</li> <li>Connection Stability: Streaming connections can be more sensitive to network issues</li> <li>Timeouts: Adjust timeout settings for long-running streams</li> </ol> <p>Here's an example of memory-efficient processing for very long responses:</p> <pre><code>// @doctest id=\"f8f4\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Generate a very long story.',\n    options: [\n        'stream' =&gt; true,\n        'max_tokens' =&gt; 10000  // Request a long response\n    ]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$outputFile = fopen('generated_story.txt', 'w');\n\nforeach ($stream as $partialResponse) {\n    // Write chunks directly to file instead of keeping them in memory\n    fwrite($outputFile, $partialResponse-&gt;contentDelta);\n\n    // Optional: Show a progress indicator\n    echo \".\";\n    flush();\n}\n\nfclose($outputFile);\necho \"\\nGeneration complete. Story saved to generated_story.txt\\n\";\n</code></pre>"},{"location":"polyglot/streaming/overview/","title":"Overview of Streaming","text":"<p>Streaming LLM responses may be preferred for user experience and system performance. Polyglot makes it easy to implement streaming with a consistent API across different providers.</p> <p>Streaming responses are a powerful feature of modern LLM APIs that allow you to receive and process model outputs incrementally as they're being generated, rather than waiting for the complete response. This chapter covers how to work with streaming responses in Polyglot, from basic setup to advanced processing techniques.</p>"},{"location":"polyglot/streaming/overview/#benefits-of-streaming","title":"Benefits of Streaming","text":"<p>Streaming responses offer several advantages:</p> <ol> <li>Improved User Experience: Display content to users as it's generated, creating a more responsive interface</li> <li>Reduced Latency Perception: Users see the beginning of a response almost immediately</li> <li>Progressive Processing: Begin processing early parts of the response while later parts are still being generated</li> <li>Handling Long Outputs: Efficiently process responses that may be very long without hitting timeout limits</li> <li>Early Termination: Stop generation early if needed, saving resources</li> </ol>"},{"location":"polyglot/streaming/overview/#enabling-streaming","title":"Enabling Streaming","text":"<p>Enabling streaming in Polyglot is straightforward - you need to set the <code>stream</code> option to <code>true</code> in your request:</p> <pre><code>// @doctest id=\"4cab\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]  // Enable streaming\n);\n</code></pre> <p>Once you have a streaming-enabled response, you can access the stream using the <code>stream()</code> method:</p> <pre><code>// @doctest id=\"3770\"\n// Get the stream of partial responses\n$stream = $response-&gt;stream();\n</code></pre>"},{"location":"polyglot/streaming/overview/#basic-stream-processing","title":"Basic Stream Processing","text":"<p>The most common way to process a stream is to iterate through the partial responses:</p> <pre><code>// @doctest id=\"7c06\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n// Get a generator that yields partial responses\n$stream = $response-&gt;stream()-&gt;responses();\n\necho \"Story: \";\nforeach ($stream as $partialResponse) {\n    // Output each chunk as it arrives\n    echo $partialResponse-&gt;contentDelta;\n\n    // Flush the output buffer to show progress in real-time (for CLI or streaming HTTP responses)\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n        flush();\n    }\n}\necho \"\\n\";\n</code></pre>"},{"location":"polyglot/streaming/overview/#understanding-partial-responses","title":"Understanding Partial Responses","text":"<p>Each iteration of the stream yields a <code>PartialInferenceResponse</code> object with these key properties:</p> <ul> <li><code>contentDelta</code>: The new content received in this chunk</li> <li><code>content</code>: The accumulated content up to this point</li> <li><code>finishReason</code>: The reason why the response finished (empty until the final chunk)</li> <li><code>usage</code>: Token usage statistics</li> </ul> <pre><code>// @doctest id=\"dc6c\"\nforeach ($stream as $partialResponse) {\n    // The new content in this chunk\n    echo \"New content: \" . $partialResponse-&gt;contentDelta . \"\\n\";\n\n    // The total content received so far\n    echo \"Total content so far: \" . $partialResponse-&gt;content() . \"\\n\";\n\n    // Check if this is the final chunk\n    if ($partialResponse-&gt;finishReason !== '') {\n        echo \"Response finished: \" . $partialResponse-&gt;finishReason . \"\\n\";\n    }\n}\n</code></pre>"},{"location":"polyglot/streaming/overview/#retrieving-the-final-response","title":"Retrieving the Final Response","text":"<p>After processing the stream, you can get the complete response:</p> <pre><code>// @doctest id=\"ceb0\"\n// Method 1: Using the original response object's get() method\n$completeText = $response-&gt;get();\n\n// Method 2: Getting the final state from the stream\n$finalResponse = $response-&gt;stream()-&gt;final();\n$completeText = $finalResponse-&gt;content();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/","title":"Debugging Requests and Responses","text":"<p>Polyglot debug mode provides a simple way to enable debugging for LLM interactions. Debugging is essential for troubleshooting and optimizing your applications. It allows you to inspect the requests sent to the LLM and the responses received, helping you identify issues and improve performance.</p>"},{"location":"polyglot/troubleshooting/debugging/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>Polyglot provides a simple way to enable debug mode:</p> <pre><code>// @doctest id=\"5327\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Enable debug mode when creating the inference object\n$inference = (new Inference())\n    -&gt;withDebugPreset('on');\n\n// Make a request - debug output will show the request and response details\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/#http-request-inspection-with-middleware","title":"HTTP Request Inspection with Middleware","text":"<p>You can manually add debugging middleware to inspect raw HTTP requests and responses.</p> <p>In this example we're using built-in middleware, but you can also create your own custom middleware.</p> <pre><code>// @doctest id=\"ed0d\"\n&lt;?php\nuse Cognesy\\Http\\Middleware\\Debug\\DebugMiddleware;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom debug middleware with specific options\n$debugMiddleware = new DebugMiddleware([\n    'requestUrl' =&gt; true,\n    'requestHeaders' =&gt; true,\n    'requestBody' =&gt; true,\n    'responseHeaders' =&gt; true,\n    'responseBody' =&gt; true,\n]);\n\n// Create an HTTP client with the debug middleware\n$httpClient = new HttpClient();\n$httpClient-&gt;withMiddleware($debugMiddleware);\n\n// Use the HTTP client with Inference\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/#event-listeners","title":"Event Listeners","text":"<p>Use event listeners to trace the flow of requests and responses:</p> <pre><code>// @doctest id=\"4165\"\n&lt;?php\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an event dispatcher\n$events = new EventDispatcher();\n\n// Add listeners\n$events-&gt;listen(InferenceRequested::class, function (InferenceRequested $event) {\n    echo \"Request sent: \" . json_encode($event-&gt;request-&gt;toArray()) . \"\\n\";\n});\n\n$events-&gt;listen(InferenceResponseCreated::class, function (InferenceResponseCreated $event) {\n    echo \"Response received: \" . substr($event-&gt;inferenceResponse-&gt;content(), 0, 50) . \"...\\n\";\n    echo \"Token usage: \" . $event-&gt;inferenceResponse-&gt;usage()-&gt;total() . \"\\n\";\n});\n\n// Create an inference object with the event dispatcher\n$inference = new Inference(events: $events);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/#logging-to-files","title":"Logging to Files","text":"<p>For more persistent debugging, you can log to files:</p> <pre><code>// @doctest id=\"5f93\"\n&lt;?php\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a function to log to file\nfunction logToFile(string $message, string $filename = 'llm_debug.log'): void {\n    $timestamp = date('Y-m-d H:i:s');\n    file_put_contents(\n        $filename,\n        \"[$timestamp] $message\" . PHP_EOL,\n        FILE_APPEND\n    );\n}\n\n// Create a custom event dispatcher\n$events = new EventDispatcher();\n\n// Listen for request events\n$events-&gt;listen(InferenceRequested::class, function (InferenceRequested $event) {\n    $request = $event-&gt;request;\n    logToFile(\"REQUEST: \" . json_encode($request-&gt;toArray()));\n});\n\n// Listen for response events\n$events-&gt;listen(InferenceResponseCreated::class, function (InferenceResponseCreated $event) {\n    $response = $event-&gt;inferenceResponse;\n    logToFile(\"RESPONSE: \" . json_encode($response-&gt;toArray()));\n});\n\n// Create an inference object with the custom event dispatcher\n$inference = new Inference(events: $events);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is artificial intelligence?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-authentication/","title":"Authentication","text":"<p>One of the most common issues when working with LLM APIs is authentication problems.</p>"},{"location":"polyglot/troubleshooting/issues-authentication/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages containing terms like \"authentication failed,\" \"invalid API key,\" or \"unauthorized\"</li> <li>HTTP status codes 401 or 403</li> </ul>"},{"location":"polyglot/troubleshooting/issues-authentication/#solutions","title":"Solutions","text":"<ol> <li> <p>Verify API Key: Ensure your API key is correctly set in your environment variables <pre><code>// @doctest id=\"3d0f\"\n// Check if API key is set\nif (empty(getenv('OPENAI_API_KEY'))) {\necho \"API key is not set in environment variables\\n\";\n}\n</code></pre></p> </li> <li> <p>Check API Key Format: Some providers require specific formats for API keys <pre><code>// @doctest id=\"f216\"\n// OpenAI keys typically start with 'sk-'\nif (!str_starts_with(getenv('OPENAI_API_KEY'), 'sk-')) {\necho \"OpenAI API key format is incorrect\\n\";\n}\n\n// Anthropic keys typically start with 'sk-ant-'\nif (!str_starts_with(getenv('ANTHROPIC_API_KEY'), 'sk-ant-')) {\necho \"Anthropic API key format is incorrect\\n\";\n}\n</code></pre></p> </li> <li> <p>Test Keys Directly: Use a simple script to test your API keys</p> </li> </ol> <pre><code>// @doctest id=\"9ef8\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction testApiKey(string $preset): bool {\n    try {\n        $llm = (new LLMFactory)-&gt;fromPreset($preset);\n        $inference = new Inference($llm);\n        $inference-&gt;with(\n            messages: 'Test message',\n            options: ['max_tokens' =&gt; 5]\n        )-&gt;get();\n\n        echo \"Connection using '$connection' is working correctly\\n\";\n        return true;\n    } catch (HttpRequestException $e) {\n        echo \"Error with connection '$connection': \" . $e-&gt;getMessage() . \"\\n\";\n        return false;\n    }\n}\n\n// Test major providers\ntestApiKey('openai');\ntestApiKey('anthropic');\ntestApiKey('mistral');\n?&gt;\n</code></pre> <ol> <li>Environment Variables: Ensure your environment variables are being loaded correctly <pre><code>// @doctest id=\"f99d\"\n&lt;?php\n// If using dotenv\n$dotenv = Dotenv\\Dotenv::createImmutable(__DIR__);\n$dotenv-&gt;load();\n$dotenv-&gt;required(['OPENAI_API_KEY'])-&gt;notEmpty();\n?&gt;\n</code></pre></li> </ol>"},{"location":"polyglot/troubleshooting/issues-configuration/","title":"Connection Configurations","text":""},{"location":"polyglot/troubleshooting/issues-configuration/#symptoms","title":"Symptoms","text":"<ul> <li>Errors like \"connection timeout,\" \"failed to connect,\" or \"network error\"</li> <li>Long delays before errors appear</li> <li>Issues with specific providers (e.g., OpenAI, Anthropic, Mistral)</li> <li>Incorrect API keys or permissions</li> <li>Missing or incorrect configuration parameters</li> </ul>"},{"location":"polyglot/troubleshooting/issues-configuration/#solutions","title":"Solutions","text":""},{"location":"polyglot/troubleshooting/issues-configuration/#1-verify-api-keys","title":"1. Verify API Keys","text":"<p>Make sure your API keys are correct and have the necessary permissions:</p> <pre><code>// @doctest id=\"d48f\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction testApiKey(string $preset): bool {\n    try {\n        $llm = LLMProvider::using($preset);\n        $inference = new Inference($preset);\n        $response = $inference-&gt;with(\n            messages: 'Test message',\n            options: ['max_tokens' =&gt; 5]\n        )-&gt;get();\n\n        echo \"Connection preset '$preset' is working.\\n\";\n        return true;\n    } catch (HttpRequestException $e) {\n        echo \"Error with connection '$preset': \" . $e-&gt;getMessage() . \"\\n\";\n        return false;\n    }\n}\n\n// Test each connection\n$presets = ['openai', 'anthropic', 'mistral'];\nforeach ($presets as $preset) {\n    testApiKey($preset);\n}\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-configuration/#2-enable-debug-mode","title":"2. Enable Debug Mode","text":"<p>Use debug mode to see the actual requests and responses:</p> <pre><code>// @doctest id=\"f230\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Enable debug mode\n$inference = new Inference()\n    -&gt;using('openai')\n    -&gt;withDebugPreset('on');\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'Test message with debug enabled'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-configuration/#3-check-provider-status","title":"3. Check Provider Status","text":"<p>Some issues might be related to the provider's service status. Check their status pages or documentation.</p>"},{"location":"polyglot/troubleshooting/issues-configuration/#4-verify-configuration-parameters","title":"4. Verify Configuration Parameters","text":"<p>Ensure all required configuration parameters are present and correctly formatted:</p> <pre><code>// @doctest id=\"dc94\"\n&lt;?php\n\nfunction verifyConfig(string $preset): void {\n    try {\n        $provider = new ConfigProvider();\n        $config = LLMConfig::fromArray($provider-&gt;getConfig($preset));\n\n        echo \"Configuration for '$preset':\\n\";\n        echo \"API URL: {$config-&gt;apiUrl}\\n\";\n        echo \"Endpoint: {$config-&gt;endpoint}\\n\";\n        echo \"Default Model: {$config-&gt;model}\\n\";\n        echo \"Provider Type: {$config-&gt;providerType}\\n\";\n\n        // Check for empty values\n        if (empty($config-&gt;apiKey)) {\n            echo \"Warning: API key is empty\\n\";\n        }\n\n        if (empty($config-&gt;model)) {\n            echo \"Warning: Default model is not set\\n\";\n        }\n    } catch (\\Exception $e) {\n        echo \"Error loading configuration for '$preset': \" . $e-&gt;getMessage() . \"\\n\";\n    }\n}\n\n// Verify configurations\n$presets = ['openai', 'anthropic', 'mistral'];\nforeach ($presets as $preset) {\n    verifyConfig($preset);\n    echo \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-connection/","title":"Connection Issues","text":"<p>Network connectivity problems can prevent successful API requests.</p>"},{"location":"polyglot/troubleshooting/issues-connection/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages like \"connection timeout,\" \"failed to connect,\" or \"network error\"</li> <li>Long delays before errors appear</li> </ul>"},{"location":"polyglot/troubleshooting/issues-connection/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Internet Connection: Ensure your server has a stable internet connection</p> </li> <li> <p>Verify API Endpoint: Make sure the API endpoint URL is correct <pre><code>// @doctest id=\"cb6f\"\n// In your configuration file (config/llm.php)\n'apiUrl' =&gt; 'https://api.openai.com/v1', // Correct URL\n</code></pre></p> </li> <li> <p>Proxy Settings: If you're behind a proxy, configure it properly</p> </li> </ol> <pre><code>// @doctest id=\"16d3\"\n// Using custom HTTP client with proxy settings\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;\n\n$config = new HttpClientConfig(\n    requestTimeout: 30,\n    connectTimeout: 10,\n    additionalOptions: ['proxy' =&gt; 'http://proxy.example.com:8080']\n);\n\n$httpClient = new HttpClient('guzzle', $config);\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n</code></pre> <ol> <li> <p>Firewall Rules: Check if your firewall is blocking outgoing connections to API endpoints</p> </li> <li> <p>DNS Resolution: Ensure your DNS is resolving the API domains correctly</p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-model-specific/","title":"Model-Specific Issues","text":"<p>When working with different LLM models, you may encounter issues that are specific to the model you're using, as different models have different capabilities and limitations. This section covers common model-specific issues and how to resolve them.</p>"},{"location":"polyglot/troubleshooting/issues-model-specific/#symptoms","title":"Symptoms","text":"<ul> <li>Errors like \"model not found,\" \"parameter not supported,\" or \"context length exceeded\"</li> <li>Unexpected responses or performance from certain models</li> </ul>"},{"location":"polyglot/troubleshooting/issues-model-specific/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Model Availability: Ensure the model you're requesting is available from the provider <pre><code>// @doctest id=\"2d2f\"\n// Check available models for each provider in their documentation\n// Example: For OpenAI 'gpt-4o-mini' is valid, but 'gpt5' is not\n</code></pre></p> </li> <li> <p>Context Length: Be aware of each model's maximum context length <pre><code>// @doctest id=\"f391\"\n// In config/llm.php, check contextLength for each model\n// Example: OpenAI models have different context windows\n// - gpt-3.5-turbo: 16K tokens\n// - gpt-4-turbo: 128K tokens\n// - claude-3-opus: 200K tokens\n</code></pre></p> </li> <li> <p>Feature Support: Different models support different features <pre><code>// @doctest id=\"6039\"\n// Some features may not work with all models\n// Example: Vision capabilities are only available in select models\n\n// Check for vision support before sending images\n$modelSupportsVision = in_array($model, [\n    'gpt-4-vision', 'gpt-4o', 'claude-3-opus', 'claude-3-sonnet'\n]);\n\nif (!$modelSupportsVision) {\n    echo \"Warning: The selected model doesn't support vision capabilities\\n\";\n}\n</code></pre></p> </li> <li> <p>Fallback Models: Implement fallbacks to other models when preferred models fail</p> </li> </ol> <pre><code>// @doctest id=\"ca21\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withModelFallback(array $models, string $prompt): string {\n    $inference = new Inference();\n    $lastException = null;\n\n    foreach ($models as $model) {\n        try {\n            return $inference-&gt;with(\n                messages: $prompt,\n                model: $model\n            )-&gt;get();\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            echo \"Model '$model' failed: \" . $e-&gt;getMessage() . \"\\n\";\n            echo \"Trying next model...\\n\";\n        }\n    }\n\n    throw new \\Exception(\"All models failed. Last error: \" .\n        ($lastException ? $lastException-&gt;getMessage() : \"Unknown error\"));\n}\n\n// Try advanced models first, then fall back to simpler ones\n$models = ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'];\n\ntry {\n    $response = withModelFallback($models, \"What is the capital of France?\");\n    echo \"Response: $response\\n\";\n} catch (\\Exception $e) {\n    echo \"Error: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-provider-specific/","title":"Provider-Specific Issues","text":"<p>Each LLM provider has unique quirks and issues. This section covers common provider-specific issues and how to resolve them.</p>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#openai","title":"OpenAI","text":"<ol> <li> <p>Organization IDs: Set the organization ID if using a shared account <pre><code>// @doctest id=\"9b48\"\n// In config/llm.php\n'metadata' =&gt; [\n    'organization' =&gt; 'org-your-organization-id',\n],\n</code></pre></p> </li> <li> <p>API Versions: Pay attention to API version changes <pre><code>// @doctest id=\"75e5\"\n// Updates to OpenAI API may require changes to your code\n// Monitor OpenAI's release notes for changes\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#anthropic","title":"Anthropic","text":"<ol> <li> <p>Message Format: Anthropic uses a different message format <pre><code>// @doctest id=\"d632\"\n// Polyglot handles this automatically, but be aware when debugging\n</code></pre></p> </li> <li> <p>Tool Support: Tool support has specific requirements <pre><code>// @doctest id=\"a60b\"\n// When using tools with Anthropic, check their latest documentation\n// for supported features and limitations\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#mistral","title":"Mistral","text":"<ol> <li>Rate Limits: Mistral has strict rate limits on free tier <pre><code>// @doctest id=\"5a5e\"\n// Implement more aggressive rate limiting for Mistral\n</code></pre></li> </ol>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#ollama","title":"Ollama","text":"<ol> <li> <p>Local Setup: Ensure Ollama is properly installed and running <pre><code># @doctest id=\"d4b6\"\n# Check if Ollama is running\ncurl http://localhost:11434/api/version\n</code></pre></p> </li> <li> <p>Model Availability: Download models before using them <pre><code># @doctest id=\"b114\"\n# Pull a model before using it\nollama pull llama2\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-rate-limits/","title":"Rate Limits","text":"<p>Provider rate limits can cause request failures during high traffic periods.</p>"},{"location":"polyglot/troubleshooting/issues-rate-limits/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages containing \"rate limit exceeded,\" \"too many requests,\" or \"quota exceeded\"</li> <li>HTTP status code 429</li> </ul>"},{"location":"polyglot/troubleshooting/issues-rate-limits/#solutions","title":"Solutions","text":"<ol> <li>Implement Retry Logic: Add automatic retries with exponential backoff</li> </ol> <pre><code>// @doctest id=\"781f\"\n&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withRetry(callable $fn, int $maxRetries = 3): mixed {\n    $attempt = 0;\n    $lastException = null;\n\n    while ($attempt &lt; $maxRetries) {\n        try {\n            return $fn();\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            $attempt++;\n\n            // Only retry on rate limit errors\n            if (strpos($e-&gt;getMessage(), 'rate limit') === false &amp;&amp;\n                $e-&gt;getCode() !== 429) {\n                throw $e;\n            }\n\n            if ($attempt &gt;= $maxRetries) {\n                break;\n            }\n\n            // Exponential backoff\n            $sleepTime = (2 ** $attempt);\n            echo \"Rate limit hit. Retrying in $sleepTime seconds...\\n\";\n            sleep($sleepTime);\n        }\n    }\n\n    throw $lastException;\n}\n\n// Usage\n$inference = new Inference();\n\ntry {\n    $response = withRetry(function() use ($inference) {\n        return $inference-&gt;with(\n            messages: 'What is the capital of France?'\n        )-&gt;get();\n    });\n\n    echo \"Response: $response\\n\";\n} catch (HttpRequestException $e) {\n    echo \"All retry attempts failed: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre> <ol> <li> <p>Request Throttling: Limit the rate of requests from your application <pre><code>// @doctest id=\"cd18\"\n&lt;?php\nclass RateLimiter {\n    private $lastRequestTime = 0;\n    private $requestsPerMinute;\n    private $minTimeBetweenRequests;\n\n    public function __construct(int $requestsPerMinute = 60) {\n        $this-&gt;requestsPerMinute = $requestsPerMinute;\n        $this-&gt;minTimeBetweenRequests = 60 / $requestsPerMinute;\n    }\n\n    public function waitIfNeeded(): void {\n        $currentTime = microtime(true);\n        $timeSinceLastRequest = $currentTime - $this-&gt;lastRequestTime;\n\n        if ($timeSinceLastRequest &lt; $this-&gt;minTimeBetweenRequests) {\n            $waitTime = $this-&gt;minTimeBetweenRequests - $timeSinceLastRequest;\n            usleep($waitTime * 1000000);\n        }\n\n        $this-&gt;lastRequestTime = microtime(true);\n    }\n}\n\n// Usage\n$limiter = new RateLimiter(30); // 30 requests per minute\n$inference = new Inference();\n\nfor ($i = 0; $i &lt; 10; $i++) {\n    $limiter-&gt;waitIfNeeded();\n    $response = $inference-&gt;with(\n        messages: \"This is request $i\"\n    )-&gt;toText();\n    echo \"Response $i: $response\\n\";\n}\n</code></pre></p> </li> <li> <p>Request Batching: Combine multiple requests into batches when possible</p> </li> </ol> <pre><code>// @doctest id=\"dbb2\"\n&lt;?php\n// Instead of making many small requests\n$responses = [];\nforeach ($questions as $question) {\n    // This would hit rate limits quickly\n    $responses[] = $inference-&gt;with(messages: $question)-&gt;get();\n}\n\n// Better: Use a context-aware batch approach\n$batchedQuestions = \"Please answer the following questions:\\n\";\nforeach ($questions as $i =&gt; $question) {\n    $batchedQuestions .= ($i + 1) . \". $question\\n\";\n}\n\n$batchResponse = $inference-&gt;with(messages: $batchedQuestions)-&gt;get();\n// Then parse the batch response into individual answers\n</code></pre> <ol> <li>Upgrade API Plan: Consider upgrading to a higher tier with increased rate limits</li> </ol>"},{"location":"polyglot/troubleshooting/issues-streaming/","title":"Streaming","text":"<p>Streaming responses can encounter specific problems.</p>"},{"location":"polyglot/troubleshooting/issues-streaming/#symptoms","title":"Symptoms","text":"<ul> <li>Streams cutting off prematurely</li> <li>Errors during stream processing</li> <li>Partial or incomplete responses</li> </ul>"},{"location":"polyglot/troubleshooting/issues-streaming/#solutions","title":"Solutions","text":"<ol> <li>Connection Timeouts: Increase timeout settings for streaming responses</li> </ol> <pre><code>// @doctest id=\"83f9\"\n&lt;?php\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;\n\n// Create a custom HTTP client with longer timeouts\n$config = new HttpClientConfig(\n    requestTimeout: 180,  // 3 minutes for the entire request\n    connectTimeout: 10,   // 10 seconds to establish connection\n    idleTimeout: 60       // 60 seconds allowed between stream chunks\n);\n\n$httpClient = new HttpClient('guzzle', $config);\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Use streaming with the custom client\n$response = $inference-&gt;with(\n    messages: 'Write a long story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n    flush();\n}\n</code></pre> <ol> <li> <p>Buffer Flushing: Ensure output buffers are properly flushed during streaming <pre><code>// @doctest id=\"4b3f\"\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n\n    // Flush output buffer to ensure content is sent immediately\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n    }\n    flush();\n}\n</code></pre></p> </li> <li> <p>Error Handling in Streams: Implement specific error handling for streams <pre><code>// @doctest id=\"06e0\"\n&lt;?php\ntry {\n    $response = $inference-&gt;with(\n        messages: 'Write a long story.',\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $stream = $response-&gt;stream()-&gt;responses();\n        $content = '';\n\n        foreach ($stream as $partial) {\n            $content .= $partial-&gt;contentDelta;\n            echo $partial-&gt;contentDelta;\n            flush();\n        }\n    } catch (\\Exception $streamException) {\n        echo \"\\nStream error: \" . $streamException-&gt;getMessage() . \"\\n\";\n\n        // If we got a partial response before the error, use it\n        if (!empty($content)) {\n            echo \"Partial content received: \" . strlen($content) . \" characters\\n\";\n        }\n    }\n} catch (RequestException $e) {\n    echo \"Request failed: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre></p> </li> <li> <p>Fallback to Non-streaming: Implement a fallback to non-streaming mode <pre><code>// @doctest id=\"0d05\"\n&lt;?php\nfunction getResponse(string $prompt, bool $preferStreaming = true): string {\n    $inference = new Inference();\n\n    try {\n        if ($preferStreaming) {\n            // Try streaming first\n            $response = $inference-&gt;with(\n                messages: $prompt,\n                options: ['stream' =&gt; true]\n            );\n\n            $content = '';\n            foreach ($response-&gt;stream()-&gt;responses() as $partial) {\n                $content .= $partial-&gt;contentDelta;\n                // Output can be done here if needed\n            }\n\n            return $content;\n        }\n    } catch (\\Exception $e) {\n        echo \"Streaming failed, falling back to non-streaming mode\\n\";\n    }\n\n    // Fallback to non-streaming\n    return $inference-&gt;with(messages: $prompt)-&gt;toText();\n}\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/overview/","title":"Overview of Troubleshooting","text":"<p>This chapter covers common issues you might encounter when working with Polyglot, along with best practices for effectively using LLMs in your applications.</p>"},{"location":"polyglot/troubleshooting/overview/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li>Authentication Issues</li> <li>Connection Issues</li> <li>Rate Limiting</li> <li>Model-Specific Issues</li> <li>Streaming Issues</li> <li>Provider-Specific Issues</li> <li>Debugging and Logging</li> </ol>"},{"location":"release-notes/v0.12.0/","title":"V0.12.0","text":"<ul> <li>Redesigned directory structure to separate the essential code from the addons, aux tools, etc.</li> <li>Directory 'src' to hold Instructor structured outputs code</li> <li>New directory <code>src-llm</code> to hold the LLM connectivity code (required for Instructor)</li> <li>New directory <code>src-utils</code> to hold the utility classes (required for Instructor)</li> <li>Directory <code>src-setup</code> to hold the Instructor setup tool</li> <li>Directory <code>src-hub</code> to hold the CLI tool for executing examples and generating documentation</li> <li>Directory 'src-tell' to hold a simple tool for prompting LLMs from CLI</li> <li>New directory <code>src-addons</code> to hold the additional capabilities (optional)</li> <li>New directory <code>src-aux</code> to hold the auxiliary tools used, e.g. by examples (optional)</li> <li>New directory <code>src-experimental</code> to hold the not yet ready, experimental work (not distributed)</li> <li>Moved package-specific events to their respective directories</li> <li>Moved embeddings API support to 'src-llm'</li> <li>Added version sync script and Github release automation</li> </ul>"},{"location":"release-notes/v0.12.10/","title":"V0.12.10","text":"<ul> <li>Excluded examples, evals and some CLI tools (hub and tell) to minimize distribution size (they are still available in the source code repository)</li> </ul>"},{"location":"release-notes/v0.12.11/","title":"V0.12.11","text":"<ul> <li>Middleware support for HTTP client layer - unified across clients</li> <li>Request / response debugging is now unified across clients</li> <li>Response and stream buffering middleware to allow for multiple reads from LLM API responses across request lifecycle</li> <li>New tests added</li> </ul>"},{"location":"release-notes/v0.12.12/","title":"V0.12.12","text":"<ul> <li>Moved HTTP connectivity layer to a separate directory (src-http)</li> <li>Added HTTP connectivity layer docs</li> <li>Minor corrections in docs</li> </ul>"},{"location":"release-notes/v0.12.13/","title":"V0.12.13","text":"<ul> <li>Polyglot dev guide added</li> </ul>"},{"location":"release-notes/v0.12.2/","title":"V0.12.2","text":"<ul> <li>Corrected .gitignore to further limit the distribution size (excluded non-essential sources, e.g. /src-hub/ and /src-aux/)</li> <li>Renamed src-llm to src-polyglot</li> <li>Renamed Cognesy/LLM package to Cognesy/Polyglot</li> <li>Skipped obsolete tests for experimental Module code</li> </ul>"},{"location":"release-notes/v0.12.3/","title":"V0.12.3","text":"<ul> <li>Corrected names in composer.json (cognesy/llm &gt; cognesy/polyglot-llm)</li> </ul>"},{"location":"release-notes/v0.12.4/","title":"V0.12.4","text":"<ul> <li>Corrected composer.json and publish script to keep single repo model until everything is ready for the split</li> </ul>"},{"location":"release-notes/v0.12.5/","title":"V0.12.5","text":"<ul> <li>Added basic Polyglot docs, some moved from Instructor documentation</li> <li>Corrected mint.json to reflect new paths</li> <li>Corrected scrips and paths after file / dir location changes</li> <li>Renamed Cognesy\\Aux to Cognesy\\Auxiliary to avoid name conflicts</li> <li>Extracted class autoloading from examples and evals to separate files</li> </ul>"},{"location":"release-notes/v0.12.6/","title":"V0.12.6","text":"<ul> <li>Continued project files reorganization</li> <li>Created README.md, LICENSE.md, composer.json, .gitattributes for subprojects to be ready for future split</li> <li>Corrected composer.json, .gitignore and .gitattributes</li> <li>Docs directory structure cleanup to work with Mintlify</li> </ul>"},{"location":"release-notes/v0.12.7/","title":"V0.12.7","text":"<ul> <li>Moved Cognesy/Addons/Prompt to Cognesy/Utils/Template to clarify purpose and remove cyclic dependency between src-addons and src-utils</li> <li>Added Anthropic thinking traces support in LLM drivers (thinking data is now available directly in LLMResponse and LLMPartialResponse object properties)</li> </ul>"},{"location":"release-notes/v0.12.8/","title":"V0.12.8","text":"<ul> <li>composer.json cleanup</li> <li>MessageRole now supports OpenAI 'developer' role, other drivers recognize and properly convert it</li> <li>Removed missed dependency on experimental code in one of Structure traits</li> <li>Moved tests into subpackage-specific directories</li> <li>Added tests for EventDispatcher class</li> <li>Added tests for Message class</li> <li>Added tests for Messages class</li> </ul>"},{"location":"release-notes/v0.12.9/","title":"V0.12.9","text":"<ul> <li>Fixed composer.json</li> </ul>"},{"location":"release-notes/v0.13.0/","title":"V0.13.0","text":"<ul> <li>Moved source code of each component to the <code>packages</code> directory</li> <li>Added Github workflow to split the monorepo into multiple packages</li> <li>Corrected sync and publish scripts</li> <li>Templates use BasePath for better path handling</li> </ul>"},{"location":"release-notes/v0.14.0/","title":"V0.14.0","text":"<ul> <li>Added license files to subpackages</li> </ul>"},{"location":"release-notes/v0.14.1/","title":"V0.14.1","text":"<ul> <li>Better Mintlify docs structure and fixes in generation</li> </ul>"},{"location":"release-notes/v0.14.2/","title":"V0.14.2","text":"<ul> <li>(utils) Env class now uses BasePath for base path resolution</li> </ul>"},{"location":"release-notes/v0.14.3/","title":"V0.14.3","text":"<ul> <li>(all) Corrected dependencies in composer.json files for all packages</li> <li>(utils) Removed circular dependency on addons package (via Image class)</li> <li>(all) Tests moved under packages/*/tests directories</li> <li>(all) Added .gitattributes file to all packages</li> <li>(all) Corrected .gitignore files in all packages</li> <li>(main) Scripts added in ./bin to install &amp; update composer dependencies and run all tests</li> <li>(main) Corrected php.yml to use the new script ./bin/run-all-tests.sh</li> <li>(docs) Corrected docs references: old example viewer &amp; launcher script (./hub.sh) to new (./bin/instructor hub)</li> </ul>"},{"location":"release-notes/v0.14.4/","title":"V0.14.4","text":"<ul> <li>(docs) Package specific docs moved to subpackage docs/ directories</li> <li>(docs) Modified Mintlify group naming to avoid conflicts</li> <li>(docs) Modified docs building process to use subpackage docs</li> </ul>"},{"location":"release-notes/v0.14.5/","title":"V0.14.5","text":"<ul> <li>(all) Added missing license information to composer.json files</li> </ul>"},{"location":"release-notes/v0.14.6/","title":"V0.14.6","text":"<ul> <li>(addons) Moved Cognesy\\Addons\\Evals to a separate package under packages/evals and new   namespace Cognesy\\Evals</li> <li>(polyglot) Corrected warning on missing 'thinking' key in the non-reasoning responses</li> <li>(docs) Minor corrections in docs</li> </ul>"},{"location":"release-notes/v0.14.7/","title":"V0.14.7","text":"<ul> <li>(utils) Moved Cognesy\\Utils\\Template to a new, separate package cognesy/templates</li> <li>(utils) Moved Debug related files to cognesy/http-client package</li> <li>(utils) Moved chat template code (Script and related classes) to cognesy/templates</li> </ul>"},{"location":"release-notes/v0.15.0/","title":"V0.15.0","text":"<ul> <li>New packages extracted from instructor/utils and instructor/addons - evals and templates</li> </ul>"},{"location":"release-notes/v0.15.1/","title":"V0.15.1","text":"<ul> <li>Corrected dependencies after extraction of <code>evals</code> and <code>templates</code> packages</li> </ul>"},{"location":"release-notes/v0.15.2/","title":"V0.15.2","text":"<ul> <li>PHP8.4 compatibility fix across the codebase (warning: \"Implicitly marking parameter $parameters as nullable is deprecated, the explicit nullable type must be used instead\")</li> </ul>"},{"location":"release-notes/v0.16.0/","title":"V0.16.0","text":"<ul> <li>(http) Modified HTTP client layer to allow pluggable drivers</li> <li>(http) Minor additions to docs</li> <li>(http) Removed hard dependency of http config file on HttpClientType enum</li> <li>(all) Removed dependencies on HttpClientType enum</li> <li>(tests) Changes in tests to allow execution from both monorepo and individual packages</li> </ul>"},{"location":"release-notes/v0.17.0/","title":"V0.17.0","text":"<ul> <li>(polyglot) Renamed class <code>Mode</code> to <code>OutputMode</code></li> <li>(polyglot) Updates in driver mappings to catch up on the provider changes / improvements (e.g. support for JSON Schema / strict mode)</li> <li>(polyglot) Fixed token usage tracking for OpenAI/Azure and Gemini OpenAI-comptible drivers</li> <li>(http) Switching debug to true in config/debug.php turns on debugging globally</li> </ul>"},{"location":"release-notes/v0.17.1/","title":"V0.17.1","text":"<ul> <li>(polyglot) Corrected response format selection in Sambanova driver</li> </ul>"},{"location":"release-notes/v0.17.10/","title":"V0.17.10","text":"<ul> <li>(utils) <code>JsonSchema</code> class - simple API to build dynamic JSON schemas in Polyglot, lean alternative to <code>Structure</code> class (which is Instructor only)</li> <li>(docs) New and updated examples</li> </ul>"},{"location":"release-notes/v0.17.11/","title":"V0.17.11","text":"<ul> <li>(polyglot) Support for DSN string containing parameters of LLM provider connection - Inference, Embeddings classes and their configs</li> <li>(instructor) Support for DSN string containing parameters of LLM provider connection - Instructor class</li> <li>(utils) DSN class with DSN-like string parsing capabilities</li> <li>(docs) DSN examples for Inference and Instructor</li> <li>(instructor) Internal refactoring - moved some Instructor/Features/Core code to Instructor namespace</li> <li>(hub) Automated build of changelog section in docs</li> </ul>"},{"location":"release-notes/v0.17.12/","title":"V0.17.12","text":"<ul> <li>(hub) Moved Mintlify helpers to instructor-aux package to make it an integration available for other components</li> </ul>"},{"location":"release-notes/v0.17.3/","title":"V0.17.3","text":"<ul> <li>(build) Run package tests separately for each package after a new version release</li> </ul>"},{"location":"release-notes/v0.17.4/","title":"V0.17.4","text":"<ul> <li>(docs) Renamed <code>Mode</code> class references to <code>OutputMode</code> in docs and examples</li> <li>(docs) Slimmed down README.md - the removed sections are in the docs</li> <li>(docs) Minor corrections of obsolete or outdated code in docs and examples</li> </ul>"},{"location":"release-notes/v0.17.5/","title":"V0.17.5","text":"<ul> <li>(main) Added release script to distribute current versions of /bin/ins-setup, /bin/ins-hub, /bin/tell to subpackages</li> <li>(main) Release script now copies examples to hub subpackage, so they can be included in the distribution</li> <li>(main) Renamed /scripts/setup.php script to /bin/ins-setup</li> <li>(main) Renamed /scripts/hub.php script to /bin/ins-hub</li> <li>(main) Removed obsolete /bin/instructor script</li> <li>(all) Updated composer.json files in main and subpackages to include new package scripts</li> <li>(docs) Updated docs</li> </ul>"},{"location":"release-notes/v0.17.6/","title":"V0.17.6","text":"<ul> <li>(setup) Renamed bin/ins-setup to bin/instructor-setup for better clarity</li> <li>(hub) Renamed bin/ins-hub to bin/instructor-hub for better clarity</li> <li>(docs) Minor corrections related to setting default config path</li> </ul>"},{"location":"release-notes/v0.17.7/","title":"V0.17.7","text":"<ul> <li>Fixed script merge issues</li> </ul>"},{"location":"release-notes/v0.17.8/","title":"V0.17.8","text":"<ul> <li>(http) Replaced enums with strings in config files - config/http.conf</li> <li>(polyglot) Replaced enums with strings in config files - config/embeddings.php, config/llm.php</li> </ul>"},{"location":"release-notes/v0.17.9/","title":"V0.17.9","text":"<ul> <li>(polyglot) Refactoring - introduced driver factories for embeddings and inference</li> <li>(polyglot) Introduced provider specific driver classes (in addition to previous modular driver)</li> <li>(polyglot) Fixed issue with Deepseek reasoning model accepted message format (no support for successive user or assistant messages)</li> </ul>"},{"location":"release-notes/v0.8.0/","title":"V0.8.0","text":"<ul> <li>'Structured-to-structured' processing - provide objects or arrays to be processed with LLM, get object as a result</li> <li>Composite language programs with Module classes (inspired by DSPy)</li> <li><code>FunctionCall</code> helper class for extracting arguments for functions, methods or closures</li> <li>(experimental) Anthropic tool calls mode support</li> <li>(experimental) Cohere API support - MdJson only, other modes unstable</li> <li>(experimental) Gemini API client - MdJson &amp; sync only, streaming is unstable</li> <li>(internals) Simplified, cleaner API client code</li> <li>(internals) Consolidated message building logic to support formats required by different APIs</li> <li>(internals) Better control over complex chat message sequences with <code>Scripts</code> and <code>Sections</code></li> <li>(internals) Code cleanup and bug fixes</li> <li>Additions to docs and examples</li> </ul>"},{"location":"release-notes/v1.0.0-RC10/","title":"v1.0.0 RC10","text":"<ul> <li>(schema) Merged schema and schema-v6 code.</li> <li>(schema) Symfony version check and dynamic adapter selection (v6 for lowest, v7 for stable).</li> </ul>"},{"location":"release-notes/v1.0.0-RC11/","title":"v1.0.0 RC11","text":"<ul> <li>(schema) Corrections in Symfony version detection and adapter behavior.</li> </ul>"},{"location":"release-notes/v1.0.0-RC12/","title":"v1.0.0 RC12","text":"<ul> <li>(schema) Refactored schema handling, cleaned up the type handling code.</li> <li>(utils) Improvements in JsonSchema handling and integration with TypeDetails and schema handling.</li> </ul>"},{"location":"release-notes/v1.0.0-RC13/","title":"v1.0.0 RC13","text":"<ul> <li>(instructor) Improved JSON Schema handling with JsonSchemaType class</li> <li>(instructor) Improved date handling</li> <li>(schema) Support for parallel handling of PropertyInfo for Symfony v6 and v7 (auto-detected)</li> </ul>"},{"location":"release-notes/v1.0.0-RC14/","title":"v1.0.0 RC14","text":"<ul> <li>Fixed #43 - Call to undefined method Cognesy\\Http\\HttpClient::makeDefaultDriver()</li> </ul>"},{"location":"release-notes/v1.0.0-RC15/","title":"v1.0.0 RC15","text":"<ul> <li>(utils) Default config path is now taken from <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable. <code>INSTRUCTOR_CONFIG_PATH</code> is still recognized for backward compatibility, but it is recommended to use <code>INSTRUCTOR_CONFIG_PATHS</code> instead. Default config location is now your project root's <code>config</code> directory, with fallback to the bundled config directory if not set.</li> </ul>"},{"location":"release-notes/v1.0.0-RC16/","title":"v1.0.0 RC16","text":"<ul> <li>(utils) Settings class now check not just for the existence of config dir, but also config files when resolving config path.</li> </ul>"},{"location":"release-notes/v1.0.0-RC17/","title":"v1.0.0 RC17","text":"<ul> <li>(utils) New, simplified Settings class with bugs fixed.</li> </ul>"},{"location":"release-notes/v1.0.0-RC18/","title":"v1.0.0 RC18","text":"<ul> <li>(all) Strict types checking is now enabled for all packages.</li> </ul>"},{"location":"release-notes/v1.0.0-RC19/","title":"v1.0.0 RC19","text":"<ul> <li>(utils) Split message and message list classes into separate package (instructor-messages)</li> </ul>"},{"location":"release-notes/v1.0.0-RC20/","title":"v1.0.0 RC20","text":"<ul> <li>(messages) Fixing: instructor/messages not split into separate repo</li> </ul>"},{"location":"release-notes/v1.0.0-RC21/","title":"v1.0.0 RC21","text":"<ul> <li>(messages) Fixing: instructor/messages not split into separate repo</li> </ul>"},{"location":"release-notes/v1.0.0-RC22/","title":"v1.0.0 RC22","text":"<ul> <li>(instructor) Moved structure class to a separate package (instructor-dynamic)</li> <li>(hub) Fix: error in summary generation (due to strict types)</li> </ul>"},{"location":"release-notes/v1.0.0-RC6/","title":"v1.0.0 RC6","text":"<ul> <li>Corrected dependencies in composer.json files.</li> <li>Corrected naming scheme for the release candidate (rc -&gt; RC) to make Packagist happy.</li> </ul>"},{"location":"release-notes/v1.0.0-RC7/","title":"v1.0.0 RC7","text":"<ul> <li>(composer/packagist) Corrected minimum-stability value to \"RC\" (case sensitive).</li> </ul>"},{"location":"release-notes/v1.0.0-RC8/","title":"v1.0.0 RC8","text":"<ul> <li>(packagist) Removed 'version' from composer.json files.</li> </ul>"},{"location":"release-notes/v1.0.0-RC9/","title":"v1.0.0 RC9","text":"<ul> <li>(utils) Corrected tests and removed Messages dependency on Template</li> <li>(scripts) Moved build scripts to <code>scripts/</code> directory</li> <li>(scripts) Corrected publish script to NOT force add version field to composer.json</li> </ul>"},{"location":"release-notes/v1.0.0-rc1/","title":"V1.0.0 rc1","text":"<ul> <li>(all) Multiple breaking changes - proceed with caution</li> <li>(instructor) the <code>Instructor</code> class is being replaced with <code>StructuredOutput</code> class; the old class will be kept for some time to allow for a smooth transition.</li> <li>(all) Common conventions for working with StructuredOutput, Inference, and Embeddings classes</li> <li>(examples) All examples have been updated to use the new <code>StructuredOutput</code> class and recommended create(), generate() methods</li> <li>(docs) Updated documentation to reflect the new <code>StructuredOutput</code> class and its usage</li> <li>(instructor) Extracted structured output config into a separate file config/structured.php (and removed from config/llm.php)</li> <li>(instructor) Added structured output config object</li> <li>(instructor) Cleaned up ChatTemplate class</li> <li>(instructor) Response and response streams are now cached after the first call, so multiple calls to <code>StructuredOutputResponse::response()</code> or <code>StructuredOutputResponse::stream()</code> do not cause re-processing (deserialization, validation, transformation) of the response value</li> <li>(instructor) StructuredOutput class offers fluent API for creating structured output requests</li> <li>(instructor) StructuredOutputResponse now offers getXxx() methods for getting the response value as a specific type (e.g. getString(), getInt(), getFloat(), getBool(), getArray(), getObject())</li> <li>(instructor) Removed <code>input</code> argument from StructuredOutput methods and StructuredOutputRequest class - use <code>messages</code> instead - (examples) Added StructuredOutput fluent API example</li> <li>(polyglot) Added fluent API calls to Inference class</li> <li>(polyglot) Added fluent API calls to Embeddings class</li> <li>(polyglot) Corrections in inference drivers, fixed defects in JSON/JSON Schema modes</li> <li>(polyglot) Fixed error in selection of embeddings driver</li> <li>(polyglot) Added <code>withDebug()</code> support to Embeddings class</li> <li>(polyglot) Added experimental support for HuggingFace inference API</li> <li>(all) Multiple changes, improvements and refactorings in the codebase</li> <li>(all) Updated docs and examples to reflect the latest changes</li> </ul>"},{"location":"release-notes/v1.0.0-rc2/","title":"V1.0.0 rc2","text":"<ul> <li>(polyglot) Cleaned up the code and interfaces - split responsibilities between inference vs HTTP layers</li> <li>(polyglot) Heavily refactored API to improve integration with HTTP layer</li> <li>(instructor) Object hydration via constructor parameters with support for parameter nullability and default values.</li> <li>(instructor) Object hydration via getters and setters (recognizes nullable parameters and default values).</li> <li>(instructor) Replaced deprecated PropertyInfo Type class with TypeInfo one.</li> <li>(instructor) Support for mixed property type.</li> <li>(schema) Introduced 2 versions of the schema package - Symfony 7 (default) and Symfony 6 (compatibility).</li> <li>(inference) Added <code>withHttpClientPreset()</code> to <code>Inference</code> and <code>StructuredOutput</code> facades</li> <li>(http) Configurable stream chunk size to optimize performance</li> <li>(http) Replaced debugging middleware with EventSourceMiddleware - generates events for HTTP requests, added 2 built-in listeners (PrintToConsole, DispatchHttpEvents)</li> </ul>"},{"location":"release-notes/v1.0.0-rc3/","title":"V1.0.0 rc3","text":"<ul> <li>(bin) Initial version of local split-packages.sh</li> </ul>"},{"location":"release-notes/v1.0.0-rc4/","title":"V1.0.0 rc4","text":"<ul> <li>(dependencies) Corrected schema-v6 version</li> </ul>"},{"location":"release-notes/v1.0.0-rc5/","title":"V1.0.0 rc5","text":"<ul> <li>(schema), (instructor) Corrected dependencies in composer.json</li> <li>Corrected version naming scheme.</li> </ul>"},{"location":"release-notes/v1.0.0/","title":"V1.0.0","text":"<ul> <li>(inference) Corrected CanHandleInference contract to enable future integration with HTTP pool mechanism</li> </ul>"},{"location":"release-notes/v1.1.0/","title":"V1.1.0","text":"<ul> <li>(messages) Added nicer API for creating message sequences (via <code>asUser()</code>, <code>asSystem()</code>, <code>asAssistant()</code>)</li> <li>(inference) <code>Inference::with()</code>, <code>Inference::withMessages()</code> now support <code>Message</code>, <code>Messages</code> objects directly</li> <li>(instructor) <code>StructuredOutput::with()</code>, <code>StructuredOutput::withMessages()</code> now support <code>Message</code>, <code>Messages</code> objects directly</li> </ul>"},{"location":"release-notes/v1.2.0/","title":"V1.2.0","text":"<p>\ud83d\udd27 Centralized Monorepo Management: - packages.json - New centralized package configuration - Scripts modernized: - load-packages.sh - Loads centralized config - generate-split-matrix.sh - Generates GitHub Actions matrix - update-split-yml.sh - Updates split.yml automatically - sync-ver.sh &amp; publish-ver.sh - Now use centralized config</p> <p>\u2699\ufe0f GitHub Actions: - split.yml - Now triggers on main branch pushes + tags (was tags only) - Matrix generation - Auto-generated from packages.json</p> <p>\ud83d\udce6 New Package: - instructor-doctor - Added to monorepo</p> <p>\ud83d\udcda Documentation: - Doc generation - Split from hub to separate system - Codeblocks - Many new HTTP examples added - CONTENTS.md &amp; CONTRIBUTOR_GUIDE.md - Updated for new workflows</p> <p>Key Impact: Eliminates manual package list maintenance across scripts and GitHub Actions.</p>"},{"location":"release-notes/v1.3.0/","title":"V1.3.0","text":""},{"location":"release-notes/v1.3.0/#new-pipeline-package","title":"New Pipeline Package","text":"<ul> <li>Pipeline processing - New <code>packages/pipeline/</code> with pipeline components</li> <li>State and Result Aware Transformations - Apply transformations while maintaining computation integrity</li> <li>Conditional Processing - Execute steps based on runtime conditions</li> <li>Tap Operations - Inspect and modify data without affecting main flow</li> <li>ProcessingState Container - New immutable state wrapper with <code>Result&lt;T&gt;</code> monad and metadata via TagMap</li> <li>PipelineBuilder &amp; PendingExecution - Lazy evaluation with fluent builder pattern for pipeline construction</li> <li>Enhanced Operator System - Comprehensive set of operators: Call, ConditionalCall, Tap, Skip, Fail, and observability operators</li> <li>Advanced Error Handling - Sophisticated error strategies with <code>ErrorTag</code> and <code>CompositeException</code> support</li> </ul>"},{"location":"release-notes/v1.3.0/#utils-package-enhancements","title":"Utils Package Enhancements","text":"<ul> <li>Result Improvements - Extended Result type with better error handling and composition support</li> <li>Composite Exceptions - Better error aggregation with <code>CompositeException</code></li> <li>FrontMatter Parser - parsing front matter in documents uses Symfony YAML (replaced <code>webuni/front-matter</code>)</li> <li>Clock and Duration - time management components (<code>ClockInterface</code>, <code>SystemClock</code>, <code>VirtualClock</code>, <code>Duration</code>)</li> </ul>"},{"location":"release-notes/v1.3.0/#core-library-improvements","title":"Core Library Improvements","text":"<ul> <li>Response Generation - Enhanced <code>ResponseGenerator</code> with better partial response validation</li> <li>Partials Generator - Improved <code>PartialsGenerator</code> with unified naming and optimized processing</li> <li>Request Materialization - Streamlined request handling and validation flows</li> </ul>"},{"location":"release-notes/v1.3.0/#doc-generation-refactoring-in-progress","title":"Doc generation Refactoring (in progress)","text":"<ul> <li>Streamlined Architecture - Cleaner separation of concerns for documentation generation</li> <li>Enhanced Documentation Config - Better configuration management for docs generation</li> <li>Archived Legacy Components - Moved old doc generation components to archived folder</li> </ul>"},{"location":"release-notes/v1.3.0/#repository-structure","title":"Repository Structure","text":"<ul> <li>Script Modernization - Renamed <code>create-package.php</code> to <code>make-package</code> script</li> <li>Data Directory - Moved empty package templates to <code>data/empty-new/</code></li> </ul>"},{"location":"release-notes/v1.3.0/#codebase-cleanup","title":"Codebase Cleanup","text":"<ul> <li>Repository Organization - Better package structure with centralized configuration in packages.json</li> <li>Improved Gitignore - Enhanced ignore patterns across all packages</li> </ul> <p>Full Changelog: v1.2.0...v1.3.0</p>"},{"location":"release-notes/v1.4.0/","title":"V1.4.0","text":""},{"location":"release-notes/v1.4.0/#core-changes","title":"Core Changes","text":"<ul> <li>Migration to <code>Pipeline</code> - Library code migrated to use new <code>Pipeline</code> replacing legacy <code>RawChain</code> and <code>ResultChain</code></li> <li>Response Generation - Enhanced <code>ResponseGenerator</code> with improved error handling using <code>CanCarryState</code> interface</li> <li>Partial Response Validation - Updated validation flow to use new state interface</li> <li>Type Safety Improvements - Better type annotations and interface compliance across pipeline components</li> </ul>"},{"location":"release-notes/v1.4.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>JsonSchema factory methods - Parameter order changed in factory methods:</li> <li>Old: <code>JsonSchema::string($name, $nullable, $description)</code></li> <li>New: <code>JsonSchema::string($name, $description, $title, $nullable)</code></li> <li>Pipeline interfaces - Introduction of <code>CanCarryState</code> interface may affect custom pipeline processors (update type hints from <code>ProcessingState</code> to <code>CanCarryState</code>)</li> </ul>"},{"location":"release-notes/v1.4.0/#jsonschema-package","title":"JsonSchema Package","text":"<ul> <li>Fixed parameter ordering - Corrected <code>JsonSchema</code> factory method parameter order for consistent API across <code>string()</code>, <code>integer()</code>, <code>number()</code>, <code>boolean()</code>, <code>enum()</code>, <code>array()</code>, and <code>collection()</code> methods</li> <li>Enhanced nullable handling - Fixed bug where nullable property was incorrectly set when description was passed as second parameter</li> </ul>"},{"location":"release-notes/v1.4.0/#pipeline-package","title":"Pipeline Package","text":"<ul> <li>CanCarryState Interface - Introduced new <code>CanCarryState</code> interface for improved state management abstraction</li> <li>ProcessingState Optimizations - Slimmed down <code>ProcessingState</code> class for cleaner API</li> <li>Enhanced Error Handling - Improved error extraction and processing with better type safety</li> </ul>"},{"location":"release-notes/v1.4.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>JsonSchema nullable property - Fixed critical bug where <code>nullable: true</code> was incorrectly set in JSON schema output when description was passed as nullable parameter</li> <li>Parameter type coercion - Resolved issue where string descriptions were being cast to boolean for nullable property</li> </ul>"},{"location":"release-notes/v1.4.0/#documentation","title":"Documentation","text":"<ul> <li>Pipeline Documentation - New <code>CHEATSHEET.md</code> and <code>OVERVIEW.md</code> with current API patterns</li> </ul> <p>Full Changelog: v1.3.0...v1.4.0</p>"},{"location":"release-notes/versions/","title":"Versions","text":""},{"location":"release-notes/versions/#versioning","title":"Versioning","text":""},{"location":"release-notes/versions/#versioning-rules","title":"Versioning Rules","text":"<p>Starting from version 1.0.0 Instructor follows semantic versioning (SemVer) with version numbers in the format x.y.z (e.g., 1.2.3, where x is the major version, y is the minor version, and z is the patch version).</p> <p>Use these rules to plan updates for the Instructor library:</p> <ul> <li>Major version (x): Incremented for significant changes, such as extensive refactoring of the core library or breaking changes to public APIs that are incompatible with previous versions. Major version updates may not be fully incompatible, but compatibility depends on the specific changes. Always consult the upgrade guide for the corresponding major version to understand the impact.</li> <li>Minor version (y): Incremented for backward-compatible additions, such as new features or components, or for breaking changes to a limited subset of public APIs (e.g., modifying or removing specific APIs). While minor versions aim to maintain compatibility, breaking changes in these releases may affect some use cases. Refer to the upgrade guide for details.</li> <li>Patch version (z): Incremented for backward-compatible bug fixes, security patches, or minor enhancements that do not affect existing functionality. In rare cases, a patch release may include breaking changes to fix a completely unusable feature, but these changes are not treated as minor version updates since the affected functionality was already broken. New features or components introduced in patch releases are designed to be backward-compatible and should not impact existing code.</li> </ul>"},{"location":"release-notes/versions/#upgrading-instructor","title":"Upgrading Instructor","text":"<p>When upgrading the Instructor library, follow these guidelines:</p> <ul> <li>Major (x) or Minor (y) Upgrades: Review the upgrade guide for the specific version in the documentation. These upgrades may include breaking changes or new features that require code adjustments.</li> <li>Patch (z) Upgrades: These are backward-compatible and can typically be applied by running composer update instructor-php in your project\u2019s root directory to update the dependency. No additional changes are usually required.</li> </ul> <p>We recommend upgrading all Instructor components together to ensure a consistent development experience, rather than updating individual components separately.</p>"},{"location":"snippets/snippet-intro/","title":"Snippet intro","text":"<p>One of the core principles of software development is DRY (Don't Repeat Yourself). This is a principle that apply to documentation as well. If you find yourself repeating the same content in multiple places, you should consider creating a custom snippet to keep your content in sync.</p>"},{"location":"snippets/snippet-intro/#setting-up","title":"Setting up","text":"<p>The first step to data processing with LLMs is setting up your editing environments.</p> <p>          Setup your API keys in .env file to access LLM API provider               Run examples to see how Instructor in action      </p>"},{"location":"snippets/snippet-intro/#using-instructor","title":"Using Instructor","text":"<p>Learn how to use Instructor to process your data with LLMs.</p> <p>          Understand basic concepts behind Instructor               Learn how to use Instructor in your projects               Find out the ways to define response data models               Use validation to automatically retry for incorrect LLM responses      </p>"}]}