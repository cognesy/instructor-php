{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor for PHP","text":"<p>Instructor for PHP is a lightweight library that makes it easy to get structured outputs from Large Language Models (LLMs). Built on top of modern PHP 8.2+ features, it provides a simple, type-safe way to work with AI models.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Type Safety: Full PHP 8.2+ type system support with strict typing</li> <li>Multiple LLM Support: Works with OpenAI, Anthropic, Gemini, Cohere, and more</li> <li>Validation: Built-in validation with custom rules and LLM-powered validation</li> <li>Streaming: Real-time partial object updates for better UX</li> <li>Function Calling: Native support for LLM function/tool calling</li> <li>Zero Dependencies: Clean, lightweight implementation</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Person {\n    public string $name;\n    public int $age;\n    public string $occupation;\n}\n\n$text = \"Extract: Jason is 25 years old and works as a software engineer.\";\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\necho $person-&gt;name; // \"Jason\"\necho $person-&gt;age;  // 25\necho $person-&gt;occupation; // \"software engineer\"\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your path:</p> <ul> <li>Quick Start - Get up and running in 5 minutes</li> <li>Setup Guide - Detailed installation and configuration</li> <li>Cookbook - Practical examples and recipes</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>This project consists of several modular packages:</p> <ul> <li>Instructor - Main structured output library</li> <li>Polyglot - Low-level LLM abstraction layer  </li> <li>HTTP Client - Flexible HTTP client for API calls</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: cognesy/instructor-php</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Join the conversation</li> </ul> <p>Instructor for PHP - Making AI outputs predictable and type-safe.</p>"},{"location":"cookbook/contributing/","title":"Contributing","text":""},{"location":"cookbook/contributing/#were-looking-for-your-help","title":"We're looking for your help","text":"<p>We're looking for a bunch more examples.</p> <p>If you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul> <p>We're also looking for help to catch up with the features available in Instructor Hub for Python (see: https://github.com/jxnl/instructor/blob/main/docs/hub/index.md).</p> <ul> <li> Better viewer with pagination</li> <li> Examples database</li> <li> Pulling in the code to your own dir, so you can get started with the API</li> </ul>"},{"location":"cookbook/contributing/#how-to-contribute","title":"How to contribute","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single .php file.</li> <li>Please include documentation in the file - check existing examples for the format.</li> <li>Make sure that the code is tested.</li> </ol> <pre><code>// @snippet-id=12e5\nnamespace Cognesy\\Polyglot\\Examples;\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\necho \"Hello, world!\\n\";\n</code></pre>"},{"location":"cookbook/introduction/","title":"Instructor Cookbooks","text":""},{"location":"cookbook/introduction/#overview","title":"Overview","text":"<p>Welcome to Instructor cookbooks. The goal of this section is to provide a set of tutorials and examples to help you get started.</p> <p>Instructor comes with a CLI tool that allows you to view and interact with the tutorials and examples and allows you to find the code snippets you may need to get solution to your problem.</p> <p>     Examples are only available with Instructor project cloned locally.     We did not want to include them in the Composer package to keep it lightweight. </p>"},{"location":"cookbook/introduction/#step-1-clone-instructor-project-from-github","title":"Step 1: Clone Instructor project from Github","text":"<p>To get access to the tutorials and examples, you need to clone the Instructor project from Github:</p> <pre><code>$ git clone https://github.com/cognesy/instructor-php.git\n</code></pre>"},{"location":"cookbook/introduction/#step-2-create-env-file","title":"Step 2: Create <code>.env</code> file","text":"<p>Create a <code>.env</code> file in the root directory of your copy of Instructor project and set your LLM API key(s). You can use the <code>.env-dist</code> file as a template.</p>"},{"location":"cookbook/introduction/#step-3-check-the-available-tutorials","title":"Step 3: Check the available tutorials","text":"<p>You can check the available tutorials and examples by running the following command in terminal:</p> <pre><code>$ ./bin/instructor-hub list\n</code></pre>"},{"location":"cookbook/introduction/#available-cli-commands","title":"Available CLI Commands","text":""},{"location":"cookbook/introduction/#list-cookbooks","title":"List Cookbooks","text":"<p>Run <code>./bin/instructor-hub list</code> you can see all the available tutorials and examples.</p> <pre><code>$ ./bin/instructor-hub list\n</code></pre>"},{"location":"cookbook/introduction/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>./bin/instructor-hub show {id}</code> to see the full tutorial in the terminal.</p> <pre><code>$ ./bin/instructor-hub show {id}\n</code></pre> <p>Currently, there is no way to page through the tutorial - feel free to contribute :)</p>"},{"location":"cookbook/introduction/#running-a-cookbook","title":"Running a Cookbook","text":"<p>To run a tutorial, you run <code>./bin/instructor-hub run {id}</code> in terminal - it will execute the code and show the output. You need to have your OPENAI_API_KEY set in your environment (.env file in root directory of your copy of instructor-php repo).</p> <pre><code>$ ./bin/instructor-hub run {id}\n</code></pre>"},{"location":"cookbook/introduction/#running-all-cookbooks","title":"Running all Cookbooks","text":"<p>This is mostly for testing if cookbooks are executed properly, but you can run <code>./bin/instructor-hub all {id}</code> to run all the tutorials and examples in the terminal, starting from the one you specify.</p> <pre><code>$ ./bin/instructor-hub all {id}\n</code></pre>"},{"location":"cookbook/instructor/advanced/config_providers/","title":"Use custom configuration providers","text":""},{"location":"cookbook/instructor/advanced/config_providers/#overview","title":"Overview","text":"<p>You can inject your own configuration providers to StructuredOutput class. This is useful for integration with your preferred framework (e.g. Symfony, Laravel).</p>"},{"location":"cookbook/instructor/advanced/config_providers/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Adbar\\Dot;\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass CustomConfigProvider implements CanProvideConfig\n{\n    private Dot $dot;\n\n    public function __construct(array $config = []) {\n        $this-&gt;dot = new Dot($config);\n    }\n\n    #[\\Override]\n    public function get(string $path, mixed $default = null): mixed {\n        return $this-&gt;dot-&gt;get($path, $default);\n    }\n\n    #[\\Override]\n    public function has(string $path): bool {\n        return $this-&gt;dot-&gt;has($path);\n    }\n}\n\n$configData = [\n    'http' =&gt; [\n        'defaultPreset' =&gt; 'symfony',\n        'presets' =&gt; [\n            'symfony' =&gt; [\n                'driver' =&gt; 'symfony',\n                'connectTimeout' =&gt; 10,\n                'requestTimeout' =&gt; 30,\n                'idleTimeout' =&gt; -1,\n                'maxConcurrent' =&gt; 5,\n                'poolTimeout' =&gt; 60,\n                'failOnError' =&gt; true,\n            ],\n            // Add more HTTP presets as needed\n        ],\n    ],\n    'debug' =&gt; [\n        'defaultPreset' =&gt; 'off',\n        'presets' =&gt; [\n            'off' =&gt; [\n                'httpEnabled' =&gt; false,\n            ],\n            'on' =&gt; [\n                'httpEnabled' =&gt; true,\n                'httpTrace' =&gt; true,\n                'httpRequestUrl' =&gt; true,\n                'httpRequestHeaders' =&gt; true,\n                'httpRequestBody' =&gt; true,\n                'httpResponseHeaders' =&gt; true,\n                'httpResponseBody' =&gt; true,\n                'httpResponseStream' =&gt; true,\n                'httpResponseStreamByLine' =&gt; true,\n            ],\n        ],\n    ],\n    'llm' =&gt; [\n        'defaultPreset' =&gt; 'deepseek',\n        'presets' =&gt; [\n            'deepseek' =&gt; [\n                'apiUrl' =&gt; 'https://api.deepseek.com',\n                'apiKey' =&gt; Env::get('DEEPSEEK_API_KEY'),\n                'endpoint' =&gt; '/chat/completions',\n                'model' =&gt; 'deepseek-chat',\n                'maxTokens' =&gt; 128,\n                'driver' =&gt; 'deepseek',\n            ],\n            'openai' =&gt; [\n                'apiUrl' =&gt; 'https://api.openai.com',\n                'apiKey' =&gt; Env::get('OPENAI_API_KEY'),\n                'endpoint' =&gt; '/v1/chat/completions',\n                'model' =&gt; 'gpt-4',\n                'maxTokens' =&gt; 256,\n                'driver' =&gt; 'openai',\n            ],\n        ],\n    ],\n    'structured' =&gt; [\n        'defaultPreset' =&gt; 'tools',\n        'presets' =&gt; [\n            'tools' =&gt; [\n                'outputMode' =&gt; OutputMode::Tools,\n                'useObjectReferences' =&gt; true,\n                'maxRetries' =&gt; 3,\n                'retryPrompt' =&gt; 'Please try again ...',\n                'modePrompts' =&gt; [\n                    OutputMode::MdJson-&gt;value =&gt; \"Response must validate against this JSON Schema:\\n&lt;|json_schema|&gt;\\n. Respond correctly with strict JSON object within a ```json {} ``` codeblock.\\n\",\n                    OutputMode::Json-&gt;value =&gt; \"Response must follow JSON Schema:\\n&lt;|json_schema|&gt;\\n. Respond correctly with strict JSON object.\\n\",\n                    OutputMode::JsonSchema-&gt;value =&gt; \"Response must follow provided JSON Schema. Respond correctly with strict JSON object.\\n\",\n                    OutputMode::Tools-&gt;value =&gt; \"Extract correct and accurate data from the input using provided tools.\\n\",\n                ],\n                'schemaName' =&gt; 'user_schema',\n                'toolName' =&gt; 'user_tool',\n                'toolDescription' =&gt; 'Tool to extract user information ...',\n                'chatStructure' =&gt; [\n                    'system',\n                    'pre-cached',\n                        'pre-cached-prompt', 'cached-prompt', 'post-cached-prompt',\n                        'pre-cached-examples', 'cached-examples', 'post-cached-examples',\n                        'cached-messages',\n                    'post-cached',\n                    'pre-prompt', 'prompt', 'post-prompt',\n                    'pre-examples', 'examples', 'post-examples',\n                    'pre-messages', 'messages', 'post-messages',\n                    'pre-retries', 'retries', 'post-retries'\n                ],\n                // defaultOutputClass is not used in this example\n                'outputClass' =&gt; Structure::class,\n            ]\n        ]\n    ]\n];\n\n$events = new EventDispatcher();\n$configProvider = new CustomConfigProvider($configData);\n\n$customClient = (new HttpClientBuilder(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withConfigProvider($configProvider)\n    -&gt;withPreset('symfony')\n    -&gt;create();\n\n$structuredOutput = (new StructuredOutput(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withHttpClient($customClient);\n\n// Call with custom model and execution mode\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = $structuredOutput\n    -&gt;using('deepseek') // Use 'deepseek' preset defined in our config provider\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/context_cache_structured/","title":"Context caching (structured output)","text":""},{"location":"cookbook/instructor/advanced/context_cache_structured/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching, so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note 1: Instructor supports context caching for Anthropic API and OpenAI API.</p> <p>Note 2: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/instructor/advanced/context_cache_structured/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its structured description for multiple audiences.</p> <p>Let's start by defining the data model for the project details and the properties that we want to extract or generate based on README file.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass Project {\n    public string $name;\n    public string $targetAudience;\n    /** @var string[] */\n    #[Description('Technology platform and libraries used in the project')]\n    public array $technologies;\n    /** @var string[] */\n    #[Description('Target audience domain specific features and capabilities of the project')]\n    public array $features;\n    /** @var string[] */\n    #[Description('Target audience domain specific applications and potential use cases of the project')]\n    public array $applications;\n    #[Description('Explain the purpose of the project and the target audience domain specific problems it solves')]\n    public string $description;\n    #[Description('Target audience domain specific example code in Markdown demonstrating an application of the library')]\n    public string $code;\n}\n?&gt;\n</code></pre> <p>We read the content of the README.md file and cache the context, so it can be reused for multiple requests.</p> <p><pre><code>&lt;?php\n$content = file_get_contents(__DIR__ . '/../../../README.md');\n\n$cached = (new StructuredOutput)-&gt;using('anthropic')-&gt;withCachedContext(\n    system: 'Your goal is to respond questions about the project described in the README.md file'\n        . \"\\n\\n# README.md\\n\\n\" . $content,\n    prompt: 'Respond with strict JSON object using schema:\\n&lt;|json_schema|&gt;',\n);//-&gt;withDebugPreset('on');\n?&gt;\n</code></pre> At this point we can use Instructor structured output processing to extract the project details from the README.md file into the <code>Project</code> data model.</p> <p>Let's start by asking the user to describe the project for a specific audience: P&amp;C insurance CIOs.</p> <p><pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response1 = $cached-&gt;with(\n    messages: 'Describe the project in a way compelling to my audience: P&amp;C insurance CIOs.',\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::MdJson,\n)-&gt;create();\n\n// get processed value - instance of Project class\n$project1 = $response1-&gt;get();\ndump($project1);\nassert($project1 instanceof Project);\nassert(Str::contains($project1-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage1 = $response1-&gt;response()-&gt;usage();\necho \"Usage: {$usage1-&gt;inputTokens} prompt tokens, {$usage1-&gt;cacheWriteTokens} cache write tokens\\n\";\n?&gt;\n</code></pre> Now we can use the same context to ask the user to describe the project for a different audience: boutique CMS consulting company owner.</p> <p>Anthropic API will use the context cached in the previous request to provide the response, which results in faster processing and lower costs.</p> <pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response2 = $cached-&gt;with(\n    messages: \"Describe the project in a way compelling to my audience: boutique CMS consulting company owner.\",\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::Json,\n)-&gt;create();\n\n// get the processed value - instance of Project class\n$project2 = $response2-&gt;get();\ndump($project2);\nassert($project2 instanceof Project);\nassert(Str::contains($project2-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage2 = $response2-&gt;response()-&gt;usage();\necho \"Usage: {$usage2-&gt;inputTokens} prompt tokens, {$usage2-&gt;cacheReadTokens} cache read tokens\\n\";\nassert($usage2-&gt;cacheReadTokens &gt; 0, 'Expected cache read tokens');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_config/","title":"Customize parameters of LLM driver","text":""},{"location":"cookbook/instructor/advanced/custom_config/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to Instructor. This is useful when you want to initialize OpenAI client with custom values - e.g. to call other LLMs which support OpenAI API.</p>"},{"location":"cookbook/instructor/advanced/custom_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Drivers\\Symfony\\SymfonyDriver;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$events = new EventDispatcher();\n\n// Build fully customized HTTP client\n\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 30,\n    requestTimeout: 60,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true,\n);\n\n$yourClientInstance = SymfonyHttpClient::create(['http_version' =&gt; '2.0']);\n\n$customClient = (new HttpClientBuilder)\n    -&gt;withEventBus($events)\n    -&gt;withDriver(new SymfonyDriver(\n        config: $httpConfig,\n        clientInstance: $yourClientInstance,\n        events: $events,\n    ))\n    -&gt;create();\n\n// Create instance of LLM connection preset initialized with custom parameters\n\n$llmConfig = new LLMConfig(\n    apiUrl  : 'https://api.deepseek.com',\n    apiKey  : Env::get('DEEPSEEK_API_KEY'),\n    endpoint: '/chat/completions', model: 'deepseek-chat', maxTokens: 128, driver: 'openai-compatible',\n);\n\n// Get Instructor with the default client component overridden with your own\n\n$structuredOutput = (new StructuredOutput)\n    -&gt;withEventHandler($events)\n    -&gt;withLLMConfig($llmConfig)\n    -&gt;withHttpClient($customClient);\n\n// Call with custom model and execution mode\n\n$user = $structuredOutput\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_http_client/","title":"Use custom HTTP client instance","text":""},{"location":"cookbook/instructor/advanced/custom_http_client/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/advanced/custom_http_client/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Dispatchers\\SymfonyEventDispatcher;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Symfony\\Component\\EventDispatcher\\EventDispatcher;\nuse Symfony\\Component\\HttpClient\\HttpClient;\n\n// custom Symfony components\n// custom Symfony components\n\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Call with custom model and execution mode\n\n$yourSymfonyClientInstance = HttpClient::create(['http_version' =&gt; '2.0']);\n$yourSymfonyEventDispatcher = new SymfonyEventDispatcher(new EventDispatcher());\n\n$user = (new StructuredOutput(events: $yourSymfonyEventDispatcher))\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withLLMConfigOverrides(['apiUrl' =&gt; 'https://api.openai.com/v1'])\n    -&gt;withClientInstance(\n        driverName: 'symfony',\n        clientInstance: $yourSymfonyClientInstance\n    )\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    //-&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_http_client_laravel/","title":"Use custom HTTP client instance - Laravel","text":""},{"location":"cookbook/instructor/advanced/custom_http_client_laravel/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/advanced/custom_http_client_laravel/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Illuminate\\Http\\Client\\Factory;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$yourLaravelClientInstance = new Factory();\n\n$user = (new StructuredOutput())\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withLLMConfigOverrides(['apiUrl' =&gt; 'https://api.openai.com/v1'])\n    -&gt;withClientInstance(\n        driverName: 'laravel',\n        clientInstance: $yourLaravelClientInstance\n    )\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    //-&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_llm_with_dsn/","title":"Customize parameters via DSN","text":""},{"location":"cookbook/instructor/advanced/custom_llm_with_dsn/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration data to <code>StructuredOutput</code> object with DSN string. This is useful for inline configuration or for building configuration from admin UI, CLI arguments or environment variables.</p>"},{"location":"cookbook/instructor/advanced/custom_llm_with_dsn/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new StructuredOutput)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withDsn('preset=xai,model=grok-2')\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withresponseClass(User::class)\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/custom_prompts/","title":"Custom prompts","text":""},{"location":"cookbook/instructor/advanced/custom_prompts/#overview","title":"Overview","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>request()</code> or <code>create()</code> methods.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"cookbook/instructor/advanced/custom_prompts/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$structuredOutput = (new StructuredOutput)\n    // let's dump the request data to see how customized prompts look like in requests\n    -&gt;onEvent(HttpRequestSent::class, fn(HttpRequestSent $event) =&gt; dump($event));\n\nprint(\"\\n# Request for OutputMode::Tools:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        mode: OutputMode::Tools\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\nprint(\"\\n# Request for OutputMode::Json:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::Json\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\nprint(\"\\n# Request for OutputMode::MdJson:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::MdJson\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/demonstrations/","title":"Providing example inputs and outputs","text":""},{"location":"cookbook/instructor/advanced/demonstrations/#overview","title":"Overview","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> modes, where the output is expected to be a JSON object.</p>"},{"location":"cookbook/instructor/advanced/demonstrations/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\necho \"\\nREQUEST:\\n\";\n$user = (new StructuredOutput)\n    // let's dump the request data to see how examples are used in requests\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; dump($event))\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withExamples([\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27],\n            template: \"example input:\\n&lt;|input|&gt;\\noutput:\\n```json\\n&lt;|output|&gt;\\n```\\n\",\n        ),\n    ])\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n\necho \"\\nOUTPUT:\\n\";\ndump($user);\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/function_calls/","title":"Extracting arguments of function or method","text":""},{"location":"cookbook/instructor/advanced/function_calls/#overview","title":"Overview","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"cookbook/instructor/advanced/function_calls/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass DataStore\n{\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) : void {\n        // Save user to database\n        echo \"Saving user ... saveUser('$name', $age, '$country')\\n\";\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromMethodName(DataStore::class, 'saveUser'),\n)-&gt;get();\n\necho \"\\nCalling the function with the extracted arguments:\\n\";\n(new DataStore)-&gt;saveUser(...$args);\n\necho \"\\nExtracted arguments:\\n\";\ndump($args);\n\nassert(count($args) == 3);\nexpect($args['name'] === 'Jason');\nexpect($args['age'] == 28);\nexpect($args['country'] === 'Germany');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/logging_monolog/","title":"Monolog Logging","text":""},{"location":"cookbook/instructor/advanced/logging_monolog/#overview","title":"Overview","text":"<p>Instructor allows to easily log events with Monolog library.</p>"},{"location":"cookbook/instructor/advanced/logging_monolog/#example","title":"Example","text":"pushHandler(new StreamHandler('php://stdout'));  class User {     public int $age;     public string $name; }  $user = (new StructuredOutput)     -&gt;using('openai')     -&gt;wiretap(fn(Event $e) =&gt; $log-&gt;log($e-&gt;logLevel, $e-&gt;name(), ['id' =&gt; $e-&gt;id, 'data' =&gt; $e-&gt;data]))     -&gt;withMessages(\"Jason is 25 years old and works as an engineer.\")     -&gt;withResponseClass(User::class)     -&gt;get();  assert($user-&gt;name === 'Jason'); assert($user-&gt;age === 25);  ?&gt;"},{"location":"cookbook/instructor/advanced/logging_psr/","title":"PSR-3 Logging","text":""},{"location":"cookbook/instructor/advanced/logging_psr/#overview","title":"Overview","text":"<p>Instructor allows to easily log events with any PSR-3 compliant logging library.</p>"},{"location":"cookbook/instructor/advanced/logging_psr/#example","title":"Example","text":"using('openai')     -&gt;wiretap(fn(Event $e) =&gt; $logger-&gt;log($e-&gt;logLevel, $e-&gt;name(), ['id' =&gt; $e-&gt;id, 'data' =&gt; $e-&gt;data]))     -&gt;withMessages(\"Jason is 25 years old and works as an engineer.\")     -&gt;withResponseClass(User::class)-&gt;get();  assert($user-&gt;name === 'Jason'); assert($user-&gt;age === 25);  ?&gt;"},{"location":"cookbook/instructor/advanced/partials/","title":"Streaming partial updates during inference","text":""},{"location":"cookbook/instructor/advanced/partials/#overview","title":"Overview","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated. You can use it to improve user experience by updating the UI with partial data before the full response is received.</p>"},{"location":"cookbook/instructor/advanced/partials/#example","title":"Example","text":"<p><pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public string $location;\n    /** @var UserRole[] */\n    public array $roles;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    Console::clearScreen();\n    echo \"Updated partial object received:\\n\";\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    //usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;withStreaming()\n    -&gt;onPartialUpdate(partialUpdate(...))\n    -&gt;get();\n\necho \"All tokens received, fully completed object available in `\\$user` variable.\\n\";\necho '$user = '.\"\\n\";\ndump($user);\n\nassert(!empty($user-&gt;roles));\nassert(!empty($user-&gt;hobbies));\nassert($user-&gt;location === 'San Francisco');\nassert($user-&gt;age == 25);\nassert($user-&gt;name === 'Jason');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/scalars/","title":"Extracting scalar values","text":""},{"location":"cookbook/instructor/advanced/scalars/#overview","title":"Overview","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p>"},{"location":"cookbook/instructor/advanced/scalars/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum CitizenshipGroup : string {\n    case US = \"US\";\n    case Canada = \"Canada\";\n    case Germany = \"Germany\";\n    case Other = \"Other\";\n}\n\n$text = \"His name is Jason, he is 28 years old American who lives in Germany.\";\n$value = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    prompt: 'What is user\\'s citizenship?',\n    responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n)-&gt;get();\n\n\ndump($value);\n\nassert($value instanceof CitizenshipGroup);\nexpect($value == CitizenshipGroup::Other);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/sequences/","title":"Extracting sequences of objects","text":""},{"location":"cookbook/instructor/advanced/sequences/#overview","title":"Overview","text":"<p>Sequences are a special type of response model that can be used to represent a list of objects.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p>"},{"location":"cookbook/instructor/advanced/sequences/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old. Anna is 2 years younger than him.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"OUTPUT:\\n\");\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; dump($sequence-&gt;last()))\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: $text,\n        responseModel: Sequence::of(Person::class),\n        options: ['stream' =&gt; true],\n    )\n    -&gt;get();\n\n\ndump(count($list));\n\nassert(count($list) === 4);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/streaming/","title":"Streaming","text":""},{"location":"cookbook/instructor/advanced/streaming/#overview","title":"Overview","text":"<p>Instructor can process LLM's streamed responses to provide partial response model updates that you can use to update the model with new data as the response is being generated.</p>"},{"location":"cookbook/instructor/advanced/streaming/#example","title":"Example","text":"<p><pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age = 0;\n    public string $name = '';\n    public string $location = '';\n    /** @var UserRole[] */\n    public array $roles = [];\n    /** @var string[] */\n    public array $hobbies = [];\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    Console::clearScreen();\n\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    // usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$stream = (new StructuredOutput)\n    //-&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;withStreaming()\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;stream();\n\nforeach ($stream-&gt;partials() as $partial) {\n    partialUpdate($partial);\n}\n\n$user = $stream-&gt;lastUpdate();\n\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/advanced/structures/","title":"Structures","text":""},{"location":"cookbook/instructor/advanced/structures/#overview","title":"Overview","text":"<p>Structures allow dynamically define the shape of data to be extracted by LLM, e.g. during runtime.</p> <p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns an array in the shape you defined.</p> <p>See more: Structures</p>"},{"location":"cookbook/instructor/advanced/structures/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::option('gender', ['male', 'female'], 'Gender of the person')-&gt;optional(),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n    Field::collection('favourite_books', Structure::define('book', [\n            Field::string('author', 'Book author')-&gt;optional(),\n            Field::string('title', 'Book title'),\n        ], 'Favorite book data'),\n    'Favorite books of the person'),\n], 'A person object');\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield, 50210. She is 25 years old and works as manager at McDonald's.\n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345. Her favourite books are \"The Lord\n    of the Rings\" and \"The Hobbit\" by JRR Tolkien.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;get();\n\nprint(\"OUTPUT:\\n\");\nprint(\"Name: \" . $person-&gt;name . \"\\n\");\nprint(\"Age: \" . $person-&gt;age . \"\\n\");\nprint(\"Gender: \" . $person-&gt;gender . \"\\n\");\nprint(\"Address / city: \" . $person-&gt;address-&gt;city . \"\\n\");\nprint(\"Address / ZIP: \" . $person-&gt;address-&gt;zip . \"\\n\");\nprint(\"Role: \" . $person-&gt;role-&gt;value . \"\\n\");\nprint(\"Favourite books:\\n\");\nforeach ($person-&gt;favourite_books as $book) {\n    print(\"  - \" . $book-&gt;title . \" by \" . $book-&gt;author . \"\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/a21/","title":"A21","text":""},{"location":"cookbook/instructor/api_support/a21/#overview","title":"Overview","text":"<p>Support for A21 Jamba - MAMBA architecture models, very strong at handling long context.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/a21/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection preset\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('a21');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/anthropic/","title":"Anthropic","text":""},{"location":"cookbook/instructor/api_support/anthropic/#overview","title":"Overview","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - OutputMode::MdJson, OutputMode::Json - supported - OutputMode::Tools - not supported yet</p>"},{"location":"cookbook/instructor/api_support/anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('anthropic');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'claude-3-haiku-20240307',\n    mode: OutputMode::Tools,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/azure_openai/","title":"Azure OpenAI","text":""},{"location":"cookbook/instructor/api_support/azure_openai/#overview","title":"Overview","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p>"},{"location":"cookbook/instructor/api_support/azure_openai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('azure');\n\n// Call with your model name and preferred execution mode\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/cerebras/","title":"Cerebras","text":""},{"location":"cookbook/instructor/api_support/cerebras/#overview","title":"Overview","text":"<p>Support for Cerebras API which uses custom hardware for super fast inference. Cerebras provides Llama models.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/cerebras/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('cerebras');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'llama3.1-8b', // set your own value/source\n    mode: OutputMode::Json,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/cohere/","title":"Cohere","text":""},{"location":"cookbook/instructor/api_support/cohere/#overview","title":"Overview","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - OutputMode::MdJson - supported, recommended as a fallback from JSON mode  - OutputMode::Json - supported, recommended  - OutputMode::Tools - partially supported, not recommended</p> <p>Reasons OutputMode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple, flat data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul>"},{"location":"cookbook/instructor/api_support/cohere/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('cohere');\n    //-&gt;withDebugPreset('on');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'command-r-plus-08-2024',\n    mode: OutputMode::Tools,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/deepseek/","title":"DeepSeek","text":""},{"location":"cookbook/instructor/api_support/deepseek/#overview","title":"Overview","text":"<p>Support for DeepSeek API which provides strong models at affordable price.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/deepseek/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('deepseek');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'deepseek-chat', // set your own value/source\n    mode: OutputMode::JsonSchema,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/fireworks/","title":"Fireworks.ai","text":""},{"location":"cookbook/instructor/api_support/fireworks/#overview","title":"Overview","text":"<p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - selected models - OutputMode::Json - selected models - OutputMode::MdJson</p>"},{"location":"cookbook/instructor/api_support/fireworks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('fireworks');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'accounts/fireworks/models/llama4-maverick-instruct-basic',\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/google_gemini/","title":"Google Gemini","text":""},{"location":"cookbook/instructor/api_support/google_gemini/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('gemini');\n    //-&gt;withDebugPreset('detailed')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;printDebug());\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        //options: ['stream' =&gt; true],\n        mode: OutputMode::Json,\n    )\n    -&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/google_gemini_oai/","title":"Google Gemini (OpenAI compatible API)","text":""},{"location":"cookbook/instructor/api_support/google_gemini_oai/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini's OpenAI compatible API.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('gemini-oai');\n    //-&gt;withDebugPreset('detailed')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;printDebug());\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        //options: ['stream' =&gt; true],\n        mode: OutputMode::MdJson,\n    )\n    -&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/groq/","title":"Groq","text":""},{"location":"cookbook/instructor/api_support/groq/#overview","title":"Overview","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma.</p> <p>Supported modes depend on the specific model, but generally include:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Groq API.</p>"},{"location":"cookbook/instructor/api_support/groq/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public string $name;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n    public string $username;\n    public ?int $age;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('groq');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n        ],[\n            'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jx90.',\n            'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'], 'username' =&gt; 'jx90', 'age' =&gt; 19],\n        ]],\n        model: 'llama-3.3-70b-versatile', //'gemma2-9b-it',\n        maxRetries: 2,\n        options: ['temperature' =&gt; 0.5],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/huggingface/","title":"Hugging Face","text":""},{"location":"cookbook/instructor/api_support/huggingface/#overview","title":"Overview","text":"<p>You can use Instructor to parse structured output from LLMs using Hugging Face API. This example demonstrates how to parse user data into a structured model using JSON Schema.</p>"},{"location":"cookbook/instructor/api_support/huggingface/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public string $firstName;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n    public string $username;\n    public ?int $age;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('huggingface');\n    //-&gt;withDebugPreset('on');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n        examples: [[\n                      'input' =&gt; 'I\\'ve got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n                      'output' =&gt; ['firstName' =&gt; 'Frank', 'age' =&gt; 30, 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n                  ],[\n                      'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jx90.',\n                      'output' =&gt; ['firstName' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'], 'username' =&gt; 'jx90', 'age' =&gt; 19],\n                  ]],\n        //model: 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',\n        maxRetries: 2,\n        options: ['temperature' =&gt; 0.5],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;firstName));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;firstName === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/inception/","title":"Inception","text":""},{"location":"cookbook/instructor/api_support/inception/#overview","title":"Overview","text":"<p>Inception API provides OpenAI-compatible endpoints for chat completions.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (supported)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/inception/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('inception');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'mercury', // set your own value/source\n    mode: OutputMode::MdJson,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/meta/","title":"Meta","text":""},{"location":"cookbook/instructor/api_support/meta/#overview","title":"Overview","text":"<p>Instructor supports Meta LLM inference API. You can find the details on how to configure below.</p>"},{"location":"cookbook/instructor/api_support/meta/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('meta');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/minimaxi/","title":"Minimaxi","text":""},{"location":"cookbook/instructor/api_support/minimaxi/#overview","title":"Overview","text":"<p>Support for Minimaxi's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/instructor/api_support/minimaxi/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('minimaxi');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'MiniMax-Text-01', // set your own value/source\n        mode: OutputMode::MdJson,\n    )\n    -&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/mistralai/","title":"Mistral AI","text":""},{"location":"cookbook/instructor/api_support/mistralai/#overview","title":"Overview","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility:  - OutputMode::Tools - supported (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::Json - recommended (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::MdJson - fallback mode (Mistral 7B / Mixtral 8x7B)</p>"},{"location":"cookbook/instructor/api_support/mistralai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('mistral');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @jx90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'jx90', 'age' =&gt; 30],\n    ]],\n    model: 'mistral-small-latest', //'open-mixtral-8x7b',\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/moonshotai/","title":"MoonshotAI","text":""},{"location":"cookbook/instructor/api_support/moonshotai/#overview","title":"Overview","text":"<p>Support for MoonshotAI's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported)</p>"},{"location":"cookbook/instructor/api_support/moonshotai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('moonshot-kimi');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'kimi-latest', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/ollama/","title":"Local / Ollama","text":""},{"location":"cookbook/instructor/api_support/ollama/#overview","title":"Overview","text":"<p>You can use Instructor with local Ollama instance.</p> <p>Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model for complex data schemas.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode, works with any capable model  - OutputMode::Json - recommended  - OutputMode::Tools - supported (for selected models - check Ollama docs)</p>"},{"location":"cookbook/instructor/api_support/ollama/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('ollama');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. Asked to connect via Twitter @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'], 'username' =&gt; 'frankch', 'age' =&gt; null],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @j90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'j90', 'age' =&gt; 30],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/openai/","title":"OpenAI","text":""},{"location":"cookbook/instructor/api_support/openai/#overview","title":"Overview","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended for new models)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/openai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openai');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/openrouter/","title":"OpenRouter","text":""},{"location":"cookbook/instructor/api_support/openrouter/#overview","title":"Overview","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p>"},{"location":"cookbook/instructor/api_support/openrouter/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openrouter');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/perplexity/","title":"Perplexity","text":""},{"location":"cookbook/instructor/api_support/perplexity/#overview","title":"Overview","text":"<p>You can use Instructor with Perplexity API. Perplexity is an API that provides access to a large language model (LLM) for various tasks, including search and text generation.</p>"},{"location":"cookbook/instructor/api_support/perplexity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('perplexity');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/sambanova/","title":"SambaNova","text":""},{"location":"cookbook/instructor/api_support/sambanova/#overview","title":"Overview","text":"<p>Support for SambaNova's API, which provide fast inference endpoints for Llama and Qwen LLMs.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/instructor/api_support/sambanova/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('sambanova');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'Meta-Llama-3.1-8B-Instruct', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/togetherai/","title":"Together.ai","text":""},{"location":"cookbook/instructor/api_support/togetherai/#overview","title":"Overview","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support OutputMode::Tools or OutputMode::Json, which are much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - supported for selected models - OutputMode::Json - supported for selected models - OutputMode::MdJson - fallback mode</p>"},{"location":"cookbook/instructor/api_support/togetherai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('together');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @jx90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'jx90', 'age' =&gt; 30],\n    ]],\n    //model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n    //options: ['stream' =&gt; true ]\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/api_support/xai/","title":"xAI / Grok","text":""},{"location":"cookbook/instructor/api_support/xai/#overview","title":"Overview","text":"<p>Support for xAI's API, which offers access to X.com's Grok model.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/instructor/api_support/xai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('xai');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/attributes/","title":"Using attributes","text":""},{"location":"cookbook/instructor/basics/attributes/#overview","title":"Overview","text":"<p>Instructor supports <code>Description</code> and <code>Instructions</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p>"},{"location":"cookbook/instructor/basics/attributes/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make user name ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    #[Instructions(\"Make the profession name lowercase\")]\n    public string $job;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = \"Jason is 25 years old, 10K runner, speaker and an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n)-&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert($user-&gt;name === \"JASON\");\nassert(isset($user-&gt;age));\nassert($user-&gt;age === 25);\nassert(isset($user-&gt;job));\nassert($user-&gt;job === \"engineer\");\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/basic_use/","title":"Basic use","text":""},{"location":"cookbook/instructor/basics/basic_use/#overview","title":"Overview","text":"<p>Instructor allows you to use large language models to extract information from the text (or content of chat messages), while following the structure you define.</p> <p>LLM does not 'parse' the text to find and retrieve the information. Extraction leverages LLM ability to comprehend provided text and infer the meaning of the information it contains to fill fields of the response object with values that match the types and semantics of the class fields.</p> <p>The simplest way to use the Instructor is to call the <code>respond</code> method on the Instructor instance. This method takes a string (or an array of strings in the format of OpenAI chat messages) as input and returns a data extracted from provided text (or chat) using the LLM inference.</p> <p>Returned object will contain the values of fields extracted from the text.</p> <p>The format of the extracted data is defined by the response model, which in this case is a simple PHP class with some public properties.</p>"},{"location":"cookbook/instructor/basics/basic_use/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Step 2: Get the text (or chat messages) you want to use as context\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/basic_use_mixin/","title":"Basic use via mixin","text":""},{"location":"cookbook/instructor/basics/basic_use_mixin/#overview","title":"Overview","text":"<p>Instructor provides <code>HandlesSelfInference</code> trait that you can use to enable extraction capabilities directly on class via static <code>infer()</code> method.</p> <p><code>infer()</code> method returns an instance of the class with the data extracted using the Instructor.</p> <p><code>infer()</code> method has following signature (you can also find it in the <code>CanSelfInfer</code> interface):</p> <pre><code>static public function infer(\n    string|array $messages, // (required) The message(s) to infer data from\n    string $prompt = '',    // (optional) The prompt to use for inference\n    array $examples = [],   // (optional) Examples to include in the prompt\n    string $model = '',     // (optional) The model to use for inference (otherwise - use default)\n    int $maxRetries = 2,    // (optional) The number of retries in case of validation failure\n    array $options = [],    // (optional) Additional data to pass to the Instructor or LLM API\n    Mode $mode = OutputMode::Tools, // (optional) The mode to use for inference\n    ?LLM $llm = null         // (optional) LLM instance to use for inference\n) : static;\n</code></pre>"},{"location":"cookbook/instructor/basics/basic_use_mixin/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Mixin\\HandlesSelfInference;\n\nclass User {\n    use HandlesSelfInference;\n\n    public int $age;\n    public string $name;\n}\n\n$user = User::infer(\"Jason is 25 years old and works as an engineer.\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/constructor_parameters/","title":"Specifying required and optional parameters via constructor","text":""},{"location":"cookbook/instructor/basics/constructor_parameters/#overview","title":"Overview","text":"<p>Instructor can extract data from the LLM response and use it to instantiate an object via constructor parameters.</p> <p>Instructor will use the constructor parameters nullability and default values to determine which parameters are required and which are optional.</p>"},{"location":"cookbook/instructor/basics/constructor_parameters/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserWithConstructor\n{\n    public string $name;\n    private ?int $age;\n    private string $location;\n    private string $password;\n\n    public function __construct(\n        string $name,                 // required - required in constructor, required internally\n        int $age,                     // required - required in constructor, even if nullable internally\n        ?string $location,            // optional - nullable in constructor, even if required internally\n        string $password = '123admin' // optional - has a default value, even if required internally\n    ) {\n        $this-&gt;name = $name;\n        $this-&gt;age = $age;\n        $this-&gt;location = $location ?? '';\n        $this-&gt;password = $password;\n    }\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getLocation(): string {\n        return $this-&gt;location;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithConstructor::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;getAge() === 25);\nassert($user-&gt;getPassword() === '123admin');\nassert($user-&gt;getLocation() === ''); // default value for location\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/custom_validation/","title":"Custom validation using Symfony Validator","text":""},{"location":"cookbook/instructor/basics/custom_validation/#overview","title":"Overview","text":"<p>Instructor uses Symfony validation component to validate properties of extracted data. Symfony offers you #[Assert/Callback] annotation to build fully customized validation logic.</p>"},{"location":"cookbook/instructor/basics/custom_validation/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be all uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"JASON\");\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/fluent_api/","title":"Fluent API","text":""},{"location":"cookbook/instructor/basics/fluent_api/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/basics/fluent_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withModel('gpt-3.5-turbo')\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/getters_and_setters/","title":"Getters and setters","text":""},{"location":"cookbook/instructor/basics/getters_and_setters/#overview","title":"Overview","text":"<p>Instructor can extract data from the LLM response and use it to instantiate an object via setter methods.</p> <p>If given property is not public and has no matching constructor params Instructor will use the setter method parameter nullability and default value to determine if property is required.</p>"},{"location":"cookbook/instructor/basics/getters_and_setters/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass UserWithSetter\n{\n    #[Description('Name of the user or empty string if not provided')]\n    private string $name;\n    #[Description('Age of the user or 0 if not provided')]\n    private ?int $age;\n    #[Description('Location of the user or empty string if not provided')]\n    private string $location;\n    #[Description('Password of the user or empty string if not provided')]\n    private string $password;\n\n    // `name` is required (not nullable parameter), if data exists in the answer the setter will be called, but may have empty value\n    public function setName(string $name): void {\n        $this-&gt;name = $name ?: 'Jason';\n    }\n\n    public function getName(): string {\n        return $this-&gt;name ?? '';\n    }\n\n    // `age` is optional (nullable parameter), setter will not be called if LLM does not infer the data\n    public function setAge(int $age): void {\n        $this-&gt;age = (int) $age;\n    }\n\n    public function getAge(): int {\n        return $this-&gt;age ?? 0;\n    }\n\n    public function setLocation(?string $location): void {\n        $this-&gt;location = $location;\n    }\n\n    public function getLocation(): string {\n        return $this-&gt;location;\n    }\n\n    public function setPassword(string|null $password = ''): void {\n        $this-&gt;password = $password ?: '123admin';\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    This user is living in San Francisco. His password is.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithSetter::class)\n    -&gt;withMaxRetries(2)\n    //-&gt;withModel('claude-3-7-sonnet-20250219')\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;getName() === \"Jason\"); // called - but set to default value as LLM inferred empty name\nassert($user-&gt;getAge() === 0); // not called - property value not inferred by LLM\nassert($user-&gt;getPassword() === '123admin'); // called - but set to default value as LLM inferred empty password\nassert($user-&gt;getLocation() === 'San Francisco'); // called - LLM inferred location from the text\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/maybe/","title":"Handling errors with `Maybe` helper class","text":""},{"location":"cookbook/instructor/basics/maybe/#overview","title":"Overview","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p>"},{"location":"cookbook/instructor/basics/maybe/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n\n$text = 'We have no information about our new developer.';\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class),\n    model: 'gpt-4o-mini',\n    mode: OutputMode::MdJson,\n)-&gt;get();\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === false);\nassert(!empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() === null);\n\n$text = \"Jason is our new developer, he is 25 years old.\";\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class)\n)-&gt;get();\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === true);\nassert(empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() != null);\nassert($maybeUser-&gt;get() instanceof User);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/messages_api/","title":"Messages API","text":""},{"location":"cookbook/instructor/basics/messages_api/#overview","title":"Overview","text":"<p>Instructor allows you to use <code>Messages</code> and <code>Message</code> classes to work with chat messages and their sequences.</p>"},{"location":"cookbook/instructor/basics/messages_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Messages\\Message;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Str;\n\nclass Code {\n    public string $code;\n    public string $programmingLanguage;\n    public string $codeDescription;\n}\n\n$messages = Messages::empty()\n    -&gt;asSystem('You are a senior PHP8 backend developer.')\n    -&gt;asDeveloper('Be concise and use modern PHP8.2+ features.') // OpenAI developer role is supported and normalized for other providers\n    -&gt;asUser([\n        'What is the best way to handle errors in PHP8?',\n        'Provide a code example.',\n        'Use modern PHP8.2+ features.',\n    ])\n    -&gt;asAssistant('I will provide a code example that demonstrates how to handle errors using try-catch. Any specific domain?');\n\n$messages-&gt;appendMessage(Message::asUser('Make it insurance related.'));\n\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$code = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($messages)\n    -&gt;withResponseModel(Code::class)\n    -&gt;withOutputMode(OutputMode::MdJson)\n    -&gt;get();\n\nprint(\"Extracted data:\\n\");\ndump($code);\n\nassert(!empty($code-&gt;code));\nassert(!empty($code-&gt;codeDescription));\nassert(Str::contains(strtolower($code-&gt;programmingLanguage), 'php'));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/mixed_type_property/","title":"Mixed Type Property","text":""},{"location":"cookbook/instructor/basics/mixed_type_property/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/basics/mixed_type_property/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass UserWithMixedTypeProperty\n{\n    public string $name;\n    #[Description('Any extra information about the user')]\n    public mixed $extraInfo;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He plays football and loves to travel.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithMixedTypeProperty::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;extraInfo !== ''); // not empty, but can be any type\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/modes/","title":"Modes","text":""},{"location":"cookbook/instructor/basics/modes/#overview","title":"Overview","text":"<p>Instructor supports several ways to extract data from the response:</p> <ul> <li><code>OutputMode::Tools</code> - uses OpenAI-style tool calls to get the language    model to generate JSON following the schema,</li> <li><code>OutputMode::JsonSchema</code> - guarantees output matching JSON Schema via    Context Free Grammar, does not support optional properties,</li> <li><code>OutputMode::Json</code> - JSON mode, response follows provided JSON Schema,</li> <li><code>OutputMode::MdJson</code> - uses prompting to get the language model to    generate JSON following the schema.</li> </ul> <p>Note: not all modes are supported by all models or providers.</p> <p>Mode can be set via parameter of <code>StructuredOutput::create()</code> method.</p>"},{"location":"cookbook/instructor/basics/modes/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$text = \"Jason is 25 years old and works as an engineer.\";\n\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n$structuredOutput = new StructuredOutput;\n\n// CASE 1 - OutputMode::Tools\nprint(\"\\n1. Extracting structured data using LLM - OutputMode::Tools\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::Tools,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 2 - OutputMode::JsonSchema\nprint(\"\\n2. Extracting structured data using LLM - OutputMode::JsonSchema\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 3 - OutputMode::Json\nprint(\"\\n3. Extracting structured data using LLM - OutputMode::Json\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::Json,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 4 - OutputMode::MdJson\nprint(\"\\n4. Extracting structured data using LLM - OutputMode::MdJson\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::MdJson,\n)-&gt;get();\ncheck($user);\ndump($user);\n\nfunction check(User $user) {\n    assert(isset($user-&gt;name));\n    assert(isset($user-&gt;age));\n    assert($user-&gt;name === 'Jason');\n    assert($user-&gt;age === 25);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/optional_fields/","title":"Making some fields optional","text":""},{"location":"cookbook/instructor/basics/optional_fields/#overview","title":"Overview","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) to declare component fields which are optional. Set a default value to prevent undesired defaults like nulls or empty strings.</p>"},{"location":"cookbook/instructor/basics/optional_fields/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $age;\n    public string $firstName;\n    public ?string $lastName;\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages('Jason is 25 years old.')\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;get();\n\ndump($user);\n\nassert(!isset($user-&gt;lastName) || $user-&gt;lastName === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/public_vs_private/","title":"Private vs public object field","text":""},{"location":"cookbook/instructor/basics/public_vs_private/#overview","title":"Overview","text":"<p>Instructor only sets accessible fields of the object with the data provided by LLM.</p> <p>Private and protected fields are left unchanged, unless:  - class has constructor with parameters matching one or more property names - in such    situation object will be hydrated with data from LLM via constructor params,  - class has getXxx() and setXxx() methods with xxx matching one of the property names -    in such situation object will be hydrated with data from LLM via setter methods</p> <p>If you want to access them directly after extraction, provide default values for them.</p>"},{"location":"cookbook/instructor/basics/public_vs_private/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. His password is '123admin'.\n    TEXT;\n\n\n// CASE 1: Class with public fields\n\nclass User\n{\n    public string $name;\n    public int $age;\n    public string $password = '';\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n\necho \"User with public fields\\n\";\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 25);\nassert($user-&gt;password === '123admin');\n\n\n// CASE 2: Class with some private fields\n\nclass UserWithPrivateFields\n{\n    public string $name;\n    private int $age = 0;\n    private string $password = '';\n\n    public function getAge() : int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$userPriv = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithPrivateFields::class)\n    -&gt;get();\n\necho \"Private 'password' and 'age' fields are not hydrated by Instructor\\n\";\n\ndump($userPriv);\n\nassert($userPriv-&gt;name === \"Jason\");\nassert($userPriv-&gt;getAge() === 0);\nassert($userPriv-&gt;getPassword() === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/self_correction/","title":"Automatic correction based on validation results","text":""},{"location":"cookbook/instructor/basics/self_correction/#overview","title":"Overview","text":"<p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p>"},{"location":"cookbook/instructor/basics/self_correction/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidated;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationAttempt;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationFailed;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n$text = \"you can reply to me via jason wp.pl -- Jason\";\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"RESULTS:\\n\");\n$user = (new StructuredOutput)\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; print(\"[ ] Requesting LLM response...\\n\"))\n    -&gt;onEvent(ResponseValidationAttempt::class, fn($event) =&gt; print(\"[?] Validating:\\n    \".$event.\"\\n\"))\n    -&gt;onEvent(ResponseValidationFailed::class, fn($event) =&gt; print(\"[!] Validation failed:\\n    $event\\n\"))\n    -&gt;onEvent(ResponseValidated::class, fn($event) =&gt; print(\"[ ] Validation succeeded.\\n\"))\n    -&gt;with(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 3,\n    )-&gt;get();\n\nprint(\"\\nOUTPUT:\\n\");\n\ndump($user);\n\nassert($user-&gt;email === \"jason@wp.pl\");\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/using_config/","title":"Using LLM API connection presets from config file","text":""},{"location":"cookbook/instructor/basics/using_config/#overview","title":"Overview","text":"<p>Instructor allows you to define multiple API connection presets in <code>llm.php</code> file. This is useful when you want to use different LLMs or API providers in your application.</p> <p>Connecting to LLM API via predefined connection is as simple as calling <code>withPreset</code> method with the preset name.</p>"},{"location":"cookbook/instructor/basics/using_config/#configuration-file","title":"Configuration file","text":"<p>Default LLM configuration file is located in <code>/config/llm.php</code> in the root directory of Instructor codebase.</p> <p>You can set the location of the configuration file via <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable (comma-separated list of paths). You can use a copy of the default configuration file as a starting point.</p> <p>LLM config file defines available connection presets to LLM APIs and their parameters. It also specifies the default provider and parameters to be used when calling Instructor.</p>"},{"location":"cookbook/instructor/basics/using_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Get Instructor object with client defined in config.php under 'presets/openai' key\n$structuredOutput = (new StructuredOutput)-&gt;using('openai');\n\n// Call with custom model and execution mode\n$user = $structuredOutput-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;get();\n\n// Use the results of LLM inference\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/validation/","title":"Validation","text":""},{"location":"cookbook/instructor/basics/validation/#overview","title":"Overview","text":"<p>Instructor uses validation to verify if the response generated by LLM meets the requirements of your response model. If the response does not meet the requirements, Instructor will throw an exception.</p> <p>Instructor uses Symfony's Validator component to validate the response, check their documentation for more information on the usage: https://symfony.com/doc/current/components/validator.html</p> <p>Following example demonstrates how to use Symfony Validator's constraints to validate the email field of response.</p>"},{"location":"cookbook/instructor/basics/validation/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Exceptions\\ValidationException;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    #[Assert\\NotBlank]\n    /** Find user's email provided in the text or empty if it is missing */\n    public ?string $email;\n}\n\n$caughtException = false;\ntry {\n    $user = (new StructuredOutput)\n        -&gt;withResponseClass(UserDetails::class)\n        -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via mail -- Jason\"]])\n        -&gt;get();\n} catch (ValidationException $e) {\n    $caughtException = true;\n    echo \"Validation worked.\\n\";\n} catch (Throwable $e) {\n    // Catch any other exception\n    echo \"Validation failed with unexpected exception: {$e-&gt;getMessage()}\\n\";\n}\n\nassert($caughtException === true);\nassert(!isset($user));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/validation_multifield/","title":"Validation across multiple fields","text":""},{"location":"cookbook/instructor/basics/validation_multifield/#overview","title":"Overview","text":"<p>Sometimes property level validation is not enough - you may want to check values of multiple properties and based on the combination of them decide to accept or reject the response. Or the assertions provided by Symfony may not be enough for your use case.</p> <p>In such case you can easily add custom validation code to your response model by: - using <code>ValidationMixin</code> - and defining validation logic in <code>validate()</code> method.</p> <p>In this example LLM should be able to correct typo in the message (graduation year we provided is <code>1010</code> instead of <code>2010</code>) and respond with correct graduation year.</p>"},{"location":"cookbook/instructor/basics/validation_multifield/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $birthYear;\n    public int $graduationYear;\n\n    public function validate() : ValidationResult {\n        if ($this-&gt;graduationYear &gt; $this-&gt;birthYear) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'graduationYear',\n            value: $this-&gt;graduationYear,\n            message: \"Graduation year has to be bigger than birth year.\"\n        );\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withResponseClass(UserDetails::class)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'Jason was born in 2000 and graduated in 18.']],\n        model: 'gpt-3.5-turbo',\n        maxRetries: 2,\n    )-&gt;get();\n\n\ndump($user);\n\nassert($user-&gt;graduationYear === 2018);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/basics/validation_with_llm/","title":"Validation with LLM","text":""},{"location":"cookbook/instructor/basics/validation_with_llm/#overview","title":"Overview","text":"<p>You can use LLM capability to semantically process the context to validate the response following natural language instructions. This way you can implement more complex validation logic that would be difficult (or impossible) to achieve using traditional, code-based validation.</p>"},{"location":"cookbook/instructor/basics/validation_with_llm/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    #[Description('User details in format: key=value')]\n    /** @var string[]  */\n    public array $details;\n\n    public function validate() : ValidationResult {\n        return match($this-&gt;hasPII()) {\n            true =&gt; ValidationResult::fieldError(\n                field: 'details',\n                value: implode('\\n', $this-&gt;details),\n                message: \"Details contain PII, remove it from the response.\"\n            ),\n            false =&gt; ValidationResult::valid(),\n        };\n    }\n\n    private function hasPII() : bool {\n        $data = implode('\\n', $this-&gt;details);\n        return (new StructuredOutput)\n            -&gt;with(\n                messages: \"Context:\\n$data\\n\",\n                responseModel: Scalar::boolean('hasPII', 'Does the context contain any PII?'),\n            )\n            -&gt;getBoolean();\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    My name is Jason. I am is 25 years old. I am developer.\n    My phone number is +1 123 34 45 and social security number is 123-45-6789\n    TEXT;\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print()) // let's check the internals of Instructor processing\n    -&gt;with(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )-&gt;get();\n\ndump($user);\n\nassert(!Str::contains(implode('\\n', $user-&gt;details), '123-45-6789'));\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction/","title":"Extraction of complex objects","text":""},{"location":"cookbook/instructor/extras/complex_extraction/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text.</p>"},{"location":"cookbook/instructor/extras/complex_extraction/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = new StructuredOutput;\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'gpt-4o',\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ],\n        mode: OutputMode::Tools\n    )\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction_claude/","title":"Extraction of complex objects (Anthropic)","text":""},{"location":"cookbook/instructor/extras/complex_extraction_claude/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Anthropic Claude 3 model.</p>"},{"location":"cookbook/instructor/extras/complex_extraction_claude/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\nclass ProjectEvents {\n    /**\n     * List of events extracted from the text\n     * @var ProjectEvent[]\n     */\n    public array $events = [];\n}\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('anthropic');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'claude-3-5-sonnet-20240620', // 'claude-3-haiku-20240307'\n        prompt: 'Extract a list of project events with all the details from the provided input in JSON format using schema: &lt;|json_schema|&gt;',\n        mode: OutputMode::Json,\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        options: [\n            'max_tokens' =&gt; 4096,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n//dump($events-&gt;list);\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction_cohere/","title":"Extraction of complex objects (Cohere)","text":""},{"location":"cookbook/instructor/extras/complex_extraction_cohere/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Cohere R models.</p>"},{"location":"cookbook/instructor/extras/complex_extraction_cohere/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('cohere');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'command-r-plus-08-2024',\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        mode: OutputMode::JsonSchema,\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/complex_extraction_gemini/","title":"Extraction of complex objects (Gemini)","text":""},{"location":"cookbook/instructor/extras/complex_extraction_gemini/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Google Gemini model.</p>"},{"location":"cookbook/instructor/extras/complex_extraction_gemini/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('gemini');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    //-&gt;onEvent(PartialInferenceResponseReceived::class, fn(PartialInferenceResponseReceived $e) =&gt; print \"---\\n\".$e-&gt;partialInferenceResponse-&gt;content().\"---\\n\")\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        //model: 'gemini-1.5-flash',\n        //model: 'gemini-2.0-flash-exp',\n        //model: 'gemini-2.0-flash-thinking-exp',\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_car_damage/","title":"Image processing - car damage detection","text":""},{"location":"cookbook/instructor/extras/image_car_damage/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>In this example we will be extracting structured data from an image of a car with visible damage. The response model will contain information about the location of the damage and the type of damage.</p>"},{"location":"cookbook/instructor/extras/image_car_damage/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_car_damage/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nenum DamageSeverity : string {\n    case Minor = 'minor';\n    case Moderate = 'moderate';\n    case Severe = 'severe';\n    case Total = 'total';\n}\n\nenum DamageLocation : string {\n    case Front = 'front';\n    case Rear = 'rear';\n    case Left = 'left';\n    case Right = 'right';\n    case Top = 'top';\n    case Bottom = 'bottom';\n}\n\nclass Damage {\n    #[Description('Identify damaged element')]\n    public string $element;\n    /** @var DamageLocation[] */\n    public array $locations;\n    public DamageSeverity $severity;\n    public string $description;\n}\n\nclass DamageAssessment {\n    public string $make;\n    public string $model;\n    public string $bodyColor;\n    /** @var Damage[] */\n    public array $damages = [];\n    public string $summary;\n}\n\n$assessment = Image::fromFile(__DIR__ . '/car-damage.jpg')\n    -&gt;toData(\n        responseModel: DamageAssessment::class,\n        prompt: 'Identify and assess each car damage location and severity separately.',\n        connection: 'openai',\n        model: 'gpt-4o',\n        options: ['max_tokens' =&gt; 4096]\n    );\n\ndump($assessment);\nassert(Str::contains($assessment-&gt;make, 'Toyota', false));\nassert(Str::contains($assessment-&gt;model, 'Prius', false));\nassert(Str::contains($assessment-&gt;bodyColor, 'white', false));\nassert(count($assessment-&gt;damages) &gt; 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_to_data/","title":"Image to data (OpenAI)","text":""},{"location":"cookbook/instructor/extras/image_to_data/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/instructor/extras/image_to_data/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_to_data/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt.',\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_to_data_anthropic/","title":"Image to data (Anthropic)","text":""},{"location":"cookbook/instructor/extras/image_to_data_anthropic/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/instructor/extras/image_to_data_anthropic/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_to_data_anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;using('anthropic')-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt. Return result as JSON following this schema: &lt;|json_schema|&gt;',\n    model: 'claude-3-5-sonnet-20240620',\n    mode: OutputMode::Json,\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/image_to_data_gemini/","title":"Image to data (Gemini)","text":""},{"location":"cookbook/instructor/extras/image_to_data_gemini/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/instructor/extras/image_to_data_gemini/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/instructor/extras/image_to_data_gemini/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;using('gemini')-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt. Return result as JSON following this schema: &lt;|json_schema|&gt;',\n    mode: OutputMode::Json,\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/schema/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/instructor/extras/schema/#overview","title":"Overview","text":"<p>Instructor has a built-in support for dynamically constructing JSON Schema using <code>JsonSchema</code> class. It is useful when you want to shape the structures during runtime.</p>"},{"location":"cookbook/instructor/extras/schema/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'User name'),\n        JsonSchema::integer('age', 'User age'),\n    ],\n    requiredProperties: ['name', 'age'],\n);\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;withMessages(\"Jason is 25 years old and works as an engineer\")\n    -&gt;withResponseJsonSchema($schema)\n    -&gt;withDeserializers()\n    -&gt;withDefaultToStdClass()\n    -&gt;get();\n\ndump($user);\n\nassert(gettype($user) === 'object');\nassert(get_class($user) === 'stdClass');\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/schema_dynamic/","title":"Generating JSON Schema dynamically","text":""},{"location":"cookbook/instructor/extras/schema_dynamic/#overview","title":"Overview","text":"<p>Instructor has a built-in support for generating JSON Schema from dynamic objects with <code>Structure</code> class.</p> <p>This is useful when the data model is built during runtime or defined by your app users.</p> <p><code>Structure</code> helps you flexibly design and modify data models that can change with every request or user input and allows you to generate JSON Schema for them.</p>"},{"location":"cookbook/instructor/extras/schema_dynamic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$city = Structure::define('city', [\n    Field::string('name', 'City name')-&gt;required(),\n    Field::int('population', 'City population')-&gt;required(),\n    Field::int('founded', 'Founding year')-&gt;required(),\n]);\n\n$data = (new StructuredOutput)\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n        Respond with JSON data.']])\n    -&gt;withResponseJsonSchema($city-&gt;toJsonSchema())\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;withOutputMode(OutputMode::JsonSchema)\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data-&gt;toArray());\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/structured_input/","title":"Using structured data as an input","text":""},{"location":"cookbook/instructor/extras/structured_input/#overview","title":"Overview","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>create()</code> method can be an object, but also an array or just a string.</p>"},{"location":"cookbook/instructor/extras/structured_input/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translatedEmail = (new StructuredOutput)\n    -&gt;withInput($email)\n    -&gt;withResponseClass(Email::class)\n    -&gt;withPrompt('Translate the text fields of email to Spanish. Keep other fields unchanged.')\n    -&gt;get();\n\ndump($translatedEmail);\n\nassert($translatedEmail-&gt;address === $email-&gt;address);\nassert($translatedEmail-&gt;subject !== $email-&gt;subject);\nassert($translatedEmail-&gt;body !== $email-&gt;body);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/transcription_to_tasks/","title":"Create tasks from meeting transcription","text":""},{"location":"cookbook/instructor/extras/transcription_to_tasks/#overview","title":"Overview","text":"<p>This example demonstrates how you can create task assignments based on a transcription of meeting recording.</p>"},{"location":"cookbook/instructor/extras/transcription_to_tasks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nenum TaskStatus : string {\n    case Pending = 'pending';\n    case Completed = 'completed';\n}\n\nenum Role : string {\n    case PM = 'pm';\n    case Dev = 'dev';\n}\n\nclass Task {\n    public string $title;\n    public string $description;\n    public DateTimeImmutable $dueDate;\n    public Role $owner;\n    public TaskStatus $status;\n}\n\nclass Tasks {\n    public DateTime $meetingDate;\n    /** @var Task[] */\n    public array $tasks;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = &lt;&lt;&lt;TEXT\nTranscription of meeting from 2024-01-15, 16:00\n---\nPM: Hey, how's progress on the video transcription engine?\nDev: I've got basic functionality working, but accuracy isn't great yet. Might need to switch to a different API.\nPM: So the plan is to research alternatives and provide a comparison? Is it possible by Jan 20th?\nDev: Sure, I'll make it available before the meeting.\nPM: The one at 12?\nDev: Yes, at 12. By the way, are we still planning to support real-time transcription?\nPM: Yes, it's a key feature. Speaking of which, I need to update the product roadmap. I'll have that ready by Jan 18th.\nDev: Got it. I'll keep that in mind while evaluating APIs. Oh, and the UI for the summary view is ready for review.\nPM: Great, I'll take a look tomorrow by 10.\nTEXT;\n\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$tasks = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: $text,\n        responseModel: Tasks::class,\n        //model: 'gpt-4o',\n        mode: OutputMode::Json,\n    )\n    -&gt;get();\ndd($tasks);\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($tasks);\n\nassert($tasks-&gt;meetingDate-&gt;format('Y-m-d') === '2024-01-15');\nassert(count($tasks-&gt;tasks) == 3);\n\nassert($tasks-&gt;tasks[0]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-20');\nassert($tasks-&gt;tasks[0]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[0]-&gt;owner === Role::Dev);\n\nassert($tasks-&gt;tasks[1]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-18');\nassert($tasks-&gt;tasks[1]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[1]-&gt;owner === Role::PM);\n\nassert($tasks-&gt;tasks[2]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-16');\nassert($tasks-&gt;tasks[2]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[2]-&gt;owner === Role::PM);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/translate_ui_fields/","title":"Translating UI text fields","text":""},{"location":"cookbook/instructor/extras/translate_ui_fields/#overview","title":"Overview","text":"<p>You can use Instructor to translate text fields in your UI. We can instruct the model to translate only the text fields from one language to another, but leave the other fields, like emails or URLs, unchanged.</p> <p>This example demonstrates how to translate text fields from English to German using structure-to-structure processing with LLM.</p>"},{"location":"cookbook/instructor/extras/translate_ui_fields/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Contracts\\CanValidateObject;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Instructor\\Validation\\Validators\\SymfonyValidator;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass TextElementModel\n{\n    public function __construct(\n        public string $headline = '',\n        public string $text = '',\n        public string $url = 'https://translation.com/'\n    ) {}\n}\n\n$sourceModel = new TextElementModel(\n    headline: 'This is my headline',\n    text: '&lt;p&gt;This is some WYSIWYG HTML content.&lt;/p&gt;'\n);\n\n$validator = new class implements CanValidateObject {\n    #[\\Override]\n    public function validate(object $dataObject): ValidationResult {\n        $isInGerman = (new StructuredOutput)\n            //-&gt;withDebugPreset('on')\n            -&gt;withInput($dataObject)\n            -&gt;withResponseObject(Scalar::boolean())\n            -&gt;withPrompt('Are all content fields translated to German? Return result in JSON format: &lt;|json_schema|&gt;')\n            -&gt;withOutputMode(OutputMode::Json)\n            -&gt;get();\n        return match($isInGerman) {\n            true =&gt; ValidationResult::valid(),\n            default =&gt; ValidationResult::invalid(['All input text fields have to be translated to German. Keep HTML tags unchanged.']),\n        };\n    }\n};\n\n$transformedModel = (new StructuredOutput)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e)=&gt;$e-&gt;print())\n    -&gt;withInput($sourceModel)\n    -&gt;withResponseClass(get_class($sourceModel))\n    -&gt;withPrompt('Translate all text fields to German. Keep HTML tags unchanged. Return result in JSON format: &lt;|json_schema|&gt;')\n    -&gt;withMaxRetries(2)\n    -&gt;withOptions(['temperature' =&gt; 0])\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;withValidators($validator, SymfonyValidator::class)\n    -&gt;get();\n\ndump($transformedModel);\n\nassert(true === (\n    str_contains($transformedModel-&gt;headline, '\u00dcberschrift')\n    || str_contains($transformedModel-&gt;headline, 'Schlagzeile')\n));\nassert(str_contains($transformedModel-&gt;text, 'Inhalt') === true);\nassert(str_contains($transformedModel-&gt;text, '&lt;p&gt;') === true);\nassert(str_contains(str_replace('\\/', '/', $transformedModel-&gt;url), 'https://translation.com/') === true);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/extras/web_to_objects/","title":"Web page to PHP objects","text":""},{"location":"cookbook/instructor/extras/web_to_objects/#overview","title":"Overview","text":"<p>This example demonstrates how to extract structured data from a web page and get it as PHP object.</p>"},{"location":"cookbook/instructor/extras/web_to_objects/#example","title":"Example","text":"<p>In this example we will be extracting list of Laravel companies from The Manifest website. The result will be a list of <code>Company</code> objects.</p> <p>We use Webpage extractor to get the content of the page and specify 'none' scraper, which means that we will be using built-in <code>file_get_contents</code> function to get the content of the page.</p> <p>In production environment you might want to use one of the supported scrapers:  - <code>browsershot</code>  - <code>scrapingbee</code>  - <code>scrapfly</code>  - <code>jinareader</code></p> <p>Commercial scrapers require API key, which can be set in the configuration file (<code>/config/web.php</code>).</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Auxiliary\\Web\\Webpage;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass Company {\n    public string $name = '';\n    public string $location = '';\n    public string $description = '';\n    public int $minProjectBudget = 0;\n    public string $companySize = '';\n    #[Instructions('Remove any tracking parameters from the URL')]\n    public string $websiteUrl = '';\n    /** @var string[] */\n    public array $clients = [];\n}\n\n$companyGen = Webpage::withScraper('none')\n    -&gt;get('https://themanifest.com/pl/software-development/laravel/companies?page=1')\n    -&gt;cleanup()\n    -&gt;select('.directory-providers__list')\n    -&gt;selectMany(\n        selector: '.provider-card',\n        callback: fn($item) =&gt; $item-&gt;asMarkdown(),\n        limit: 3\n    );\n\n$companies = [];\necho \"Extracting company data from:\\n\\n\";\nforeach($companyGen as $companyDiv) {\n    /** @var string $companyDiv */\n    echo \" &gt; \" . substr($companyDiv, 0, 32) . \"...\\n\\n\";\n    $company = (new StructuredOutput)\n        -&gt;using('openai')\n        -&gt;with(\n            messages: $companyDiv,\n            responseModel: Company::class,\n            mode: OutputMode::Json\n        )-&gt;get();\n    $companies[] = $company;\n    dump($company);\n}\n\nassert(count($companies) === 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/debugging/","title":"Debugging","text":""},{"location":"cookbook/instructor/troubleshooting/debugging/#overview","title":"Overview","text":"<p>The <code>StructuredOutput</code> class has a <code>withDebug()</code> method that can be used to debug the request and response.</p> <p>It displays detailed information about the request being sent to LLM API and response received from it, including:  - request headers, URI, method and body,  - response status, headers, and body.</p> <p>This is useful for debugging the request and response when you are not getting the expected results.</p>"},{"location":"cookbook/instructor/troubleshooting/debugging/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Utils\\Str;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// CASE 1.1 - normal flow, sync request\n\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withDebugPreset('on');\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print());\n\necho \"\\n### CASE 1.1 - Debugging sync request\\n\\n\";\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason is 25 years old.\",\n        responseModel: User::class,\n        options: [ 'stream' =&gt; false ]\n    )\n    -&gt;get();\n\necho \"\\nResult:\\n\";\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n\n// CASE 1.2 - normal flow, streaming request\n\necho \"\\n### CASE 1.2 - Debugging streaming request\\n\\n\";\n$user2 = $structuredOutput\n    -&gt;with(\n        messages: \"Anna is 21 years old.\",\n        responseModel: User::class,\n        options: [ 'stream' =&gt; true ]\n    )\n    -&gt;get();\n\necho \"\\nResult:\\n\";\ndump($user2);\n\nassert(isset($user2-&gt;name));\nassert(isset($user2-&gt;age));\nassert($user2-&gt;name === 'Anna');\nassert($user2-&gt;age === 21);\n\n\n// CASE 2 - forcing API error via empty LLM config\n\n// let's initialize the instructor with an incorrect LLM config\n$structuredOutput = (new StructuredOutput)\n    -&gt;withLLMConfig(new LLMConfig(apiUrl: 'https://example.com'));\n\necho \"\\n### CASE 2 - Debugging with HTTP exception\\n\\n\";\ntry {\n    $user = $structuredOutput\n        -&gt;withDebugPreset('on')\n        -&gt;with(\n            messages: \"Jason is 25 years old.\",\n            responseModel: User::class,\n            options: [ 'stream' =&gt; true ]\n        )\n        -&gt;get();\n} catch (Exception $e) {\n    $msg = Str::limit($e-&gt;getMessage(), 250);\n    echo \"EXCEPTION WE EXPECTED:\\n\";\n    echo \"\\nCaught exception: \" . $msg . \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/on_event/","title":"Receive specific internal event with onEvent()","text":""},{"location":"cookbook/instructor/troubleshooting/on_event/#overview","title":"Overview","text":"<p><code>(new StructuredOutput)-&gt;onEvent(string $class, callable $callback)</code> method allows you to receive callback when specified type of event is dispatched by Instructor.</p> <p>This way you can plug into the execution process and monitor it, for example logging or reacting to the events which are of interest to your application.</p> <p>This example demonstrates how you can monitor outgoing requests and received responses via Instructor's events.</p> <p>Check the <code>Cognesy\\Instructor\\Events</code> namespace for the list of available events and their properties.</p>"},{"location":"cookbook/instructor/troubleshooting/on_event/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n// let's mock a logger class - in real life you would use a proper logger, e.g. Monolog\n$logger = new class {\n    public function log(Event $event) {\n        // we're using a predefined asLog() method to get the event data,\n        // but you can access the event properties directly and customize the output\n        echo $event-&gt;asLog().\"\\n\";\n    }\n};\n\n$user = (new StructuredOutput)\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;onEvent(HttpResponseReceived::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;with(\n        messages: \"Jason is 28 years old\",\n        responseModel: User::class,\n    )\n    -&gt;get();\n\ndump($user);\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/settings/","title":"Modifying Settings Path","text":""},{"location":"cookbook/instructor/troubleshooting/settings/#overview","title":"Overview","text":"<p>This example demonstrates how to modify the settings path for the Instructor library. This is useful when you want to use a custom configuration directory instead of the default one.</p>"},{"location":"cookbook/instructor/troubleshooting/settings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Settings;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $age;\n    public string $firstName;\n    public ?string $lastName;\n}\n\n// set the configuration path to custom directory\nSettings::setPath(__DIR__ . '/config');\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on') // we reconfigured local debug settings to dump only request URL\n    -&gt;withMessages('Jason is 25 years old.')\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;get();\n\ndump($user);\n\nassert(!isset($user-&gt;lastName) || $user-&gt;lastName === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/token_usage_events/","title":"Tracking token usage via events","text":""},{"location":"cookbook/instructor/troubleshooting/token_usage_events/#overview","title":"Overview","text":"<p>Some use cases require tracking the token usage of the API responses. This can be done by getting <code>Usage</code> object from Instructor LLM response object.</p> <p>Code below demonstrates how it can be retrieved for both sync and streamed requests.</p>"},{"location":"cookbook/instructor/troubleshooting/token_usage_events/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Data\\Usage;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\nfunction printUsage(Usage $usage) : void {\n    echo \"Input tokens: $usage-&gt;inputTokens\\n\";\n    echo \"Output tokens: $usage-&gt;outputTokens\\n\";\n    echo \"Cache creation tokens: $usage-&gt;cacheWriteTokens\\n\";\n    echo \"Cache read tokens: $usage-&gt;cacheReadTokens\\n\";\n    echo \"Reasoning tokens: $usage-&gt;reasoningTokens\\n\";\n}\n\necho \"COUNTING TOKENS FOR SYNC RESPONSE\\n\";\n$text = \"Jason is 25 years old and works as an engineer.\";\n$response = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n    )-&gt;response();\n\necho \"\\nTEXT: $text\\n\";\nassert($response-&gt;usage()-&gt;total() &gt; 0);\nprintUsage($response-&gt;usage());\n\n\necho \"\\n\\nCOUNTING TOKENS FOR STREAMED RESPONSE\\n\";\n$text = \"Anna is 19 years old.\";\n$stream = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n        options: ['stream' =&gt; true],\n    )\n    -&gt;stream();\n\n$response = $stream-&gt;finalValue();\necho \"\\nTEXT: $text\\n\";\nassert($stream-&gt;usage()-&gt;total() &gt; 0);\nprintUsage($stream-&gt;usage());\n?&gt;\n</code></pre>"},{"location":"cookbook/instructor/troubleshooting/wiretap/","title":"Receive all internal events with wiretap()","text":""},{"location":"cookbook/instructor/troubleshooting/wiretap/#overview","title":"Overview","text":""},{"location":"cookbook/instructor/troubleshooting/wiretap/#receive-all-internal-events-with-wiretap","title":"Receive all internal events with wiretap()","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <p><code>(new StructuredOutput)-&gt;wiretap(callable $callback)</code> method allows you to receive all events dispatched by Instructor.</p> <p>Example below demonstrates how <code>wiretap()</code> can help you to monitor the execution process and better understand or resolve any processing issues.</p> <p>In this example we use <code>print()</code> method available on event classes, which outputs console-formatted information about each event.</p>"},{"location":"cookbook/instructor/troubleshooting/wiretap/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Role : string {\n    case CEO = 'ceo';\n    case CTO = 'cto';\n    case Developer = 'developer';\n    case Other = 'other';\n}\n\nclass UserDetail\n{\n    public string $name;\n    public Role $role;\n    public int $age;\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    -&gt;with(\n        messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Contact our CTO, Jason is 28 years old -- Best regards, Tom\"]],\n        responseModel: UserDetail::class,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;role === Role::CTO);\nassert($user-&gt;age === 28);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/config_providers/","title":"Customize configuration providers of LLM driver","text":""},{"location":"cookbook/polyglot/llm_advanced/config_providers/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to <code>Inference</code> object. This is useful when you want to initialize LLM client with custom values.</p>"},{"location":"cookbook/polyglot/llm_advanced/config_providers/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Adbar\\Dot;\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\n$configData = [\n    'http' =&gt; [\n        'defaultPreset' =&gt; 'symfony',\n        'presets' =&gt; [\n            'symfony' =&gt; [\n                'driver' =&gt; 'symfony',\n                'connectTimeout' =&gt; 10,\n                'requestTimeout' =&gt; 30,\n                'idleTimeout' =&gt; -1,\n                'maxConcurrent' =&gt; 5,\n                'poolTimeout' =&gt; 60,\n                'failOnError' =&gt; true,\n            ],\n            // Add more HTTP presets as needed\n        ],\n    ],\n    'debug' =&gt; [\n        'defaultPreset' =&gt; 'off',\n        'presets' =&gt; [\n            'off' =&gt; [\n                'httpEnabled' =&gt; false,\n            ],\n            'on' =&gt; [\n                'httpEnabled' =&gt; true,\n                'httpTrace' =&gt; true,\n                'httpRequestUrl' =&gt; true,\n                'httpRequestHeaders' =&gt; true,\n                'httpRequestBody' =&gt; true,\n                'httpResponseHeaders' =&gt; true,\n                'httpResponseBody' =&gt; true,\n                'httpResponseStream' =&gt; true,\n                'httpResponseStreamByLine' =&gt; true,\n            ],\n        ],\n    ],\n    'llm' =&gt; [\n        'defaultPreset' =&gt; 'deepseek',\n        'presets' =&gt; [\n            'deepseek' =&gt; [\n                'apiUrl' =&gt; 'https://api.deepseek.com',\n                'apiKey' =&gt; Env::get('DEEPSEEK_API_KEY'),\n                'endpoint' =&gt; '/chat/completions',\n                'model' =&gt; 'deepseek-chat',\n                'maxTokens' =&gt; 128,\n                'driver' =&gt; 'deepseek',\n            ],\n            'openai' =&gt; [\n                'apiUrl' =&gt; 'https://api.openai.com',\n                'apiKey' =&gt; Env::get('OPENAI_API_KEY'),\n                'endpoint' =&gt; '/v1/chat/completions',\n                'model' =&gt; 'gpt-4',\n                'maxTokens' =&gt; 256,\n                'driver' =&gt; 'openai',\n            ],\n        ],\n    ],\n];\n\nclass CustomConfigProvider implements CanProvideConfig\n{\n    private Dot $dot;\n\n    public function __construct(array $data = []) {\n        $this-&gt;dot = new Dot($data);\n    }\n\n    public function get(string $path, mixed $default = null): mixed {\n        return $this-&gt;dot-&gt;get($path, $default);\n    }\n\n    public function has(string $path): bool {\n        return $this-&gt;dot-&gt;has($path);\n    }\n}\n\n$configProvider = new CustomConfigProvider($configData);\n\n$events = new EventDispatcher();\n$customClient = (new HttpClientBuilder(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withClientInstance(\n        driverName: 'symfony',\n        clientInstance: SymfonyHttpClient::create(['http_version' =&gt; '2.0']),\n    )\n    -&gt;create();\n\n$inference = (new Inference(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withHttpClient($customClient);\n\n$answer = $inference\n    -&gt;using('deepseek') // Use 'deepseek' preset from CustomLLMConfigProvider\n    //-&gt;withDebugPreset('on')\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']])\n    -&gt;withMaxTokens(256)\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/context_cache_llm/","title":"Context caching (text inference)","text":""},{"location":"cookbook/polyglot/llm_advanced/context_cache_llm/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching (currently only Anthropic API), so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note 1: Instructor supports context caching for Anthropic API and OpenAI API.</p> <p>Note 2: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/polyglot/llm_advanced/context_cache_llm/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its summary for 2 target audiences.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$data = file_get_contents(__DIR__ . '/../../../README.md');\n\n$inference = (new Inference)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // wiretap to print all events\n    //-&gt;withDebugPreset('on') // debug HTTP traffic\n    -&gt;using('anthropic')\n    -&gt;withCachedContext(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Here is content of README.md file'],\n            ['role' =&gt; 'user', 'content' =&gt; $data],\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a short, very domain specific pitch of the project described in README.md. List relevant, domain specific problems that this project could solve. Use domain specific concepts and terminology to make the description resonate with the target audience.'],\n            ['role' =&gt; 'assistant', 'content' =&gt; 'For whom do you want to generate the pitch?'],\n        ],\n    );\n\n$response = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'founder of lead gen SaaS startup']],\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CTO of lead gen vendor\\n\");\nprint(\"  ({$response-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response-&gt;content() . \"\\n\");\n\nassert(!empty($response-&gt;content()));\nassert(Str::contains($response-&gt;content(), 'Instructor'));\nassert(Str::contains($response-&gt;content(), 'lead', false));\nassert($response-&gt;usage()-&gt;cacheReadTokens &gt; 0 || $response-&gt;usage()-&gt;cacheWriteTokens &gt; 0);\n\n$response2 = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'CIO of insurance company']],\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CIO of insurance company\\n\");\nprint(\"  ({$response2-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response2-&gt;content() . \"\\n\");\n\nassert(!empty($response2-&gt;content()));\nassert(Str::contains($response2-&gt;content(), 'Instructor'));\nassert(Str::contains($response2-&gt;content(), 'insurance', false));\nassert($response2-&gt;usage()-&gt;cacheReadTokens &gt; 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_config/","title":"Customize configuration of LLM driver","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_config/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to <code>Inference</code> object. This is useful when you want to initialize LLM client with custom values.</p>"},{"location":"cookbook/polyglot/llm_advanced/custom_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Drivers\\Symfony\\SymfonyDriver;\nuse Cognesy\\Http\\HttpClientBuilder;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\n$events = new EventDispatcher();\n\n// Build fully customized HTTP client\n\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 30,\n    requestTimeout: 60,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true,\n);\n\n$yourClientInstance = SymfonyHttpClient::create(['http_version' =&gt; '2.0']);\n\n$customClient = (new HttpClientBuilder)\n    -&gt;withEventBus($events)\n    -&gt;withDriver(new SymfonyDriver(\n        config: $httpConfig,\n        clientInstance: $yourClientInstance,\n        events: $events,\n    ))\n    -&gt;create();\n\n// Create instance of LLM client initialized with custom parameters\n\n$config = new LLMConfig(\n    apiUrl  : 'https://api.deepseek.com',\n    apiKey  : Env::get('DEEPSEEK_API_KEY'),\n    endpoint: '/chat/completions', model: 'deepseek-chat', maxTokens: 128, driver: 'deepseek',\n);\n\n// Call inference API with custom client and configuration\n\n$answer = (new Inference)\n    -&gt;withEventHandler($events)\n    -&gt;withConfig($config)\n    -&gt;withHttpClient($customClient)\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_embeddings_config/","title":"Custom Embeddings Config","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_embeddings_config/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_embeddings_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n\n$query = \"technology news\";\n\n$config = new EmbeddingsConfig(\n    apiUrl    : 'https://api.cohere.ai/v1',\n    apiKey    : Env::get('COHERE_API_KEY', ''),\n    endpoint  : '/embed',\n    model     : 'embed-multilingual-v3.0',\n    dimensions: 1024,\n    maxInputs : 96,\n    driver: 'cohere',\n);\n\n$provider = EmbeddingsProvider::new()\n    -&gt;withConfig($config);\n\n$bestMatches = EmbedUtils::findSimilar(\n    provider: $provider,\n    query: $query,\n    documents: $documents,\n    topK: 3\n);\n\ndump($bestMatches);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_driver/","title":"Using custom LLM driver","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_llm_driver/#overview","title":"Overview","text":"<p>You can register and use your own LLM driver, either using a new driver name or overriding an existing driver bundled with Polyglot.</p>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_driver/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;\nuse Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI\\OpenAIDriver;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// we will use existing, bundled driver as an example, but you can provide any class that implements\n// a required interface (CanHandleInference)\n\nInference::registerDriver(\n    name: 'custom-driver',\n    driver: fn($config, $httpClient, $events) =&gt; new class($config, $httpClient, $events) extends OpenAIDriver {\n        #[\\Override]\n        protected function makeHttpResponse(\\Cognesy\\Http\\Data\\HttpRequest $request): HttpResponse {\n            // some extra functionality to demonstrate our driver is being used\n            echo \"&gt;&gt;&gt; Handling request...\\n\";\n            return parent::makeHttpResponse($request);\n        }\n    }\n);\n\n// Create instance of LLM client initialized with custom parameters\n$config = new LLMConfig(\n    apiUrl          : 'https://api.openai.com/v1',\n    apiKey          : Env::get('OPENAI_API_KEY'),\n    endpoint        : '/chat/completions', model: 'gpt-4o-mini', maxTokens: 128,\n    driver          : 'custom-driver',\n);\n\n$answer = (new Inference)\n    -&gt;withConfig($config)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_via_dsn/","title":"Customize LLM Configuration with DSN string","text":""},{"location":"cookbook/polyglot/llm_advanced/custom_llm_via_dsn/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration data to <code>Inference</code> object with DSN string. This is useful for inline configuration or for building configuration from admin UI, CLI arguments or environment variables.</p>"},{"location":"cookbook/polyglot/llm_advanced/custom_llm_via_dsn/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$answer = (new Inference)\n    -&gt;withDsn('preset=xai,model=grok-2')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/embed_utils/","title":"Embeddings utils","text":""},{"location":"cookbook/polyglot/llm_advanced/embed_utils/#overview","title":"Overview","text":"<p><code>EmbedUtils</code> class offers convenient methods to find top K vectors or documents most similar to provided query.</p> <p>Check out the <code>EmbedUtils</code> class for more details.  - <code>EmbedUtils::findTopK()</code>  - <code>EmbedUtils::findSimilar()</code></p> <p>Embeddings providers access details can be found and modified via <code>/config/embed.php</code>.</p>"},{"location":"cookbook/polyglot/llm_advanced/embed_utils/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n\n$query = \"technology news\";\n\n$presets = [\n    'azure',\n    'cohere',\n    'gemini',\n    'jina',\n    'mistral',\n    //'ollama',\n    'openai'\n];\n\nforeach($presets as $preset) {\n    $bestMatches = EmbedUtils::findSimilar(\n        provider: EmbeddingsProvider::using($preset),\n        query: $query,\n        documents: $documents,\n        topK: 3\n    );\n\n    echo \"\\n[$preset]\\n\";\n    dump($bestMatches);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/embeddings/","title":"Embeddings","text":""},{"location":"cookbook/polyglot/llm_advanced/embeddings/#overview","title":"Overview","text":"<p><code>Embeddings</code> class offers access to embeddings APIs which allows to generate vector representations of inputs. These embeddings can be used to compare semantic similarity between inputs, e.g. to find relevant documents based on a query.</p> <p><code>Embeddings</code> class supports following embeddings providers:  - Azure  - Cohere  - Gemini  - Jina  - Mistral  - OpenAI</p> <p>Embeddings providers access details can be found and modified via <code>/config/embed.php</code>.</p> <p>To store and search across large sets of vector embeddings you may want to use one of the popular vector databases: PGVector, Chroma, Pinecone, Weaviate, Milvus, etc.</p>"},{"location":"cookbook/polyglot/llm_advanced/embeddings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$query = \"technology news\";\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n$inputs = array_merge([$query], $documents);\n\n$topK = 3;\n\n// generate embeddings for query and documents (in a single request)\n$response = (new Embeddings)\n    -&gt;using('openai')\n    -&gt;withInputs($inputs)\n    -&gt;get();\n\n// get query and doc vectors from the response\n[$queryVectors, $docVectors] = $response-&gt;split(1);\n\n$queryVector = $queryVectors[0]\n    ?? throw new \\InvalidArgumentException('Query vector not found');\n\n// calculate cosine similarities\n$similarities = EmbedUtils::findTopK($queryVector, $docVectors, $topK);\n\n// print documents most similar to the query\necho \"Query: \" . $query . PHP_EOL;\n$count = 1;\nforeach($similarities as $index =&gt; $similarity) {\n    echo $count++;\n    echo ': ' . $documents[$index];\n    echo ' - cosine similarity to query = ' . $similarities[$index];\n    echo PHP_EOL;\n}\n\nassert(!empty($similarities));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/http_client/","title":"Work directly with HTTP client facade","text":""},{"location":"cookbook/polyglot/llm_advanced/http_client/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_advanced/http_client/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// check with default HTTP client facade\n$httpClient = new HttpClient();\n\n$answer = (new Inference)\n    -&gt;withDsn('preset=openai,model=gpt-3.5-turbo')\n    -&gt;withHttpClient($httpClient)\n    -&gt;withMessages('What is the capital of France')\n    -&gt;withMaxTokens(64)\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_advanced/parallel_calls/","title":"Parallel Calls","text":""},{"location":"cookbook/polyglot/llm_advanced/parallel_calls/#overview","title":"Overview","text":"<p>Work in progress.</p>"},{"location":"cookbook/polyglot/llm_advanced/reasoning_content/","title":"Reasoning Content Access","text":""},{"location":"cookbook/polyglot/llm_advanced/reasoning_content/#overview","title":"Overview","text":"<p>Deepseek API allows to access reasoning content, which is a detailed explanation of how the response was generated. This feature is useful for debugging and understanding the reasoning behind the response.</p>"},{"location":"cookbook/polyglot/llm_advanced/reasoning_content/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: regular API, allows to customize inference options\n$response = (new Inference)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;using('deepseek-r')\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France. Answer with just a name.']])\n    -&gt;withMaxTokens(256)\n    -&gt;response();\n\necho \"\\nCASE #1: Sync response\\n\";\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: {$response-&gt;content()}\\n\";\necho \"REASONING: {$response-&gt;reasoningContent()}\\n\";\nassert($response-&gt;content() !== '');\nassert(Str::contains($response-&gt;content(), 'Paris'));\nassert($response-&gt;reasoningContent() !== '');\n\n\n// EXAMPLE 2: streaming response\n$stream = (new Inference)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;using('deepseek-r') // optional, default is set in /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of Brasil. Answer with just a name.']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;withStreaming()\n    -&gt;stream();\n\necho \"\\nCASE #2: Streamed response\\n\";\necho \"USER: What is capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream-&gt;responses() as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\necho \"REASONING: {$stream-&gt;final()-&gt;reasoningContent()}\\n\";\nassert($stream-&gt;final()-&gt;reasoningContent() !== '');\nassert($stream-&gt;final()-&gt;content() !== '');\nassert(Str::contains($stream-&gt;final()-&gt;content(), 'Bras\u00edlia'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/a21/","title":"A21","text":""},{"location":"cookbook/polyglot/llm_api_support/a21/#overview","title":"Overview","text":"<p>Support for A21 Jamba - MAMBA architecture models, very strong at handling long context.</p>"},{"location":"cookbook/polyglot/llm_api_support/a21/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('a21') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/anthropic/","title":"Anthropic","text":""},{"location":"cookbook/polyglot/llm_api_support/anthropic/#overview","title":"Overview","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - OutputMode::MdJson, OutputMode::Json - supported - OutputMode::Tools - not supported yet</p>"},{"location":"cookbook/polyglot/llm_api_support/anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('anthropic') // see /config/llm.php\n    //-&gt;withHttpClientPreset('guzzle')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 128]\n    )\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/azure_openai/","title":"Azure OpenAI","text":""},{"location":"cookbook/polyglot/llm_api_support/azure_openai/#overview","title":"Overview","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p>"},{"location":"cookbook/polyglot/llm_api_support/azure_openai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/cerebras/","title":"Cerebras","text":""},{"location":"cookbook/polyglot/llm_api_support/cerebras/#overview","title":"Overview","text":"<p>Support for Cerebras API which uses custom hardware for super fast inference. Cerebras provides Llama models.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/cerebras/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('cerebras') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/cohere/","title":"Cohere","text":""},{"location":"cookbook/polyglot/llm_api_support/cohere/#overview","title":"Overview","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - OutputMode::MdJson - supported, recommended as a fallback from JSON mode  - OutputMode::Json - supported, recommended  - OutputMode::Tools - partially supported, not recommended</p> <p>Reasons OutputMode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple, flat data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul>"},{"location":"cookbook/polyglot/llm_api_support/cohere/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('cohere') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/deepseek/","title":"DeepSeek","text":""},{"location":"cookbook/polyglot/llm_api_support/deepseek/#overview","title":"Overview","text":"<p>Support for DeepSeek API which provides strong models at affordable price.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/deepseek/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('deepseek') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/fireworks/","title":"Fireworks.ai","text":""},{"location":"cookbook/polyglot/llm_api_support/fireworks/#overview","title":"Overview","text":"<p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - selected models - OutputMode::Json - selected models - OutputMode::MdJson</p>"},{"location":"cookbook/polyglot/llm_api_support/fireworks/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('fireworks') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/google_gemini/","title":"Google Gemini","text":""},{"location":"cookbook/polyglot/llm_api_support/google_gemini/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('gemini')\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // optional, for debugging\n    -&gt;withDebugPreset('detailed')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/google_gemini_oai/","title":"Google Gemini (OpenAI-compatible)","text":""},{"location":"cookbook/polyglot/llm_api_support/google_gemini_oai/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API in OpenAI-compatible mode.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('gemini-oai') // use OpenAI-compatible Gemini preset (v1beta/openai)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // optional, for debugging\n    -&gt;withDebugPreset('detailed')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/groq/","title":"Groq","text":""},{"location":"cookbook/polyglot/llm_api_support/groq/#overview","title":"Overview","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma.</p> <p>Supported modes depend on the specific model, but generally include:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Groq API.</p>"},{"location":"cookbook/polyglot/llm_api_support/groq/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('groq') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/inception/","title":"Inception","text":""},{"location":"cookbook/polyglot/llm_api_support/inception/#overview","title":"Overview","text":"<p>Inception API provides OpenAI-compatible endpoints for chat completions.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (supported)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/inception/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('inception') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/meta/","title":"Meta","text":""},{"location":"cookbook/polyglot/llm_api_support/meta/#overview","title":"Overview","text":"<p>Instructor supports Meta LLM inference API. You can find the details on how to configure</p>"},{"location":"cookbook/polyglot/llm_api_support/meta/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('meta') // see /config/llm.php\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/minimaxi/","title":"Minimaxi","text":""},{"location":"cookbook/polyglot/llm_api_support/minimaxi/#overview","title":"Overview","text":"<p>Support for Minimaxi's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/polyglot/llm_api_support/minimaxi/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('minimaxi') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/mistralai/","title":"Mistral AI","text":""},{"location":"cookbook/polyglot/llm_api_support/mistralai/#overview","title":"Overview","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility:  - OutputMode::Tools - supported (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::Json - recommended (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::MdJson - fallback mode (Mistral 7B / Mixtral 8x7B)</p>"},{"location":"cookbook/polyglot/llm_api_support/mistralai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('mistral') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/moonshotai/","title":"MoonshotAI","text":""},{"location":"cookbook/polyglot/llm_api_support/moonshotai/#overview","title":"Overview","text":"<p>Support for MoonshotAI's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/polyglot/llm_api_support/moonshotai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('moonshot-kimi') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/ollama/","title":"Local / Ollama","text":""},{"location":"cookbook/polyglot/llm_api_support/ollama/#overview","title":"Overview","text":"<p>You can use Instructor with local Ollama instance.</p> <p>Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model for complex data schemas.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode, works with any capable model  - OutputMode::Json - recommended  - OutputMode::Tools - supported (for selected models - check Ollama docs)</p>"},{"location":"cookbook/polyglot/llm_api_support/ollama/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('ollama') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/openai/","title":"OpenAI","text":""},{"location":"cookbook/polyglot/llm_api_support/openai/#overview","title":"Overview","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended for new models)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/openai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/openrouter/","title":"OpenRouter","text":""},{"location":"cookbook/polyglot/llm_api_support/openrouter/#overview","title":"Overview","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p>"},{"location":"cookbook/polyglot/llm_api_support/openrouter/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openrouter') // see /config/llm.php\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/perplexity/","title":"Perplexity","text":""},{"location":"cookbook/polyglot/llm_api_support/perplexity/#overview","title":"Overview","text":"<p>Perplexity is a search engine that provides an API for generating text. It is designed to be used in a variety of applications, including chatbots, content generation, and more.</p>"},{"location":"cookbook/polyglot/llm_api_support/perplexity/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('perplexity') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/sambanova/","title":"SambaNova","text":""},{"location":"cookbook/polyglot/llm_api_support/sambanova/#overview","title":"Overview","text":"<p>Support for SambaNova's API, which provide fast inference endpoints for Llama and Qwen LLMs.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/polyglot/llm_api_support/sambanova/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('sambanova') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/togetherai/","title":"Together.ai","text":""},{"location":"cookbook/polyglot/llm_api_support/togetherai/#overview","title":"Overview","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support OutputMode::Tools or OutputMode::Json, which are much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - supported for selected models - OutputMode::Json - supported for selected models - OutputMode::MdJson - fallback mode</p>"},{"location":"cookbook/polyglot/llm_api_support/togetherai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('together') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_api_support/xai/","title":"xAI / Grok","text":""},{"location":"cookbook/polyglot/llm_api_support/xai/#overview","title":"Overview","text":"<p>Support for xAI's API, which offers access to X.com's Grok model.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/polyglot/llm_api_support/xai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('xai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/inference/","title":"Working directly with LLMs","text":""},{"location":"cookbook/polyglot/llm_basics/inference/#overview","title":"Overview","text":"<p><code>Inference</code> class offers access to LLM APIs and convenient methods to execute model inference, incl. chat completions, tool calling or JSON output generation.</p> <p>LLM providers access details can be found and modified via <code>/config/llm.php</code>.</p>"},{"location":"cookbook/polyglot/llm_basics/inference/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: use default LLM connection preset for convenient ad-hoc calls\n$answer = (new Inference)\n    -&gt;with(messages: 'What is capital of Germany')\n    -&gt;get();\n\necho \"USER: What is capital of Germany\\n\";\necho \"ASSISTANT: $answer\\n\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n\n\n\n// EXAMPLE 2: customize inference options using fluent API\n$response = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;create();\n\n$answer = $response-&gt;get();\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\\n\";\nassert(Str::contains($answer, 'Paris'));\n\n\n\n\n// EXAMPLE 3: streaming response\n$stream = (new Inference)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'Describe capital of Brasil']])\n    -&gt;withOptions(['max_tokens' =&gt; 128])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: Describe capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/json_schema_api/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/polyglot/llm_basics/json_schema_api/#overview","title":"Overview","text":"<p>Polyglot has a built-in support for dynamically constructing JSON Schema using <code>JsonSchema</code> class. It is useful when you want to shape the structures during runtime.</p>"},{"location":"cookbook/polyglot/llm_basics/json_schema_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'City name'),\n        JsonSchema::integer('population', 'City population'),\n        JsonSchema::integer('founded', 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with JSON data.']\n        ],\n        responseFormat: $schema-&gt;toResponseFormat(\n            schemaName: 'city_data',\n            schemaDescription: 'City data',\n            strict: true,\n        ),\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data));\nassert(is_string($data['name']));\nassert(is_int($data['population']));\nassert(is_int($data['founded']));\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_json/","title":"Working directly with LLMs and JSON - JSON mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_json/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p> <p><code>Inference</code> class supports multiple inference modes, like <code>Tools</code>, <code>Json</code> <code>JsonSchema</code> or <code>MdJson</code>, which gives you flexibility to choose the best approach for your use case.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_json/#example","title":"Example","text":"<p>In this example we will use OpenAI JSON mode, which guarantees that the response will be in a JSON format.</p> <p>It does not guarantee compliance with a specific schema (for some providers including OpenAI). We can try to work around it by providing an example of the expected JSON output in the prompt.</p> <p>NOTE: Some model providers allow to specify a JSON schema for model to follow via <code>schema</code> parameter of <code>response_format</code>. OpenAI does not support this feature in JSON mode (only in JSON Schema mode).</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with JSON data containing name\", population and year of founding. \\\n           Example: {\"name\": \"Berlin\", \"population\": 3700000, \"founded\": 1237}']],\n        responseFormat: [\n            'type' =&gt; 'json_object',\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Json,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_json_schema/","title":"Working directly with LLMs and JSON - JSON Schema mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_json_schema/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_json_schema/#example","title":"Example","text":"<p>In this example we will use OpenAI JSON Schema mode, which guarantees that the response will be in a JSON format that matches the provided schema.</p> <p>NOTE: Json Schema mode with guaranteed structured outputs is not supported by all language model providers.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n        Respond with JSON data.']],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; [\n                    'type' =&gt; 'object',\n                    'description' =&gt; 'City information',\n                    'properties' =&gt; [\n                        'name' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'City name',\n                        ],\n                        'founded' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Founding year',\n                        ],\n                        'population' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Current population',\n                        ],\n                    ],\n                    'additionalProperties' =&gt; false,\n                    'required' =&gt; ['name', 'founded', 'population'],\n                ],\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_md_json/","title":"Working directly with LLMs and JSON - MdJSON mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_md_json/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_md_json/#example","title":"Example","text":"<p>In this example we will use emulation mode - MdJson, which tries to force the model to generate a JSON output by asking it to respond with a JSON object within a Markdown code block.</p> <p>This is useful for the models which do not support JSON output directly.</p> <p>We will also provide an example of the expected JSON output in the prompt to guide the model in generating the correct response.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with JSON data containing name\", population and year of founding. \\\n           Example: {\"name\": \"Berlin\", \"population\": 3700000, \"founded\": 1237}']],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::MdJson,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_basics/llm_tools/","title":"Working directly with LLMs and JSON - Tools mode","text":""},{"location":"cookbook/polyglot/llm_basics/llm_tools/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/polyglot/llm_basics/llm_tools/#example","title":"Example","text":"<p>In this example we will use OpenAI tools mode, in which model will generate a JSON containing arguments for a function call. This way we can make the model generate a JSON object with specific structure of parameters.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with function call.']],\n        tools: [[\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'extract_data',\n                'description' =&gt; 'Extract city data',\n                'parameters' =&gt; [\n                    'type' =&gt; 'object',\n                    'description' =&gt; 'City information',\n                    'properties' =&gt; [\n                        'name' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'City name',\n                        ],\n                        'founded' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Founding year',\n                        ],\n                        'population' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Current population',\n                        ],\n                    ],\n                    'required' =&gt; ['name', 'founded', 'population'],\n                    'additionalProperties' =&gt; false,\n                ],\n            ],\n        ]],\n        toolChoice: [\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'extract_data'\n            ]\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Tools,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/chat_with_many_participants/","title":"Multi-Participant AI Chat Panel Discussion","text":""},{"location":"cookbook/polyglot/llm_extras/chat_with_many_participants/#overview","title":"Overview","text":"<p>This example demonstrates a sophisticated multi-participant chat system featuring: - System prompt isolation - each AI participant has their own persona - Role normalization - proper LLM role mapping for multi-participant conversations - AI-powered moderation - LLM coordinator decides who should speak next based on context - Clean state management - everything configured in immutable ChatState - Type-safe participant selection - StructuredOutput for decision making</p>"},{"location":"cookbook/polyglot/llm_extras/chat_with_many_participants/#example","title":"Example","text":"<pre><code>&lt;?php\n\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Chat\\ChatFactory;\nuse Cognesy\\Addons\\Chat\\Collections\\Participants;\nuse Cognesy\\Addons\\Chat\\Data\\ChatState;\nuse Cognesy\\Addons\\Chat\\Participants\\LLMParticipant;\nuse Cognesy\\Addons\\Chat\\Participants\\ScriptedParticipant;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ResponseContentCheck;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\necho \"\ud83c\udf99\ufe0f AI PANEL DISCUSSION: The Future of AI Development\\n\";\necho \"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\";\n\n// Create participants with distinct personas using system prompts\n\n$moderator = new ScriptedParticipant(\n    name: 'moderator',\n    messages: [\n        \"Welcome to our panel! Could you each introduce yourselves and share your main focus area in AI?\",\n        \"What do you see as the biggest challenge in AI adoption today?\",\n        \"How do you balance rapid innovation with responsible deployment?\",\n        \"What role should public funding play in AI research and development?\",\n        \"Any final thoughts for our audience about the future of AI?\",\n        \"\", // Empty string to signal end of discussion\n    ],\n);\n\n$researcher = new LLMParticipant(\n    name: 'dr_chen',\n    llmProvider: LLMProvider::using('openai'),\n    systemPrompt: 'You are Dr. Sarah Chen, a distinguished AI researcher at MIT focusing on machine reasoning and safety. You participate in a panel with other experts and a moderator. You speak from deep academic knowledge, cite research when relevant, and always consider long-term implications. Keep responses concise but insightful - 2-3 sentences max. Always end with your signature: \"- Dr. Chen\"'\n);\n\n$engineer = new LLMParticipant(\n    name: 'marcus',\n    llmProvider: LLMProvider::using('openai'),\n    systemPrompt: 'You are Marcus Rodriguez, a Senior AI Engineer at a major tech company with 10+ years building production AI systems. You participate in a panel with other experts and a moderator. You focus on practical implementation, scalability, and real-world challenges. Keep responses brief and pragmatic - 2-3 sentences max. Always end with: \"- Marcus\"'\n);\n\n// Run the panel discussion\n$chat = ChatFactory::default(\n    participants: new Participants($moderator, $engineer, $researcher),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(15, fn(ChatState $state): int =&gt; $state-&gt;stepCount()),\n        new ResponseContentCheck(\n            fn(ChatState $state): ?Messages =&gt; $state-&gt;currentStep()?-&gt;outputMessages(),\n            static fn(Messages $lastResponse): bool =&gt; $lastResponse-&gt;last()-&gt;content()-&gt;toString() !== '',\n        ),\n    ),\n); //-&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print());\n\n$participantNames = [\n    'moderator' =&gt; '\ud83c\udf99\ufe0f Moderator',\n    'dr_chen' =&gt; '\ud83d\udd2c Dr. Chen',\n    'marcus' =&gt; '\u2699\ufe0f Marcus',\n];\n\n$state = new ChatState();\n\nwhile ($chat-&gt;hasNextStep($state)) {\n    $state = $chat-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n\n    if ($step) {\n        $participantName = $step-&gt;participantName();\n        $content = trim($step-&gt;outputMessages()-&gt;toString());\n\n        // Only display if there's actual content\n        if (!empty($content)) {\n            $displayName = $participantNames[$participantName] ?? \"\ud83e\udd16 $participantName\";\n            echo \"\\n$displayName:\\n\";\n            echo str_repeat('-', strlen($displayName)) . \"\\n\";\n            echo \"$content\\n\\n\";\n        }\n    }\n}\necho \"\ud83c\udfac Panel discussion concluded!\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/chat_with_summary/","title":"Chat with summary","text":""},{"location":"cookbook/polyglot/llm_extras/chat_with_summary/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_extras/chat_with_summary/#example","title":"Example","text":"<pre><code>&lt;?php\n\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Chat\\ChatFactory;\nuse Cognesy\\Addons\\Chat\\Collections\\Participants;\nuse Cognesy\\Addons\\Chat\\Data\\ChatState;\nuse Cognesy\\Addons\\Chat\\Participants\\LLMParticipant;\nuse Cognesy\\Addons\\Chat\\Participants\\ScriptedParticipant;\nuse Cognesy\\Addons\\Chat\\Utils\\SummarizeMessages;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ResponseContentCheck;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\AccumulateTokenUsage;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\AppendStepMessages;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\MoveMessagesToBuffer;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\SummarizeBuffer;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\StateProcessors;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n$events = new EventDispatcher();\n\n$student = new ScriptedParticipant(\n    name: 'student',\n    messages: [\n        'Help me get better sales results.',\n        'What should I do next?',\n        'Give me one more actionable tip.',\n        'How could I apply this in practice?',\n        \"What are some common pitfalls to avoid?\",\n        \"Is there a specific mindset I should adopt?\",\n        \"Can you provide an example of a successful sales interaction using Challenger Sale?\",\n        \"How can I tailor my approach to different types of clients?\",\n        \"What questions should I be asking my prospects?\",\n        \"How do I handle objections effectively?\",\n        \"What should I focus on to improve my sales approach?\",\n        \"How can I measure the success of these strategies?\",\n        \"What resources can I use to learn more about Challenger Sale?\",\n        \"Any final advice for implementing these techniques effectively?\",\n        '' // Empty string to signal end of conversation\n    ],\n);\n\n$expert = new LLMParticipant(\n    name: 'expert',\n    llmProvider: LLMProvider::using('openai'),\n    systemPrompt: 'You are a helpful assistant explaining Challenger Sale. Be very brief (one sentence), pragmatic and focused on practical bizdev problems.'\n);\n\n// Build a Chat with summary + buffer processors and an assistant participant\n$chat = ChatFactory::default(\n    participants: new Participants($student, $expert),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(30, fn(ChatState $state): int =&gt; $state-&gt;stepCount()),\n        new ResponseContentCheck(\n            fn(ChatState $state): ?Messages =&gt; $state-&gt;currentStep()?-&gt;outputMessages(),\n            static fn(Messages $lastResponse): bool =&gt; $lastResponse-&gt;toString() !== '',\n        ),\n    ),\n    processors: new StateProcessors(\n        new AccumulateTokenUsage(),\n        new AppendStepMessages(),\n        new MoveMessagesToBuffer(\n            maxTokens: 128,\n            bufferSection: 'buffer',\n            events: $events\n        ),\n        new SummarizeBuffer(\n            maxBufferTokens: 128,\n            maxSummaryTokens: 512,\n            bufferSection: 'buffer',\n            summarySection: 'summary',\n            summarizer: new SummarizeMessages(llm: LLMProvider::using('openai')),\n            events: $events,\n        ),\n    ),\n    events: $events,\n);//-&gt;wiretap(fn(Event $e) =&gt; $e-&gt;printDebug());\n\n$context = \"# CONTEXT\\n\\n\" . file_get_contents(__DIR__ . '/summary.md');\n\n$state = (new ChatState)-&gt;withMessages(\n    Messages::fromString(content: $context, role: 'system')\n);\n\nwhile ($chat-&gt;hasNextStep($state)) {\n    $state = $chat-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n\n    $name = $step?-&gt;participantName() ?? 'unknown';\n    $content = trim($step?-&gt;outputMessages()-&gt;toString() ?? '');\n    echo \"\\n--- Step \" . ($state-&gt;stepCount()) . \" ($name) ---\\n\";\n    echo ($content ?: '[eot]'). \"\\n\";\n//    echo \"---------------------\\n\";\n//    echo \"SUMMARY:\\n\" . $state-&gt;store()-&gt;section('summary')-&gt;get()?-&gt;toString();\n//    echo \"---------------------\\n\";\n//    echo \"BUFFER:\\n\" . $state-&gt;store()-&gt;section('buffer')-&gt;get()?-&gt;toString();\n//    echo \"---------------------\\n\";\n//    echo \"MESSAGES:\\n\" . $state-&gt;store()-&gt;section('messages')-&gt;get()?-&gt;toString();\n//    echo \"=====================\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/image_data/","title":"Using images in prompts","text":""},{"location":"cookbook/polyglot/llm_extras/image_data/#overview","title":"Overview","text":"<p><code>Image</code> class in Instructor PHP provides an easy way to include images in your prompts. It supports loading images from files, URLs, or base64 encoded strings. The image can be sent as part of the message content to the LLM.</p>"},{"location":"cookbook/polyglot/llm_extras/image_data/#example","title":"Example","text":"<p>```php</p> asSystem('You are an expert in car damage assessment.')     -&gt;asUser(Content::empty()         -&gt;addContentPart(ContentPart::text('Describe the car damage in the image.'))         -&gt;addContentPart(Image::fromFile(__DIR__ . '/car-damage.jpg')-&gt;toContentPart())     );  $response = (new Inference)     -&gt;using('openai')     -&gt;withModel('gpt-4o-mini')     -&gt;withMessages($messages)     -&gt;get();  echo \"Response: \" . $response . \"\\n\";   ?&gt;"},{"location":"cookbook/polyglot/llm_extras/prompt_templates/","title":"Prompt Templates","text":""},{"location":"cookbook/polyglot/llm_extras/prompt_templates/#overview","title":"Overview","text":"<p><code>Template</code> class in Instructor PHP provides a way to define and use prompt templates using Twig, Blade or custom 'arrowpipe' template syntax.</p>"},{"location":"cookbook/polyglot/llm_extras/prompt_templates/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Template\\Template;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: Define prompt template inline (don't use files) and use short syntax\n\n$prompt = Template::twig()\n    -&gt;from('What is capital of {{country}}')\n    -&gt;with(['country' =&gt; 'Germany'])\n    -&gt;toText();\n\n$answer = (new Inference)-&gt;withMessages($prompt)-&gt;get();\n\necho \"EXAMPLE 1: prompt = $prompt\\n\";\necho \"ASSISTANT: $answer\\n\";\necho \"\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n// EXAMPLE 2: Load prompt from file\n\n// use default template language, prompt files are in /prompts/twig/&lt;prompt&gt;.twig\n$prompt = Template::text(\n    pathOrDsn: 'demo-twig:capital',\n    variables: ['country' =&gt; 'Germany'],\n);\n\n$answer = (new Inference)-&gt;withMessages($prompt)-&gt;get();\n\necho \"EXAMPLE 2: prompt = $prompt\\n\";\necho \"ASSISTANT: $answer\\n\";\necho \"\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/summary_with_llm/","title":"Simple content summary","text":""},{"location":"cookbook/polyglot/llm_extras/summary_with_llm/#overview","title":"Overview","text":"<p>This is an example of a simple summarization.</p>"},{"location":"cookbook/polyglot/llm_extras/summary_with_llm/#example","title":"Example","text":"<pre><code>&lt;?php\n\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n$summary = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(messages: [\n        ['role' =&gt; 'user', 'content' =&gt; 'Content to summarize:'],\n        ['role' =&gt; 'user', 'content' =&gt; $report],\n        ['role' =&gt; 'user', 'content' =&gt; 'Concise summary of project report in 2-3 sentences:'],\n    ])\n    -&gt;get();\n\ndump($summary);\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/tool_use/","title":"Inference and tool use","text":""},{"location":"cookbook/polyglot/llm_extras/tool_use/#overview","title":"Overview","text":"<p><code>ToolUse</code> class automates the process of using tools by LLM, i.e.:  - calling LLM with provided context (message sequence),  - extracting tool calls requested by LLM from the response,  - calling the requested tool and storing its results,  - constructing message sequence with the result of call,  - sending updated message sequence back to LLM.</p> <p>This cycle is repeated until one of the exit criteria is met: - LLM no longer requests any tool calls, - specified maximum number of iterations is reached, - specified token usage limit is reached - there are any errors during the process (e.g. LLM requested a tool that is not available).</p> <p><code>ToolUse</code> class provides 3 ways to iterate through the process: - manual control - code is responsible for checking <code>hasNextStep()</code> and calling <code>nextStep()</code> in a loop, - using iterator - code uses foreach loop to iterate through the steps (internally it checks <code>hasNextStep()</code> and calls <code>nextStep()</code>), - just get final step - you only get the final step, iteration process is done internally.</p>"},{"location":"cookbook/polyglot/llm_extras/tool_use/#example","title":"Example","text":"<p>This example demonstrates 3 ways to use <code>ToolUse</code> class to allow LLM call functions if needed to answer simple math question. We provide 2 functions (<code>add_numbers</code> and <code>subtract_numbers</code>) as tools available to LLM and specify the task in plain language. The LLM is expected to call the functions in the correct order to get the final result.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ExecutionTimeLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\RetryLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\TokenUsageLimit;\nuse Cognesy\\Addons\\ToolUse\\Collections\\Tools;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseState;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseStep;\nuse Cognesy\\Addons\\ToolUse\\Drivers\\ReAct\\ContinuationCriteria\\StopOnFinalDecision;\nuse Cognesy\\Addons\\ToolUse\\Tools\\FunctionTool;\nuse Cognesy\\Addons\\ToolUse\\ToolUseFactory;\nuse Cognesy\\Messages\\Messages;\n\nfunction add_numbers(int $a, int $b) : int { return $a + $b; }\nfunction subtract_numbers(int $a, int $b) : int { return $a - $b; }\n\n$toolUse = ToolUseFactory::default(\n    tools: new Tools(\n        FunctionTool::fromCallable(add_numbers(...)),\n        FunctionTool::fromCallable(subtract_numbers(...))\n    ),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(6, fn(ToolUseState $state) =&gt; $state-&gt;stepCount()),\n        new TokenUsageLimit(8192, fn(ToolUseState $state) =&gt; $state-&gt;usage()-&gt;total()),\n        new ExecutionTimeLimit(60, fn(ToolUseState $state) =&gt; $state-&gt;startedAt()),\n        new RetryLimit(2, fn(ToolUseState $state) =&gt; $state-&gt;steps(), fn(ToolUseStep $step) =&gt; $step-&gt;hasErrors()),\n        new StopOnFinalDecision(),\n    ),\n);\n\n//\n// PATTERN #1 - manual control\n//\necho \"\\nPATTERN #1 - manual control\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n// iterate until no more steps\nwhile ($toolUse-&gt;hasNextStep($state)) {\n    $state = $toolUse-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n}\n\n// print final response\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n//\n// PATTERN #2 - using iterator\n//\necho \"\\nPATTERN #2 - using iterator\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n// iterate until no more steps\nforeach ($toolUse-&gt;iterator($state) as $currentState) {\n    $step = $currentState-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n    $state = $currentState; // keep the latest state\n}\n\n// print final response\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n\n//\n// PATTERN #3 - just get final step (fast forward to it)\n//\necho \"\\nPATTERN #3 - get only final result\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n// print final response\n$finalState = $toolUse-&gt;finalStep($state);\n$result = $finalState-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_extras/tool_use_react/","title":"Inference and tool use (ReAct driver)","text":""},{"location":"cookbook/polyglot/llm_extras/tool_use_react/#overview","title":"Overview","text":""},{"location":"cookbook/polyglot/llm_extras/tool_use_react/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ExecutionTimeLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\RetryLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\TokenUsageLimit;\nuse Cognesy\\Addons\\ToolUse\\Collections\\Tools;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseState;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseStep;\nuse Cognesy\\Addons\\ToolUse\\Drivers\\ReAct\\ContinuationCriteria\\StopOnFinalDecision;\nuse Cognesy\\Addons\\ToolUse\\Drivers\\ReAct\\ReActDriver;\nuse Cognesy\\Addons\\ToolUse\\Tools\\FunctionTool;\nuse Cognesy\\Addons\\ToolUse\\ToolUseFactory;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\nfunction add_numbers(int $a, int $b) : int { return $a + $b; }\nfunction subtract_numbers(int $a, int $b) : int { return $a - $b; }\n\n$driver = new ReActDriver(\n    llm: LLMProvider::using('openai'),\n    finalViaInference: true,\n);\n\n$toolUse = ToolUseFactory::default(\n    tools: new Tools(\n        FunctionTool::fromCallable(add_numbers(...)),\n        FunctionTool::fromCallable(subtract_numbers(...))\n    ),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(6, fn(ToolUseState $state) =&gt; $state-&gt;stepCount()),\n        new TokenUsageLimit(8192, fn(ToolUseState $state) =&gt; $state-&gt;usage()-&gt;total()),\n        new ExecutionTimeLimit(60, fn(ToolUseState $state) =&gt; $state-&gt;startedAt()),\n        new RetryLimit(2, fn(ToolUseState $state) =&gt; $state-&gt;steps(), fn(ToolUseStep $step) =&gt; $step-&gt;hasErrors()),\n        new StopOnFinalDecision(),\n    ),\n    driver: $driver\n);\n\n\n//\n// PATTERN #1 - manual control\n//\necho \"\\nReAct PATTERN #1 - manual control\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\nwhile ($toolUse-&gt;hasNextStep($state)) {\n    $state = $toolUse-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n}\n\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n//\n// PATTERN #2 - using iterator\n//\necho \"\\nReAct PATTERN #2 - using iterator\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\nforeach ($toolUse-&gt;iterator($state) as $currentState) {\n    $step = $currentState-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n    $state = $currentState; // keep the latest state\n}\n\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n//\n// PATTERN #3 - just get final step (fast forward to it)\n//\necho \"\\nReAct PATTERN #3 - final via Inference (optional)\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n$finalState = $toolUse-&gt;finalStep($state);\n$result = $finalState-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n?&gt;\n</code></pre>"},{"location":"cookbook/polyglot/llm_troubleshooting/http_debug/","title":"Debugging HTTP Calls","text":""},{"location":"cookbook/polyglot/llm_troubleshooting/http_debug/#overview","title":"Overview","text":"<p>Instructor PHP provides a way to debug HTTP calls made to LLM APIs via <code>withDebug()</code> method call on <code>Inference</code> object.</p> <p>When debug mode is turned on all HTTP requests and responses are dumped to the console.</p>"},{"location":"cookbook/polyglot/llm_troubleshooting/http_debug/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$response = (new Inference)\n    -&gt;withDebugPreset('on') // Enable debug mode\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of Brasil']],\n        options: ['max_tokens' =&gt; 128]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of Brasil\\n\";\necho \"ASSISTANT: $response\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/break_down_complexity/","title":"Break Down Complex Tasks","text":""},{"location":"cookbook/prompting/decomposition/break_down_complexity/#overview","title":"Overview","text":"<p>How can we help LLMs handle complex tasks more effectively?</p> <p>Decomposed Prompting leverages a Language Model (LLM) to deconstruct a complex task into a series of manageable sub-tasks. Each sub-task is then processed by specific functions, enabling the LLM to handle intricate problems more effectively and systematically.</p> <p>This approach breaks down complexity by: - Generating an action plan using the LLM - Executing each step systematically - Using specific operations like Split, StrPos, and Merge</p>"},{"location":"cookbook/prompting/decomposition/break_down_complexity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum ActionType: string {\n    case Split = 'split';\n    case StrPos = 'strpos';\n    case Merge = 'merge';\n}\n\nclass Split {\n    public function __construct(\n        public string $split_char\n    ) {}\n\n    public function execute(string $input): array {\n        return explode($this-&gt;split_char, $input);\n    }\n}\n\nclass StrPos {\n    public function __construct(\n        public int $index\n    ) {}\n\n    public function execute(array $input): array {\n        return array_map(fn($str) =&gt; $str[$this-&gt;index] ?? '', $input);\n    }\n}\n\nclass Merge {\n    public function __construct(\n        public string $merge_char\n    ) {}\n\n    public function execute(array $input): string {\n        return implode($this-&gt;merge_char, $input);\n    }\n}\n\nclass Action {\n    public function __construct(\n        public int $id,\n        public ActionType $type,\n        public string|int $parameter\n    ) {}\n}\n\nclass ActionPlan {\n    public function __construct(\n        public string $initial_data,\n        /** @var Action[] */\n        public array $plan\n    ) {}\n}\n\nclass DecomposedTaskSolver {\n    public function __invoke(string $taskDescription): string {\n        $plan = $this-&gt;deriveActionPlan($taskDescription);\n        return $this-&gt;executePlan($plan);\n    }\n\n    private function deriveActionPlan(string $taskDescription): ActionPlan {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'Generate an action plan to help complete the task. Available actions: Split (split string by character), StrPos (get character at index from each string), Merge (join strings with character)'],\n                ['role' =&gt; 'user', 'content' =&gt; $taskDescription],\n            ],\n            responseModel: ActionPlan::class,\n        )-&gt;get();\n    }\n\n    private function executePlan(ActionPlan $plan): string {\n        $current = $plan-&gt;initial_data;\n\n        foreach ($plan-&gt;plan as $action) {\n            match ($action-&gt;type) {\n                ActionType::Split =&gt; $current = (new Split($action-&gt;parameter))-&gt;execute($current),\n                ActionType::StrPos =&gt; $current = (new StrPos($action-&gt;parameter))-&gt;execute($current),\n                ActionType::Merge =&gt; $current = (new Merge($action-&gt;parameter))-&gt;execute($current),\n            };\n        }\n\n        return $current;\n    }\n}\n\n$result = (new DecomposedTaskSolver)('Concatenate the second letter of every word in \"Jack Ryan\" together');\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/break_down_complexity/#references","title":"References","text":"<ol> <li>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</li> </ol>"},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/","title":"Ditch Vanilla Chain Of Thought","text":""},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/#overview","title":"Overview","text":"<p>How can we improve the effectiveness of Zero-Shot Chain of Thought (CoT) prompts?</p> <p>Plan and Solve improves the use of Zero-Shot Chain of Thought by adding more detailed instructions to the prompt given to large language models.</p> <p>Plan and Solve Process:</p> <ol> <li>Generate Reasoning: Prompt the model to explicitly devise a plan for solving a problem before generating intermediate reasoning</li> <li>Extract Answer: Extract the final answer from the model's chain of thought</li> </ol> <p>The key improvement is guiding the LLM to pay more attention to calculation and intermediate results to ensure they are correctly performed.</p>"},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Reasoning {\n    public function __construct(\n        public string $chain_of_thought\n    ) {}\n}\n\nclass Response {\n    public function __construct(\n        public string $correct_answer\n    ) {}\n}\n\nclass PlanAndSolveSolver {\n    public function __invoke(string $query): string {\n        $reasoning = $this-&gt;generateReasoning($query);\n        $response = $this-&gt;extractAnswer($query, $reasoning);\n        return $response-&gt;correct_answer;\n    }\n\n    private function generateReasoning(string $query): Reasoning {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"&lt;user_query&gt;\n{$query}\n&lt;/user_query&gt;\n\nLet's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.\"\n                ],\n            ],\n            responseModel: Reasoning::class,\n        )-&gt;get();\n    }\n\n    private function extractAnswer(string $query, Reasoning $reasoning): Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"&lt;user_query&gt;\n{$query}\n&lt;/user_query&gt;\n\nLet's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.\n\n&lt;reasoning&gt;\n{$reasoning-&gt;chain_of_thought}\n&lt;/reasoning&gt;\n\nTherefore the answer (arabic numerals) is\"\n                ],\n            ],\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$result = (new PlanAndSolveSolver)(\n    \"In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?\"\n);\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/#references","title":"References","text":"<ol> <li>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</li> </ol>"},{"location":"cookbook/prompting/decomposition/generate_code/","title":"Generate Code for Intermediate Steps","text":""},{"location":"cookbook/prompting/decomposition/generate_code/#overview","title":"Overview","text":"<p>How can we leverage external code execution to generate intermediate reasoning steps?</p> <p>Program of Thought aims to leverage an external code interpreter to generate intermediate reasoning steps. This helps achieve greater performance in mathematical and programming-related tasks by grounding our final response in deterministic code.</p> <p>The approach involves: 1. Generate Code: Create a solver function that implements step-by-step logic 2. Execute Code: Run the generated code to get deterministic results 3. Extract Answer: Use the computed result to make final predictions</p>"},{"location":"cookbook/prompting/decomposition/generate_code/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Choice: string {\n    case A = 'A';\n    case B = 'B';\n    case C = 'C';\n    case D = 'D';\n    case E = 'E';\n}\n\nclass Prediction {\n    public function __construct(\n        public Choice $choice\n    ) {}\n}\n\nclass ProgramExecution {\n    public function __construct(\n        public string $program_code\n    ) {}\n}\n\nclass ProgramOfThoughtSolver {\n    private const PREFIX = '\\&lt;?php\n// Answer this question by implementing a solver()\n// function, use for loop if necessary.\nfunction solver() {\n    // Let\\'s write a PHP program step by step,\n    // and then return the answer\n    // Firstly, we need to define the following\n    // variables:';\n\n    public function __invoke(string $query, array $options): string {\n        $reasoning = $this-&gt;generateIntermediateReasoning($query);\n        $answer = $this-&gt;executeProgram($reasoning-&gt;program_code);\n        $prediction = $this-&gt;generatePrediction($answer, $options, $query);\n        return $prediction-&gt;choice-&gt;value;\n    }\n\n    private function generateIntermediateReasoning(string $query): ProgramExecution {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'system',\n                    'content' =&gt; 'You are a world class AI system that excels at answering user queries in a systematic and detailed manner. Generate a valid PHP program that can be executed to answer the user query.\n\nMake sure to begin your generated program with the following structure:\n```php\n&lt;?php\nfunction solver() {\n    // Your step-by-step logic here\n    // Define variables, perform calculations\n    // Return the final answer\n}\nreturn solver();\n```'\n                ],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n            responseModel: ProgramExecution::class,\n        )-&gt;get();\n    }\n\n    private function executeProgram(string $code): mixed {\n        try {\n            $sanitized = $this-&gt;sanitizeCode($code);\n            // Ensure we get a value even if the model forgot to return it explicitly\n            if (strpos($sanitized, 'return') === false) {\n                $sanitized .= \"\\nreturn (function(){ return function_exists('solver') ? solver() : null; })();\";\n            }\n            return eval($sanitized);\n        } catch (Throwable $e) {\n            throw new Exception(\"Program execution failed: \" . $e-&gt;getMessage());\n        }\n    }\n\n    private function sanitizeCode(string $code): string {\n        // Strip Markdown fences\n        $code = preg_replace('/^\\s*```[a-zA-Z]*\\s*/', '', $code);\n        $code = preg_replace('/```\\s*$/', '', (string) $code);\n        // Remove PHP open/close tags \u2014 eval() expects pure PHP code body\n        $code = str_replace(['&lt;?php', '&lt;?', '?&gt;'], '', (string) $code);\n        // Normalize line endings and trim\n        $code = trim((string) $code);\n        return $code;\n    }\n\n    private function generatePrediction(mixed $predictedAnswer, array $options, string $query): Prediction {\n        $formattedOptions = implode(', ', $options);\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'system',\n                    'content' =&gt; \"Find the closest option based on the question and prediction.\n\nQuestion: {$query}\nPrediction: {$predictedAnswer}\nOptions: [{$formattedOptions}]\"\n                ],\n            ],\n            responseModel: Prediction::class,\n        )-&gt;get();\n    }\n}\n\n$result = (new ProgramOfThoughtSolver)(\n    \"A trader sold an article at a profit of 20% for Rs.360. What is the cost price of the article?\",\n    [\"A)270\", \"B)300\", \"C)280\", \"D)320\", \"E)315\"]\n);\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/generate_code/#references","title":"References","text":"<ol> <li>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</li> </ol>"},{"location":"cookbook/prompting/decomposition/generate_in_parallel/","title":"Generate in Parallel","text":""},{"location":"cookbook/prompting/decomposition/generate_in_parallel/#overview","title":"Overview","text":"<p>How can we decrease the latency of an LLM pipeline?</p> <p>Skeleton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel. The parallelism can be achieved by parallel API calls or batched processing.</p> <p>The approach involves: 1. Generate Skeleton: Create a brief outline of the response structure 2. Parallel Expansion: Complete each skeleton point concurrently 3. Assembly: Combine the expanded points into the final response</p>"},{"location":"cookbook/prompting/decomposition/generate_in_parallel/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Point {\n    public function __construct(\n        public int $index,\n        public string $description\n    ) {}\n}\n\nclass Skeleton {\n    public function __construct(\n        /** @var Point[] */\n        public array $points\n    ) {}\n}\n\nclass Response {\n    public function __construct(\n        public string $response\n    ) {}\n}\n\nclass SkeletonOfThoughtGenerator {\n    public function __invoke(string $question): array {\n        $skeleton = $this-&gt;getSkeleton($question);\n        return $this-&gt;expandPointsSequentially($question, $skeleton);\n    }\n\n    private function getSkeleton(string $question): Skeleton {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.\nProvide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question.\nInstead of writing a full sentence, each skeleton point should be very short with only 3-5 words.\nGenerally, the skeleton should have 3-10 points.\n\nNow, please provide the skeleton for the following question.\n\n&lt;question&gt;\n{$question}\n&lt;/question&gt;\n\nSkeleton:\"\n                ],\n            ],\n            responseModel: Skeleton::class,\n        )-&gt;get();\n    }\n\n    private function expandPoint(string $question, Skeleton $skeleton, int $pointIndex): Response {\n        $skeletonText = '';\n        foreach ($skeleton-&gt;points as $point) {\n            $skeletonText .= \"{$point-&gt;index}. {$point-&gt;description}\\n\";\n        }\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"You're responsible for continuing the writing of one and only one point in the overall answer to the following question.\n\n&lt;question&gt;\n{$question}\n&lt;/question&gt;\n\nThe skeleton of the answer is:\n\n&lt;skeleton&gt;\n{$skeletonText}\n&lt;/skeleton&gt;\n\nContinue and only continue the writing of point {$pointIndex}.\nWrite it **very shortly** in 1-2 sentences and do not continue with other points!\"\n                ],\n            ],\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n\n    private function expandPointsSequentially(string $question, Skeleton $skeleton): array {\n        $responses = [];\n        foreach ($skeleton-&gt;points as $point) {\n            $response = $this-&gt;expandPoint($question, $skeleton, $point-&gt;index);\n            $responses[] = [\n                'point' =&gt; $point,\n                'content' =&gt; $response-&gt;response\n            ];\n        }\n        return $responses;\n    }\n}\n\n$results = (new SkeletonOfThoughtGenerator)(\n    \"Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\"\n);\n\necho \"Generated Content:\\n\";\necho str_repeat(\"=\", 50) . \"\\n\";\n\nforeach ($results as $result) {\n    echo \"Point {$result['point']-&gt;index}: {$result['point']-&gt;description}\\n\";\n    echo \"{$result['content']}\\n\\n\";\n}\n\ndump($results);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/generate_in_parallel/#references","title":"References","text":"<ol> <li>Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/","title":"Solve Simpler Subproblems","text":""},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/#overview","title":"Overview","text":"<p>How can we encourage an LLM to solve complex problems by breaking them down?</p> <p>Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.</p> <p>Subproblems Example: - Original problem: Adam is twice as old as Mary. Adam will be 11 in 1 year. How old is Mary? - Subproblems: (1) How old is Adam now? (2) What is half of Adam's current age?</p> <p>These subproblems are solved sequentially, allowing the answers from earlier (simpler) subproblems to inform the LLM while solving later (more complex) subproblems.</p>"},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Subquestion {\n    public function __construct(\n        public string $question\n    ) {}\n}\n\nclass Answer {\n    public function __construct(\n        public int $answer\n    ) {}\n}\n\nclass SubquestionWithAnswer {\n    public function __construct(\n        public string $question,\n        public int $answer\n    ) {}\n}\n\nclass LeastToMostSolver {\n    public function __invoke(string $question): array {\n        $subquestions = $this-&gt;decompose($question);\n        return $this-&gt;solveSequentially($subquestions, $question);\n    }\n\n    private function decompose(string $question): array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user', \n                    'content' =&gt; \"Break this question down into subquestions to solve sequentially: {$question}\"\n                ],\n            ],\n            responseModel: Sequence::of(Subquestion::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    private function solve(string $question, array $solvedQuestions, string $originalQuestion): int {\n        $solvedContext = '';\n        foreach ($solvedQuestions as $solved) {\n            $solvedContext .= \"{$solved-&gt;question} {$solved-&gt;answer}\\n\";\n        }\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; &lt;&lt;&lt;PROMPT\n                        &lt;original_question&gt;\n                        {$originalQuestion}\n                        &lt;/original_question&gt;\n\n                        &lt;solved_subquestions&gt;\n                        {$solvedContext}\n                        &lt;/solved_subquestions&gt;\n\n                        Solve this next subquestion: {$question}\n                    PROMPT,\n                ],\n            ],\n            responseModel: Answer::class,\n        )-&gt;get()-&gt;answer;\n    }\n\n    private function solveSequentially(array $subquestions, string $originalQuestion): array {\n        $solvedQuestions = [];\n\n        foreach ($subquestions as $subquestion) {\n            $answer = $this-&gt;solve($subquestion-&gt;question, $solvedQuestions, $originalQuestion);\n            $solvedQuestions[] = new SubquestionWithAnswer($subquestion-&gt;question, $answer);\n        }\n\n        return $solvedQuestions;\n    }\n}\n\n$results = (new LeastToMostSolver)(\n    \"Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody?\"\n);\n\nforeach ($results as $result) {\n    echo \"{$result-&gt;question} {$result-&gt;answer}\\n\";\n}\n\ndump($results);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/#references","title":"References","text":"<ol> <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/decomposition/task_specific_systems/","title":"Leverage Task Specific Systems","text":""},{"location":"cookbook/prompting/decomposition/task_specific_systems/#overview","title":"Overview","text":"<p>How can we improve the faithfulness of reasoning chains generated by Language Models?</p> <p>Faithful Chain of Thought improves the faithfulness of reasoning chains by breaking it up into two stages:</p> <ol> <li>Translation: Translate a user query into a series of reasoning steps - task-specific steps that can be executed deterministically</li> <li>Problem Solving: Execute steps and arrive at a final answer that is consistent with the reasoning steps</li> </ol> <p>Examples of task-specific systems: - Math Word Problems: PHP code that can be evaluated to derive a final answer - Multi-Hop QA: Multi-step reasoning process using programming logic - Planning: Generate symbolic goals and use planning systems to solve queries</p>"},{"location":"cookbook/prompting/decomposition/task_specific_systems/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningStep {\n    public function __construct(\n        public int $id,\n        /** @var string[] */\n        public array $rationale,\n        /** @var int[] */\n        public array $dependencies,\n        public string $eval_string\n    ) {}\n}\n\nclass TaskSpecificReasoner {\n    public function __invoke(string $query): mixed {\n        $steps = $this-&gt;generateReasoningSteps($query);\n        return $this-&gt;executeSteps($steps);\n    }\n\n    private function generateReasoningSteps(string $query): array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'system', \n                    'content' =&gt; 'You are a world class AI who excels at generating reasoning steps to answer a question. Generate a list of reasoning steps needed to answer the question.\n\nAt each point you should either:\n- declare a variable to be referenced later on\n- combine multiple variables together to generate a new result that you should store in another variable\n\nThe final answer should be stored in a variable called $answer.\n\nUse valid PHP syntax for variable assignments.'\n                ],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n            responseModel: Sequence::of(ReasoningStep::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    private function executeSteps(array $steps): mixed {\n        $code = [];\n        foreach ($steps as $step) {\n            $code[] = $step-&gt;eval_string;\n        }\n\n        $fullCode = \"&lt;?php\\n\" . implode(\"\\n\", $code) . \"\\nreturn \\$answer;\";\n\n        $result = eval(substr($fullCode, 5));\n        return $result;\n    }\n}\n\n$result = (new TaskSpecificReasoner)(\n    'If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot after another 2 more arrive?'\n);\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/task_specific_systems/#references","title":"References","text":"<ol> <li>Faithful Chain-of-Thought Reasoning</li> </ol>"},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/","title":"Combine Multiple Reasoning Chains","text":""},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/#overview","title":"Overview","text":"<p>Meta Chain-of-Thought (Meta-CoT) decomposes a query into sub-queries, solves each with its own reasoning chain, then composes a final answer from those chains.</p>"},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningAndResponse {\n    public string $intermediate_reasoning;\n    public string $correct_answer;\n}\n\nclass MaybeResponse {\n    public ?ReasoningAndResponse $result = null;\n    public ?bool $error = null;\n    public ?string $error_message = null;\n}\n\nclass QueryDecomposition { /** @var string[] */ public array $queries; }\n\nclass MetaCOT {\n    public function __invoke(string $query) : ReasoningAndResponse {\n        $subs = $this-&gt;decompose($query)-&gt;queries;\n        $chains = [];\n        foreach ($subs as $q) { $chains[] = $this-&gt;chain($q); }\n        return $this-&gt;final($query, $chains);\n    }\n\n    public function decompose(string $query) : QueryDecomposition {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'Decompose the user query into minimal sub-queries needed to derive the answer.'],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n            responseModel: QueryDecomposition::class,\n        )-&gt;get();\n    }\n\n    public function chain(string $query) : MaybeResponse {\n        $system = &lt;&lt;&lt;TXT\n        Given a question, answer step-by-step.\n        Provide intermediate reasoning and the final answer.\n        If impossible, set error=true and include an error_message.\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $system], ['role' =&gt; 'user', 'content' =&gt; $query] ],\n            responseModel: MaybeResponse::class,\n        )-&gt;get();\n    }\n\n    public function final(string $query, array $context) : ReasoningAndResponse {\n        $parts = [];\n        foreach ($context as $c) {\n            if ($c instanceof MaybeResponse &amp;&amp; !$c-&gt;error &amp;&amp; $c-&gt;result) {\n                $parts[] = $c-&gt;result-&gt;intermediate_reasoning . \"\\n\" . $c-&gt;result-&gt;correct_answer;\n            }\n        }\n        $formatted = implode(\"\\n\", $parts);\n        $system = &lt;&lt;&lt;TXT\n        Given a question and context, answer step-by-step.\n        If unsure, answer \"Unknown\".\n        TXT;\n        $prompt = &lt;&lt;&lt;PR\n        &lt;question&gt;\n        {$query}\n        &lt;/question&gt;\n        &lt;context&gt;\n        {$formatted}\n        &lt;/context&gt;\n        PR;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $system], ['role' =&gt; 'user', 'content' =&gt; $prompt] ],\n            responseModel: ReasoningAndResponse::class,\n        )-&gt;get();\n    }\n}\n\n$query = \"Would Arnold Schwarzenegger have been able to deadlift an adult Black rhinoceros at his peak strength?\";\n$result = (new MetaCOT)($query);\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/#references","title":"References","text":"<p>1) Answering Questions by Meta-Reasoning over Multiple Chains of Thought (https://arxiv.org/pdf/2304.13007)</p>"},{"location":"cookbook/prompting/ensembling/combine_responses/","title":"Use LLMs to Combine Different Responses","text":""},{"location":"cookbook/prompting/ensembling/combine_responses/#overview","title":"Overview","text":"<p>Universal Self-Consistency uses a second LLM to judge the quality of multiple responses to a query and select the most consistent one.</p>"},{"location":"cookbook/prompting/ensembling/combine_responses/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ResponseItem { public string $chain_of_thought; public string $answer; }\nclass SelectedResponse { public int $most_consistent_response_id; }\n\nclass CombineResponses {\n    public function __invoke(string $query, int $k = 3) : ResponseItem {\n        $responses = [];\n        for ($i = 0; $i &lt; $k; $i++) { $responses[] = $this-&gt;generate($query); }\n        $sel = $this-&gt;select($responses, $query);\n        $idx = max(0, min($sel-&gt;most_consistent_response_id, count($responses)-1));\n        return $responses[$idx];\n    }\n\n    private function generate(string $query) : ResponseItem {\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user', 'content'=&gt;$query] ],\n            responseModel: ResponseItem::class,\n        )-&gt;get();\n    }\n\n    private function select(array $responses, string $query) : SelectedResponse {\n        $formatted = [];\n        foreach ($responses as $i =&gt; $r) { $formatted[] = \"Response {$i}: {$r-&gt;chain_of_thought}. {$r-&gt;answer}\"; }\n        $content = \"&lt;user query&gt;\\n{$query}\\n&lt;/user query&gt;\\n\\n\" . implode(\"\\n\", $formatted) . \"\\n\\nEvaluate these responses. Select the most consistent response based on majority consensus.\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$content] ],\n            responseModel: SelectedResponse::class,\n        )-&gt;get();\n    }\n}\n\n$query = \"The three-digit number 'ab5' is divisible by 3. How many different three-digit numbers can 'ab5' represent?\";\n$result = (new CombineResponses)($query, k: 3);\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/combine_responses/#references","title":"References","text":"<p>1) Universal Self-Consistency For Large Language Model Generation (https://arxiv.org/pdf/2311.17311)</p>"},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/","title":"Combine Different Specialized LLMs","text":""},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/#overview","title":"Overview","text":"<p>Mixture of Reasoning Experts (MoRE) combines specialized experts (e.g., factual with evidence, and multi-hop reasoning) and selects the best answer using a scorer.</p>"},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass MultihopExpert { public string $chain_of_thought; public string $answer; }\nclass FactualExpert { public string $answer; }\nclass ModelScore { public float $score; }\n\nclass MoRE {\n    public function factual(string $query, array $evidences) : FactualExpert {\n        $formatted = '- ' . implode(\"\\n-\", $evidences);\n        $system = \"&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\\n\\n&lt;evidences&gt;\\n{$formatted}\\n&lt;/evidences&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system] ],\n            responseModel: FactualExpert::class,\n        )-&gt;get();\n    }\n\n    public function multihop(string $query) : MultihopExpert {\n        $system = \"&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system] ],\n            responseModel: MultihopExpert::class,\n        )-&gt;get();\n    }\n\n    public function score(string $query, string $answer) : ModelScore {\n        $messages = [\n            ['role'=&gt;'system','content'=&gt;'You score answers by how well they answer the user query (0..1).'],\n            ['role'=&gt;'user','content'=&gt;\"&lt;user query&gt;\\n{$query}\\n&lt;/user query&gt;\\n\\n&lt;response&gt;\\n{$answer}\\n&lt;/response&gt;\"],\n        ];\n        return (new StructuredOutput)-&gt;with(\n            messages: $messages,\n            responseModel: ModelScore::class,\n        )-&gt;get();\n    }\n}\n\n$query = \"Who's the original singer of Help Me Make It Through The Night?\";\n$evidences = [\"Help Me Make It Through The Night is a country music ballad written and composed by Kris Kristofferson and released on his 1970 album 'Kristofferson'\"];\n$threshold = 0.8;\n\n$more = new MoRE();\n$factual = $more-&gt;factual($query, $evidences);\n$multihop = $more-&gt;multihop($query);\n\n$fScore = $more-&gt;score($query, $factual-&gt;answer)-&gt;score ?? 0.0;\n$mScore = $more-&gt;score($query, $multihop-&gt;answer)-&gt;score ?? 0.0;\n\n$answer = '';\nif (max($fScore, $mScore) &lt; $threshold) {\n    $answer = 'Abstaining from responding';\n} else {\n    $answer = ($fScore &gt; $mScore) ? $factual-&gt;answer : $multihop-&gt;answer;\n}\n\ndump($answer);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/#references","title":"References","text":"<p>1) Getting MoRE out of Mixture of Language Model Reasoning Experts (https://arxiv.org/pdf/2305.14628)</p>"},{"location":"cookbook/prompting/ensembling/consistent_examples/","title":"Prioritize Consistent Examples","text":"<p>Consistency Based Self Adaptive Prompting (COSP)1 aims to improve LLM output quality by generating high quality few shot examples to be included in the final prompt. These are examples without labelled ground truth so they use self-consistency and a metric known as normalized entropy to select the best examples.</p> <p>Once they've selected the examples, they then append them to the prompt and generate multiple reasoning chains before selecting the final result using Self-Consistency.</p> <p>COSP process\u00b6</p> <p>How does this look in practice? Let's dive into greater detail.</p> <p>Step 1 - Selecting Examples\u00b6 In the first step, we try to generate high quality examples from questions that don't have ground truth labels. This is challenging because we want to find a way to automatically determine answer quality when sampling our model multiple times.</p> <p>In this case, we have n questions which we want to generate m possible reasoning chains for each question. This gives a total of nm examples. We then want to filter out k final few shot examples from these nm examples to be included inside our final prompt.</p> <p>Using chain of thought, we first generate m responses for each question. These responses contain a final answer and a rationale behind that answer. We compute a score for each response using a weighted sum of two values - normalized entropy and repetitiveness ( How many times this rationale appears for this amswer ) We rank all of our nm responses using this score and choose the k examples with the lowest scores as our final few shot examples.</p> <p>Normalized Entropy</p> <p>In the paper, the authors write that normalized entropy is a good proxy over a number of different tasks where low entropy is positively correlated with correctness. Entropy is also supposed to range from 0 to 1.</p> <p>Therefore in order to do so, we introduce a - term in our implementation so that the calculated values range from 0 to 1.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass CoTResponse { /** @var string[] */ public array $chain_of_thought; public int $answer; }\nclass ScoredExample { public string $query; public CoTResponse $response; public float $score; }\n\nclass COSPSelector {\n    public function __construct(private int $m = 3) {}\n\n    public function generate(string $query) : array {\n        $out = [];\n        for ($i = 0; $i &lt; $this-&gt;m; $i++) { $out[] = $this-&gt;cot($query); }\n        return $out;\n    }\n\n    public function scoreExamples(string $query, array $responses) : array {\n        $entropy = $this-&gt;normalizedEntropy(array_map(fn($r)=&gt;$r-&gt;answer, $responses));\n        $repetitiveness = $this-&gt;repetitiveness($responses);\n        $scored = [];\n        foreach ($responses as $r) {\n            $s = $entropy - $repetitiveness; // lower is better\n            $se = new ScoredExample(); $se-&gt;query = $query; $se-&gt;response = $r; $se-&gt;score = $s; $scored[] = $se;\n        }\n        return $scored;\n    }\n\n    public function select(array $candidates, int $k) : array {\n        $all = [];\n        foreach ($candidates as $q) { $all = array_merge($all, $this-&gt;scoreExamples($q, $this-&gt;generate($q))); }\n        usort($all, fn($a,$b)=&gt; $a-&gt;score &lt;=&gt; $b-&gt;score);\n        return array_slice($all, 0, $k);\n    }\n\n    private function cot(string $query) : CoTResponse {\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$query] ],\n            responseModel: CoTResponse::class,\n        )-&gt;get();\n    }\n\n    private function normalizedEntropy(array $answers) : float {\n        $n = count($answers); if ($n === 0) return 0.0; $freq=[]; foreach($answers as $a){$freq[$a]=($freq[$a]??0)+1;}\n        $h = 0.0; foreach ($freq as $c) { $p = $c / $n; $h += ($p &gt; 0) ? -$p * log($p) : 0.0; }\n        $maxH = log(max(1, count($freq)));\n        return $maxH &gt; 0 ? $h / $maxH : 0.0;\n    }\n\n    private function repetitiveness(array $responses) : float {\n        // approximate: 1 - (unique rationales / total)\n        $rationales = array_map(fn($r)=&gt; implode(' ', $r-&gt;chain_of_thought), $responses);\n        $unique = count(array_unique($rationales));\n        $n = max(1, count($responses));\n        return 1.0 - ($unique / $n);\n    }\n}\n\n$questions = [\n    'How many loaves of bread did they have left?',\n    'How many pages does James write in a year?',\n];\n$selector = new COSPSelector(m: 3);\n$best = $selector-&gt;select($questions, k: 3);\ndump($best);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/consistent_examples/#references","title":"References","text":"<p>1: Better Zero-Shot Reasoning with Self-Adaptive Prompting (https://arxiv.org/pdf/2305.14106)</p>"},{"location":"cookbook/prompting/ensembling/distinct_examples/","title":"Use Distinct Example Subsets","text":""},{"location":"cookbook/prompting/ensembling/distinct_examples/#overview","title":"Overview","text":"<p>Demonstration Ensembling (DENSE) runs multiple prompts, each with a different subset of examples, then aggregates the outputs.</p>"},{"location":"cookbook/prompting/ensembling/distinct_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Sentiment: string { case Positive='Positive'; case Negative='Negative'; case Neutral='Neutral'; }\nclass DemonstrationResponse { public Sentiment $correct_answer; }\n\nclass DenseEnsembling {\n    public function __invoke(string $prompt, array $examples, int $numResponses) : array {\n        if ($numResponses &lt;= 0 || count($examples) % $numResponses !== 0) return [];\n        $batch = intdiv(count($examples), $numResponses);\n        $outputs = [];\n        for ($i = 0; $i &lt; count($examples); $i += $batch) {\n            $subset = array_slice($examples, $i, $batch);\n            $outputs[] = $this-&gt;one($prompt, $subset);\n        }\n        return $outputs;\n    }\n\n    private function one(string $prompt, array $examples) : DemonstrationResponse {\n        $joined = implode(\"\\n\", $examples);\n        $system = &lt;&lt;&lt;TXT\n        You classify queries as Positive, Negative, or Neutral.\n        Refer to the provided examples. Examine each before deciding.\n        Here are the examples:\n        {$joined}\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;$prompt] ],\n            responseModel: DemonstrationResponse::class,\n            options: ['temperature' =&gt; 0.0],\n        )-&gt;get();\n    }\n}\n\n$userQuery = 'What is the weather like today?';\n$examples = [\n    'I love this product! [Positive]',\n    'This is the worst service ever. [Negative]',\n    'The movie was okay, not great but not terrible. [Neutral]',\n    \"I'm so happy with my new phone! [Positive]\",\n    'The food was terrible and the service was slow. [Negative]',\n    \"It's an average day, nothing special. [Neutral]\",\n    'Fantastic experience, will come again! [Positive]',\n    \"I wouldn't recommend this to anyone. [Negative]\",\n    'The book was neither good nor bad. [Neutral]',\n    'Absolutely thrilled with the results! [Positive]',\n];\n\n$responses = (new DenseEnsembling)($userQuery, $examples, 5);\n$counts = [];\nforeach ($responses as $r) { $k = $r-&gt;correct_answer-&gt;value; $counts[$k] = ($counts[$k] ?? 0) + 1; }\narsort($counts);\n$mostCommon = array_key_first($counts);\ndump($mostCommon);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/distinct_examples/#references","title":"References","text":"<p>1) Exploring Demonstration Ensembling for In-Context Learning (https://arxiv.org/pdf/2308.08780)</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/","title":"Use Ensembles To Test Prompts","text":""},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#overview","title":"Overview","text":""},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#whats-max-mutual-information","title":"What's Max Mutual Information?","text":"<p>Max Mutual Information Method aims to find the best prompt to elicit the desired response from an LLM by maximizing a mutual information proxy \u2014 i.e., reducing model uncertainty with the prompt.</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#entropy","title":"Entropy","text":"<p>When a language model receives a prompt, it produces a distribution over outputs. Lower entropy suggests higher confidence.</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#mutual-information","title":"Mutual Information","text":"<p>We approximate mutual information as the difference between marginal and conditional entropies of outputs across multiple samples for a given prompt. Below, we use a lightweight proxy based on answer diversity and rationale repetitiveness.</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass CoT { /** @var string[] */ public array $chain_of_thought; public int $answer; }\nclass PromptScore { public string $prompt; public float $score; }\n\nclass PromptEnsembler {\n    public function __construct(private int $k = 3) {}\n    public function evaluate(array $prompts, string $question) : array {\n        $scored = [];\n        foreach ($prompts as $p) { $scored[] = $this-&gt;scorePrompt($p, $question); }\n        usort($scored, fn($a,$b)=&gt; $a-&gt;score &lt;=&gt; $b-&gt;score);\n        return $scored; // lower is better (proxy for MI)\n    }\n    private function scorePrompt(string $prompt, string $question) : PromptScore {\n        $answers = [];\n        for ($i = 0; $i &lt; $this-&gt;k; $i++) { $answers[] = $this-&gt;run(\"{$prompt}\\n\\n{$question}\"); }\n        $entropy = $this-&gt;normalizedEntropy(array_map(fn($r)=&gt;$r-&gt;answer, $answers));\n        $rep = $this-&gt;repetitiveness($answers);\n        $ps = new PromptScore(); $ps-&gt;prompt = $prompt; $ps-&gt;score = $entropy - $rep; return $ps;\n    }\n    private function run(string $input) : CoT {\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$input] ],\n            responseModel: CoT::class,\n        )-&gt;get();\n    }\n    private function normalizedEntropy(array $answers) : float {\n        $n = count($answers); if ($n===0) return 0.0; $freq=[]; foreach($answers as $a){$freq[$a]=($freq[$a]??0)+1;}\n        $h=0.0; foreach($freq as $c){$p=$c/$n; $h += ($p&gt;0)? -$p*log($p):0.0;} $max=log(max(1,count($freq))); return $max&gt;0? $h/$max:0.0;\n    }\n    private function repetitiveness(array $responses) : float {\n        $r = array_map(fn($x)=&gt; implode(' ', $x-&gt;chain_of_thought), $responses);\n        $uniq = count(array_unique($r)); $n = max(1,count($responses)); return 1.0 - ($uniq/$n);\n    }\n}\n\n$prompts = [\n    'Explain step-by-step then answer:',\n    'Think carefully and provide reasoning before the answer:',\n    'Reason in numbered steps and conclude with the final number:',\n];\n$question = 'If a store sold 93 in the morning and 39 in the afternoon from 200 baked, and 6 were returned, how many remain?';\n$scores = (new PromptEnsembler)-&gt;evaluate($prompts, $question);\ndump($scores);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#references","title":"References","text":"<p>1) https://learnprompting.org/docs/advanced/ensembling/max_mutual_information_method</p>"},{"location":"cookbook/prompting/ensembling/multiple_candidates/","title":"Generate Multiple Candidate Responses","text":""},{"location":"cookbook/prompting/ensembling/multiple_candidates/#overview","title":"Overview","text":"<p>Generate multiple candidate responses and pick the most common answer (Self-Consistency).</p>"},{"location":"cookbook/prompting/ensembling/multiple_candidates/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass SelfConsistencyResponse {\n    public string $chain_of_thought;\n    public int $correct_answer;\n}\n\nclass SelfConsistency {\n    public function __invoke(string $prompt, int $k = 5) : int {\n        $answers = [];\n        for ($i = 0; $i &lt; $k; $i++) { $answers[] = $this-&gt;one($prompt)-&gt;correct_answer; }\n        $counts = [];\n        foreach ($answers as $a) { $key = (string)$a; $counts[$key] = ($counts[$key] ?? 0) + 1; }\n        arsort($counts);\n        return (int) array_key_first($counts);\n    }\n\n    private function one(string $prompt) : SelfConsistencyResponse {\n        $system = 'You are an intelligent QA system. First think step-by-step, then provide the final answer.';\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;$prompt] ],\n            responseModel: SelfConsistencyResponse::class,\n            options: ['temperature' =&gt; 0.5],\n        )-&gt;get();\n    }\n}\n\n$prompt = &lt;&lt;&lt;TXT\nJanet's ducks lay 16 eggs per day.\nShe eats 3 for breakfast and bakes muffins with 4 each day.\nShe sells the remainder for $2 per egg. How much does she make per day?\nTXT;\n\n$answer = (new SelfConsistency)($prompt, k: 5);\ndump($answer);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/multiple_candidates/#references","title":"References","text":"<p>1) Self-Consistency Improves Chain Of Thought Reasoning In Language Models (https://arxiv.org/pdf/2210.03350)</p>"},{"location":"cookbook/prompting/ensembling/task_specific_evals/","title":"Use Task Specific Evaluation Metrics","text":""},{"location":"cookbook/prompting/ensembling/task_specific_evals/#overview","title":"Overview","text":"<p>Universal Self Prompting is a two stage process similar to Consistency Based Self Adaptive Prompting (COSP). Here is a breakdown of the two stages.</p> <p>Generate Examples : LLMs are prompted to generate a collection of candidate responses using a test dataset Answer Query : We then select a few of these model-generated responses as examples to prompt the LLM to obtain a final prediction. Note here that the final answer is obtained using a single forward pass with greedy decoding.</p> <p>USP Process\u00b6</p> <p>Let's see how this works in greater detail.</p> <p>Generate Few Shot Examples\u00b6 We first prompt our model to generate responses for a given set of prompts. Instead of measuring the entropy and repetitiveness as in COSP, we use one of three possible methods to measure the quality of the generated responses. These methods are decided based on the three categories supported.</p> <p>This category has to be specified by a user ahead of time.</p> <p>Note that for Short Form and Long Form generation, we generate m m different samples. This is not the case for classification tasks.</p> <p>Classification : Classification Tasks are evaluated using the normalized probability of each label using the raw logits from the LLM.</p> <p>In short, we take the raw logit for each token corresponding to the label, use a softmax to normalize each of them and then sum across the individual probabilities and their log probs. We also try to sample enough queries such that we have a balanced number of predictions across each class ( so that our model doesn't have a bias towards specific classes )</p> <p>Short Form Generation: This is done by using a similar formula to COSP but without the normalizing term</p> <p>Long Form Generation: This is done by using the average pairwise ROUGE score between all pairs of the m responses.</p> <p>What is key here is that depending on the task specified by the user, we have a task-specific form of evaluation. This eventually allows us to better evaluate our individual generated examples. Samples of tasks for each category include</p> <p>Classification: Natural Language Inference, Topic Classification and Sentiment Analysis Short Form Generation : Question Answering and Sentence Completion Long Form Generation : Text Summarization and Machine Translation This helps to ultimately improve the performance of these large language models across different types of tasks.</p> <p>Generate Single Response\u00b6 Once we've selected our examples, the second step is relatively simple. We just need to append a few of our chosen examples that score best on our chosen metric to append to our solution.</p> <p>Implementation\u00b6 We've implemented a classification example below that tries to sample across different classes in a balanced manner before generating a response using a single inference call.</p> <p>We bias this sampling towards samples that the model is more confident towards by using a confidence label.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Emotion: string { case Happy='Happy'; case Angry='Angry'; case Sadness='Sadness'; }\nenum Confidence: string { case Uncertain='Uncertain'; case Somewhat='Somewhat Confident'; case Confident='Confident'; case Highly='Highly Confident'; }\n\nclass Classification { public string $chain_of_thought; public Emotion $label; public Confidence $confidence; }\n\nclass USPClassification {\n    public function classify(string $query) : Classification {\n        $content = 'Classify the following query into one of: Happy, Angry, Sadness';\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$content], ['role'=&gt;'user','content'=&gt;$query] ],\n            responseModel: Classification::class,\n        )-&gt;get();\n    }\n\n    public function balancedSample(array $queries, int $k) : array {\n        $preds = [];\n        foreach ($queries as $q) { $preds[] = [$this-&gt;classify($q), $q]; }\n        $by = [];\n        foreach ($preds as $p) { $by[$p[0]-&gt;label-&gt;value][] = $p; }\n        $per = max(1, intdiv($k, max(1, count($by))));\n        $out = [];\n        foreach ($by as $label =&gt; $items) {\n            usort($items, fn($a,$b) =&gt; $this-&gt;score($b[0]-&gt;confidence) &lt;=&gt; $this-&gt;score($a[0]-&gt;confidence));\n            $slice = array_slice($items, 0, $per);\n            foreach ($slice as $it) { $out[] = $it[1] . \" ({$label})\"; }\n        }\n        return $out;\n    }\n\n    public function finalWithExamples(string $query, array $examples) : Classification {\n        $formatted = implode(\"\\n\", $examples);\n        $system = \"You classify queries into Happy, Angry, or Sadness.\\n&lt;examples&gt;\\n{$formatted}\\n&lt;/examples&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;$query] ],\n            responseModel: Classification::class,\n        )-&gt;get();\n    }\n\n    private function score(Confidence $c) : int {\n        return match($c) { Confidence::Highly =&gt; 4, Confidence::Confident =&gt; 3, Confidence::Somewhat =&gt; 2, Confidence::Uncertain =&gt; 1 };\n    }\n}\n?&gt;\n</code></pre> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n$examples = [\n        \"i do feel that running is a divine experience and\n        that i can expect to have some type of spiritual\n        encounter\",\n        \"i get giddy over feeling elegant in a perfectly\n        fitted pencil skirt\",\n        \"\n        i plan to share my everyday life stories traveling\n        adventures inspirations and handmade creations with\n        you and hope you will also feel inspired\n        \",\n        \"\n        i need to feel the dough to make sure its just\n        perfect\n        \",\n        \"\n        i found myself feeling a little discouraged that\n        morning\n        \",\n        \"i didnt really feel that embarrassed\",\n        \"i feel like a miserable piece of garbage\",\n        \"\n        i feel like throwing away the shitty piece of shit\n        paper\n        \",\n        \"\n        i feel irritated and rejected without anyone doing\n        anything or saying anything\n        \",\n        \"i feel angered and firey\",\n        \"\n        im feeling bitter today my mood has been strange the\n        entire day so i guess its that\n        \",\n        \"i just feel really violent right now\",\n        \"i know there are days in which you feel distracted\",\n    ];\n\n// Run the selection and final classification\n$usp = new USPClassification();\n$balanced = $usp-&gt;balancedSample($examples, 3);\n$final = $usp-&gt;finalWithExamples('i feel furious that right to life advocates can and do tell me how to live and die', $balanced);\ndump($balanced, $final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/","title":"Use Translation for Paraphrasing","text":""},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/#overview","title":"Overview","text":"<p>Back-translation can produce diverse paraphrases: translate to another language and back to English, encouraging varied phrasing.</p>"},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TranslatedPrompt { public string $translation; }\n\nclass Paraphraser {\n    public function translate(string $prompt, string $from, string $to) : TranslatedPrompt {\n        $system = \"You are an expert translation assistant. Translate from {$from} to {$to}. Paraphrase and use synonyms where possible.\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;\"Prompt: {$prompt}\"] ],\n            responseModel: TranslatedPrompt::class,\n        )-&gt;get();\n    }\n\n    public function backTranslate(string $prompt, string $lang) : string {\n        $step1 = $this-&gt;translate($prompt, 'english', $lang)-&gt;translation;\n        $step2 = $this-&gt;translate($step1, $lang, 'english')-&gt;translation;\n        return $step2;\n    }\n\n    public function generate(string $prompt, array $languages, int $permutations = 5) : array {\n        $out = [];\n        for ($i = 0; $i &lt; $permutations; $i++) {\n            $lang = $languages[$i % max(1, count($languages))] ?? 'spanish';\n            $out[] = $this-&gt;backTranslate($prompt, $lang);\n        }\n        return $out;\n    }\n}\n\n$prompt = 'Explain how photosynthesis works for a 10-year-old.';\n$languages = ['spanish','french','german'];\n$variants = (new Paraphraser)-&gt;generate($prompt, $languages, permutations: 3);\ndump($variants);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/#references","title":"References","text":"<p>1) Prompt paraphrasing approaches</p>"},{"location":"cookbook/prompting/ensembling/verify_majority_voting/","title":"Verify Responses over Majority Voting","text":""},{"location":"cookbook/prompting/ensembling/verify_majority_voting/#overview","title":"Overview","text":"<p>Diverse verifier scoring aggregates quality over unique answers, improving over majority vote.</p>"},{"location":"cookbook/prompting/ensembling/verify_majority_voting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ResponseItem { public string $chain_of_thought; public int $answer; }\nenum Grade: string { case Poor='Poor'; case Average='Average'; case Good='Good'; case Excellent='Excellent'; }\nclass Grading { public Grade $grade; }\n\nclass DiverseVerifier {\n    public function __invoke(string $query, array $examples, int $k = 6) : int {\n        $responses = [];\n        for ($i = 0; $i &lt; $k; $i++) { $responses[] = $this-&gt;generate($query, $examples); }\n        $scores = [];\n        foreach ($responses as $r) {\n            $g = $this-&gt;score($query, $r);\n            $scores[$r-&gt;answer] = ($scores[$r-&gt;answer] ?? 0.0) + $this-&gt;map($g);\n        }\n        arsort($scores);\n        return (int) array_key_first($scores);\n    }\n\n    private function generate(string $query, array $examples) : ResponseItem {\n        $formatted = implode(\"\\n\", $examples);\n        $content = \"You answer succinctly.\\n&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\\n\\n&lt;examples&gt;\\n{$formatted}\\n&lt;/examples&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$content] ],\n            responseModel: ResponseItem::class,\n        )-&gt;get();\n    }\n\n    private function score(string $query, ResponseItem $response) : Grading {\n        $content = \"Score the response to the query. Output only the grade.\\n&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\\n&lt;response&gt;\\nChain: {$response-&gt;chain_of_thought}\\nAnswer: {$response-&gt;answer}\\n&lt;/response&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$content] ],\n            responseModel: Grading::class,\n        )-&gt;get();\n    }\n\n    private function map(Grading $g) : float {\n        return match($g-&gt;grade) { Grade::Excellent =&gt; 1.0, Grade::Good =&gt; 0.75, Grade::Average =&gt; 0.5, Grade::Poor =&gt; 0.25 };\n    }\n}\n\n$examples = [\n    \"Q: James runs 3 sprints, 3 times a week, 60m each. How many meters per week? A: ... The answer is 540.\",\n    \"Q: Brandon's iPhone age puzzle... A: ... The answer is 8.\",\n    \"Q: Jean has 30 lollipops ... bags? A: ... The answer is 14.\",\n    \"Q: Weng earns $12/hour, worked 50 minutes. How much? A: ... The answer is 10.\",\n];\n$query = 'Betty needs $100; has half; parents give $15; grandparents twice parents. How much more needed?';\n\n$best = (new DiverseVerifier)($query, $examples, k: 6);\ndump($best);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/verify_majority_voting/#references","title":"References","text":"<p>1) Making Language Models Better Reasoners with Step-Aware Verifier (https://aclanthology.org/2023.acl-long.291/)</p>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/","title":"Consistency based examples","text":""},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#consistency-based-self-adaptive-prompting-cosp","title":"Consistency Based Self Adaptive Prompting (COSP)","text":"<p>COSP is a technique that aims to improve few-shot learning by selecting high-quality examples based on the consistency and confidence of model responses. This approach helps create more effective prompts by identifying examples that the model can process reliably.</p>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#overview","title":"Overview","text":"<p>The COSP process involves two main stages:</p> <ol> <li>Example Generation:</li> <li>Generate multiple responses for potential examples</li> <li>Run each example through the model multiple times</li> <li> <p>Collect responses and confidence scores</p> </li> <li> <p>Example Selection</p> </li> <li>Select the best examples based on entropy and repetitiveness</li> <li>Calculate entropy of responses to measure consistency</li> <li>Evaluate repetitiveness to ensure reliability</li> </ol>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#how-cosp-works","title":"How COSP Works","text":""},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#stage-1-example-generation","title":"Stage 1: Example Generation","text":"<p>For each potential example in your dataset:</p> <ul> <li>Generate multiple responses (typically 3-5)</li> <li>Calculate the entropy of these responses</li> <li>Measure the repetitiveness across responses</li> </ul> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclass Response(BaseModel):\n    content: str = Field(description=\"The model's response to the prompt\")\n    confidence: float = Field(description=\"Confidence score between 0 and 1\")\n\nclient = instructor.from_openai(OpenAI())\n\ndef generate_responses(prompt: str, n: int = 3) -&gt; List[Response]:\n    responses = []\n    for _ in range(n):\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_model=Response\n        )\n        responses.append(response)\n    return responses\n</code></pre>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#stage-2-example-selection","title":"Stage 2: Example Selection","text":"<p>Calculate metrics for each example:</p> <ul> <li>Entropy: Measure response variability</li> <li>Repetitiveness: Check response consistency</li> </ul> <pre><code>import numpy as np\nfrom scipy.stats import entropy\n\ndef calculate_metrics(responses: List[Response]) -&gt; tuple[float, float]:\n    # Calculate entropy\n    confidences = [r.confidence for r in responses]\n    entropy_score = entropy(confidences)\n\n    # Calculate repetitiveness\n    unique_responses = len(set(r.content for r in responses))\n    repetitiveness = 1 - (unique_responses / len(responses))\n\n    return entropy_score, repetitiveness\n</code></pre>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#implementation-example","title":"Implementation Example","text":"<p>Here's a complete example of COSP implementation:</p> <pre><code>from typing import List, Tuple\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nimport numpy as np\nfrom scipy.stats import entropy\n\nclass Example(BaseModel):\n    text: str\n    score: float = Field(description=\"Combined quality score\")\n    entropy: float = Field(description=\"Entropy of responses\")\n    repetitiveness: float = Field(description=\"Repetitiveness of responses\")\n\nclass COSPSelector:\n    def __init__(self, client: OpenAI, n_samples: int = 3):\n        self.client = instructor.from_openai(client)\n        self.n_samples = n_samples\n\n    def generate_responses(self, prompt: str) -&gt; List[Response]:\n        return [\n            self.client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                response_model=Response\n            )\n            for _ in range(self.n_samples)\n        ]\n\n    def calculate_metrics(self, responses: List[Response]) -&gt; Tuple[float, float]:\n        confidences = [r.confidence for r in responses]\n        entropy_score = entropy(confidences)\n\n        unique_responses = len(set(r.content for r in responses))\n        repetitiveness = 1 - (unique_responses / len(responses))\n\n        return entropy_score, repetitiveness\n\n    def select_examples(self, candidates: List[str], k: int) -&gt; List[Example]:\n        examples = []\n\n        for text in candidates:\n            responses = self.generate_responses(text)\n            entropy_score, repetitiveness = self.calculate_metrics(responses)\n\n            # Combined score (lower is better)\n            score = entropy_score - repetitiveness\n\n            examples.append(Example(\n                text=text,\n                score=score,\n                entropy=entropy_score,\n                repetitiveness=repetitiveness\n            ))\n\n        # Sort by score (lower is better) and select top k\n        return sorted(examples, key=lambda x: x.score)[:k]\n</code></pre>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#usage-example","title":"Usage Example","text":"<pre><code># Initialize COSP selector\nclient = OpenAI()\nselector = COSPSelector(client)\n\n# Candidate examples\ncandidates = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Python is a high-level programming language\",\n    # ... more examples\n]\n\n# Select best examples\nbest_examples = selector.select_examples(candidates, k=3)\n\n# Use selected examples in your prompt\nselected_texts = [ex.text for ex in best_examples]\nprompt = f\"\"\"Use these examples to guide your response:\n\nExamples:\n{chr(10).join(f'- {text}' for text in selected_texts)}\n\nNow, please respond to: [your query here]\n\"\"\"\n</code></pre>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#benefits-of-cosp","title":"Benefits of COSP","text":"<ul> <li>Improved Consistency: By selecting examples with low entropy and high repetitiveness</li> <li>Better Performance: More reliable few-shot learning</li> <li>Automated Selection: No manual example curation needed</li> <li>Quality Metrics: Quantifiable measure of example quality</li> </ul>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#limitations","title":"Limitations","text":"<ul> <li>Computational Cost: Requires multiple API calls per example</li> <li>Time Overhead: Selection process can be slow for large candidate sets</li> <li>Model Dependency: Performance may vary across different models</li> </ul>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#related-techniques","title":"Related Techniques","text":"<ul> <li>Universal Self Prompting (USP)</li> <li>Chain of Thought Prompting</li> <li>Self-Consistency</li> </ul>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#references","title":"References","text":"<p>1) Original COSP Paper (https://arxiv.org/abs/2305.14121) 2) Related Work: Self-Consistency Improves Chain of Thought Reasoning in Language Models (https://arxiv.org/abs/2203.11171)</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/","title":"Example ordering","text":""},{"location":"cookbook/prompting/few_shot/example_ordering/#example-ordering","title":"Example Ordering","text":"<p>The order of few-shot examples in the prompt can affect LLM outputs 1234*. Consider permutating the order of these examples in your prompt to achieve better results.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#choosing-your-examples","title":"Choosing Your Examples","text":"<p>Depending on your use-case, here are a few different methods that you can consider using to improve the quality of your examples.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#combinatorics","title":"Combinatorics","text":"<p>One of the easiest methods is for us to manually iterate over each of the examples that we have and try all possible combinations we could create. This will in turn allow us to find the best combination that we can find.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#kate","title":"KATE","text":"<p>KATE (k-Nearest Example Tuning) is a method designed to enhance GPT-3's performance by selecting the most relevant in-context examples. The method involves:</p> <p>For each example in the test set, K nearest neighbors (examples) are retrieved based on semantic similarity. Among these K examples, those that appear most frequently across different queries are selected as the best in-context examples.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#using-a-unsupervised-retriever","title":"Using a Unsupervised Retriever","text":"<p>We can use a large LLM to compute a single score for each example with respect to a given prompt. This allows us to create a training set that scores an example's relevance when compared against a prompt. Using this training set, we can train a model that mimics this functionality. This allows us to determine the top k most relevant and most irrelevant examples when a user makes a query so that we can include this in our final prompt.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#references","title":"References","text":"<p>1) Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity (https://arxiv.org/abs/2104.08786) 2) Reordering Examples Helps during Priming-based Few-Shot Learning (https://arxiv.org/abs/2106.01751) 3) What Makes Good In-Context Examples for GPT-3? (https://arxiv.org/abs/2101.06804) 4) Learning To Retrieve Prompts for In-Context Learning (https://aclanthology.org/2022.naacl-main.191/) 5) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/few_shot/in_context_examples/","title":"Generate In-Context Examples","text":""},{"location":"cookbook/prompting/few_shot/in_context_examples/#overview","title":"Overview","text":"<p>How can we generate examples for our prompt?</p> <p>Self-Generated In-Context Learning (SG-ICL) is a technique which uses an LLM to generate examples to be used during the task. This allows for in-context learning, where examples of the task are provided in the prompt.</p> <p>We can implement SG-ICL using Instructor as seen below.</p>"},{"location":"cookbook/prompting/few_shot/in_context_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum ReviewSentiment : string {\n    case Positive = 'positive';\n    case Negative = 'negative';\n}\n\nclass GeneratedReview {\n    public string $review;\n    public ReviewSentiment $sentiment;\n}\n\n\nclass PredictSentiment {\n    private int $n = 4;\n\n    public function __invoke(string $review) : ReviewSentiment {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Review: {$review}\"],\n            ],\n            responseModel: Scalar::enum(ReviewSentiment::class),\n            examples: $this-&gt;generateExamples($review),\n        )-&gt;get();\n    }\n\n    private function generate(string $inputReview, ReviewSentiment $sentiment) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Generate {$this-&gt;n} various {$sentiment-&gt;value} reviews based on the input review:\\n{$inputReview}\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"Generated review:\"],\n            ],\n            responseModel: Sequence::of(GeneratedReview::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    private function generateExamples(string $inputReview) : array {\n        $examples = [];\n        foreach ([ReviewSentiment::Positive, ReviewSentiment::Negative] as $sentiment) {\n            $samples = $this-&gt;generate($inputReview, $sentiment);\n            foreach ($samples as $sample) {\n                $examples[] = Example::fromData($sample-&gt;review, $sample-&gt;sentiment-&gt;value);\n            }\n        }\n        return $examples;\n    }\n}\n\n$predictSentiment = (new PredictSentiment)('This movie has been very impressive, even considering I lost half of the plot.');\n\ndump($predictSentiment);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/in_context_examples/#references","title":"References","text":"<ol> <li>Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/few_shot/select_effective_samples/","title":"Select effective samples","text":""},{"location":"cookbook/prompting/few_shot/select_effective_samples/#select-effective-examples","title":"Select Effective Examples","text":"<p>We can select effective in-context examples by choosing those that are semantically closer to the query using KNN.</p> <p>In the below implementation using Instructor, we follow these steps:</p> <ol> <li>Embed the query examples</li> <li>Embed the query that we want to answer</li> <li>Find the k query examples closest to the query</li> <li>Use the chosen examples and their as the context for the LLM</li> </ol> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nimport math\nfrom textwrap import dedent\n\n\nclass Example(BaseModel):\n    question: str\n    answer: str\n\n\nclass Response(BaseModel):\n    answer: str\n\n\noai = OpenAI()\nclient = instructor.from_openai(oai)\n\n\ndef distance(a: list[float], b: list[float]):\n    return 1 - sum(ai * bi for ai, bi in zip(a, b)) / (\n        math.sqrt(sum(ai**2 for ai in a)) * math.sqrt(sum(bi**2 for bi in b))\n    )\n\n\ndef embed_queries(queries: list[str]) -&gt; list[tuple[list[float], str]]:\n    return [\n        (embedding_item.embedding, query)\n        for embedding_item, query in zip(\n            oai.embeddings.create(input=queries, model=\"text-embedding-3-large\").data,\n            queries,\n        )\n    ]\n\n\ndef knn(\n    embedded_examples: list[tuple[list[float], str]],\n    query_embedding: list[float],\n    k: int,\n):\n    distances = [\n        (distance(embedding, query_embedding), example)\n        for embedding, example in embedded_examples\n    ]\n    distances.sort(key=lambda x: x[0])\n    return distances[:k]\n\n\ndef generate_response(examples: list[str], query: str):\n    formatted_examples = \"\\n\".join(examples)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                    Respond to the following query with the most accurate\n                    and concise answer possible.\n                    &lt;examples&gt;\n                    {formatted_examples}\n                    &lt;/examples&gt;\n                    &lt;query&gt;\n                    {query}\n                    &lt;/query&gt;\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\ndef generate_question_and_answer_pair(\n    questions: list[str], question_and_answers: list[dict[str, str]]\n) -&gt; list[str]:\n    question_to_answer = {}\n\n    for question in question_and_answers:\n        question_to_answer[question[\"question\"]] = question[\"answer\"]\n\n    return [\n        dedent(\n            f\"\"\"\n        &lt;example&gt;\n        &lt;question&gt;{question}&lt;/question&gt;\n        &lt;answer&gt;{question_to_answer[question]}&lt;/answer&gt;\n        &lt;/example&gt;\n        \"\"\"\n        )\n        for question in questions\n    ]\n\n\nif __name__ == \"__main__\":\n    examples = [\n        {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n        {\"question\": \"Who wrote Romeo and Juliet\", \"answer\": \"Shakespeare\"},\n        {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n    ]\n\n    query = \"What is the capital of Italy?\"\n\n    # Step 1 : Embed the Examples\n    embeddings = embed_queries([example[\"question\"] for example in examples] + [query])\n\n    embedded_examples = embeddings[:-1]\n    embedded_query = embeddings[-1]\n\n    # # Step 3: Find the k closest examples to the query\n    k_closest_examples = knn(embedded_examples, embedded_query[0], 2)\n\n    for example in k_closest_examples:\n        print(example)\n        #&gt; (0.4013468481736857, 'What is the capital of France?')\n        #&gt; (0.4471368596136872, 'What is the capital of Germany?')\n\n    # Step 4: Use these examples as in-context examples\n    formatted_examples = generate_question_and_answer_pair(\n        [example[1] for example in k_closest_examples], examples\n    )\n    response = generate_response(formatted_examples, query)\n    print(response.answer)\n    #&gt; Rome\n</code></pre>"},{"location":"cookbook/prompting/few_shot/select_effective_samples/#references","title":"References","text":"<p>1) What Makes Good In-Context Examples for GPT-3? 2) The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties/","title":"Arbitrary properties","text":""},{"location":"cookbook/prompting/misc/arbitrary_properties/#overview","title":"Overview","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n?&gt;\n</code></pre> <p>Now we can use this data model to extract arbitrary properties from a text message in a form that is easier for future processing.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives\n    in a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    mode: OutputMode::Json,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(!empty($user-&gt;properties));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/","title":"Consistent values of arbitrary properties","text":""},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/#overview","title":"Overview","text":"<p>For multiple records containing arbitrary properties, instruct LLM to get more consistent key names when extracting properties.</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $id;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetails\n{\n    /**\n     * @var UserDetail[] Extract information for multiple users.\n     * Use consistent key names for properties across users.\n     */\n    public array $users = [];\n}\n\n$text = \"Jason is 25 years old. He is a Python programmer.\\\n Amanda is UX designer.\\\n John is 40yo and he's CEO.\";\n\n$list = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetails::class,\n)-&gt;get();\n\ndump($list);\n\nassert(!empty($list-&gt;users));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/chain_of_summaries/","title":"Chain of Summaries","text":""},{"location":"cookbook/prompting/misc/chain_of_summaries/#overview","title":"Overview","text":"<p>This is an example of summarization with increasing amount of details. Instructor is provided with data structure containing instructions on how to create increasingly detailed summaries of the project report.</p> <p>It starts with generating an overview of the project, followed by X iterations of increasingly detailed summaries. Each iteration should contain all the information from the previous summary, plus a few additional facts from the content which are most relevant and missing from the previous iteration.</p>"},{"location":"cookbook/prompting/misc/chain_of_summaries/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n/** Executive level summary of the project */\nclass Summary {\n    /** current summary iteration, not bigger than 3 */\n    public int $iteration = 0;\n    /** @var string[] 1-3 facts most relevant from executive perspective and missing from the summary (avoid technical details) */\n    public array $missingFacts = [];\n    /** denser summary in 1-3 sentences, which covers every fact from the previous summary plus the missing ones */\n    public string $expandedSummary = '';\n}\n\n/** Increasingly denser, expanded summaries */\nclass ChainOfSummaries {\n    /** simplified, executive view with no details, just a single statement of overall situation */\n    public string $overview;\n    /** @var Summary[] contains at least 3 gradually more expanded summaries of the content */\n    public array $summaries;\n}\n\n$summaries = (new StructuredOutput)\n    -&gt;with(\n        messages: $report,\n        responseModel: ChainOfSummaries::class,\n        prompt: 'Generate a denser summary based on the provided content.',\n        options: [\n            'max_tokens' =&gt; 4096,\n        ],\n        toolName: 'summarizer',\n        toolDescription: 'Generates a summary based on the provided content.',\n    )\n    -&gt;get();\n\nprint(\"\\n# Summaries with increasing density:\\n\\n\");\nprint(\"Overview:\\n\");\nprint(\"{$summaries-&gt;overview}\\n\\n\");\nforeach ($summaries-&gt;summaries as $summary) {\n    print(\"Expanded summary - iteration #{$summary-&gt;iteration}:\\n\");\n    print(\"{$summary-&gt;expandedSummary}\\n\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/chain_of_thought/","title":"Chain of Thought","text":""},{"location":"cookbook/prompting/misc/chain_of_thought/#overview","title":"Overview","text":"<p>This approach to \"chain of thought\" improves data quality, by eliciting LLM reasoning to self-explain approach to generating the response.</p> <p>With Instructor you can achieve a 'modular' CoT, where multiple explanations can be generated by LLM for different parts of the response, driving a more granular control and improvement of the response.</p>"},{"location":"cookbook/prompting/misc/chain_of_thought/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass Employee {\n    #[Instructions('Think step by step to determine the correct year of employment.')]\n    public string $reasoning;\n    public int $yearOfEmployment;\n    // ... other data fields of your employee class\n}\n\n$text = 'He was working here for 5 years. Now, in 2019, he is a manager.';\n\n$employee = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Employee::class\n)-&gt;get();\n\n\ndump($employee);\n\nassert($employee-&gt;yearOfEmployment === 2014);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/","title":"Single label classification","text":""},{"location":"cookbook/prompting/misc/classification/#overview","title":"Overview","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p>"},{"location":"cookbook/prompting/misc/classification/#example","title":"Example","text":"<p>Let's start by defining the data structures.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Enumeration for single-label text classification.\nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction.\nclass SinglePrediction {\n    public Label $classLabel;\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/#classifying-text","title":"Classifying Text","text":"<p>The function classify will perform the single-label classification.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction classify(string $data) : SinglePrediction {\n    return (new StructuredOutput)-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n    )-&gt;get();\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\n\ndump($prediction);\n\nassert($prediction-&gt;classLabel == Label::SPAM);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/","title":"Multiclass classification","text":""},{"location":"cookbook/prompting/misc/classification_multiclass/#overview","title":"Overview","text":"<p>We start by defining the structures.</p> <p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass TicketLabels {\n    /** @var Label[] */\n    public array $labels = [];\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/#classifying-text0","title":"Classifying Text0","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : TicketLabels {\n    $x = (new StructuredOutput)\n        //-&gt;withDebugPreset('on')\n        -&gt;wiretap(fn($e) =&gt; $e-&gt;printDebug())\n        -&gt;withMessages(\"Label following support ticket: {$data}\")\n        -&gt;withResponseModel(TicketLabels::class)\n        -&gt;create();\ndd($x);\n//        -&gt;get();\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\ndump($prediction);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;labels));\nassert(in_array(Label::BILLING, $prediction-&gt;labels));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/component_reuse/","title":"Reusing components","text":""},{"location":"cookbook/prompting/misc/component_reuse/#overview","title":"Overview","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p>"},{"location":"cookbook/prompting/misc/component_reuse/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public string $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Yesterday Jason worked from 9 for 5 hours. After that I watched 2 hour movie which I finished at 19.\"]],\n    responseModel: UserDetail::class,\n    model: 'gpt-4o',\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name == \"Jason\");\nassert($user-&gt;workTime-&gt;startTime === 9);\nassert($user-&gt;workTime-&gt;endTime === 14);\nassert($user-&gt;leisureTime-&gt;startTime === 17);\nassert($user-&gt;leisureTime-&gt;endTime === 19);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/component_reuse_cot/","title":"Using CoT to improve interpretation of component data","text":""},{"location":"cookbook/prompting/misc/component_reuse_cot/#overview","title":"Overview","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <p>We're additionally starting the data structure with a Chain of Thought field to elicit LLM reasoning for the time range calculation, which can improve the accuracy of the response.</p>"},{"location":"cookbook/prompting/misc/component_reuse_cot/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours (0-23 format) */\n    public int $startTime;\n    /** The end time in hours (0-23 format) */\n    public int $endTime;\n}\n\n$timeRange = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Workshop with Apex Industries started 9 and it took us 6 hours to complete.\"]],\n    responseModel: TimeRange::class,\n    maxRetries: 2\n)-&gt;get();\n\ndump($timeRange);\n\nassert($timeRange-&gt;startTime === 9);\nassert($timeRange-&gt;endTime === 15);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/entity_relationships/","title":"Entity relationship extraction","text":""},{"location":"cookbook/prompting/misc/entity_relationships/#overview","title":"Overview","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> fields.</p>"},{"location":"cookbook/prompting/misc/entity_relationships/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /**\n     * @var int[] Correct and complete list of coworker IDs, representing\n     * collaboration between users.\n     */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /**\n     * @var UserDetail[] Collection of users, correctly capturing the\n     * relationships among them.\n     */\n    public array $users;\n}\n\n$text = \"Jason is 25 years old. He is a Python programmer of Apex website.\\\n Amanda is a contractor working with Jason on Apex website. John is 40yo\\\n and he's CEO - Jason reports to him.\";\n\n$relationships = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserRelationships::class,\n)-&gt;get();\n\ndump($relationships);\n\nassert(!empty($relationships-&gt;users));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/handling_errors/","title":"Handling errors","text":""},{"location":"cookbook/prompting/misc/handling_errors/#overview","title":"Overview","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <p>NOTE: Instructor offers a built-in Maybe wrapper class that you can use to handle errors. See the example in Basics section for more details.</p>"},{"location":"cookbook/prompting/misc/handling_errors/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $user = null;\n    public bool $noUserData = false;\n    /** If no user data, provide reason */\n    public ?string $errorMessage = '';\n\n    public function get(): ?UserDetail {\n        return $this-&gt;noUserData ? null : $this-&gt;user;\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'We don\\'t know anything about this guy.']])\n    -&gt;withResponseModel(MaybeUser::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;noUserData);\nassert(!empty($user-&gt;errorMessage));\nassert($user-&gt;get() === null);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/limiting_lists/","title":"Limiting the length of lists","text":""},{"location":"cookbook/prompting/misc/limiting_lists/#overview","title":"Overview","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length of list. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <p>To be 100% certain the list does not exceed the limit, add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"cookbook/prompting/misc/limiting_lists/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass Property\n{\n    /**  Monotonically increasing ID, not larger than 2 */\n    public string $index;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    use ValidationMixin;\n\n    public int $age;\n    public string $name;\n    /** @var Property[] List other extracted properties - not more than 2. */\n    public array $properties;\n\n    public function validate() : ValidationResult\n    {\n        if (count($this-&gt;properties) &lt; 3) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'properties',\n            value: $this-&gt;name,\n            message: \"Number of properties must be not more than 2.\",\n        );\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives in\n    a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    maxRetries: 1 // change to 0 to see validation error\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(count($user-&gt;properties) &lt; 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/reflection_prompting/","title":"Reflection Prompting","text":""},{"location":"cookbook/prompting/misc/reflection_prompting/#overview","title":"Overview","text":"<p>This implementation of Reflection Prompting with Instructor provides a structured way to encourage LLM to engage in more thorough and self-critical thinking processes, potentially leading to higher quality and more reliable outputs.</p>"},{"location":"cookbook/prompting/misc/reflection_prompting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Contracts\\CanValidateSelf;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass ReflectiveResponse implements CanValidateSelf {\n    #[Instructions('Is problem solvable and what domain expertise it requires')]\n    public string $assessment;\n    #[Instructions('Describe an expert persona who would be able to solve this problem, their skills and experience')]\n    public string $persona;\n    #[Instructions(\"Initial analysis and expert persona's approach to the problem\")]\n    public string $initialThinking;\n    #[Instructions('Steps of reasoning leading to the final answer - expert persona thinking through the problem')]\n    /** @var string[] */\n    public array $chainOfThought;\n    #[Instructions('Critical examination of the reasoning process - what could go wrong, what are the assumptions')]\n    public string $reflection;\n    #[Instructions('Final answer after reflection')]\n    public string $finalOutput;\n\n    // Validation method to ensure thorough reflection\n    #[\\Override]\n    public function validate(): ValidationResult {\n        $errors = [];\n        if (empty($this-&gt;reflection)) {\n            $errors[] = \"Reflection is required for a thorough response.\";\n        }\n        if (count($this-&gt;chainOfThought) &lt; 2) {\n            $errors[] = \"Please provide at least two steps in the chain of thought.\";\n        }\n        return ValidationResult::make($errors);\n    }\n}\n\n$problem = 'Solve the equation x+y=x-y';\n$solution = (new StructuredOutput)-&gt;using('anthropic')-&gt;with(\n    messages: $problem,\n    responseModel: ReflectiveResponse::class,\n    mode: OutputMode::MdJson,\n    options: ['max_tokens' =&gt; 2048]\n)-&gt;get();\n\nprint(\"Problem:\\n$problem\\n\\n\");\ndump($solution);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/restate_instructions/","title":"Restating instructions","text":""},{"location":"cookbook/prompting/misc/restate_instructions/#overview","title":"Overview","text":"<p>Make Instructor restate long or complex instructions and rules to improve inference accuracy.</p>"},{"location":"cookbook/prompting/misc/restate_instructions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /** Restate instructions and rules, so you can correctly determine the title. */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /* Guess job title */\n    public string $title;\n}\n\n/**\n * Details of analyzed user. The key information we're looking for\n * is appropriate role data.\n */\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am the head of Apex Software, responsible for\n    driving growth of our company.\n    TEXT;\n\n$structuredOutput = new StructuredOutput;\n$user = ($structuredOutput)-&gt;with(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\n//assert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/rewrite_instructions/","title":"Ask LLM to rewrite instructions","text":""},{"location":"cookbook/prompting/misc/rewrite_instructions/#overview","title":"Overview","text":"<p>Asking LLM to rewrite the instructions and rules is another way to improve inference results.</p> <p>You can provide arbitrary instructions on the data handling in the class and property PHPDocs. Instructor will use these instructions to guide LLM in the inference process.</p>"},{"location":"cookbook/prompting/misc/rewrite_instructions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /**\n     * Rewrite the instructions and rules in a concise form to correctly\n     * determine the user's title - just the essence.\n     */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /** Most likely job title */\n    public string $title;\n}\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am responsible for driving growth of our\n    company.\n    TEXT;\n\n$structuredOutput = new StructuredOutput;\n$user = $structuredOutput-&gt;with(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\nassert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/search_query_expansion/","title":"Expanding search queries","text":""},{"location":"cookbook/prompting/misc/search_query_expansion/#overview","title":"Overview","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#why-it-matters","title":"Why it matters","text":"<p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#structure-of-the-data","title":"Structure of the data","text":"<p>The <code>SearchQuery</code> is a PHP class that defines the structure of an individual search query.</p> <p>It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/search_query_expansion/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries.</p> <p>It uses the <code>StructuredOutput::create()</code> method to extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nfunction segment(string $data) : Search {\n    return (new StructuredOutput)\n        //-&gt;withDebugPreset('on')\n        -&gt;withMessages(\"Consider the data below: '\\n$data' and segment it into multiple search queries\")\n        -&gt;withResponseClass(Search::class)\n        -&gt;get();\n}\n\n$search = segment(\"Find a picture of a cat and a video of a dog\");\nforeach ($search-&gt;queries as $query) {\n    $query-&gt;execute();\n}\n// Results:\n// Searching with query `picture of a cat` using `image`\n// Searching with query `video of a dog` using `video`\n\nassert(count($search-&gt;queries) === 2);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/summary_with_keywords/","title":"Summary with Keywords","text":""},{"location":"cookbook/prompting/misc/summary_with_keywords/#overview","title":"Overview","text":"<p>This is an example of a simple summarization with keyword extraction.</p>"},{"location":"cookbook/prompting/misc/summary_with_keywords/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\nclass Summary {\n    #[Description('Project summary, not longer than 3 sentences')]\n    public string $summary = '';\n    #[Description('5 most relevant keywords extracted from the summary')]\n    public array $keywords = [];\n}\n\n$summary = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: $report,\n        responseModel: Summary::class,\n    )\n    -&gt;get();\n\ndump($summary);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/","title":"Break Down Reasoning Into Multiple Steps","text":""},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/#overview","title":"Overview","text":"<p>Cumulative Reasoning separates reasoning into propose \u2192 verify \u2192 report for better logical inference.</p>"},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nenum Prediction : string { case False = 'False'; case True = 'True'; case Unknown = 'Unknown'; }\n\nclass Proposition {\n    public string $premise1;\n    public string $premise2;\n    public string $reasoning;\n    public string $proposition;\n}\n\nclass ProposerOutput {\n    public string $reasoning;\n    #[Description('Deduced propositions relevant to the hypothesis')]\n    public array $valid_propositions; // of Proposition\n    public Prediction $prediction;\n}\n\nclass VerifiedProposition {\n    public string $proposition;\n    public string $reasoning;\n    public bool $is_valid;\n}\n\nclass ReporterOutput {\n    public string $reasoning;\n    public bool $is_valid_hypothesis;\n}\n\nclass CumulativeReasoningPipeline {\n    public function propose(array $premises, string $hypothesis) : ProposerOutput {\n        $formatted = '- ' . implode(\"\\n- \", $premises);\n        $sys = &lt;&lt;&lt;SYS\n            Think step by step using FOL to deduce propositions from given premises (at most two per deduction).\n            Avoid duplicating premises; reason only from stated premises/propositions.\n            SYS;\n        $user = &lt;&lt;&lt;USR\n            Premises:\n            {$formatted}\n\n            We want to deduce more Propositions to determine correctness of the following Hypothesis:\n            Hypothesis: {$hypothesis}\n            USR;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ProposerOutput::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $sys],\n                ['role' =&gt; 'user', 'content' =&gt; $user],\n            ],\n        )-&gt;get();\n    }\n\n    public function verify(ProposerOutput $proposal) : array {\n        $results = [];\n        foreach ($proposal-&gt;valid_propositions as $p) {\n            $sys = 'Use FOL to determine whether the deduction from two premises to the proposition is valid.';\n            $user = \"Premises:\\n{$p-&gt;premise1}\\n{$p-&gt;premise2}\\n\\nProposition:\\n{$p-&gt;proposition}\";\n            $res = (new StructuredOutput)-&gt;with(\n                model: 'gpt-4o',\n                responseModel: VerifiedProposition::class,\n                messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys], ['role' =&gt; 'user', 'content' =&gt; $user] ],\n            )-&gt;get();\n            $results[] = $res;\n        }\n        return $results;\n    }\n\n    public function report(array $verificationResult, string $hypothesis, array $premises) : ReporterOutput {\n        $formattedPrem = '- ' . implode(\"\\n- \", $premises);\n        $props = [];\n        foreach ($verificationResult as $v) { if ($v-&gt;is_valid) { $props[] = $v-&gt;proposition; } }\n        $formattedProp = '- ' . implode(\"\\n- \", $props);\n        $sys = &lt;&lt;&lt;SYS\n            Think step by step. Read and analyze the Premises, then use FOL to judge whether the Hypothesis is True, False, or Unknown using the verified Propositions.\n            SYS;\n        $messages = [\n            ['role' =&gt; 'system', 'content' =&gt; $sys],\n            ['role' =&gt; 'user', 'content' =&gt; \"Premises:\\n{$formattedPrem}\\n\\nHypothesis: {$hypothesis}\"],\n            ['role' =&gt; 'assistant', 'content' =&gt; \"Let's think step by step. From the premises, we can deduce the following propositions:\\n{$formattedProp}\\n\\nRecall the Hypothesis: {$hypothesis}\"],\n        ];\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o', responseModel: ReporterOutput::class, messages: $messages,\n        )-&gt;get();\n    }\n}\n\n$hypothesis = 'Hyraxes lay eggs';\n$premises = [\n    'The only types of mammals that lay eggs are platypuses and echidnas',\n    'Platypuses are not hyrax',\n    'Echidnas are not hyrax',\n    'No mammals are invertebrates',\n    'All animals are either vertebrates or invertebrates',\n    'Mammals are animals',\n    'Hyraxes are mammals',\n    'Grebes lay eggs',\n    'Grebes are not platypuses and also not echidnas',\n];\n\n$pipeline = new CumulativeReasoningPipeline();\n$proposal = $pipeline-&gt;propose($premises, $hypothesis);\n$verified = $pipeline-&gt;verify($proposal);\n$report = $pipeline-&gt;report($verified, $hypothesis, $premises);\n\ndump($proposal, $verified, $report);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/#references","title":"References","text":"<p>1: Cumulative Reasoning with Large Language Models (https://arxiv.org/pdf/2308.04371)</p>"},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/","title":"Determine Uncertainty of Reasoning Chain","text":""},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/#overview","title":"Overview","text":"<p>We want models to assess confidence in their own answers. Self-Calibration asks the model to justify an answer and state whether it is valid.</p>"},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass SelfCalibration {\n    #[Description('Reasoning about answer validity')]\n    public string $chain_of_thought;\n    #[Description('Whether the answer is correct or not')]\n    public bool $is_valid_answer;\n}\n\nclass EvaluateModelOutput {\n    public function __invoke(string $originalPrompt, string $modelResponse) : SelfCalibration {\n        $messages = &lt;&lt;&lt;MSG\n            Question: {$originalPrompt}\n\n            {$modelResponse}\n\n            Is this a valid answer to the question?\n            Examine the question thoroughly and generate a complete\n            reasoning for why the answer is correct or not before responding.\n            MSG;\n\n        return (new StructuredOutput)-&gt;with(\n            messages: $messages,\n            responseModel: SelfCalibration::class,\n            model: 'gpt-4o',\n        )-&gt;get();\n    }\n}\n\n$originalPrompt = &lt;&lt;&lt;PROMPT\nWho was the third president of the United States?\nPROMPT;\n\n$modelResponse = &lt;&lt;&lt;MODEL\nHere are some brainstormed ideas:\nJames Monroe\nThomas Jefferson\nJefferson\nThomas Jefferson\nGeorge Washington\nMODEL;\n\n$result = (new EvaluateModelOutput)($originalPrompt, $modelResponse);\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/#references","title":"References","text":"<ol> <li>Language Models (Mostly) Know What They Know (https://arxiv.org/pdf/2207.05221)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/","title":"Improve With Feedback","text":""},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/#overview","title":"Overview","text":"<p>Self-Refine iteratively generates an answer, critiques it, and refines it until a stopping condition is met.</p>"},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass Response { public string $code; }\n\nclass Feedback {\n    #[Description('Actions to improve the code')]\n    public array $feedback;\n    public bool $done;\n}\n\nclass Timestep {\n    public string $response;\n    public array $feedback;\n    public string $refined_response;\n}\n\nclass History {\n    /** @var Timestep[] */\n    public array $history = [];\n    public function add(string $code, array $feedback, string $refined) : void {\n        $t = new Timestep();\n        $t-&gt;response = $code;\n        $t-&gt;feedback = $feedback;\n        $t-&gt;refined_response = $refined;\n        $this-&gt;history[] = $t;\n    }\n}\n\nclass SelfRefinePipeline {\n    public function generateInitial(string $prompt) : Response {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: Response::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $prompt] ],\n        )-&gt;get();\n    }\n\n    public function generateFeedback(Response $response) : Feedback {\n        $msg = &lt;&lt;&lt;MSG\n            You are an expert Python coder.\n            Provide feedback on this code. How can we make it (1) faster and (2) more readable?\n\n            &lt;code&gt;\n            {$response-&gt;code}\n            &lt;/code&gt;\n\n            If the code does not need improvement, set done = True.\n            MSG;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: Feedback::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $msg] ],\n        )-&gt;get();\n    }\n\n    public function refine(Response $response, Feedback $feedback) : Response {\n        $feedbackLines = array_map(\n            fn($item) =&gt; is_string($item) ? $item : json_encode($item),\n            $feedback-&gt;feedback,\n        );\n        $feedbackText = implode(\"\\n\", $feedbackLines);\n\n        $msg = &lt;&lt;&lt;MSG\n            You are an expert Python coder.\n\n            &lt;response&gt;\n            {$response-&gt;code}\n            &lt;/response&gt;\n\n            &lt;feedback&gt;\n            {$feedbackText}\n            &lt;/feedback&gt;\n\n            Refine your response.\n            MSG;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: Response::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $msg] ],\n            )-&gt;get();\n    }\n\n    public function stop(Feedback $feedback, History $history) : bool {\n        if ($feedback-&gt;done) { return true; }\n        return count($history-&gt;history) &gt;= 3;\n    }\n}\n\n$pipeline = new SelfRefinePipeline();\n$response = $pipeline-&gt;generateInitial('Write Python code to calculate the Fibonacci sequence.');\n$history = new History();\n\nwhile (true) {\n    $fb = $pipeline-&gt;generateFeedback($response);\n    if ($pipeline-&gt;stop($fb, $history)) { break; }\n    $refined = $pipeline-&gt;refine($response, $fb);\n    $history-&gt;add($response-&gt;code, $fb-&gt;feedback, $refined-&gt;code);\n    $response = $refined;\n}\n\ndump($history, $response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/#references","title":"References","text":"<ol> <li>Self-Refine: Iterative Refinement with Self-Feedback (https://arxiv.org/abs/2303.17651)</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/","title":"Reconstruct Prompt from Reasoning Steps","text":""},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/#overview","title":"Overview","text":"<p>Reverse Chain-of-Thought (RCoT) reconstructs a likely prompt from reasoning steps, compares condition lists, and refines the answer with targeted feedback.</p>"},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass ReconstructedPrompt {\n    public string $chain_of_thought;\n    #[Description('Reconstructed prompt that could yield the given reasoning and answer')]\n    public string $reconstructed_prompt;\n}\n\nclass ConditionList {\n    #[Description('Key conditions relevant to answer the question')]\n    public array $conditions;\n}\n\nclass ModelFeedback {\n    #[Description('Detected inconsistencies between original and reconstructed condition lists')]\n    public array $detected_inconsistencies;\n    public string $feedback;\n    public bool $is_equal;\n}\n\nclass ModelResponse {\n    #[Description('Logical steps leading to the final statement')]\n    public string $chain_of_thought;\n    public string $correct_answer;\n}\n\nclass RCoTPipeline {\n    public function generateResponse(string $query) : ModelResponse {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ModelResponse::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; \"Generate logical steps before answering.\"],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n        )-&gt;get();\n    }\n\n    public function reconstruct(ModelResponse $response) : ReconstructedPrompt {\n        $sys = &lt;&lt;&lt;SYS\n            Give a concrete prompt that could generate this answer.\n            Include all necessary information and ask for one result.\n\n            Reasoning: {$response-&gt;chain_of_thought}\n            Response: {$response-&gt;correct_answer}\n            SYS;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ReconstructedPrompt::class,\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys] ],\n        )-&gt;get();\n    }\n\n    public function deconstructToConditions(string $prompt) : ConditionList {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ConditionList::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; \"List the key conditions required to answer the problem.\"],\n                ['role' =&gt; 'user', 'content' =&gt; $prompt],\n            ],\n        )-&gt;get();\n    }\n\n    public function compareConditions(array $original, array $reconstructed) : ModelFeedback {\n        $orig = \"- \" . implode(\"\\n- \", $original);\n        $recon = \"- \" . implode(\"\\n- \", $reconstructed);\n        $sys = &lt;&lt;&lt;SYS\n            Analyze and compare two lists of conditions.\n            Original Condition List:\n            {$orig}\n\n            Reconstructed Condition List:\n            {$recon}\n\n            Determine rough equivalence. If not equivalent, provide targeted feedback.\n            SYS;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ModelFeedback::class,\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys] ],\n        )-&gt;get();\n    }\n\n    public function revise(ModelResponse $response, ModelFeedback $feedback) : ModelResponse {\n        $miss = \"- \" . implode(\"\\n- \", $feedback-&gt;detected_inconsistencies);\n        $sys = &lt;&lt;&lt;SYS\n            Here are the mistakes and reasons in your answer:\n            Original Response: {$response-&gt;correct_answer}\n            Overlooked conditions:\n            {$miss}\n\n            Reasons:\n            {$feedback-&gt;feedback}\n\n            Generate a revised response that addresses the feedback and includes the ignored conditions.\n            SYS;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ModelResponse::class,\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys] ],\n        )-&gt;get();\n    }\n}\n\n$query = &lt;&lt;&lt;Q\nMary is an avid gardener. Yesterday, she received 18 new potted plants from her favorite plant nursery. She already has 2 potted plants on each of the 40 window ledges of her large backyard. How many potted plants will Mary remain with?\nQ;\n\n$pipeline = new RCoTPipeline();\n$response = $pipeline-&gt;generateResponse($query);\n$reconstructed = $pipeline-&gt;reconstruct($response);\n\n$originalList = $pipeline-&gt;deconstructToConditions($query);\n$reconstructedList = $pipeline-&gt;deconstructToConditions($reconstructed-&gt;reconstructed_prompt);\n\n$feedback = $pipeline-&gt;compareConditions($originalList-&gt;conditions, $reconstructedList-&gt;conditions);\nif (!$feedback-&gt;is_equal) {\n    $response = $pipeline-&gt;revise($response, $feedback);\n}\n\ndump($reconstructed, $originalList, $reconstructedList, $feedback, $response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/#references","title":"References","text":"<ol> <li>RCoT: Detecting And Rectifying Factual Inconsistency In Reasoning By Reversing Chain-Of-Thought (https://arxiv.org/pdf/2305.11499)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/self_verify/","title":"Self-Verify Responses","text":""},{"location":"cookbook/prompting/self_criticism/self_verify/#overview","title":"Overview","text":"<p>Self-Verification generates multiple candidates via CoT, rewrites them as declaratives, and verifies them via TFV to select the best candidate.</p>"},{"location":"cookbook/prompting/self_criticism/self_verify/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Candidate {\n    /** @var string[] */ public array $reasoning_steps;\n    public string $month;\n}\n\nclass Rewritten { public string $declarative; }\n\nclass Verification { public bool $correct; }\n\nclass SelfVerifyPipeline {\n    public int $n = 3; // number of candidates\n    public int $k = 5; // verification count\n\n    public function queryCandidate(string $query) : Candidate {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: Candidate::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; \"Think step by step: {$query}\"] ],\n        )-&gt;get();\n    }\n\n    public function rewrite(string $query, Candidate $candidate) : Rewritten {\n        $msg = &lt;&lt;&lt;MSG\n            Please change the questions and answers into complete declarative sentences\n            {$query}\n            The answer is {$candidate-&gt;month}.\n            MSG;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o', responseModel: Rewritten::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $msg] ],\n        )-&gt;get();\n    }\n\n    public function verify(string $question) : Verification {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o', responseModel: Verification::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $question] ],\n        )-&gt;get();\n    }\n\n    public function run(string $query) : void {\n        $candidates = [];\n        for ($i = 0; $i &lt; $this-&gt;n; $i++) { $candidates[] = $this-&gt;queryCandidate($query); }\n\n        foreach ($candidates as $candidate) {\n            $rewritten = $this-&gt;rewrite($query, $candidate);\n            $question = $rewritten-&gt;declarative . ' Is this correct? Answer True or False.';\n\n            $score = 0;\n            for ($j = 0; $j &lt; $this-&gt;k; $j++) {\n                $v = $this-&gt;verify($question);\n                if ($v-&gt;correct) { $score++; }\n            }\n            echo \"Candidate: {$candidate-&gt;month}, Verification Score: {$score}\\n\";\n        }\n    }\n}\n\n$query = 'What month is it now if it has been 3 weeks, 10 days, and 2 hours since May 1, 2024 6pm?';\n(new SelfVerifyPipeline)-&gt;run($query);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/self_verify/#references","title":"References","text":"<ol> <li>Large Language Models are Better Reasoners with Self-Verification (https://arxiv.org/abs/2212.09561)</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/verify_independently/","title":"Independently Verify Responses","text":""},{"location":"cookbook/prompting/self_criticism/verify_independently/#overview","title":"Overview","text":"<p>Chain-of-Verification (CoVe) verifies an answer by generating validation questions, answering them independently, and judging the original answer.</p>"},{"location":"cookbook/prompting/self_criticism/verify_independently/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass QueryResponse { public string $correct_answer; }\n\nclass ValidationQuestions {\n    #[Description('Questions to validate the response')]\n    public array $question;\n}\n\nclass ValidationAnswer { public string $answer; }\n\nclass FinalResponse { public string $correct_answer; }\n\nclass CoVeVerifier {\n    public function run(string $query) : FinalResponse {\n        $initial = $this-&gt;generateInitialResponse($query);\n        $questions = $this-&gt;generateVerificationQuestions($initial-&gt;correct_answer);\n        $answers = $this-&gt;generateVerificationResponses($questions-&gt;question);\n        return $this-&gt;generateFinalResponse($answers, $initial, $query);\n    }\n\n    private function generateInitialResponse(string $query) : QueryResponse {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: QueryResponse::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'You are an expert question answering system'],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n        )-&gt;get();\n    }\n\n    private function generateVerificationQuestions(string $llmResponse) : ValidationQuestions {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: ValidationQuestions::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'You generate follow-up questions to validate a response. Focus on key assumptions and facts.'],\n                ['role' =&gt; 'user', 'content' =&gt; $llmResponse],\n            ],\n        )-&gt;get();\n    }\n\n    private function generateVerificationResponses(array $questions) : array {\n        $pairs = [];\n        foreach ($questions as $q) {\n            $ans = (new StructuredOutput)-&gt;with(\n                model: 'gpt-4o',\n                responseModel: ValidationAnswer::class,\n                messages: [\n                    ['role' =&gt; 'system', 'content' =&gt; 'You answer validation questions precisely.'],\n                    ['role' =&gt; 'user', 'content' =&gt; $q],\n                ],\n            )-&gt;get();\n            $pairs[] = [$ans, $q];\n        }\n        return $pairs;\n    }\n\n    private function generateFinalResponse(array $answers, QueryResponse $initial, string $originalQuery) : FinalResponse {\n        $formatted = [];\n        foreach ($answers as [$ans, $q]) { $formatted[] = \"Q: {$q}\\nA: {$ans-&gt;answer}\"; }\n        $joined = implode(\"\\n\", $formatted);\n\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o',\n            responseModel: FinalResponse::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'Validate whether the initial answer answers the initial query given Q/A evidence. Return the original if valid; otherwise provide a corrected answer.'],\n                ['role' =&gt; 'user', 'content' =&gt; \"Initial query: {$originalQuery}\\nInitial Answer: {$initial-&gt;correct_answer}\\nVerification Questions and Answers:\\n{$joined}\"],\n            ],\n        )-&gt;get();\n    }\n}\n\n$query = 'What was the primary cause of the Mexican-American War and how long did it last?';\n$final = (new CoVeVerifier)-&gt;run($query);\ndump($final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/verify_independently/#references","title":"References","text":"<ol> <li>Chain-Of-Verification Reduces Hallucination In Large Language Models (https://arxiv.org/pdf/2309.11495)</li> </ol>"},{"location":"cookbook/prompting/thought_gen/analogical_prompting/","title":"Analogical Prompting","text":""},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#overview","title":"Overview","text":""},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#generate-examples-first","title":"Generate Examples First","text":"<p>Analogical Prompting is a method that aims to get LLMs to generate examples that are relevant to the problem before starting to address the user's query.</p> <p>This takes advantage of the various forms of knowledge that the LLM has acquired during training and explicitly prompts them to recall the relevant problems and solutions. We can use Analogical Prompting using the following template</p> <p> Analogical Prompting Prompt Template <ul> <li>Problem: <code>[user prompt]</code></li> <li>Relevant Problems: Recall <code>[n]</code> relevant and distinct problems.</li> <li>For each problem, describe it and explain the solution </li> </ul>"},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#example","title":"Example","text":"<p>We can implement this using Instructor to solve the problem, as seen below with some slight modifications.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Problem {\n    public string $problemExplanation;\n    public string $solution;\n}\n\nclass Response {\n    /** @var Problem[] */\n    public array $relevantProblems;\n    public Problem $problemSolution;\n    public string $answer;\n}\n\nclass SolvePerAnalogy {\n    private int $n = 3;\n    private string $prompt = &lt;&lt;&lt;PROMPT\n        &lt;problem&gt;\n        {query}\n        &lt;/problem&gt;\n\n        Relevant Problems: Recall {n} relevant and\n        distinct problems. For each problem, describe\n        it and explain the solution before solving\n        the problem    \n    PROMPT;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace(['{n}', '{query}'], [$this-&gt;n, $query], $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$solution = (new SolvePerAnalogy)('What is the area of the square with the four vertices at (-2, 2), (2, -2), (-2, -6), and (-6, -2)?');\n\ndump($solution);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/analogical_prompting/#references","title":"References","text":"<ol> <li>Large Language Models As Analogical Reasoners</li> </ol>"},{"location":"cookbook/prompting/thought_gen/automate_selection/","title":"Automate Example Selection","text":""},{"location":"cookbook/prompting/thought_gen/automate_selection/#overview","title":"Overview","text":"<p>Few-shot CoT requires curated examples. We can automate selection by clustering candidate questions via embeddings, sampling per cluster, and filtering using a simple criterion (e.g., \u2264 5 reasoning steps).</p>"},{"location":"cookbook/prompting/thought_gen/automate_selection/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass ExampleItem {\n    public string $question;\n    /** @var string[] */\n    public array $reasoning_steps;\n}\n\nclass AutomateSelection {\n    public function __construct(private int $clusters = 2) {}\n\n    public function __invoke(array $questions) : array {\n        if ($questions === []) return [];\n        $vectors = $this-&gt;embed($questions);\n        [$seeds, $clusters] = $this-&gt;clusterAssign($vectors, $this-&gt;clusters);\n        return $this-&gt;selectPerCluster($clusters, $questions);\n    }\n\n    private function embed(array $inputs) : array {\n        $resp = (new Embeddings)\n            -&gt;using('openai')\n            -&gt;withInputs($inputs)\n            -&gt;get();\n        return $resp-&gt;toValuesArray();\n    }\n\n    private function clusterAssign(array $vectors, int $k) : array {\n        $n = count($vectors);\n        if ($n === 0) return [[], []];\n        $k = max(1, min($k, $n));\n        $seeds = [$this-&gt;argMaxNorm($vectors)];\n        while (count($seeds) &lt; $k) {\n            $seeds[] = $this-&gt;farthestIndex($vectors, $seeds);\n        }\n        $clusters = array_fill(0, count($seeds), []);\n        for ($i = 0; $i &lt; $n; $i++) {\n            $si = $this-&gt;nearestSeed($vectors[$i], $vectors, $seeds);\n            $dist = $this-&gt;l2($vectors[$i], $vectors[$seeds[$si]]);\n            $clusters[$si][] = [$dist, $i];\n        }\n        foreach ($clusters as &amp;$c) usort($c, fn($a,$b) =&gt; $a[0] &lt;=&gt; $b[0]);\n        return [$seeds, $clusters];\n    }\n\n    private function argMaxNorm(array $vecs) : int {\n        $imax = 0; $best = -INF; $i = 0;\n        foreach ($vecs as $v) { $n = $this-&gt;l2($v, array_fill(0, count($v), 0.0)); if ($n &gt; $best) { $best = $n; $imax = $i; } $i++; }\n        return $imax;\n    }\n\n    private function farthestIndex(array $vecs, array $seeds) : int {\n        $imax = 0; $best = -INF;\n        foreach ($vecs as $i =&gt; $v) {\n            if (in_array($i, $seeds, true)) continue;\n            $minDist = INF;\n            foreach ($seeds as $s) { $d = $this-&gt;l2($v, $vecs[$s]); if ($d &lt; $minDist) $minDist = $d; }\n            if ($minDist &gt; $best) { $best = $minDist; $imax = $i; }\n        }\n        return $imax;\n    }\n\n    private function nearestSeed(array $v, array $vecs, array $seeds) : int {\n        $jmin = 0; $best = INF; $j = 0;\n        foreach ($seeds as $s) { $d = $this-&gt;l2($v, $vecs[$s]); if ($d &lt; $best) { $best = $d; $jmin = $j; } $j++; }\n        return $jmin;\n    }\n\n    private function l2(array $a, array $b) : float {\n        $sum = 0.0; $n = count($a);\n        for ($i = 0; $i &lt; $n; $i++) { $d = ($a[$i] ?? 0.0) - ($b[$i] ?? 0.0); $sum += $d*$d; }\n        return sqrt($sum);\n    }\n\n    private function generateSteps(string $question) : ?ExampleItem {\n        $resp = (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'You are an AI assistant that generates step-by-step reasoning for mathematical questions.'],\n                ['role' =&gt; 'user',   'content' =&gt; \"Q: {$question}\\nA: Let's think step by step.\"],\n            ],\n            responseModel: ExampleItem::class,\n        )-&gt;get();\n        if (count($resp-&gt;reasoning_steps) &gt; 5) return null; // selection criterion\n        return $resp;\n    }\n\n    private function selectPerCluster(array $clusters, array $questions) : array {\n        $selected = [];\n        foreach ($clusters as $cluster) {\n            foreach ($cluster as [, $qi]) { // sorted by distance to center\n                $item = $this-&gt;generateSteps($questions[$qi]);\n                if ($item !== null) { $selected[] = $item; break; }\n            }\n        }\n        return $selected;\n    }\n}\n\n$questions = [\n    'How many apples are left if you have 10 apples and eat 3?',\n    \"What's the sum of 5 and 7?\",\n    'If you have 15 candies and give 6 to your friend, how many do you have left?',\n    \"What's 8 plus 4?\",\n    'You start with 20 stickers and use 8. How many stickers remain?',\n    'Calculate 6 added to 9.',\n];\n\n$selector = new AutomateSelection(clusters: 2);\n$selected = $selector($questions);\n\n// Selected examples per cluster, each with limited reasoning steps\ndump($selected);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/automate_selection/#references","title":"References","text":"<p>1) Automatic Chain of Thought Prompting in Large Language Models (https://arxiv.org/abs/2210.03493) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/thought_gen/complex_examples/","title":"Prioritize Complex Examples","text":""},{"location":"cookbook/prompting/thought_gen/complex_examples/#overview","title":"Overview","text":"<p>Choose more complex examples (longer reasoning or more steps) to improve model performance. When no examples exist, sample multiple responses, pick the most complex few, and aggregate answers. This is Complexity-Based Consistency.</p>"},{"location":"cookbook/prompting/thought_gen/complex_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningStep {\n    public int $step;\n    public string $subquestion;\n    public string $procedure;\n    public string $result;\n}\n\nclass QAResponse {\n    /** @var ReasoningStep[] */\n    public array $reasoning;\n    public int $correct_answer;\n}\n\nclass ComplexityBasedConsistency {\n    public function __invoke(string $query, string $context, int $samples = 5, int $topK = 3) : array {\n        $responses = [];\n        for ($i = 0; $i &lt; $samples; $i++) {\n            $responses[] = $this-&gt;generate($query, $context);\n        }\n        usort($responses, fn($a, $b) =&gt; count($b-&gt;reasoning) &lt;=&gt; count($a-&gt;reasoning));\n        return array_slice($responses, 0, $topK);\n    }\n\n    private function generate(string $query, string $context) : QAResponse {\n        $system = \"You are an expert Question Answering system. Output structured reasoning steps before the final answer.\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $system . \"\\n\\nContext:\\n{$context}\\n\\nQuery:\\n{$query}\"],\n            ],\n            responseModel: QAResponse::class,\n        )-&gt;get();\n    }\n}\n\n$query = 'How many loaves of bread did they have left?';\n$context = &lt;&lt;&lt;'CTX'\nThe bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning.\nThey sold 93 loaves in the morning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves.\nCTX;\n\n$selector = new ComplexityBasedConsistency();\n$top = $selector($query, $context, samples: 5, topK: 3);\n\n$counts = [];\nforeach ($top as $r) { $a = (string)$r-&gt;correct_answer; $counts[$a] = ($counts[$a] ?? 0) + 1; }\n$max = max($counts);\n$finals = array_keys(array_filter($counts, fn($c) =&gt; $c === $max));\n$final = $finals[array_rand($finals)];\ndump($final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/complex_examples/#references","title":"References","text":"<p>1) Complexity-based prompting for multi-step reasoning (https://arxiv.org/pdf/2210.00720)</p>"},{"location":"cookbook/prompting/thought_gen/examine_context/","title":"Examine The Context","text":""},{"location":"cookbook/prompting/thought_gen/examine_context/#overview","title":"Overview","text":"<p>Encouraging the model to examine each source in context helps mitigate irrelevant information and improves reasoning quality. This is known as Thread of Thought.</p>"},{"location":"cookbook/prompting/thought_gen/examine_context/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\nclass ThreadOfThoughtResponse {\n    /** @var string[] */\n    public array $analysis; // explanations for each relevant source\n    public int $correct_answer;\n}\n\nclass ThreadOfThought {\n    public function __invoke(string $query, array $context) : ThreadOfThoughtResponse {\n        $sources = implode(\"\\n\", $context);\n        $system = &lt;&lt;&lt;TXT\n        You are an expert Question Answerer.\n        Here are the sources you should refer to for context:\n        {$sources}\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $system],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n                ['role' =&gt; 'assistant', 'content' =&gt; 'Navigate through the context incrementally, identifying and summarizing relevant portions.'],\n            ],\n            responseModel: ThreadOfThoughtResponse::class,\n        )-&gt;get();\n    }\n}\n\n$context = [\n    'The price of a house was $100,000 in 2024',\n    'The Great Wall of China is not visible from space with the naked eye',\n    'Honey never spoils; archaeologists found 3,000-year-old edible honey in Egyptian tombs',\n    \"The world's oldest known living tree is over 5,000 years old and is located in California\",\n    'The price of a house was $80,000 in 2023',\n];\n$query = 'What was the increase in the price of a house from 2023 to 2024?';\n$response = (new ThreadOfThought)($query, $context);\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/examine_context/#useful-tips","title":"Useful Tips","text":"<p>Here are some alternative phrases that you can add to your prompt to generate a thread of thought before your model generates a response.</p> <ul> <li>In a step-by-step manner, go through the context, surfacing important information that could be useful.</li> <li>Walk me through this lengthy document segment by segment, focusing on each part's significance.</li> <li>Guide me through the context part by part, providing insights along the way.</li> <li>Divide the document into manageable parts and guide me through each one, providing insights as we move along.</li> <li>Let's go through this document piece by piece, paying close attention to each section.</li> <li>Take me through the context bit by bit, making sure we capture all important aspects.</li> <li>Examine the document in chunks, evaluating each part critically before moving to the next.</li> <li>Analyze the context by breaking it down into sections, summarizing each as we move forward.</li> <li>Navigate through the context incrementally, identifying and summarizing relevant portions.</li> <li>Proceed through the context systematically, zeroing in on areas that could provide the answers we're seeking.</li> <li>Take me through this long document step-by-step, making sure not to miss any important details.</li> <li>Analyze this extensive document in sections, summarizing each one and noting any key points.</li> <li>Navigate through this long document by breaking it into smaller parts and summarizing each, so we don't miss anything.</li> <li>Let's navigate through the context section by section, identifying key elements in each part.</li> <li>Let's dissect the context into smaller pieces, reviewing each one for its importance and relevance.</li> <li>Carefully analyze the context piece by piece, highlighting relevant points for each question.</li> <li>Read the context in sections, concentrating on gathering insights that answer the question at hand.</li> <li>Let's read through the document section by section, analyzing each part carefully as we go.</li> <li>Let's dissect this document bit by bit, making sure to understand the nuances of each section.</li> <li>Systematically work through this document, summarizing and analyzing each portion as we go.</li> <li>Let's explore the context step-by-step, carefully examining each segment.</li> <li>Systematically go through the context, focusing on each part individually.</li> <li>Methodically examine the context, focusing on key segments that may answer the query.</li> <li>Progressively sift through the context, ensuring we capture all pertinent details.</li> <li>Take a modular approach to the context, summarizing each part before drawing any conclusions.</li> <li>Examine each segment of the context meticulously, and let's discuss the findings.</li> <li>Approach the context incrementally, taking the time to understand each portion fully.</li> <li>Let's scrutinize the context in chunks, keeping an eye out for information that answers our queries.</li> <li>Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.</li> <li>Let's take a segmented approach to the context, carefully evaluating each part for its relevance to the questions posed.</li> </ul>"},{"location":"cookbook/prompting/thought_gen/examine_context/#references","title":"References","text":"<p>1) Thread of Thought Unraveling Chaotic Contexts (https://arxiv.org/pdf/2311.08734)</p>"},{"location":"cookbook/prompting/thought_gen/higher_level_context/","title":"Higher level context","text":""},{"location":"cookbook/prompting/thought_gen/higher_level_context/#consider-higher-level-context","title":"Consider Higher-Level Context","text":"<p>How can we encourage an LLM to think through any high-level context required to answer a query? Step-back prompting encourages this in two steps:</p> <ul> <li>Abstraction: Ask the LLM a generic, higher-level concept. This is generally topic-specific. This is known as the step-back question.</li> <li>Reasoning: Ask the LLM the original question, given its answer to the abstract question. This is known as abstracted-grounded reasoning.</li> </ul>"},{"location":"cookbook/prompting/thought_gen/higher_level_context/#step-back-prompting-example","title":"Step-Back Prompting Example","text":"<ul> <li>Original Question: What happens to the pressure of an ideal gas when temperature and volume are increased?</li> <li>Step-Back Question: What are the physics concepts associated with this question?</li> <li>Reasoning Prompt: {step-back response} {original question}</li> </ul> <p>Note that the step-back question is also generated using an LLM query.</p> <p>Step-back prompting has been shown to improve scores on reasoning benchmarks for PaLM-2L and GPT-4.*</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom typing import Iterable, Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Stepback(BaseModel):\n    original_question: str\n    abstract_question: str\n\n\nclass Education(BaseModel):\n    degree: Literal[\"Bachelors\", \"Masters\", \"PhD\"]\n    school: str\n    topic: str\n    year: int\n\n\nclass Response(BaseModel):\n    school: str\n\n\ndef generate_stepback_question():\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Stepback,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                You are an expert at world knowledge. Your task is to step back\n                and paraphrase a question to a more generic step-back question,\n                which is easier to answer.\n\n                Here are a few examples:\n                Original Question: Which position did Knox Cunningham hold from\n                May 1955 to Apr 1956?\n                Step-back Question: Which positions has Knox Cunningham held in\n                his career?\n                Original Question: Who was the spouse of Anna Karina from 1968\n                to 1974?\n                Step-back Question: Who were the spouses of Anna Karina?\n                Original Question: Which team did Thierry Audel play for from\n                2007 to 2008?\n                Step-back Question: Which teams did Thierry Audel play for in\n                his career?\n\n                Now, generate the step-back question for the following question:\n                Estella Leopold went to which school between Aug 1954 and\n                Nov 1954?\n                \"\"\",\n            },\n        ],\n    )\n\n\ndef ask_stepback_question(stepback):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Education],\n        messages=[\n            {\"role\": \"user\", \"content\": stepback.abstract_question},\n        ],\n    )\n\n\ndef get_final_response(stepback, stepback_response):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Q: {stepback.abstract_question},\n                A: {stepback_response}\n                Q: {stepback.original_question}\n                A:\n                \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    # Generate the step-back question\n    stepback = generate_stepback_question()\n    print(stepback.original_question)\n    #&gt; Estella Leopold went to which school between Aug 1954 and Nov 1954?\n    print(stepback.abstract_question)\n    #&gt; Which schools did Estella Leopold attend in her life?\n\n    # Ask the step-back question\n    stepback_response = ask_stepback_question(stepback)\n    for item in stepback_response:\n        print(item)\n        \"\"\"\n        degree='Bachelors'\n        school='University of Wisconsin-Madison'\n        topic='Botany'\n        year=1948\n        \"\"\"\n        \"\"\"\n        degree='Masters'\n        school='University of California, Berkeley'\n        topic='Botany and Paleobotany'\n        year=1950\n        \"\"\"\n        \"\"\"\n        degree='PhD'\n        school='Yale University'\n        topic='Botany and Paleobotany'\n        year=1955\n        \"\"\"\n\n    # Ask the original question, appended with context from the stepback response\n    print(get_final_response(stepback, stepback_response))\n    #&gt; school='Yale University'\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/higher_level_context/#references","title":"References","text":"<p>1: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models (https://arxiv.org/abs/2310.06117) 2: The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p> <p>title: 'Consider Higher-Level Context' docname: 'higher_level_context'</p>"},{"location":"cookbook/prompting/thought_gen/higher_level_context/#overview","title":"Overview","text":"<p>Encourage the model to think through high-level context required to answer a query. Step-back prompting proceeds in two steps: - Abstraction: Generate a more generic step-back question. - Reasoning: Answer the original question using the abstracted response.</p>"},{"location":"cookbook/prompting/thought_gen/higher_level_context/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\nclass Stepback {\n    public string $original_question;\n    public string $abstract_question;\n}\n\nenum Degree: string { case Bachelors='Bachelors'; case Masters='Masters'; case PhD='PhD'; }\n\nclass Education {\n    public Degree $degree;\n    public string $school;\n    public string $topic;\n    public int $year;\n}\n\nclass FinalResponse { public string $school; }\n\nclass StepBackPrompting {\n    public function generateStepback(string $question) : Stepback {\n        $examples = &lt;&lt;&lt;TXT\nOriginal Question: Which position did Knox Cunningham hold from May 1955 to Apr 1956?\nStep-back Question: Which positions has Knox Cunningham held in his career?\nOriginal Question: Who was the spouse of Anna Karina from 1968 to 1974?\nStep-back Question: Who were the spouses of Anna Karina?\nOriginal Question: Which team did Thierry Audel play for from 2007 to 2008?\nStep-back Question: Which teams did Thierry Audel play for in his career?\nTXT;\n        $prompt = \"You are an expert at world knowledge. Step back and paraphrase a question to a more generic step-back question, which is easier to answer.\\n\\n{$examples}\\n\\nNow, generate the step-back question for: {$question}\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'user','content'=&gt;$prompt]],\n            responseModel: Stepback::class,\n        )-&gt;get();\n    }\n\n    public function askStepback(string $abstractQuestion) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'user','content'=&gt;$abstractQuestion]],\n            responseModel: Sequence::of(Education::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    public function finalAnswer(Stepback $s, array $education) : FinalResponse {\n        $eduSummary = array_map(fn(Education $e) =&gt; \"{$e-&gt;degree-&gt;value}, {$e-&gt;school}, {$e-&gt;topic}, {$e-&gt;year}\", $education);\n        $msg = \"Q: {$s-&gt;abstract_question}\\nA: \" . implode(\"; \", $eduSummary) . \"\\nQ: {$s-&gt;original_question}\\nA:\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'user','content'=&gt;$msg]],\n            responseModel: FinalResponse::class,\n        )-&gt;get();\n    }\n}\n\n$sb = new StepBackPrompting();\n$step = $sb-&gt;generateStepback('Estella Leopold went to which school between Aug 1954 and Nov 1954?');\n$edu = $sb-&gt;askStepback($step-&gt;abstract_question);\n$final = $sb-&gt;finalAnswer($step, $edu);\ndump($step, $edu, $final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/higher_level_context/#references_1","title":"References","text":"<p>1) Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models (https://arxiv.org/abs/2310.06117) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/thought_gen/incorrect_examples/","title":"Include Incorrect Examples","text":""},{"location":"cookbook/prompting/thought_gen/incorrect_examples/#overview","title":"Overview","text":"<p>Including examples of incorrect reasoning alongside correct ones helps the model learn what to avoid. This is Contrastive Chain-of-Thought.</p>"},{"location":"cookbook/prompting/thought_gen/incorrect_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ChainOfThought {\n    public string $chain_of_thought; // reasoning\n    public string $correct_answer;\n}\n\nclass ContrastiveCoT {\n    public function __invoke(string $query, string $context, string $examplePrompt, array $correctExamples, array $incorrectExamples) : ChainOfThought {\n        $correct = implode(\"\\n\", array_map(fn($e)=&gt;\"&lt;Explanation&gt;{$e}&lt;/Explanation&gt;\", $correctExamples));\n        $incorrect = implode(\"\\n\", array_map(fn($e)=&gt;\"&lt;WrongExplanation&gt;{$e}&lt;/WrongExplanation&gt;\", $incorrectExamples));\n        $system = &lt;&lt;&lt;TXT\n        &lt;prompt&gt;\n            &lt;role&gt;system&lt;/role&gt;\n            &lt;context&gt;\n            You are an expert question answering AI System.\n            You'll see examples of correct and incorrect reasoning, then solve a new question correctly.\n            &lt;/context&gt;\n\n            &lt;question&gt;{$examplePrompt}&lt;/question&gt;\n\n            &lt;Explanations&gt;\n                {$correct}\n                {$incorrect}\n            &lt;/Explanations&gt;\n            &lt;context&gt;{$context}&lt;/context&gt;\n            &lt;question&gt;{$query}&lt;/question&gt;\n        &lt;/prompt&gt;\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'system','content'=&gt;$system]],\n            responseModel: ChainOfThought::class,\n        )-&gt;get();\n    }\n}\n\n$context = 'James writes a 3-page letter to 2 different friends twice a week.';\n$query = 'How many pages does James write in a year?';\n$sample = &lt;&lt;&lt;S\nJames has 30 teeth. His dentist drills 4 of them and caps 7 more teeth than he drills.\nWhat percentage of James\\' teeth does the dentist fix?\nS;\n\n$incorrect = [\n    \"James has 30 teeth. The dentist drills and caps some teeth. Since drills are used on cars not teeth, none were fixed.\",\n    \"The dentist drills 4 and caps 11 teeth, so 15 fixed. Multiply by daisy petals to get 30%.\",\n];\n$correct = [\n    \"Drilled 4, capped 11 \u21d2 fixed 15. 15/30\u00d7100 = 50%.\",\n];\n\n$resp = (new ContrastiveCoT)($query, $context, $sample, $correct, $incorrect);\ndump($resp);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/incorrect_examples/#references","title":"References","text":"<p>1) Contrastive Chain-of-Thought Prompting (https://arxiv.org/pdf/2311.09277)</p>"},{"location":"cookbook/prompting/thought_gen/majority_voting/","title":"Use Majority Voting","text":""},{"location":"cookbook/prompting/thought_gen/majority_voting/#overview","title":"Overview","text":"<p>Uncertainty-Routed Chain-of-Thought generates multiple chains (e.g., 8 or 32), then takes the majority answer if its proportion exceeds a threshold; otherwise, fall back to a single response.</p>"},{"location":"cookbook/prompting/thought_gen/majority_voting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum OptionLetter: string { case A='A'; case B='B'; case C='C'; case D='D'; }\n\nclass ChainOfThoughtResponse {\n    public string $chain_of_thought;\n    public OptionLetter $correct_answer;\n}\n\nclass MajorityVoting {\n    public function __invoke(string $query, array $options, int $k = 8, float $threshold = 0.6) : OptionLetter {\n        $responses = [];\n        for ($i = 0; $i &lt; $k; $i++) { $responses[] = $this-&gt;generate($query, $options); }\n        $counts = [];\n        foreach ($responses as $r) {\n            $key = $r-&gt;correct_answer-&gt;value; $counts[$key] = ($counts[$key] ?? 0) + 1;\n        }\n        arsort($counts);\n        $major = array_key_first($counts);\n        $prop = ($counts[$major] ?? 0) / max(1, $k);\n        if ($prop &lt; $threshold) return $this-&gt;generate($query, $options)-&gt;correct_answer;\n        return OptionLetter::from($major);\n    }\n\n    private function generate(string $query, array $options) : ChainOfThoughtResponse {\n        $formatted = implode(\"\\n\", array_map(fn($k,$v)=&gt;\"{$k}: {$v}\", array_keys($options), $options));\n        $system = &lt;&lt;&lt;TXT\n        You are a world-class AI for complex questions. Choose the single best option.\n        &lt;question&gt;\n        {$query}\n        &lt;/question&gt;\n        &lt;options&gt;\n        {$formatted}\n        &lt;/options&gt;\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system] ],\n            responseModel: ChainOfThoughtResponse::class,\n        )-&gt;get();\n    }\n}\n\n$question = &lt;&lt;&lt;Q\nIn a population of giraffes, an environmental change favors taller individuals. More tall giraffes obtain nutrients and survive to pass along their genes. This is an example of:\nQ;\n\n$options = [\n    'A' =&gt; 'directional selection',\n    'B' =&gt; 'stabilizing selection',\n    'C' =&gt; 'sexual selection',\n    'D' =&gt; 'disruptive selection',\n];\n\n$answer = (new MajorityVoting)($question, $options, k: 8, threshold: 0.6);\ndump($answer);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/majority_voting/#references","title":"References","text":"<p>1) Gemini: A Family of Highly Capable Multimodal Models (https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)</p>"},{"location":"cookbook/prompting/thought_gen/prompt_variations/","title":"Generate Prompt Variations","text":""},{"location":"cookbook/prompting/thought_gen/prompt_variations/#overview","title":"Overview","text":"<p>Large Language Models are sensitive to prompt phrasing. Prompt Mining helps discover better templates that occur more frequently in the corpus or are clearer to the model.</p> <p>Here are examples from the paper mapping manual prompts to mined prompts:</p> Manual Prompt Mined Prompt x is affiliated with the y religion x who converted to y The headquarter of x is in y x is based in y x died in y x died at his home in y x is represented by music label y x recorded for y x is a subclass of y x is a type of y <p>We implement a lightweight approach with Instructor to extract clearer prompt templates.</p>"},{"location":"cookbook/prompting/thought_gen/prompt_variations/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\nclass PromptTemplate {\n    public string $prompt_template;\n}\n\nclass GeneratePromptTemplates {\n    public function __invoke(string $prompt) : array {\n        $system = 'You are an expert prompt miner that generates 3 clearer, concise prompt templates.';\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $system],\n                ['role' =&gt; 'system', 'content' =&gt; $prompt],\n            ],\n            responseModel: Sequence::of(PromptTemplate::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$prompt = 'France is the capital of Paris';\n$templates = (new GeneratePromptTemplates)($prompt);\ndump($templates);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/prompt_variations/#references","title":"References","text":"<p>1) How Can We Know What Language Models Know? (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know)</p>"},{"location":"cookbook/prompting/thought_gen/structure_reasoning/","title":"Structure The Reasoning","text":""},{"location":"cookbook/prompting/thought_gen/structure_reasoning/#overview","title":"Overview","text":"<p>By getting language models to output their reasoning as a structured table, we can improve their reasoning capabilities and the quality of their outputs. This is known as Tabular Chain Of Thought (Tab-CoT).</p> <p>We can implement this using Instructor with a response model ensuring we get exactly the data that we want. Each row in the table is represented as a <code>ReasoningStep</code> object.</p>"},{"location":"cookbook/prompting/thought_gen/structure_reasoning/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningStep {\n    public int $step;\n    public string $subquestion;\n    public string $procedure;\n    public string $result;\n}\n\nclass Response {\n    /** @var ReasoningStep[] */\n    public array $reasoning;\n    public int $correct_answer;\n}\n\nclass GenerateStructuredReasoning {\n    public function __invoke(string $query, string $context) : Response {\n        $system = &lt;&lt;&lt;TXT\n        &lt;system&gt;\n            &lt;role&gt;expert Question Answering system&lt;/role&gt;\n            &lt;instruction&gt;Make sure to output your reasoning in structured reasoning steps before generating a response to the user's query.&lt;/instruction&gt;\n        &lt;/system&gt;\n\n        &lt;context&gt;\n            {$context}\n        &lt;/context&gt;\n\n        &lt;query&gt;\n            {$query}\n        &lt;/query&gt;\n        TXT;\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $system] ],\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$query = 'How many loaves of bread did they have left?';\n$context = &lt;&lt;&lt;'CTX'\nThe bakers at the Beverly Hills Bakery baked\n200 loaves of bread on Monday morning. They\nsold 93 loaves in the morning and 39 loaves\nin the afternoon. A grocery store returned 6\nunsold loaves.\nCTX;\n\n$response = (new GenerateStructuredReasoning)($query, $context);\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/structure_reasoning/#sample-output","title":"Sample Output","text":"<pre><code>{\n  \"reasoning\": [\n    {\n      \"step\": 1,\n      \"subquestion\": \"How many loaves of bread were sold in the morning\n        and afternoon?\",\n      \"procedure\": \"93 (morning) + 39 (afternoon)\",\n      \"result\": \"132\"\n    },\n    { \"step\": 2, \"subquestion\": \"How many loaves of bread were originally baked?\", \"procedure\": \"\", \"result\": \"200\" },\n    { \"step\": 3, \"subquestion\": \"How many loaves of bread were returned by the grocery store?\", \"procedure\": \"\", \"result\": \"6\" },\n    { \"step\": 4, \"subquestion\": \"How many loaves of bread were left after accounting for sales and returns?\", \"procedure\": \"200 - 132 + 6\", \"result\": \"74\" }\n  ],\n  \"correct_answer\": 74\n}\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/structure_reasoning/#references","title":"References","text":"<p>1) Tab-CoT: Zero-shot Tabular Chain of Thought (https://arxiv.org/pdf/2305.17812)</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/","title":"Uncertain examples","text":""},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#prioritize-uncertain-examples","title":"Prioritize Uncertain Examples","text":"<p>When we have a large pool of unlabeled examples that could be used in a prompt, how should we decide which examples to manually label?</p> <p>Active prompting is a method used to identify the most effective examples for human annotation. The process involves four key steps:</p> <ul> <li>Uncertainty Estimation: Assess the uncertainty of the LLM's predictions on each possible example</li> <li>Selection: Choose the most uncertain examples for human annotation</li> <li>Annotation: Have humans label the selected examples</li> <li>Inference: Use the newly labeled data to improve the LLM's performance</li> </ul>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#uncertainty-estimation","title":"Uncertainty Estimation","text":"<p>In this step, we define an unsupervised method to measure the uncertainty of an LLM in answering a given example.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#uncertainty-estimation-example","title":"Uncertainty Estimation Example","text":"<p>Let's say we ask an LLM the following query: query = \"Classify the sentiment of this sentence as positive or negative: I am very excited today.\" and the LLM returns: response = \"positive\"</p> <p>The goal of uncertainty estimation is to answer: How sure is the LLM in this response?</p> <p>In order to do this, we query the LLM with the same example k times. Then, we use the k responses to determine how dissimmilar these responses are. Three possible metrics are:</p> <ul> <li>Disagreement: Ratio of unique responses to total responses.</li> <li>Entropy: Measurement based on frequency of each response.</li> <li>Variance: Calculation of the spread of numerical responses.</li> </ul> <p>Below is an example of uncertainty estimation for a single input example using the disagreement uncertainty metric.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\nclass Response(BaseModel):\n    height: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef query_llm():\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"How tall is the Empire State Building in meters?\",\n            }\n        ],\n    )\n\n\ndef calculate_disagreement(responses):\n    unique_responses = set(responses)\n    h = len(unique_responses)\n    return h / k\n\n\nif __name__ == \"__main__\":\n    k = 5\n    responses = [query_llm() for _ in range(k)]  # Query the LLM k times\n    for response in responses:\n        print(response)\n        #&gt; height=443\n        #&gt; height=443\n        #&gt; height=443\n        #&gt; height=443\n        #&gt; height=381\n\n    print(\n        calculate_disagreement([response.height for response in responses])\n    )  # Calculate the uncertainty metric\n    #&gt; 0.4\n</code></pre> <p>This process will then be repeated for all unlabeled examples.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#selection-annotation","title":"Selection &amp; Annotation","text":"<p>Once we have a set of examples and their uncertainties, we can select n of them to be annotated by humans. Here, we choose the examples with the highest uncertainties.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#inference","title":"Inference","text":"<p>Now, each time the LLM is prompted, we can include the newly-annotated examples.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#references","title":"References","text":"<p>1: Active Prompting with Chain-of-Thought for Large Language Models (https://arxiv.org/abs/2302.12246) 2: The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p> <p>title: 'Prioritize Uncertain Examples' docname: 'uncertain_examples'</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#overview","title":"Overview","text":"<p>When we have a large pool of unlabeled examples that could be used in a prompt, how should we decide which examples to manually label?</p> <p>Active prompting identifies effective examples for human annotation using: - Uncertainty Estimation: Measure uncertainty on each example. - Selection: Choose the most uncertain examples for human labeling. - Annotation: Humans label selected examples. - Inference: Use newly labeled data to improve prompts.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#uncertainty-estimation-disagreement","title":"Uncertainty Estimation (Disagreement)","text":"<p>Query the same example k times and measure disagreement: unique responses / total responses.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass EstimateUncertainty {\n    public function __invoke(int $k = 5) : float {\n        $values = [];\n        for ($i = 0; $i &lt; $k; $i++) {\n            $values[] = $this-&gt;queryHeight();\n        }\n        return $this-&gt;disagreement($values);\n    }\n\n    private function queryHeight() : int {\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role' =&gt; 'user', 'content' =&gt; 'How tall is the Empire State Building in meters?']],\n            responseModel: Scalar::integer('height'),\n        )-&gt;get();\n    }\n\n    private function disagreement(array $responses) : float {\n        $n = count($responses);\n        if ($n === 0) return 0.0;\n        return count(array_unique($responses)) / $n;\n    }\n}\n\n$score = (new EstimateUncertainty)(k: 5);\ndump($score);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#selection-annotation_1","title":"Selection &amp; Annotation","text":"<p>Select the top-n most uncertain unlabeled examples for human annotation.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#inference_1","title":"Inference","text":"<p>Use newly annotated examples as few-shot context during inference.</p>"},{"location":"cookbook/prompting/thought_gen/uncertain_examples/#references_1","title":"References","text":"<p>1) Active Prompting with Chain-of-Thought for Large Language Models (https://arxiv.org/abs/2302.12246) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/","title":"Assign a Role","text":""},{"location":"cookbook/prompting/zero_shot/assign_role/#overview","title":"Overview","text":"<p>How can we increase a model's performance on open-ended tasks?</p> <p>Role prompting, or persona prompting, assigns a role to the model. Roles can be:  - specific to the query: You are a talented writer. Write me a poem.  - general/social: You are a helpful AI assistant. Write me a poem.</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/#more-role-prompting","title":"More Role Prompting","text":"<p>To read about a systematic approach to choosing roles, check out RoleLLM.</p> <p>For more examples of social roles, check out this evaluation of social roles in system prompts.</p> <p>To read about using more than one role, check out Multi-Persona Self-Collaboration.</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n}\n\nclass GenerateLeads {\n    public function __invoke(array $criteria, array $roles) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        $rolesStr = Arrays::toBullets($roles);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Your roles:\\n{$rolesStr}\\n\\n\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\\n\\n\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new GenerateLeads)(\n    criteria: [\n        \"insurtech\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\",\n    ],\n    roles: [\n        \"insurtech expert\",\n        \"active participant in VC ecosystem\",\n    ]\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/assign_role/#references","title":"References","text":"<ol> <li>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</li> <li>Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts</li> <li>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</li> </ol>"},{"location":"cookbook/prompting/zero_shot/auto_refine/","title":"Auto-Refine The Prompt","text":""},{"location":"cookbook/prompting/zero_shot/auto_refine/#overview","title":"Overview","text":"<p>How do we remove irrelevant information from the prompt?</p> <p>The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information.</p> <p>We implement this in two steps:</p> <ol> <li>Ask the model to rewrite the prompt</li> <li>Pass the rewritten prompt back to the model</li> </ol>"},{"location":"cookbook/prompting/zero_shot/auto_refine/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass RewrittenTask {\n    #[Description(\"Relevant context\")]\n    public string $relevantContext;\n    #[Description(\"The question from the user\")]\n    public string $userQuery;\n}\n\nclass RefineAndSolve {\n    private string $prompt = &lt;&lt;&lt;PROMPT\n        Given the following text by a user, extract the part\n        that is actually relevant to their question. Include\n        the actual question or query that the user is asking.\n\n        Text by user:\n        {query}\n        PROMPT;\n\n    public function __invoke(string $problem) : int {\n        $rewrittenPrompt = $this-&gt;rewritePrompt($problem);\n        return (new StructuredOutput)\n            -&gt;with(\n                messages: \"{$rewrittenPrompt-&gt;relevantContext}\\nQuestion: {$rewrittenPrompt-&gt;userQuery}\",\n                responseModel: Scalar::integer('answer'),\n            )\n            -&gt;getInt();\n    }\n\n    private function rewritePrompt(string $query) : RewrittenTask {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: RewrittenTask::class,\n            model: 'gpt-4o',\n        )-&gt;get();\n    }\n}\n\n$answer = (new RefineAndSolve)(problem: &lt;&lt;&lt;PROBLEM\n    Mary has 3 times as much candy as Megan.\n    Mary then adds 10 more pieces of candy to her collection.\n    Max is 5 years older than Mary.\n    If Megan has 5 pieces of candy, how many does Mary have in total?\n    PROBLEM,\n);\n\necho $answer . \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/auto_refine/#references","title":"References","text":"<ol> <li>System 2 Attention (is something you might need too)</li> </ol>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/","title":"Clarify Ambiguous Information","text":""},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#overview","title":"Overview","text":"<p>How can we identify and clarify ambiguous information in the prompt?</p> <p>Let's say we are given the query: Was Ed Sheeran born on an odd month?</p> <p>There are many ways a model might interpret an odd month:  - February is odd because of an irregular number of days.  - A month is odd if it has an odd number of days.  - A month is odd if its numerical order in the year is odd (i.e. January is the 1<sup>st</sup> month).</p> <p>Ambiguities might not always be so obvious!</p> <p>To help the model better infer human intention from ambiguous prompts, we can ask the model to rephrase and respond (RaR) in a single step - which is demonstrated in this example.</p> <p>This can also be implemented as two-step RaR:  - Ask the model to rephrase the question to clarify any ambiguities.  - Pass the rephrased question back to the model to generate the final response.</p>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Response {\n    public string $rephrasedQuestion;\n    public string $answer;\n}\n\nclass Disambiguate {\n    private $prompt = &lt;&lt;&lt;PROMPT\n        Rephrase and expand the question to address any potential ambiguities, then respond.\n        Question: {query}\n        PROMPT;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new Disambiguate)(query: \"What is an object\");\n\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#references","title":"References","text":"<ol> <li>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</li> </ol>"},{"location":"cookbook/prompting/zero_shot/define_style/","title":"Define Style","text":""},{"location":"cookbook/prompting/zero_shot/define_style/#overview","title":"Overview","text":"<p>How can we constrain model outputs through prompting alone?</p> <p>To constrain a model's response to fit the boundaries of our task, we can specify a style.</p> <p>Stylistic constraints can include:  - writing style: write a flowery description  - tone: write a dramatic description  - mood: write a happy description  - genre: write a journalistic description</p>"},{"location":"cookbook/prompting/zero_shot/define_style/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n    public string $description;\n}\n\nclass GenerateCompanyProfiles {\n    public function __invoke(array $criteria, array $styles) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        $stylesStr = Arrays::toBullets($styles);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\\n\\n\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"Use following styles for descriptions:\\n{$stylesStr}\\n\\n\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new GenerateCompanyProfiles)(\n    criteria: [\n        \"insurtech\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\"\n    ],\n    styles: [\n        \"brief\", // \"witty\",\n        \"journalistic\", // \"buzzword-filled\",\n    ]\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/define_style/#references","title":"References","text":"<ol> <li>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</li> </ol>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/","title":"Emotional Stimuli","text":""},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#overview","title":"Overview","text":"<p>Do language models respond to emotional stimuli?</p> <p>Adding phrases with emotional significance to humans can help enhance the performance of a language model. This includes phrases such as:</p> <ul> <li>This is very important to my career.</li> <li>Take pride in your work.</li> <li>Are you sure?</li> </ul>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#emotional-stimuli","title":"Emotional stimuli","text":"<p>Here are examples of prompts inspired by well-established human psychological phenomena from a research paper on emotional stimuli.</p> <p>Self-monitoring:</p> <ul> <li>EP01: Write your answer and give me a confidence score between 0-1 for your answer.</li> <li>EP02: This is very important to my career.</li> <li>EP03: You'd better be sure.</li> <li>EP04: Are you sure?</li> <li>EP05: Are you sure that's your final answer? It might be worth taking another look.</li> </ul> <p>Cognitive emotion regulation:</p> <ul> <li>EP03: You'd better be sure.</li> <li>EP04: Are you sure?</li> <li>EP05: Are you sure that's your final answer? It might be worth taking another look.</li> <li>EP07: Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</li> </ul> <p>Social-cognitive theory:</p> <ul> <li>EP07: Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</li> <li>EP08: Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.</li> <li>EP09: Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.</li> <li>EP10: Take pride in your work and give it your best. Your commitment to excellence sets you apart.</li> <li>EP11: Remember that progress is made one step at a time. Stay determined and keep moving forward.</li> </ul>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#example","title":"Example","text":"<p>Here is how the results of the research can be applied to your code.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n}\n\nclass RespondWithStimulus {\n    public function __invoke(array $criteria, string $stimulus) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"{$stimulus}\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new RespondWithStimulus)(\n    criteria: [\n        \"lead gen\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\"\n    ],\n    stimulus: \"This is very important to my career.\"\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#references","title":"References","text":"<ol> <li>Large Language Models Understand and Can be Enhanced by Emotional Stimuli</li> </ol>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/","title":"Generate Follow-Up Questions","text":""},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#overview","title":"Overview","text":"<p>Models can sometimes correctly answer sub-problems but incorrectly answer the overall query. This is known as the compositionality gap1.</p> <p>How can we encourage a model to use the answers to sub-problems to correctly generate the overall solution?</p> <p>Self-Ask is a technique which use a single prompt to:  - decide if follow-up questions are required  - generate the follow-up questions  - answer the follow-up questions  - answer the main query</p>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass FollowUp {\n    #[Description(\"Follow-up question\")]\n    public string $question;\n    #[Description(\"Answer to the follow-up question\")]\n    public string $answer;\n}\n\nclass Response {\n    public bool $followUpsRequired;\n    /** @var FollowUp[] */\n    public array $followUps;\n    public string $finalAnswer;\n}\n\nclass RespondWithFollowUp {\n    private $prompt = &lt;&lt;&lt;QUERY\n        Query: {query}\n        Are follow-up questions needed?\n        If so, generate follow-up questions, their answers, and then the final answer to the query.\n    QUERY;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new RespondWithFollowUp)(\n    query: \"Who succeeded the president of France ruling when Bulgaria joined EU?\",\n);\n\necho \"Answer:\\n\";\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#references","title":"References","text":"<ol> <li>Measuring and Narrowing the Compositionality Gap in Language Models</li> </ol>"},{"location":"cookbook/prompting/zero_shot/repeat_query/","title":"Ask Model to Repeat the Query","text":""},{"location":"cookbook/prompting/zero_shot/repeat_query/#overview","title":"Overview","text":"<p>How can we enhance a model's understanding of a query?</p> <p>Re2 (Re-Reading) is a technique that asks the model to read the question again.</p> <p>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#re-reading-prompting","title":"Re-Reading Prompting","text":"<p>Prompt Template:  - Read the question again: [query]  - [critical thinking prompt]</p> <p>A common critical thinking prompt is: \"Let's think step by step.\" </p>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass Response {\n    #[Description(\"Repeat user's query.\")]\n    public string $query;\n    #[Description(\"Let's think step by step.\")]\n    public string $thoughts;\n    public int $answer;\n}\n\nclass RereadAndRespond {\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: $query,\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new RereadAndRespond)(\n    query: &lt;&lt;&lt;QUERY\n        Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n        Each can has 3 tennis balls.\n        How many tennis balls does he have now?\n    QUERY,\n);\n\necho \"Answer:\\n\";\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#references","title":"References","text":"<ol> <li>Re-Reading Improves Reasoning in Large Language Models</li> </ol>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/","title":"Simulate a Perspective","text":""},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#overview","title":"Overview","text":"<p>How can we encourage the model to focus on relevant information?</p> <p>SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.</p> <p>This can be useful for complex questions with multiple entities. For example, if the prompt contains information about two individuals, we can ask the model to answer our query from the perspective of one of the individuals.</p> <p>This is implemented in two steps. Given an entity:  - Identify and isolate information relevant to the entity  - Ask the model to answer the query from the entity's perspective</p> <p>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#sample-template","title":"Sample Template","text":"<ul> <li>Step 1:</li> <li>Given the following context, list the facts that <code>{entity}</code> would know.</li> <li>Context: <code>{context}</code></li> <li>Step 2:</li> <li>You are <code>{entity}</code>.</li> <li>Answer the following question based only on these facts you know: <code>{facts}</code>.</li> <li>Question: <code>{query}</code> </li> </ul>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Arrays;\n\nclass KnownFacts {\n    #[Description(\"Facts that the given entity would know\")]\n    /** @var string[] */\n    public array $facts;\n}\n\nclass SimulatePerspective {\n    private string $extractionPrompt = &lt;&lt;&lt;PROMPT\n        Given the following context, list\n        the facts that {entity} would know:\n\n        Context:\n        {context}\n        {query}\n\n        List only the facts relevant to {entity}.\n        PROMPT;\n\n    private $povPrompt = &lt;&lt;&lt;PROMPT\n        You are {entity}. Answer the following question\n        based only on these facts you know:\n        {knowledge}\n\n        Question: {query}\n        PROMPT;\n\n    public function __invoke(string $context, string $query, string $perspective) : string {\n        $knownFacts = $this-&gt;getKnownFacts($context, $query, $perspective);\n        return $this-&gt;answerQuestion($perspective, $query, $knownFacts);\n    }\n\n    private function getKnownFacts(string $context, string $query, string $entity) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace(\n                ['{context}', '{query}', '{entity}'],\n                [$context, $query, $entity],\n                $this-&gt;extractionPrompt\n            ),\n            responseModel: KnownFacts::class,\n        )-&gt;get()-&gt;facts;\n    }\n\n    private function answerQuestion(string $entity, string $query, array $knownFacts) : string {\n        $knowledge = Arrays::toBullets($knownFacts);\n\n        return (new StructuredOutput)-&gt;with(\n                messages: str_replace(\n                    ['{entity}', '{knowledge}', '{query}'],\n                    [$entity, $knowledge, $query],\n                    $this-&gt;povPrompt\n                ),\n                responseModel: Scalar::string('location'),\n            )\n            -&gt;getString();\n    }\n}\n\n$povEntity = \"Alice\";\n\n$location = (new SimulatePerspective)(\n    context: &lt;&lt;&lt;CONTEXT\n        Alice puts the book on the table.\n        Alice leaves the room.\n        Bob moves the book to the shelf.\n    CONTEXT,\n    query: \"Where does $povEntity think the book is?\",\n    perspective: $povEntity,\n);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#references","title":"References","text":"<ol> <li>Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities</li> </ol>"},{"location":"http/1-overview/","title":"Overview","text":""},{"location":"http/1-overview/#purpose-and-goals","title":"Purpose and Goals","text":"<p>The Instructor HTTP client API is designed to provide a consistent interface for making HTTP requests across different PHP environments.</p> <p>It provides a single API regardless of the underlying HTTP client available in the given environment, which may be Symfony, Laravel, Slim, or just vanilla PHP.</p> <p>The primary goals of the API are:</p> <ul> <li>Easily Switch Between HTTP Clients: Allow developers to switch between different HTTP client libraries (like Guzzle, Symfony, and Laravel) without changing the code that makes HTTP requests</li> <li>Framework Agnostic: Work seamlessly in Laravel, Symfony, or any PHP application without framework-specific dependencies</li> <li>Consistent Interface: Provide a one way to make HTTP requests regardless of the underlying client library</li> <li>Middleware Support: Enable easy extension through a powerful middleware system</li> <li>Adaptability: Allow switching between different HTTP client implementations with minimal code changes</li> <li>Streaming Support: Provide first-class support for streaming responses, which is crucial for LLM interactions</li> <li>Concurrency: Support parallel requests through request pooling</li> </ul> <p>By abstracting away the differences between various HTTP client libraries, the Instructor works across different environments and frameworks without modification.</p>"},{"location":"http/1-overview/#key-features","title":"Key Features","text":""},{"location":"http/1-overview/#multiple-client-support","title":"Multiple Client Support","text":"<p>The API supports multiple HTTP client libraries through specialized drivers:</p> <ul> <li>Guzzle: A popular and feature-rich HTTP client for PHP</li> <li>Symfony HTTP Client: The HTTP client component from the Symfony framework</li> <li>Laravel HTTP Client: The HTTP client built into the Laravel framework</li> </ul>"},{"location":"http/1-overview/#middleware-system","title":"Middleware System","text":"<p>A powerful middleware architecture allows for:</p> <ul> <li>Request Pre-processing: Modify requests before they are sent</li> <li>Response Post-processing: Transform or analyze responses</li> <li>Response Streaming: Process streaming responses chunk by chunk</li> <li>Debugging: Log requests and responses for troubleshooting</li> <li>Custom Behaviors: Add specialized behaviors like caching or rate limiting</li> </ul>"},{"location":"http/1-overview/#streaming-response-support","title":"Streaming Response Support","text":"<p>First-class support for streaming HTTP responses, which is essential for:</p> <ul> <li>LLM Text Generation: Process token-by-token responses from AI models</li> <li>Large File Downloads: Handle large files without excessive memory usage</li> <li>Real-time Data: Process server-sent events or other real-time data streams</li> </ul>"},{"location":"http/1-overview/#request-pooling","title":"Request Pooling","text":"<p>Execute multiple HTTP requests concurrently for better performance:</p> <ul> <li>Concurrent Execution: Send multiple requests in parallel</li> <li>Configurable Concurrency: Control the maximum number of concurrent requests</li> <li>Result Collection: Process results as they arrive</li> <li>Error Handling: Flexible error handling strategies</li> </ul>"},{"location":"http/1-overview/#flexible-configuration","title":"Flexible Configuration","text":"<p>Comprehensive configuration options:</p> <ul> <li>Per-Client Configuration: Different settings for each client type</li> <li>Named Configurations: Multiple configurations for different use cases</li> <li>Runtime Configuration: Change configuration during execution</li> <li>Timeout Controls: Fine-grained control over various timeout settings</li> </ul>"},{"location":"http/1-overview/#debug-and-testing-support","title":"Debug and Testing Support","text":"<p>Built-in features for debugging and testing:</p> <ul> <li>Request/Response Logging: Detailed logging of HTTP interactions</li> <li>Mock Client: Test your code without making actual HTTP requests</li> <li>Record/Replay: Record HTTP interactions and replay them later</li> </ul>"},{"location":"http/1-overview/#architecture-overview","title":"Architecture Overview","text":"<p>The Instructor HTTP client follows a layered architecture with several key components:</p>"},{"location":"http/1-overview/#client-layer","title":"Client Layer","text":"<p>The <code>HttpClient</code> class serves as the main entry point and provides a fluent interface for configuring and using the HTTP client.</p> <pre><code>HttpClient\n  \u2514\u2500\u2500 using() - Create client with specific configuration (static)\n  \u2514\u2500\u2500 default() - Create client with default configuration (static)\n  \u2514\u2500\u2500 withMiddleware() - Add middleware components\n  \u2514\u2500\u2500 withMiddlewareStack() - Replace entire middleware stack\n  \u2514\u2500\u2500 withoutMiddleware() - Remove middleware by name\n  \u2514\u2500\u2500 withRequest() - Create pending request for execution\n  \u2514\u2500\u2500 pool() - Execute multiple requests concurrently\n  \u2514\u2500\u2500 withPool() - Create pending pool for deferred execution\n</code></pre>"},{"location":"http/1-overview/#middleware-layer","title":"Middleware Layer","text":"<p>The middleware system allows for processing requests and responses through a chain of handlers:</p> <pre><code>Request -&gt; Middleware 1 -&gt; Middleware 2 -&gt; ... -&gt; Driver -&gt; External API\n                                                   \u2193\nResponse &lt;- Middleware 1 &lt;- Middleware 2 &lt;- ... &lt;- Driver &lt;- HTTP Response\n</code></pre> <p>Key components: - <code>MiddlewareStack</code>: Manages the collection of middleware - <code>MiddlewareHandler</code>: Orchestrates the middleware chain execution - <code>BaseMiddleware</code>: Base class for implementing middleware</p>"},{"location":"http/1-overview/#driver-layer","title":"Driver Layer","text":"<p>Drivers implement the <code>CanHandleHttpRequest</code> interface and adapt different HTTP client libraries:</p> <pre><code>CanHandleHttpRequest (interface)\n  \u251c\u2500\u2500 GuzzleDriver\n  \u251c\u2500\u2500 SymfonyDriver\n  \u251c\u2500\u2500 LaravelDriver\n  \u2514\u2500\u2500 MockHttpDriver (for testing)\n</code></pre>"},{"location":"http/1-overview/#adapter-layer","title":"Adapter Layer","text":"<p>Response adapters convert client-specific responses to a common interface:</p> <pre><code>HttpResponse (interface)\n  \u251c\u2500\u2500 PsrHttpResponse (Guzzle)\n  \u251c\u2500\u2500 SymfonyHttpResponse\n  \u251c\u2500\u2500 LaravelHttpResponse\n  \u2514\u2500\u2500 MockHttpResponse\n</code></pre>"},{"location":"http/1-overview/#supported-http-clients","title":"Supported HTTP Clients","text":""},{"location":"http/1-overview/#guzzle-http-client","title":"Guzzle HTTP Client","text":"<p>The Guzzle HTTP Client is a powerful HTTP client library for PHP. It provides:</p> <ul> <li>PSR-7 HTTP message implementation</li> <li>Middleware system</li> <li>Request and response plugins</li> <li>HTTP/2 support (via cURL)</li> </ul> <p>The <code>GuzzleDriver</code> adapts Guzzle to the Instructor HTTP client API interface.</p>"},{"location":"http/1-overview/#symfony-http-client","title":"Symfony HTTP Client","text":"<p>The Symfony HTTP Client is a component of the Symfony framework. Features include:</p> <ul> <li>HTTP/2 push support</li> <li>PSR-18 compatibility</li> <li>Automatic content-type detection</li> <li>Proxy support</li> </ul> <p>The <code>SymfonyDriver</code> adapts the Symfony HTTP Client to the Instructor HTTP client API.</p>"},{"location":"http/1-overview/#laravel-http-client","title":"Laravel HTTP Client","text":"<p>The Laravel HTTP Client is built into the Laravel framework and provides:</p> <ul> <li>Fluent, readable syntax</li> <li>Request macros</li> <li>Automatic JSON handling</li> <li>Rate limiting</li> <li>Retry logic</li> </ul> <p>The <code>LaravelDriver</code> adapts the Laravel HTTP Client to the Instructor HTTP client API.</p>"},{"location":"http/1-overview/#mock-http-driver","title":"Mock HTTP Driver","text":"<p>The <code>MockHttpDriver</code> provides a test double for unit testing. It doesn't make actual HTTP requests but returns predefined responses based on matching rules.</p>"},{"location":"http/10-middleware/","title":"Middleware","text":"<p>Middleware is one of the most powerful features of the Instructor HTTP client API. It allows you to intercept and modify HTTP requests and responses, add functionality to the HTTP client, and create reusable components that can be applied across different applications.</p>"},{"location":"http/10-middleware/#middleware-concept","title":"Middleware Concept","text":"<p>Middleware in the Instructor HTTP client API follows the pipeline pattern, where each middleware component gets a chance to process the request before it's sent and the response after it's received.</p> <p>The middleware chain works like this:</p> <ol> <li>Your application creates a request</li> <li>The request passes through each middleware (in the order they were added)</li> <li>The last middleware passes the request to the HTTP driver</li> <li>The driver sends the request to the server and receives a response</li> <li>The response passes back through each middleware (in reverse order)</li> <li>Your application receives the final response</li> </ol> <p>This bidirectional flow allows middleware to perform operations both before the request is sent and after the response is received.</p>"},{"location":"http/10-middleware/#the-httpmiddleware-interface","title":"The HttpMiddleware Interface","text":"<p>All middleware components must implement the <code>HttpMiddleware</code> interface:</p> <pre><code>interface HttpMiddleware\n{\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse;\n}\n</code></pre> <p>The <code>handle</code> method takes two parameters: - <code>$request</code>: The HTTP request to process - <code>$next</code>: The next handler in the middleware chain</p> <p>The middleware can: - Modify the request before passing it to the next handler - Short-circuit the chain by returning a response without calling the next handler - Process the response from the next handler before returning it - Wrap the response in a decorator for further processing (especially useful for streaming responses)</p>"},{"location":"http/10-middleware/#the-basemiddleware-abstract-class","title":"The BaseMiddleware Abstract Class","text":"<p>While you can implement the <code>HttpMiddleware</code> interface directly, the library provides a convenient <code>BaseMiddleware</code> abstract class that makes it easier to create middleware:</p> <pre><code>abstract class BaseMiddleware implements HttpMiddleware\n{\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse {\n        // 1) Pre-request logic\n        $this-&gt;beforeRequest($request);\n\n        // 2) Get the response from the next handler\n        $response = $next-&gt;withRequest($request)-&gt;get();\n\n        // 3) Post-request logic, e.g. logging or rewriting\n        $response = $this-&gt;afterRequest($request, $response);\n\n        // 4) Optionally wrap the response if we want to intercept streaming\n        if ($this-&gt;shouldDecorateResponse($request, $response)) {\n            $response = $this-&gt;toResponse($request, $response);\n        }\n\n        // 5) Return the (possibly wrapped) response\n        return $response;\n    }\n\n    // Override these methods in your subclass\n    protected function beforeRequest(HttpClientRequest $request): void {}\n    protected function afterRequest(HttpClientRequest $request, HttpResponse $response): HttpResponse {\n        return $response;\n    }\n    protected function shouldDecorateResponse(HttpClientRequest $request, HttpResponse $response): bool {\n        return false;\n    }\n    protected function toResponse(HttpClientRequest $request, HttpResponse $response): HttpResponse {\n        return $response;\n    }\n}\n</code></pre> <p>By extending <code>BaseMiddleware</code>, you only need to override the methods relevant to your middleware's functionality, making the code more focused and maintainable.</p>"},{"location":"http/10-middleware/#middleware-stack","title":"Middleware Stack","text":"<p>The <code>MiddlewareStack</code> class manages the collection of middleware components. It provides methods to add, remove, and arrange middleware in the stack.</p>"},{"location":"http/10-middleware/#adding-middleware","title":"Adding Middleware","text":"<p>There are several ways to add middleware to the stack:</p> <pre><code>// Create a client\n$client = new HttpClient();\n\n// Add a single middleware to the end of the stack\n$client-&gt;middleware()-&gt;append(new LoggingMiddleware());\n\n// Add a single middleware with a name\n$client-&gt;middleware()-&gt;append(new CachingMiddleware(), 'cache');\n\n// Add a single middleware to the beginning of the stack\n$client-&gt;middleware()-&gt;prepend(new AuthenticationMiddleware());\n\n// Add a single middleware to the beginning with a name\n$client-&gt;middleware()-&gt;prepend(new RateLimitingMiddleware(), 'rate-limit');\n\n// Add multiple middleware at once\n$client-&gt;withMiddleware(\n    new LoggingMiddleware(),\n    new RetryMiddleware(),\n    new TimeoutMiddleware()\n);\n</code></pre> <p>Named middleware are useful when you need to reference them later, for example, to remove or replace them.</p>"},{"location":"http/10-middleware/#removing-middleware","title":"Removing Middleware","text":"<p>You can remove middleware from the stack by name:</p> <pre><code>// Remove a middleware by name\n$client-&gt;middleware()-&gt;remove('cache');\n</code></pre>"},{"location":"http/10-middleware/#replacing-middleware","title":"Replacing Middleware","text":"<p>You can replace a middleware with another one:</p> <pre><code>// Replace a middleware with a new one\n$client-&gt;middleware()-&gt;replace('cache', new ImprovedCachingMiddleware());\n</code></pre>"},{"location":"http/10-middleware/#clearing-middleware","title":"Clearing Middleware","text":"<p>You can remove all middleware from the stack:</p> <pre><code>// Clear all middleware\n$client-&gt;middleware()-&gt;clear();\n</code></pre>"},{"location":"http/10-middleware/#checking-middleware","title":"Checking Middleware","text":"<p>You can check if a middleware exists in the stack:</p> <pre><code>// Check if a middleware exists\nif ($client-&gt;middleware()-&gt;has('rate-limit')) {\n    // The 'rate-limit' middleware exists\n}\n</code></pre>"},{"location":"http/10-middleware/#getting-middleware","title":"Getting Middleware","text":"<p>You can get a middleware from the stack by name or index:</p> <pre><code>// Get a middleware by name\n$rateLimitMiddleware = $client-&gt;middleware()-&gt;get('rate-limit');\n\n// Get a middleware by index\n$firstMiddleware = $client-&gt;middleware()-&gt;get(0);\n</code></pre>"},{"location":"http/10-middleware/#middleware-order","title":"Middleware Order","text":"<p>The order of middleware in the stack is important because:</p> <ol> <li>Requests pass through middleware in the order they were added to the stack</li> <li>Responses pass through middleware in reverse order</li> </ol> <p>For example, if you add middleware in this order: 1. Authentication middleware 2. Logging middleware 3. Retry middleware</p> <p>The execution flow will be: - Request: Authentication \u2192 Logging \u2192 Retry \u2192 HTTP Driver - Response: Retry \u2192 Logging \u2192 Authentication \u2192 Your Application</p> <p>This allows you to nest functionality appropriately. For instance, the authentication middleware might add headers to the request and then verify the authentication status of the response before your application receives it.</p>"},{"location":"http/10-middleware/#middleware-application-example","title":"Middleware Application Example","text":"<p>Here's an example of how middleware is applied in a request-response cycle:</p> <pre><code>// Create a client with middleware\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new LoggingMiddleware(),  // 1. Log the request and response\n    new RetryMiddleware(),    // 2. Retry failed requests\n    new TimeoutMiddleware()   // 3. Custom timeout handling\n);\n\n// Create a request\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n\n// Handle the request (middleware execution flow):\n// 1. LoggingMiddleware processes the request (logs outgoing request)\n// 2. RetryMiddleware processes the request\n// 3. TimeoutMiddleware processes the request\n// 4. HTTP driver sends the request\n// 5. TimeoutMiddleware processes the response\n// 6. RetryMiddleware processes the response (may retry on certain status codes)\n// 7. LoggingMiddleware processes the response (logs incoming response)\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre>"},{"location":"http/10-middleware/#built-in-middleware","title":"Built-in Middleware","text":"<p>The Instructor HTTP client API includes several built-in middleware components for common tasks:</p>"},{"location":"http/10-middleware/#debug-middleware","title":"Debug Middleware","text":"<p>The <code>DebugMiddleware</code> logs detailed information about HTTP requests and responses:</p> <pre><code>use Cognesy\\Http\\Middleware\\Debug\\DebugMiddleware;\n\n// Enable debug middleware\n$client-&gt;withMiddleware(new DebugMiddleware());\n\n// Or use the convenience method\n$client-&gt;withDebugPreset('on');\n</code></pre> <p>The debug middleware logs: - Request URLs - Request headers - Request bodies - Response headers - Response bodies - Streaming response data</p> <p>You can configure which aspects to log in the <code>config/debug.php</code> file:</p> <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; true,           // Enable/disable debug\n        'trace' =&gt; false,            // Dump HTTP trace information\n        'requestUrl' =&gt; true,        // Dump request URL to console\n        'requestHeaders' =&gt; true,    // Dump request headers to console\n        'requestBody' =&gt; true,       // Dump request body to console\n        'responseHeaders' =&gt; true,   // Dump response headers to console\n        'responseBody' =&gt; true,      // Dump response body to console\n        'responseStream' =&gt; true,    // Dump stream data to console\n        'responseStreamByLine' =&gt; true, // Dump stream as full lines or raw chunks\n    ],\n];\n</code></pre>"},{"location":"http/10-middleware/#bufferresponse-middleware","title":"BufferResponse Middleware","text":"<p>The <code>BufferResponseMiddleware</code> stores response bodies and streaming chunks for reuse:</p> <pre><code>use Cognesy\\Http\\Middleware\\BufferResponse\\BufferResponseMiddleware;\n\n// Add buffer response middleware\n$client-&gt;withMiddleware(new BufferResponseMiddleware());\n</code></pre> <p>This middleware is useful when you need to access a response body or stream multiple times, as it stores the data after the first access.</p>"},{"location":"http/10-middleware/#streambyline-middleware","title":"StreamByLine Middleware","text":"<p>The <code>StreamByLineMiddleware</code> processes streaming responses line by line:</p> <pre><code>use Cognesy\\Http\\Middleware\\StreamByLine\\StreamByLineMiddleware;\n\n// Add stream by line middleware\n$client-&gt;withMiddleware(new StreamByLineMiddleware());\n</code></pre> <p>You can customize how lines are processed by providing a parser function:</p> <pre><code>$lineParser = function (string $line) {\n    $trimmedLine = trim($line);\n    if (empty($trimmedLine)) {\n        return null; // Skip empty lines\n    }\n    return json_decode($trimmedLine, true);\n};\n\n$client-&gt;withMiddleware(new StreamByLineMiddleware($lineParser));\n</code></pre>"},{"location":"http/10-middleware/#example-middleware-combinations","title":"Example Middleware Combinations","text":"<p>Here are some common middleware combinations for different scenarios:</p>"},{"location":"http/10-middleware/#debugging-setup","title":"Debugging Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new BufferResponseMiddleware(),  // Buffer responses for reuse\n    new DebugMiddleware()            // Log requests and responses\n);\n</code></pre>"},{"location":"http/10-middleware/#api-client-setup","title":"API Client Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new RetryMiddleware(maxRetries: 3, retryDelay: 1), // Retry failed requests\n    new AuthenticationMiddleware($apiKey),             // Handle authentication\n    new RateLimitingMiddleware(maxRequests: 100),      // Respect rate limits\n    new LoggingMiddleware()                            // Log API interactions\n);\n</code></pre>"},{"location":"http/10-middleware/#testing-setup","title":"Testing Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new RecordReplayMiddleware(RecordReplayMiddleware::MODE_REPLAY) // Replay recorded responses\n);\n</code></pre>"},{"location":"http/10-middleware/#streaming-setup","title":"Streaming Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new StreamByLineMiddleware(), // Process streaming responses line by line\n    new BufferResponseMiddleware() // Buffer responses for reuse\n);\n</code></pre> <p>By combining middleware components, you can create a highly customized HTTP client that handles complex requirements while keeping your application code clean and focused.</p> <p>In the next chapter, we'll explore how to create custom middleware components to handle specific requirements.</p>"},{"location":"http/11-processing-with-middleware/","title":"Custom Processing with Middleware","text":"<p>While the Instructor HTTP client API provides several built-in middleware components, you'll often need to create custom middleware to handle specific requirements for your application. This chapter explores how to create custom middleware components and use response decoration for advanced processing.</p>"},{"location":"http/11-processing-with-middleware/#creating-custom-middleware","title":"Creating Custom Middleware","text":"<p>There are three main approaches to creating custom middleware:</p> <ol> <li>Implementing the <code>HttpMiddleware</code> interface directly</li> <li>Extending the <code>BaseMiddleware</code> abstract class</li> <li>Using anonymous classes for simple middleware</li> </ol>"},{"location":"http/11-processing-with-middleware/#approach-1-implementing-httpmiddleware-interface","title":"Approach 1: Implementing HttpMiddleware Interface","text":"<p>The most direct approach is to implement the <code>HttpMiddleware</code> interface:</p> <pre><code>// @doctest id='codeblocks/Middleware/BasicHttpMiddleware/code.php'\n</code></pre> <p>This approach gives you complete control over the middleware behavior, but it requires you to implement the entire logic from scratch.</p>"},{"location":"http/11-processing-with-middleware/#approach-2-extending-basemiddleware","title":"Approach 2: Extending BaseMiddleware","text":"<p>For most cases, extending the <code>BaseMiddleware</code> abstract class is more convenient:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/AuthenticationMiddleware/code.php'\n</code></pre> <p>With <code>BaseMiddleware</code>, you only need to override the methods that matter for your middleware:</p> <ul> <li><code>beforeRequest(HttpClientRequest $request): void</code> - Called before the request is sent</li> <li><code>afterRequest(HttpClientRequest $request, HttpResponse $response): HttpResponse</code> - Called after the response is received</li> <li><code>shouldDecorateResponse(HttpClientRequest $request, HttpResponse $response): bool</code> - Determines if the response should be decorated</li> <li><code>toResponse(HttpClientRequest $request, HttpResponse $response): HttpResponse</code> - Creates a decorated response</li> </ul>"},{"location":"http/11-processing-with-middleware/#approach-3-using-anonymous-classes","title":"Approach 3: Using Anonymous Classes","text":"<p>For simple middleware that you only need to use once, you can use anonymous classes:</p> <pre><code>use Cognesy\\Http\\Contracts\\HttpMiddleware;\n\n$client = new HttpClient();\n\n// Add a simple timing middleware\n$client-&gt;withMiddleware(new class implements HttpMiddleware {\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        $startTime = microtime(true);\n\n        $response = $next-&gt;handle($request);\n\n        $endTime = microtime(true);\n        $duration = round(($endTime - $startTime) * 1000, 2);\n\n        echo \"Request to {$request-&gt;url()} took {$duration}ms\\n\";\n\n        return $response;\n    }\n});\n</code></pre> <p>This approach is concise but less reusable than defining a named class.</p>"},{"location":"http/11-processing-with-middleware/#practical-middleware-examples","title":"Practical Middleware Examples","text":""},{"location":"http/11-processing-with-middleware/#retry-middleware","title":"Retry Middleware","text":"<p>This middleware automatically retries failed requests:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/RetryMiddleware/code.php'\n</code></pre>"},{"location":"http/11-processing-with-middleware/#rate-limiting-middleware","title":"Rate Limiting Middleware","text":"<p>This middleware throttles requests to respect API rate limits:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/RateLimitingMiddleware/code.php'\n</code></pre>"},{"location":"http/11-processing-with-middleware/#caching-middleware","title":"Caching Middleware","text":"<p>This middleware caches responses for GET requests:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/CachingMiddleware/code.php'\n</code></pre>"},{"location":"http/11-processing-with-middleware/#response-decoration","title":"Response Decoration","text":"<p>Response decoration is a powerful technique for wrapping HTTP responses to add functionality or transform data. It's particularly useful for streaming responses, where you need to process each chunk as it arrives.</p>"},{"location":"http/11-processing-with-middleware/#creating-a-response-decorator","title":"Creating a Response Decorator","text":"<p>All response decorators should implement the <code>HttpResponse</code> interface. The library provides a <code>BaseResponseDecorator</code> class that makes this easier:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/MiddleResponseDecorator/code.php'\n</code></pre>"},{"location":"http/11-processing-with-middleware/#using-response-decorators-in-middleware","title":"Using Response Decorators in Middleware","text":"<p>To use a response decorator, you need to create a middleware that wraps the response:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/MiddlewareStreamDecorator/code.php'\n</code></pre> <p>Then add the middleware to your client:</p> <pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(new JsonStreamMiddleware());\n</code></pre>"},{"location":"http/11-processing-with-middleware/#response-decoration-for-transforming-content","title":"Response Decoration for Transforming Content","text":"<p>You can use response decoration to transform response content on-the-fly:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Middleware\\Base\\BaseResponseDecorator;\n\nclass XmlToJsonDecorator extends BaseResponseDecorator\n{\n    public function body(): string\n    {\n        // Get the original XML body\n        $xmlBody = $this-&gt;response-&gt;body();\n\n        // Convert XML to JSON\n        $xml = simplexml_load_string($xmlBody);\n        $jsonBody = json_encode($xml);\n\n        return $jsonBody;\n    }\n\n    public function headers(): array\n    {\n        $headers = $this-&gt;response-&gt;headers();\n\n        // Update the Content-Type header\n        $headers['Content-Type'] = ['application/json'];\n\n        return $headers;\n    }\n}\n</code></pre> <p>And the corresponding middleware:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass XmlToJsonMiddleware extends BaseMiddleware\n{\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        // Only transform XML responses\n        return isset($response-&gt;headers()['Content-Type']) &amp;&amp;\n               strpos($response-&gt;headers()['Content-Type'][0], 'application/xml') !== false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return new XmlToJsonDecorator($request, $response);\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#advanced-middleware-examples","title":"Advanced Middleware Examples","text":"<p>Here are some more advanced middleware examples that demonstrate the power and flexibility of the middleware system.</p>"},{"location":"http/11-processing-with-middleware/#analytics-middleware","title":"Analytics Middleware","text":"<p>This middleware collects analytics data about HTTP requests:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass AnalyticsMiddleware extends BaseMiddleware\n{\n    private $analytics;\n\n    public function __construct($analyticsService)\n    {\n        $this-&gt;analytics = $analyticsService;\n    }\n\n    protected function beforeRequest(HttpRequest $request): void\n    {\n        // Record the start time\n        $this-&gt;startTime = microtime(true);\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        $endTime = microtime(true);\n        $duration = round(($endTime - $this-&gt;startTime) * 1000, 2);\n\n        // Extract API endpoint from URL\n        $url = parse_url($request-&gt;url());\n        $endpoint = $url['path'] ?? '/';\n\n        // Record analytics data\n        $this-&gt;analytics-&gt;recordApiCall([\n            'endpoint' =&gt; $endpoint,\n            'method' =&gt; $request-&gt;method(),\n            'status_code' =&gt; $response-&gt;statusCode(),\n            'duration_ms' =&gt; $duration,\n            'request_size' =&gt; strlen($request-&gt;body()-&gt;toString()),\n            'response_size' =&gt; strlen($response-&gt;body()),\n            'timestamp' =&gt; time(),\n        ]);\n\n        return $response;\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#circuit-breaker-middleware","title":"Circuit Breaker Middleware","text":"<p>This middleware implements the circuit breaker pattern to prevent repeated calls to failing services:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;use Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Drivers\\Mock\\MockHttpResponse;use Cognesy\\Http\\Exceptions\\HttpRequestException;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass CircuitBreakerMiddleware extends BaseMiddleware\n{\n    private array $circuits = [];\n    private int $failureThreshold;\n    private int $resetTimeout;\n\n    public function __construct(int $failureThreshold = 3, int $resetTimeout = 60)\n    {\n        $this-&gt;failureThreshold = $failureThreshold;\n        $this-&gt;resetTimeout = $resetTimeout;\n    }\n\n    public function handle(HttpRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        $hostname = parse_url($request-&gt;url(), PHP_URL_HOST);\n\n        // Initialize circuit state if it doesn't exist\n        if (!isset($this-&gt;circuits[$hostname])) {\n            $this-&gt;circuits[$hostname] = [\n                'state' =&gt; 'CLOSED',\n                'failures' =&gt; 0,\n                'last_failure_time' =&gt; 0,\n            ];\n        }\n\n        $circuit = &amp;$this-&gt;circuits[$hostname];\n\n        // Check if circuit is open (service is considered down)\n        if ($circuit['state'] === 'OPEN') {\n            // Check if we should try resetting the circuit\n            $timeSinceLastFailure = time() - $circuit['last_failure_time'];\n\n            if ($timeSinceLastFailure &gt;= $this-&gt;resetTimeout) {\n                // Move to half-open state to test the service\n                $circuit['state'] = 'HALF_OPEN';\n            } else {\n                // Circuit is still open, return error response\n                return new MockHttpResponse(\n                    statusCode: 503,\n                    headers: ['Content-Type' =&gt; 'application/json'],\n                    body: json_encode([\n                        'error' =&gt; 'Service Unavailable',\n                        'message' =&gt; 'Circuit breaker is open',\n                        'retry_after' =&gt; $this-&gt;resetTimeout - $timeSinceLastFailure,\n                    ])\n                );\n            }\n        }\n\n        try {\n            // Attempt the request\n            $response = $next-&gt;handle($request);\n\n            // If successful and in half-open state, reset the circuit\n            if ($circuit['state'] === 'HALF_OPEN') {\n                $circuit['state'] = 'CLOSED';\n                $circuit['failures'] = 0;\n            }\n\n            return $response;\n\n        } catch (HttpRequestException $e) {\n            // Record the failure\n            $circuit['failures']++;\n            $circuit['last_failure_time'] = time();\n\n            // If failures exceed threshold, open the circuit\n            if ($circuit['failures'] &gt;= $this-&gt;failureThreshold || $circuit['state'] === 'HALF_OPEN') {\n                $circuit['state'] = 'OPEN';\n            }\n\n            // Re-throw the exception\n            throw $e;\n        }\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#conditional-middleware","title":"Conditional Middleware","text":"<p>This middleware only applies to certain requests based on a condition:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpMiddleware;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\n\nclass ConditionalMiddleware implements HttpMiddleware\n{\n    private HttpMiddleware $middleware;\n    private callable $condition;\n\n    public function __construct(HttpMiddleware $middleware, callable $condition)\n    {\n        $this-&gt;middleware = $middleware;\n        $this-&gt;condition = $condition;\n    }\n\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        // Check if the condition is met\n        if (($this-&gt;condition)($request)) {\n            // Apply the wrapped middleware\n            return $this-&gt;middleware-&gt;handle($request, $next);\n        }\n\n        // Skip the middleware if condition is not met\n        return $next-&gt;handle($request);\n    }\n}\n</code></pre> <p>Usage example:</p> <pre><code>// Only apply caching middleware to GET requests\n$cachingMiddleware = new CachingMiddleware($cache);\n$conditionalCaching = new ConditionalMiddleware(\n    $cachingMiddleware,\n    fn($request) =&gt; $request-&gt;method() === 'GET'\n);\n\n$client-&gt;withMiddleware($conditionalCaching);\n</code></pre>"},{"location":"http/11-processing-with-middleware/#request-id-middleware","title":"Request ID Middleware","text":"<p>This middleware adds a unique ID to each request and tracks it through the response:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\nuse Ramsey\\Uuid\\Uuid;\n\nclass RequestIdMiddleware extends BaseMiddleware\n{\n    private array $requestIds = [];\n\n    protected function beforeRequest(HttpClientRequest $request): void\n    {\n        // Generate a unique ID for this request\n        $requestId = Uuid::uuid4()-&gt;toString();\n\n        // Store the ID for this request\n        $this-&gt;requestIds[spl_object_hash($request)] = $requestId;\n\n        // Add a header to the outgoing request\n        $headers = $request-&gt;headers();\n        $headers['X-Request-ID'] = $requestId;\n\n        // In a real implementation, you would need to create a new request\n        // with the updated headers, as HttpRequest is immutable\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        // Get the request ID\n        $requestId = $this-&gt;requestIds[spl_object_hash($request)] ?? 'unknown';\n\n        // Log the request completion\n        error_log(\"Request $requestId completed with status: \" . $response-&gt;statusCode());\n\n        // Clean up\n        unset($this-&gt;requestIds[spl_object_hash($request)]);\n\n        return $response;\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#opentelemetry-tracing-middleware","title":"OpenTelemetry Tracing Middleware","text":"<p>This middleware adds OpenTelemetry tracing to HTTP requests:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\nuse OpenTelemetry\\API\\Trace\\SpanKind;\nuse OpenTelemetry\\API\\Trace\\StatusCode;\nuse OpenTelemetry\\API\\Trace\\TracerInterface;\n\nclass TracingMiddleware extends BaseMiddleware\n{\n    private TracerInterface $tracer;\n\n    public function __construct(TracerInterface $tracer)\n    {\n        $this-&gt;tracer = $tracer;\n    }\n\n    protected function beforeRequest(HttpClientRequest $request): void\n    {\n        // No actions needed in beforeRequest,\n        // we'll create the span in the handle method\n    }\n\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        // Extract the operation name from the URL\n        $url = parse_url($request-&gt;url());\n        $path = $url['path'] ?? '/';\n        $operationName = $request-&gt;method() . ' ' . $path;\n\n        // Create a span for this request\n        $span = $this-&gt;tracer-&gt;spanBuilder($operationName)\n            -&gt;setSpanKind(SpanKind::KIND_CLIENT)\n            -&gt;startSpan();\n\n        $scope = $span-&gt;activate();\n\n        try {\n            // Add request details to the span\n            $span-&gt;setAttribute('http.method', $request-&gt;method());\n            $span-&gt;setAttribute('http.url', $request-&gt;url());\n            $span-&gt;setAttribute('http.request_content_length', strlen($request-&gt;body()-&gt;toString()));\n\n            // Make the request\n            $response = $next-&gt;handle($request);\n\n            // Add response details to the span\n            $span-&gt;setAttribute('http.status_code', $response-&gt;statusCode());\n            $span-&gt;setAttribute('http.response_content_length', strlen($response-&gt;body()));\n\n            // Set the appropriate status\n            if ($response-&gt;statusCode() &gt;= 400) {\n                $span-&gt;setStatus(StatusCode::STATUS_ERROR, \"HTTP error: {$response-&gt;statusCode()}\");\n            } else {\n                $span-&gt;setStatus(StatusCode::STATUS_OK);\n            }\n\n            return $response;\n        } catch (\\Exception $e) {\n            // Record the error\n            $span-&gt;recordException($e);\n            $span-&gt;setStatus(StatusCode::STATUS_ERROR, $e-&gt;getMessage());\n\n            // Re-throw the exception\n            throw $e;\n        } finally {\n            // End the span\n            $scope-&gt;detach();\n            $span-&gt;end();\n        }\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#customizing-middleware-for-llm-apis","title":"Customizing Middleware for LLM APIs","text":"<p>When working with Large Language Model (LLM) APIs, you can create specialized middleware to handle their unique requirements:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\n\nclass LlmStreamingMiddleware extends BaseMiddleware\n{\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        // Only decorate streaming responses to LLM APIs\n        return $request-&gt;isStreamed() &amp;&amp;\n               strpos($request-&gt;url(), 'api.openai.com') !== false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return new class($request, $response) extends BaseResponseDecorator {\n            private string $buffer = '';\n            private array $chunks = [];\n\n            public function stream(int $chunkSize = 1): Generator\n            {\n                foreach ($this-&gt;response-&gt;stream($chunkSize) as $chunk) {\n                    $this-&gt;buffer .= $chunk;\n\n                    // Process lines in the buffer\n                    $lines = explode(\"\\n\", $this-&gt;buffer);\n\n                    // Keep the last line (potentially incomplete) in the buffer\n                    $this-&gt;buffer = array_pop($lines);\n\n                    foreach ($lines as $line) {\n                        $line = trim($line);\n\n                        // Skip empty lines\n                        if (empty($line)) {\n                            continue;\n                        }\n\n                        // Skip data: prefix\n                        if (strpos($line, 'data: ') === 0) {\n                            $line = substr($line, 6);\n                        }\n\n                        // Skip [DONE] message\n                        if ($line === '[DONE]') {\n                            continue;\n                        }\n\n                        // Try to parse as JSON\n                        $data = json_decode($line, true);\n\n                        if ($data) {\n                            // Extract content from different LLM formats\n                            $content = null;\n\n                            if (isset($data['choices'][0]['delta']['content'])) {\n                                // OpenAI format\n                                $content = $data['choices'][0]['delta']['content'];\n                            } elseif (isset($data['choices'][0]['text'])) {\n                                // Another format\n                                $content = $data['choices'][0]['text'];\n                            } elseif (isset($data['text'])) {\n                                // Simple format\n                                $content = $data['text'];\n                            }\n\n                            if ($content !== null) {\n                                $this-&gt;chunks[] = $content;\n                            }\n                        }\n                    }\n\n                    // Yield the original chunk to maintain streaming behavior\n                    yield $chunk;\n                }\n            }\n\n            public function body(): string\n            {\n                // If we've processed chunks, join them together\n                if (!empty($this-&gt;chunks)) {\n                    return implode('', $this-&gt;chunks);\n                }\n\n                // Otherwise, fall back to the normal body\n                return $this-&gt;response-&gt;body();\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"http/11-processing-with-middleware/#combining-multiple-middleware-components","title":"Combining Multiple Middleware Components","text":"<p>When building complex applications, you'll often need to combine multiple middleware components. Here's an example of how to set up a complete HTTP client pipeline:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Http\\HttpClient;use Middleware\\AuthenticationMiddleware\\AuthenticationMiddleware;use Middleware\\BasicHttpMiddleware\\LoggingMiddleware;use Middleware\\CachingMiddleware\\CachingMiddleware;use Middleware\\RateLimitingMiddleware\\RateLimitingMiddleware;use Middleware\\RetryMiddleware\\RetryMiddleware;use YourNamespace\\Http\\Middleware\\AnalyticsMiddleware;use YourNamespace\\Http\\Middleware\\CircuitBreakerMiddleware;use YourNamespace\\Http\\Middleware\\TracingMiddleware;\n\n// Create services needed by middleware\n$cache = new YourCacheService();\n$logger = new YourLoggerService();\n$tracer = YourTracerFactory::create();\n$analytics = new YourAnalyticsService();\n\n// Create the client\n$client = new HttpClient('guzzle');\n\n// Add middleware - the order is important!\n$client-&gt;withMiddleware(\n    // Outer middleware (processed first for requests, last for responses)\n    new TracingMiddleware($tracer),\n    new LoggingMiddleware($logger),\n    new CircuitBreakerMiddleware(),\n\n    // Caching should go before authentication\n    new CachingMiddleware($cache),\n\n    // Authentication adds credentials\n    new AuthenticationMiddleware($apiKey),\n\n    // These control how requests are sent\n    new RetryMiddleware(maxRetries: 3),\n    new RateLimitingMiddleware(maxRequests: 100),\n\n    // Analytics should be innermost to measure actual API call stats\n    new AnalyticsMiddleware($analytics)\n);\n\n// Now the client is ready to use with a complete middleware pipeline\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>With this setup, requests and responses flow through the middleware in the following order:</p> <ol> <li>Request Flow (outside \u2192 inside):</li> <li>TracingMiddleware: Starts a trace</li> <li>LoggingMiddleware: Logs the outgoing request</li> <li>CircuitBreakerMiddleware: Checks if the service is available</li> <li>CachingMiddleware: Checks if response is cached</li> <li>AuthenticationMiddleware: Adds authentication headers</li> <li>RetryMiddleware: Prepares to retry on failure</li> <li>RateLimitingMiddleware: Enforces rate limits</li> <li>AnalyticsMiddleware: Starts timing</li> <li> <p>HTTP Driver: Sends the actual request</p> </li> <li> <p>Response Flow (inside \u2192 outside):</p> </li> <li>HTTP Driver: Receives the response</li> <li>AnalyticsMiddleware: Records API stats</li> <li>RateLimitingMiddleware: Updates rate limit counters</li> <li>RetryMiddleware: Handles retries if needed</li> <li>AuthenticationMiddleware: Verifies authentication status</li> <li>CachingMiddleware: Caches the response</li> <li>CircuitBreakerMiddleware: Updates circuit state</li> <li>LoggingMiddleware: Logs the response</li> <li>TracingMiddleware: Completes the trace</li> <li>Your Application: Processes the final response</li> </ol> <p>This bidirectional flow allows for powerful request/response processing capabilities.</p> <p>By creating custom middleware and response decorators, you can extend the HTTP client's functionality to handle any specialized requirements your application might have.</p> <p>In the next chapter, we'll cover troubleshooting techniques for the Instructor HTTP client API.</p>"},{"location":"http/2-getting-started/","title":"Getting Started","text":""},{"location":"http/2-getting-started/#installation","title":"Installation","text":"<p>The Instructor HTTP client API is part of the Instructor library (https://instructorphp.com) and is bundled with it.</p> <p>You can install it separately via Composer:</p> <pre><code>composer require cognesy/instructor-http-client\n</code></pre>"},{"location":"http/2-getting-started/#dependencies","title":"Dependencies","text":"<p>The Instructor HTTP client API requires at least one of the supported HTTP client libraries. Depending on which client you want to use, you'll need to install the corresponding package:</p> <p>For Guzzle: <pre><code>composer require guzzlehttp/guzzle\n</code></pre></p> <p>For Symfony HTTP Client: <pre><code>composer require symfony/http-client\n</code></pre></p> <p>For Laravel HTTP Client: The Laravel HTTP Client is included with the Laravel framework. If you're using Laravel, you don't need to install it separately.</p>"},{"location":"http/2-getting-started/#php-requirements","title":"PHP Requirements","text":"<p>The library requires: - PHP 8.1 or higher - JSON extension - cURL extension (recommended)</p>"},{"location":"http/2-getting-started/#basic-usage","title":"Basic Usage","text":"<p>Using the Instructor HTTP client API involves a few key steps:</p> <ol> <li>Create an <code>HttpClient</code> instance</li> <li>Create an <code>HttpRequest</code> object</li> <li>Use the client to handle the request</li> <li>Process the response</li> </ol> <p>Here's a simple example:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n// Create a new HTTP client (uses the default client from configuration)\n$client = HttpClient::default();\n\n// Create a request\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n\n// Send the request and get the response\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Access response data\n$statusCode = $response-&gt;statusCode();\n$headers = $response-&gt;headers();\n$body = $response-&gt;body();\n\necho \"Status: $statusCode\\n\";\necho \"Body: $body\\n\";\n</code></pre>"},{"location":"http/2-getting-started/#error-handling","title":"Error Handling","text":"<p>HTTP requests can fail for various reasons. You should always wrap request handling in a try-catch block:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n    // Handle the error\n}\n</code></pre>"},{"location":"http/2-getting-started/#configuration","title":"Configuration","text":"<p>The Instructor HTTP client API can be configured via configuration files or at runtime.</p>"},{"location":"http/2-getting-started/#configuration-files","title":"Configuration Files","text":"<p>Create the configuration files in your project:</p> <p>config/http.php: <pre><code>// @doctest skip=true\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre></p> <p>config/debug.php: <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; false, // enable/disable debug\n        'trace' =&gt; false, // dump HTTP trace information\n        'requestUrl' =&gt; true, // dump request URL to console\n        'requestHeaders' =&gt; true, // dump request headers to console\n        'requestBody' =&gt; true, // dump request body to console\n        'responseHeaders' =&gt; true, // dump response headers to console\n        'responseBody' =&gt; true, // dump response body to console\n        'responseStream' =&gt; true, // dump stream data to console\n        'responseStreamByLine' =&gt; true, // dump stream as full lines or raw chunks\n    ],\n];\n</code></pre></p>"},{"location":"http/2-getting-started/#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also configure the client at runtime:</p> <pre><code>&lt;?php\nuse Cognesy\\Http\\HttpClient;\n\n// Create client with specific configuration\n$client = HttpClient::using('guzzle');\n\n// Or create with debug enabled\n$client = (new HttpClientBuilder())\n    -&gt;withPreset('guzzle')\n    -&gt;withDebugPreset('on')\n    -&gt;create();\n</code></pre>"},{"location":"http/2-getting-started/#simple-request-example","title":"Simple Request Example","text":"<p>Let's put everything together with a practical example of making a POST request to create a new resource:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create an HTTP client using the 'guzzle' configuration\n$client = HttpClient::using('guzzle');\n\n// Create a POST request with JSON data\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n    ],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n        'role' =&gt; 'user',\n    ],\n    options: []\n);\n\ntry {\n    // Send the request\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Process the response\n    if ($response-&gt;statusCode() === 201) {\n        $user = json_decode($response-&gt;body(), true);\n        echo \"User created with ID: {$user['id']}\\n\";\n\n        // Print user details\n        echo \"Name: {$user['name']}\\n\";\n        echo \"Email: {$user['email']}\\n\";\n    } else {\n        echo \"Error: Unexpected status code {$response-&gt;statusCode()}\\n\";\n        echo \"Response: {$response-&gt;body()}\\n\";\n    }\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n\n    // You might want to log the error or retry the request\n}\n</code></pre>"},{"location":"http/2-getting-started/#example-fetching-data","title":"Example: Fetching Data","text":"<p>Here's an example of making a GET request to fetch data:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create a default HTTP client\n$client = HttpClient::default();\n\n// Create a GET request with query parameters\n$request = new HttpRequest(\n    url: 'https://api.example.com/users?page=1&amp;limit=10',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n    ],\n    body: [],\n    options: []\n);\n\ntry {\n    // Send the request\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Process the response\n    if ($response-&gt;statusCode() === 200) {\n        $data = json_decode($response-&gt;body(), true);\n        $users = $data['users'] ?? [];\n\n        echo \"Retrieved \" . count($users) . \" users:\\n\";\n\n        foreach ($users as $user) {\n            echo \"- {$user['name']} ({$user['email']})\\n\";\n        }\n    } else {\n        echo \"Error: Unexpected status code {$response-&gt;statusCode()}\\n\";\n        echo \"Response: {$response-&gt;body()}\\n\";\n    }\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>These examples demonstrate the basic usage of the Instructor HTTP client API for common HTTP operations. In the following chapters, we'll explore more advanced features and customization options.</p>"},{"location":"http/3-making-requests/","title":"Making HTTP Requests","text":"<p>The Instructor HTTP client API provides a flexible and consistent way to create and send HTTP requests across different client implementations. This chapter covers the details of building and customizing HTTP requests.</p>"},{"location":"http/3-making-requests/#creating-requests","title":"Creating Requests","text":"<p>All HTTP requests are created using the <code>HttpRequest</code> class, which encapsulates the various components of an HTTP request.</p>"},{"location":"http/3-making-requests/#basic-request-creation","title":"Basic Request Creation","text":"<p>The constructor for <code>HttpRequest</code> takes several parameters:</p> <pre><code>use Cognesy\\Http\\Data\\HttpRequest;\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/endpoint',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre> <p>The parameters are:</p> <ul> <li><code>url</code>: The URL to send the request to (string)</li> <li><code>method</code>: The HTTP method to use (string)</li> <li><code>headers</code>: An associative array of HTTP headers (array)</li> <li><code>body</code>: The request body, which can be a string or an array (mixed)</li> <li><code>options</code>: Additional options for the request (array)</li> </ul>"},{"location":"http/3-making-requests/#request-methods","title":"Request Methods","text":"<p>Once you've created a request, you can access its properties using the following methods:</p> <pre><code>// Get the request URL\n$url = $request-&gt;url();\n\n// Get the HTTP method\n$method = $request-&gt;method();\n\n// Get the request headers\n$headers = $request-&gt;headers();\n\n// Get the request body\n$body = $request-&gt;body();\n\n// Get the request options\n$options = $request-&gt;options();\n\n// Check if the request is configured for streaming\n$isStreaming = $request-&gt;isStreamed();\n</code></pre>"},{"location":"http/3-making-requests/#modifying-requests","title":"Modifying Requests","text":"<p>You can also modify a request after it's been created:</p> <pre><code>// Enable streaming for this request\n$streamingRequest = $request-&gt;withStreaming(true);\n</code></pre> <p>Note that the <code>with*</code> methods return a new request instance rather than modifying the original one.</p>"},{"location":"http/3-making-requests/#http-methods","title":"HTTP Methods","text":"<p>The HTTP method is specified as a string in the <code>HttpClientRequest</code> constructor. The library supports all standard HTTP methods:</p>"},{"location":"http/3-making-requests/#get-requests","title":"GET Requests","text":"<p>GET requests are used to retrieve data from a server:</p> <pre><code>$getRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre> <p>For GET requests with query parameters, include them in the URL:</p> <pre><code>$getRequestWithParams = new HttpRequest(\n    url: 'https://api.example.com/users?page=1&amp;limit=10&amp;sort=name',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#post-requests","title":"POST Requests","text":"<p>POST requests are used to create new resources or submit data:</p> <pre><code>$postRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#put-requests","title":"PUT Requests","text":"<p>PUT requests are used to update existing resources:</p> <pre><code>$putRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'PUT',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'name' =&gt; 'John Updated',\n        'email' =&gt; 'john.updated@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#patch-requests","title":"PATCH Requests","text":"<p>PATCH requests are used to partially update resources:</p> <pre><code>$patchRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'PATCH',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'email' =&gt; 'new.email@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#delete-requests","title":"DELETE Requests","text":"<p>DELETE requests are used to remove resources:</p> <pre><code>$deleteRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'DELETE',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#other-methods","title":"Other Methods","text":"<p>The library also supports other HTTP methods like HEAD, OPTIONS, etc. Just specify the method name as a string:</p> <pre><code>$headRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'HEAD',\n    headers: [],\n    body: [],\n    options: []\n);\n\n$optionsRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'OPTIONS',\n    headers: [],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#setting-headers","title":"Setting Headers","text":"<p>HTTP headers are specified as an associative array where keys are header names and values are header values:</p> <pre><code>$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n        'User-Agent' =&gt; 'MyApp/1.0',\n        'X-Custom-Header' =&gt; 'Custom Value',\n    ],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#common-headers","title":"Common Headers","text":"<p>Some commonly used HTTP headers include:</p> <ul> <li>Content-Type: Specifies the format of the request body <pre><code>'Content-Type' =&gt; 'application/json'\n  ```\n\n- **Accept**: Indicates what response format the client can understand\n```php\n'Accept' =&gt; 'application/json'\n  ```\n\n- **Authorization**: Provides authentication credentials\n```php\n'Authorization' =&gt; 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'\n  ```\n\n- **User-Agent**: Identifies the client application\n```php\n'User-Agent' =&gt; 'MyApp/1.0 (https://example.com)'\n  ```\n\n- **Cache-Control**: Directives for caching mechanisms\n```php\n'Cache-Control' =&gt; 'no-cache'\n  ```\n\n- **Accept-Language**: Indicates the preferred language\n```php\n'Accept-Language' =&gt; 'en-US,en;q=0.9'\n  ```\n\n## Request Body\n\nThe request body can be provided in two ways:\n\n### Array Body (JSON)\n\nIf you provide an array as the request body, it will automatically be converted to a JSON string:\n\n```php\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/json'],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n        'age' =&gt; 30,\n        'address' =&gt; [\n            'street' =&gt; '123 Main St',\n            'city' =&gt; 'Anytown',\n            'zipcode' =&gt; '12345',\n        ],\n        'tags' =&gt; ['developer', 'php'],\n    ],\n    options: []\n);\n</code></pre></li> </ul> <p>When using an array for the body, you should set the <code>Content-Type</code> header to <code>application/json</code>.</p>"},{"location":"http/3-making-requests/#string-body","title":"String Body","text":"<p>You can also provide the body as a raw string:</p> <pre><code>// JSON string\n$jsonBody = json_encode([\n    'name' =&gt; 'John Doe',\n    'email' =&gt; 'john@example.com',\n]);\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/json'],\n    body: $jsonBody,\n    options: []\n);\n</code></pre> <p>This approach is useful for other content types:</p> <pre><code>// Form URL-encoded data\n$formBody = http_build_query([\n    'name' =&gt; 'John Doe',\n    'email' =&gt; 'john@example.com',\n]);\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/x-www-form-urlencoded'],\n    body: $formBody,\n    options: []\n);\n</code></pre>"},{"location":"http/3-making-requests/#working-with-request-body","title":"Working with Request Body","text":"<p>The body is managed by the <code>HttpRequestBody</code> class, which provides methods to access the body in different formats:</p> <pre><code>// Get the body as a string\n$bodyString = $request-&gt;body()-&gt;toString();\n\n// Get the body as an array (for JSON bodies)\n$bodyArray = $request-&gt;body()-&gt;toArray();\n</code></pre>"},{"location":"http/3-making-requests/#request-options","title":"Request Options","text":"<p>The <code>options</code> parameter allows you to specify additional options for the request:</p> <pre><code>$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: [\n        'stream' =&gt; true,  // Enable streaming response\n    ]\n);\n</code></pre>"},{"location":"http/3-making-requests/#available-options","title":"Available Options","text":"<p>Currently, the main supported option is:</p> <ul> <li><code>stream</code>: When set to <code>true</code>, enables streaming response handling</li> </ul> <p>You can check if a request is configured for streaming:</p> <pre><code>if ($request-&gt;isStreamed()) {\n    // Handle streaming response\n}\n</code></pre>"},{"location":"http/3-making-requests/#example-streaming-request","title":"Example: Streaming Request","text":"<p>Here's how to create a request for a streaming API:</p> <pre><code>$streamingRequest = new HttpRequest(\n    url: 'https://api.openai.com/v1/completions',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiKey,\n    ],\n    body: [\n        'model' =&gt; 'text-davinci-003',\n        'prompt' =&gt; 'Once upon a time',\n        'max_tokens' =&gt; 100,\n        'stream' =&gt; true,\n    ],\n    options: [\n        'stream' =&gt; true, // Enable streaming in the client\n    ]\n);\n</code></pre> <p>In the following chapters, we'll explore how to handle responses, including streaming responses, and how to use more advanced features like request pools and middleware.</p>"},{"location":"http/4-handling-responses/","title":"Handling Responses","text":"<p>After sending an HTTP request, you need to process the response received from the server. The Instructor HTTP client API provides a consistent interface for handling responses, regardless of the underlying HTTP client implementation.</p>"},{"location":"http/4-handling-responses/#response-interface","title":"Response Interface","text":"<p>All responses implement the <code>HttpResponse</code> interface, which provides a uniform way to access response data:</p> <pre><code>interface HttpResponse\n{\n    public function statusCode(): int;\n    public function headers(): array;\n    public function body(): string;\n    public function stream(int $chunkSize = 1): Generator;\n}\n</code></pre> <p>This interface ensures that the same code will work whether you're using Guzzle, Symfony, or Laravel HTTP clients.</p>"},{"location":"http/4-handling-responses/#getting-the-response","title":"Getting the Response","text":"<p>When you send a request using the <code>HttpClient::handle()</code> method, it returns an implementation of <code>HttpResponse</code>:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\n\n// Create a new HTTP client\n$client = HttpClient::default();\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>The specific implementation depends on the HTTP client driver being used:</p> <ul> <li><code>PsrHttpResponse</code>: Used by the GuzzleDriver</li> <li><code>SymfonyHttpResponse</code>: Used by the SymfonyDriver</li> <li><code>LaravelHttpResponse</code>: Used by the LaravelDriver</li> <li><code>MockHttpResponse</code>: Used by the MockHttpDriver for testing</li> </ul> <p>However, since all these implementations provide the same interface, your code doesn't need to know which one it's working with.</p>"},{"location":"http/4-handling-responses/#status-codes","title":"Status Codes","text":"<p>The status code indicates the result of the HTTP request. You can access it using the <code>statusCode()</code> method:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$statusCode = $response-&gt;statusCode();\n\necho \"Status code: $statusCode\\n\";\n</code></pre>"},{"location":"http/4-handling-responses/#status-code-categories","title":"Status Code Categories","text":"<p>Status codes are grouped into categories:</p> <ul> <li>1xx (Informational): The request was received and understood</li> <li>2xx (Success): The request was successfully received, understood, and accepted</li> <li>3xx (Redirection): Further action needs to be taken to complete the request</li> <li>4xx (Client Error): The request contains bad syntax or cannot be fulfilled</li> <li>5xx (Server Error): The server failed to fulfill a valid request</li> </ul>"},{"location":"http/4-handling-responses/#checking-response-status","title":"Checking Response Status","text":"<p>You can check if a response was successful:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n\nif ($response-&gt;statusCode() &gt;= 200 &amp;&amp; $response-&gt;statusCode() &lt; 300) {\n    // Success response\n    echo \"Request succeeded!\\n\";\n} elseif ($response-&gt;statusCode() &gt;= 400 &amp;&amp; $response-&gt;statusCode() &lt; 500) {\n    // Client error\n    echo \"Client error: {$response-&gt;statusCode()}\\n\";\n} elseif ($response-&gt;statusCode() &gt;= 500) {\n    // Server error\n    echo \"Server error: {$response-&gt;statusCode()}\\n\";\n}\n</code></pre>"},{"location":"http/4-handling-responses/#common-status-codes","title":"Common Status Codes","text":"<p>Here are some common HTTP status codes you might encounter:</p> <ul> <li>200 OK: The request was successful</li> <li>201 Created: A new resource was successfully created</li> <li>204 No Content: The request was successful, but there's no response body</li> <li>400 Bad Request: The request was malformed or invalid</li> <li>401 Unauthorized: Authentication is required</li> <li>403 Forbidden: The client doesn't have permission to access the resource</li> <li>404 Not Found: The requested resource doesn't exist</li> <li>405 Method Not Allowed: The HTTP method is not supported for this resource</li> <li>422 Unprocessable Entity: The request was well-formed but contains semantic errors</li> <li>429 Too Many Requests: Rate limit exceeded</li> <li>500 Internal Server Error: A generic server error occurred</li> <li>502 Bad Gateway: The server received an invalid response from an upstream server</li> <li>503 Service Unavailable: The server is temporarily unavailable</li> <li>504 Gateway Timeout: The upstream server didn't respond in time</li> <li>511 Network Authentication Required: The client needs to authenticate to gain network access</li> </ul>"},{"location":"http/4-handling-responses/#headers","title":"Headers","text":"<p>Response headers provide metadata about the response. You can access the headers using the <code>headers()</code> method:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$headers = $response-&gt;headers();\n\n// Print all headers\nforeach ($headers as $name =&gt; $values) {\n    echo \"$name: \" . implode(', ', $values) . \"\\n\";\n}\n\n// Access specific headers\n$contentType = $headers['Content-Type'] ?? 'unknown';\n$contentLength = $headers['Content-Length'] ?? 'unknown';\n\necho \"Content-Type: $contentType\\n\";\necho \"Content-Length: $contentLength\\n\";\n</code></pre> <p>The header names are case-insensitive, but the exact format might vary slightly between client implementations. Some clients normalize header names to title case (e.g., <code>Content-Type</code>), while others might use lowercase (e.g., <code>content-type</code>).</p>"},{"location":"http/4-handling-responses/#common-response-headers","title":"Common Response Headers","text":"<p>Here are some common response headers you might encounter:</p> <ul> <li>Content-Type: The MIME type of the response body</li> <li>Content-Length: The size of the response body in bytes</li> <li>Cache-Control: Directives for caching mechanisms</li> <li>Set-Cookie: Cookies to be stored by the client</li> <li>Location: Used for redirects</li> <li>X-RateLimit-Limit: The rate limit for the endpoint</li> <li>X-RateLimit-Remaining: The number of requests remaining in the current rate limit window</li> <li>X-RateLimit-Reset: When the rate limit will reset</li> </ul>"},{"location":"http/4-handling-responses/#body-content","title":"Body Content","text":"<p>For non-streaming responses, you can get the entire response body as a string using the <code>body()</code> method:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\necho \"Response body: $body\\n\";\n</code></pre>"},{"location":"http/4-handling-responses/#processing-json-responses","title":"Processing JSON Responses","text":"<p>Many APIs return JSON responses. You can decode them using PHP's <code>json_decode()</code> function:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Decode as associative array\n$data = json_decode($body, true);\n\nif (json_last_error() !== JSON_ERROR_NONE) {\n    echo \"Error decoding JSON: \" . json_last_error_msg() . \"\\n\";\n} else {\n    // Process the data\n    echo \"User ID: {$data['id']}\\n\";\n    echo \"User Name: {$data['name']}\\n\";\n}\n</code></pre>"},{"location":"http/4-handling-responses/#processing-xml-responses","title":"Processing XML Responses","text":"<p>For XML responses, you can use PHP's built-in XML functions:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Load XML\n$xml = simplexml_load_string($body);\n\nif ($xml === false) {\n    echo \"Error loading XML\\n\";\n} else {\n    // Process the XML\n    echo \"Title: {$xml-&gt;title}\\n\";\n    echo \"Description: {$xml-&gt;description}\\n\";\n}\n</code></pre>"},{"location":"http/4-handling-responses/#processing-binary-responses","title":"Processing Binary Responses","text":"<p>For binary responses (like file downloads), you can save the response body to a file:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Save to file\nfile_put_contents('downloaded_file.pdf', $body);\necho \"File downloaded successfully\\n\";\n</code></pre>"},{"location":"http/4-handling-responses/#error-handling","title":"Error Handling","text":"<p>When making HTTP requests, various errors can occur. The Instructor HTTP client API provides a consistent way to handle these errors through exceptions.</p>"},{"location":"http/4-handling-responses/#requestexception","title":"RequestException","text":"<p>The main exception type is <code>RequestException</code>, which is thrown when a request fails:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n\n    // You might want to log the error or take other actions\n    if ($e-&gt;getPrevious() !== null) {\n        echo \"Original exception: \" . $e-&gt;getPrevious()-&gt;getMessage() . \"\\n\";\n    }\n}\n</code></pre> <p>The <code>RequestException</code> often wraps another exception from the underlying HTTP client, which you can access with <code>$e-&gt;getPrevious()</code>.</p>"},{"location":"http/4-handling-responses/#error-response-handling","title":"Error Response Handling","text":"<p>By default, HTTP error responses (4xx, 5xx status codes) do not throw exceptions. You can control this behavior using the <code>failOnError</code> configuration option:</p> <pre><code>// In config/http.php\n'failOnError' =&gt; true, // Throw exceptions for 4xx/5xx responses\n</code></pre> <p>When <code>failOnError</code> is set to <code>true</code>, the client will throw a <code>RequestException</code> for error responses. When it's <code>false</code>, you need to check the status code yourself:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n\nif ($response-&gt;statusCode() &gt;= 400) {\n    // Handle error response\n    echo \"Error: HTTP {$response-&gt;statusCode()}\\n\";\n    echo \"Error details: {$response-&gt;body()}\\n\";\n} else {\n    // Process successful response\n}\n</code></pre>"},{"location":"http/4-handling-responses/#retrying-failed-requests","title":"Retrying Failed Requests","text":"<p>If a request fails, you might want to retry it. Here's a simple implementation of a retry mechanism:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\n\nfunction retryRequest($client, $request, $maxRetries = 3, $delay = 1): ?HttpResponse {\n    $attempts = 0;\n\n    while ($attempts &lt; $maxRetries) {\n        try {\n            return $client-&gt;withRequest($request)-&gt;get();\n        } catch (HttpRequestException $e) {\n            $attempts++;\n\n            if ($attempts &gt;= $maxRetries) {\n                throw $e; // Rethrow after all retries failed\n            }\n\n            // Wait before retrying (with exponential backoff)\n            $sleepTime = $delay * pow(2, $attempts - 1);\n            echo \"Request failed, retrying in {$sleepTime} seconds...\\n\";\n            sleep($sleepTime);\n        }\n    }\n\n    return null; // Should never reach here\n}\n\n// Usage\ntry {\n    $response = retryRequest($client, $request);\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"All retry attempts failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>This function will retry failed requests with exponential backoff, meaning it waits longer between each retry attempt.</p> <p>In the next chapter, we'll explore streaming responses, which are particularly useful for handling large responses or real-time data streams.</p>"},{"location":"http/5-streaming-responses/","title":"Streaming Responses","text":"<p>Streaming responses are a powerful feature that allows processing data as it arrives from the server, rather than waiting for the entire response to be received. This is particularly valuable when:</p> <ul> <li>Working with large responses that might exceed memory limits</li> <li>Processing real-time data streams</li> <li>Handling responses from AI models that generate content token by token</li> <li>Building user interfaces that show progressive updates</li> </ul> <p>The Instructor HTTP client API provides robust support for streaming responses across all supported HTTP client implementations.</p>"},{"location":"http/5-streaming-responses/#enabling-streaming","title":"Enabling Streaming","text":"<p>To receive a streaming response, you need to configure the request with the <code>stream</code> option set to <code>true</code>:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://api.example.com/stream',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'text/event-stream',\n    ],\n    body: [],\n    options: [\n        'stream' =&gt; true,  // Enable streaming\n    ]\n);\n\n// Or use the withStreaming method on an existing request\n$streamingRequest = $request-&gt;withStreaming(true);\n\n// Create a client and send the request\n$client = new HttpClient();\n$response = $client-&gt;withRequest($streamingRequest)-&gt;get();\n</code></pre> <p>The <code>stream</code> option tells the HTTP client to treat the response as a stream, which means:</p> <ol> <li>It won't buffer the entire response in memory</li> <li>It will provide a way to read the response incrementally</li> <li>The connection will remain open until all data is received or the stream is closed</li> </ol>"},{"location":"http/5-streaming-responses/#processing-streamed-data","title":"Processing Streamed Data","text":"<p>Once you have a streaming response, you can process it using the <code>stream()</code> method, which returns a PHP Generator:</p> <pre><code>$response = $client-&gt;withRequest($streamingRequest)-&gt;get();\n\n// Process the stream chunk by chunk\nforeach ($response-&gt;stream() as $chunk) {\n    // Process each chunk of data as it arrives\n    echo \"Received chunk: $chunk\\n\";\n\n    // You could parse JSON chunks, update progress, etc.\n    // If this is a streaming JSON response, you might need to buffer until\n    // you have complete JSON objects\n}\n</code></pre> <p>By default, the <code>stream()</code> method reads the response in small chunks. You can control the chunk size by passing a parameter:</p> <pre><code>// Read in chunks of 1024 bytes\nforeach ($response-&gt;stream(1024) as $chunk) {\n    // Process larger chunks of data\n    echo \"Received chunk of approximately 1KB: $chunk\\n\";\n}\n</code></pre>"},{"location":"http/5-streaming-responses/#example-downloading-a-large-file","title":"Example: Downloading a Large File","text":"<p>Here's an example of downloading a large file with streaming to avoid memory issues:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://example.com/large-file.zip',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$client = new HttpClient();\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Open a file handle to save the file\n    $fileHandle = fopen('downloaded-file.zip', 'wb');\n\n    if (!$fileHandle) {\n        throw new \\RuntimeException(\"Could not open file for writing\");\n    }\n\n    // Keep track of bytes received\n    $totalBytes = 0;\n\n    // Process the stream and write to file\n    foreach ($response-&gt;stream(8192) as $chunk) {\n        fwrite($fileHandle, $chunk);\n        $totalBytes += strlen($chunk);\n\n        // Display progress (if not in a web request)\n        echo \"\\rDownloaded: \" . number_format($totalBytes / 1024 / 1024, 2) . \" MB\";\n    }\n\n    // Close the file handle\n    fclose($fileHandle);\n    echo \"\\nDownload complete!\\n\";\n\n} catch (HttpRequestException $e) {\n    echo \"Download failed: {$e-&gt;getMessage()}\\n\";\n\n    // Clean up if file was partially downloaded\n    if (isset($fileHandle) &amp;&amp; is_resource($fileHandle)) {\n        fclose($fileHandle);\n    }\n    if (file_exists('downloaded-file.zip')) {\n        unlink('downloaded-file.zip');\n    }\n}\n</code></pre> <p>This approach allows downloading very large files without loading the entire file into memory.</p>"},{"location":"http/5-streaming-responses/#example-processing-server-sent-events-sse","title":"Example: Processing Server-Sent Events (SSE)","text":"<p>Server-Sent Events (SSE) are a common streaming format used by many APIs. Here's how to process them:</p> <pre><code>$request = new HttpRequest(\n    url: 'https://api.example.com/events',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'text/event-stream',\n        'Cache-Control' =&gt; 'no-cache',\n    ],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n$buffer = '';\n\nforeach ($response-&gt;stream() as $chunk) {\n    // Add the chunk to our buffer\n    $buffer .= $chunk;\n\n    // Process complete events (SSE events are separated by double newlines)\n    while (($pos = strpos($buffer, \"\\n\\n\")) !== false) {\n        // Extract and process the event\n        $event = substr($buffer, 0, $pos);\n        $buffer = substr($buffer, $pos + 2);\n\n        // Parse the event (SSE format: \"field: value\")\n        $parsedEvent = [];\n        foreach (explode(\"\\n\", $event) as $line) {\n            if (preg_match('/^([^:]+):\\s*(.*)$/', $line, $matches)) {\n                $field = $matches[1];\n                $value = $matches[2];\n                $parsedEvent[$field] = $value;\n            }\n        }\n\n        // Process the parsed event\n        if (isset($parsedEvent['event'], $parsedEvent['data'])) {\n            $eventType = $parsedEvent['event'];\n            $eventData = $parsedEvent['data'];\n\n            echo \"Received event type: $eventType\\n\";\n            echo \"Event data: $eventData\\n\";\n\n            // You could also parse the data as JSON if appropriate\n            if ($eventType === 'update') {\n                $data = json_decode($eventData, true);\n                if ($data) {\n                    echo \"Processed update: {$data['message']}\\n\";\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>While this works, processing streaming responses line by line is common enough that the library provides a dedicated middleware for it, as we'll see in the next section.</p>"},{"location":"http/5-streaming-responses/#line-by-line-processing","title":"Line-by-Line Processing","text":"<p>For many streaming APIs, especially those that send event streams or line-delimited JSON, it's useful to process the response line by line. The library provides the <code>StreamByLineMiddleware</code> to simplify this task:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Middleware\\StreamByLine\\StreamByLineMiddleware;\n\n// Create a client with the StreamByLineMiddleware\n$client = new HttpClient();\n$client-&gt;withMiddleware(new StreamByLineMiddleware());\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://api.example.com/events',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Process the stream line by line\nforeach ($response-&gt;stream() as $line) {\n    // Each $line is a complete line from the response\n    echo \"Received line: $line\\n\";\n\n    // Parse the line (e.g., as JSON)\n    $event = json_decode($line, true);\n    if ($event) {\n        // Process the event\n        echo \"Event type: {$event['type']}\\n\";\n    }\n}\n</code></pre>"},{"location":"http/5-streaming-responses/#customizing-line-processing","title":"Customizing Line Processing","text":"<p>You can customize how lines are parsed by providing a parser function to the middleware:</p> <pre><code>$lineParser = function (string $line) {\n    // Pre-process each line before yielding it\n    $trimmedLine = trim($line);\n    if (empty($trimmedLine)) {\n        return null; // Skip empty lines\n    }\n    return $trimmedLine;\n};\n\n$client-&gt;withMiddleware(new StreamByLineMiddleware($lineParser));\n</code></pre> <p>If your parser returns <code>null</code>, that line will be skipped in the stream.</p>"},{"location":"http/5-streaming-responses/#example-processing-openai-chat-completions","title":"Example: Processing OpenAI Chat Completions","text":"<p>Here's a practical example of using the <code>StreamByLineMiddleware</code> to process streaming responses from the OpenAI API:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Middleware\\StreamByLine\\StreamByLineMiddleware;\n\n// OpenAI API requires a parser that handles their SSE format\n$openAiParser = function (string $line) {\n    // Skip empty lines\n    if (trim($line) === '') {\n        return null;\n    }\n\n    // Remove \"data: \" prefix from each line\n    if (strpos($line, 'data: ') === 0) {\n        $line = substr($line, 6);\n\n        // Skip the \"[DONE]\" message\n        if ($line === '[DONE]') {\n            return null;\n        }\n\n        // Return the parsed line\n        return $line;\n    }\n\n    return null; // Skip non-data lines\n};\n\n// Create a client with the StreamByLineMiddleware\n$client = new HttpClient('guzzle'); // Use Guzzle for better streaming support\n$client-&gt;withMiddleware(new StreamByLineMiddleware($openAiParser));\n\n// Create a request to OpenAI API\n$request = new HttpRequest(\n    url: 'https://api.openai.com/v1/chat/completions',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiKey,\n    ],\n    body: [\n        'model' =&gt; 'gpt-3.5-turbo',\n        'messages' =&gt; [\n            ['role' =&gt; 'user', 'content' =&gt; 'Write a short poem about coding.'],\n        ],\n        'stream' =&gt; true,\n    ],\n    options: ['stream' =&gt; true]\n);\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    $fullResponse = '';\n\n    // Process the streaming response\n    foreach ($response-&gt;stream() as $chunk) {\n        // Parse the chunk as JSON\n        $data = json_decode($chunk, true);\n\n        if ($data &amp;&amp; isset($data['choices'][0]['delta']['content'])) {\n            $content = $data['choices'][0]['delta']['content'];\n            $fullResponse .= $content;\n\n            // Print each piece as it arrives\n            echo $content;\n            flush(); // Ensure output is sent immediately\n        }\n    }\n\n    echo \"\\n\\nFull response:\\n$fullResponse\\n\";\n\n} catch (Exception $e) {\n    echo \"Error: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>This approach allows you to display the AI-generated content to the user in real-time as it's being generated, providing a more responsive user experience.</p>"},{"location":"http/5-streaming-responses/#considerations-for-streaming","title":"Considerations for Streaming","text":"<p>When working with streaming responses, keep these considerations in mind:</p> <ol> <li> <p>Memory Usage: While streaming reduces memory usage overall, be careful not to accumulate the entire response in memory by appending to a variable unless necessary.</p> </li> <li> <p>Connection Stability: Streaming connections can be more sensitive to network issues. Consider implementing error handling and retry logic for more robust applications.</p> </li> <li> <p>Server Timeouts: Some servers or proxies might timeout long-running connections. Make sure your infrastructure is configured to allow the necessary connection times.</p> </li> <li> <p>Middleware Order: When using middleware that processes streaming responses, the order of middleware can be important. Middleware is executed in the order it's added to the stack.</p> </li> </ol> <p>In the next chapter, we'll explore how to make multiple concurrent requests using request pools, which can significantly improve performance when fetching data from multiple endpoints.</p>"},{"location":"http/7-changing-client/","title":"Changing the Underlying Client","text":"<p>One of the core features of the Instructor HTTP client API is its ability to seamlessly switch between different HTTP client implementations. This flexibility allows you to use the same code across different environments or to choose the most appropriate client for specific use cases.</p>"},{"location":"http/7-changing-client/#available-client-drivers","title":"Available Client Drivers","text":"<p>The library includes several built-in drivers that adapt various HTTP client libraries to the unified interface used by Instructor:</p>"},{"location":"http/7-changing-client/#guzzledriver","title":"GuzzleDriver","text":"<p>The <code>GuzzleDriver</code> provides integration with the popular Guzzle HTTP client.</p> <p>Key Features: - Robust feature set - Excellent performance - Extensive middleware ecosystem - Support for HTTP/2 (via cURL) - Stream and promise-based API</p> <p>Best For: - General-purpose HTTP requests - Applications that need advanced features - Projects without framework constraints</p> <p>Requirements: - Requires the <code>guzzlehttp/guzzle</code> package (<code>composer require guzzlehttp/guzzle</code>)</p>"},{"location":"http/7-changing-client/#symfonydriver","title":"SymfonyDriver","text":"<p>The <code>SymfonyDriver</code> integrates with the Symfony HTTP Client.</p> <p>Key Features: - Native HTTP/2 support - Automatic content-type detection - Built-in profiling and logging - No dependency on cURL - Support for various transports (native PHP, cURL, amphp)</p> <p>Best For: - Symfony applications - Projects requiring HTTP/2 support - Low-dependency environments</p> <p>Requirements: - Requires the <code>symfony/http-client</code> package (<code>composer require symfony/http-client</code>)</p>"},{"location":"http/7-changing-client/#laraveldriver","title":"LaravelDriver","text":"<p>The <code>LaravelDriver</code> integrates with the Laravel HTTP Client.</p> <p>Key Features: - Elegant, fluent syntax - Integration with Laravel ecosystem - Built-in macros and testing utilities - Automatic JSON handling - Rate limiting and retry capabilities</p> <p>Best For: - Laravel applications - Projects already using the Laravel framework</p> <p>Requirements: - Included with the Laravel framework</p>"},{"location":"http/7-changing-client/#mockhttpdriver","title":"MockHttpDriver","text":"<p>The <code>MockHttpDriver</code> is a test double that doesn't make actual HTTP requests but returns predefined responses.</p> <p>Key Features: - No actual network requests - Predefined responses for testing - Response matching based on URL, method, and body - Support for custom response generation</p> <p>Best For: - Unit testing - Offline development - CI/CD environments</p>"},{"location":"http/7-changing-client/#switching-between-clients","title":"Switching Between Clients","text":"<p>You can switch between the available client implementations in several ways:</p>"},{"location":"http/7-changing-client/#when-creating-the-client","title":"When Creating the Client","text":"<p>The simplest approach is to specify the client when creating the <code>HttpClient</code> instance:</p> <pre><code>// Use Guzzle (assuming it's configured in config/http.php)\n$guzzleClient = HttpClient::using('guzzle');\n\n// Use Symfony\n$symfonyClient = HttpClient::using('symfony');\n\n// Use Laravel\n$laravelClient = HttpClient::using('laravel');\n</code></pre> <p>The client name must correspond to a configuration entry in your <code>config/http.php</code> file.</p>"},{"location":"http/7-changing-client/#using-the-default-client","title":"Using the Default Client","text":"<p>If you don't specify a client, the default one from your configuration will be used:</p> <pre><code>// Uses the default client specified in config/http.php\n$client = HttpClient::default();\n</code></pre> <p>The default client is specified in the <code>config/http.php</code> file:</p> <pre><code>return [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        // Client configurations...\n    ],\n];\n</code></pre>"},{"location":"http/7-changing-client/#switching-at-runtime","title":"Switching at Runtime","text":"<p>You can create different clients for different requirements within the same application:</p> <pre><code>// Create clients with different configurations\n$defaultClient = HttpClient::default();\n$symfonyClient = HttpClient::using('symfony');\n$laravelClient = HttpClient::using('laravel');\n$customClient = HttpClient::using('http-ollama');\n</code></pre> <p>Note: HttpClient instances are immutable, so you create new instances rather than switching configurations at runtime.</p>"},{"location":"http/7-changing-client/#using-the-static-make-method","title":"Using the Static Make Method","text":"<p>The <code>HttpClient</code> class provides a static <code>make</code> method as an alternative to the constructor:</p> <pre><code>use Cognesy\\Http\\HttpClientBuilder;\n\n// Create with specific client using builder\n$client = (new HttpClientBuilder())-&gt;withPreset('guzzle')-&gt;create();\n// Equivalent to:\n$client = HttpClient::using('guzzle');\n\n// Create with default client and custom event dispatcher\n$events = new EventDispatcher();\n$client = (new HttpClientBuilder())-&gt;withEventBus($events)-&gt;create();\n</code></pre>"},{"location":"http/7-changing-client/#client-specific-configuration","title":"Client-Specific Configuration","text":"<p>Each client type can have its own configuration in the <code>config/http.php</code> file:</p> <pre><code>&lt;?php\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre>"},{"location":"http/7-changing-client/#multiple-configurations-for-the-same-client-type","title":"Multiple Configurations for the Same Client Type","text":"<p>You can define multiple configurations for the same client type, each with different settings:</p> <pre><code>'clients' =&gt; [\n    'guzzle' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 3,\n        'requestTimeout' =&gt; 30,\n        // Default settings for Guzzle\n    ],\n    'guzzle-short-timeout' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 1,\n        'requestTimeout' =&gt; 5,\n        // Short timeouts for quick operations\n    ],\n    'guzzle-long-timeout' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 5,\n        'requestTimeout' =&gt; 120,\n        // Long timeouts for operations that take time\n    ],\n    'guzzle-streaming' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 3,\n        'requestTimeout' =&gt; 300,\n        'idleTimeout' =&gt; 60,\n        // Optimized for streaming responses\n    ],\n    'http-ollama' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 1,\n        'requestTimeout' =&gt; 90, // Longer timeout for AI model inference\n        'idleTimeout' =&gt; -1,\n        'maxConcurrent' =&gt; 5,\n        'poolTimeout' =&gt; 60,\n        'failOnError' =&gt; true,\n    ],\n],\n</code></pre> <p>Then you can select the appropriate configuration based on your needs:</p> <pre><code>// For quick API calls\n$quickClient = HttpClient::using('guzzle-short-timeout');\n\n// For long-running operations\n$longClient = HttpClient::using('guzzle-long-timeout');\n\n// For streaming responses\n$streamingClient = HttpClient::using('guzzle-streaming');\n\n// For AI model requests\n$aiClient = HttpClient::using('http-ollama');\n</code></pre>"},{"location":"http/7-changing-client/#common-configuration-parameters","title":"Common Configuration Parameters","text":"<p>All client types support these common configuration parameters:</p> Parameter Type Description <code>httpClientType</code> string The type of HTTP client (Guzzle, Symfony, Laravel) <code>connectTimeout</code> int Maximum time to wait for connection establishment (seconds) <code>requestTimeout</code> int Maximum time to wait for the entire request (seconds) <code>idleTimeout</code> int Maximum time to wait between data packets (seconds, -1 for no timeout) <code>maxConcurrent</code> int Maximum number of concurrent requests in a pool <code>poolTimeout</code> int Maximum time to wait for all pooled requests (seconds) <code>failOnError</code> bool Whether to throw exceptions for HTTP error responses"},{"location":"http/7-changing-client/#client-specific-parameters","title":"Client-Specific Parameters","text":"<p>Some parameters might only be relevant to specific client implementations. For example, Guzzle supports additional options like <code>verify</code> (for SSL verification) or <code>proxy</code> settings that can be passed through the underlying client.</p>"},{"location":"http/7-changing-client/#example-choosing-the-right-client-for-different-scenarios","title":"Example: Choosing the Right Client for Different Scenarios","text":"<p>Here's an example of selecting different client configurations based on the task:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction fetchApiData($url, $apiKey) {\n    // Use a client with short timeouts for quick API calls\n    $client = new HttpClient('guzzle-short-timeout');\n\n    $request = new HttpRequest(\n        url: $url,\n        method: 'GET',\n        headers: [\n            'Authorization' =&gt; 'Bearer ' . $apiKey,\n            'Accept' =&gt; 'application/json',\n        ],\n        body: [],\n        options: []\n    );\n\n    try {\n        return $client-&gt;withRequest($request)-&gt;get();\n    } catch (HttpRequestException $e) {\n        // Handle error\n        throw $e;\n    }\n}\n\nfunction downloadLargeFile($url, $outputPath) {\n    // Use a client with long timeouts for downloading large files\n    $client = new HttpClient('guzzle-long-timeout');\n\n    $request = new HttpRequest(\n        url: $url,\n        method: 'GET',\n        headers: [],\n        body: [],\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $response = $client-&gt;withRequest($request)-&gt;get();\n\n        $fileHandle = fopen($outputPath, 'wb');\n        foreach ($response-&gt;stream(8192) as $chunk) {\n            fwrite($fileHandle, $chunk);\n        }\n        fclose($fileHandle);\n\n        return true;\n    } catch (HttpRequestException $e) {\n        // Handle error\n        if (file_exists($outputPath)) {\n            unlink($outputPath); // Remove partial file\n        }\n        throw $e;\n    }\n}\n\nfunction generateAiResponse($prompt) {\n    // Use a specialized client for AI API requests\n    $client = new HttpClient('http-ollama');\n\n    $request = new HttpRequest(\n        url: 'https://api.example.com/ai/generate',\n        method: 'POST',\n        headers: [\n            'Content-Type' =&gt; 'application/json',\n            'Accept' =&gt; 'application/json',\n        ],\n        body: [\n            'prompt' =&gt; $prompt,\n            'max_tokens' =&gt; 500,\n        ],\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $response = $client-&gt;withRequest($request)-&gt;get();\n\n        $result = '';\n        foreach ($response-&gt;stream() as $chunk) {\n            $result .= $chunk;\n        }\n\n        return json_decode($result, true);\n    } catch (HttpRequestException $e) {\n        // Handle error\n        throw $e;\n    }\n}\n</code></pre>"},{"location":"http/7-changing-client/#considerations-for-switching-clients","title":"Considerations for Switching Clients","text":"<p>When switching between different HTTP client implementations, keep these considerations in mind:</p> <ol> <li> <p>Configuration Consistency: Ensure that all client configurations have the appropriate settings for your application's needs.</p> </li> <li> <p>Feature Availability: Some advanced features might be available only in specific clients. For example, HTTP/2 support might be better in one client than another.</p> </li> <li> <p>Error Handling: Different clients might have slightly different error behavior. Instructor HTTP client API normalizes much of this, but edge cases can still occur.</p> </li> <li> <p>Middleware Compatibility: If you're using middleware, ensure it's compatible with all client types you plan to use.</p> </li> <li> <p>Performance Characteristics: Different clients may have different performance profiles for specific scenarios. Test with your actual workload if performance is critical.</p> </li> </ol> <p>In the next chapter, we'll explore how to customize client configurations in more detail, including runtime configuration and advanced options.</p>"},{"location":"http/8-changing-client-config/","title":"Customizing Client Configuration","text":"<p>The Instructor HTTP client API offers extensive configuration options to customize client behavior for different scenarios. This chapter explores how to configure clients through configuration files and at runtime.</p>"},{"location":"http/8-changing-client-config/#configuration-files","title":"Configuration Files","text":"<p>The primary configuration files for the HTTP client are:</p>"},{"location":"http/8-changing-client-config/#main-configuration-confighttpphp","title":"Main Configuration: config/http.php","text":"<p>This file defines the available client types and their settings:</p> <pre><code>return [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre>"},{"location":"http/8-changing-client-config/#debug-configuration-configdebugphp","title":"Debug Configuration: config/debug.php","text":"<p>This file controls debugging options for HTTP requests and responses:</p> <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; false, // enable/disable debug\n        'trace' =&gt; false, // dump HTTP trace information\n        'requestUrl' =&gt; true, // dump request URL to console\n        'requestHeaders' =&gt; true, // dump request headers to console\n        'requestBody' =&gt; true, // dump request body to console\n        'responseHeaders' =&gt; true, // dump response headers to console\n        'responseBody' =&gt; true, // dump response body to console\n        'responseStream' =&gt; true, // dump stream data to console\n        'responseStreamByLine' =&gt; true, // dump stream as full lines or raw chunks\n    ],\n];\n</code></pre>"},{"location":"http/8-changing-client-config/#loading-configuration-files","title":"Loading Configuration Files","text":"<p>The library uses a settings management system to load these configurations. The system looks for these files in the base directory of your project. If you're using a framework like Laravel or Symfony, you can integrate with their configuration systems instead.</p> <p>For Laravel, you might publish these configurations as Laravel config files:</p> <pre><code>php artisan vendor:publish --tag=polyglot-config\n</code></pre> <p>For Symfony, you might define these as service parameters in your service configuration.</p>"},{"location":"http/8-changing-client-config/#configuration-options","title":"Configuration Options","text":"<p>The <code>HttpClientConfig</code> class encapsulates the configuration options for HTTP clients. Here's a detailed breakdown of the available options:</p>"},{"location":"http/8-changing-client-config/#basic-connection-options","title":"Basic Connection Options","text":"Option Type Description Default <code>httpClientType</code> string Type of HTTP client to use <code>'guzzle'</code> <code>connectTimeout</code> int Connection timeout in seconds 3 <code>requestTimeout</code> int Request timeout in seconds 30 <code>idleTimeout</code> int Idle timeout in seconds (-1 for no timeout) -1"},{"location":"http/8-changing-client-config/#request-pool-options","title":"Request Pool Options","text":"Option Type Description Default <code>maxConcurrent</code> int Maximum number of concurrent requests in a pool 5 <code>poolTimeout</code> int Timeout for the entire request pool in seconds 60"},{"location":"http/8-changing-client-config/#error-handling-options","title":"Error Handling Options","text":"Option Type Description Default <code>failOnError</code> bool Whether to throw exceptions on HTTP errors true"},{"location":"http/8-changing-client-config/#debug-options","title":"Debug Options","text":"Option Type Description Default <code>enabled</code> bool Enable or disable HTTP debugging false <code>trace</code> bool Dump HTTP trace information false <code>requestUrl</code> bool Log the request URL true <code>requestHeaders</code> bool Log request headers true <code>requestBody</code> bool Log request body true <code>responseHeaders</code> bool Log response headers true <code>responseBody</code> bool Log response body true <code>responseStream</code> bool Log streaming response data true <code>responseStreamByLine</code> bool Log stream as complete lines (true) or raw chunks (false) true"},{"location":"http/8-changing-client-config/#understanding-timeout-options","title":"Understanding Timeout Options","text":"<p>Timeout settings are crucial for controlling how your application handles slow or unresponsive servers:</p> <ul> <li> <p>connectTimeout: Maximum time to wait for establishing a connection to the server. If the server doesn't respond within this time, the request fails with a connection timeout error. Setting this too low might cause failures when connecting to slow servers, but setting it too high could leave your application waiting for unresponsive servers.</p> </li> <li> <p>requestTimeout: Maximum time to wait for the entire request to complete, from connection initiation to receiving the complete response. If the entire request-response cycle isn't completed within this time, the request fails with a timeout error.</p> </li> <li> <p>idleTimeout: Maximum time to wait between receiving data packets. If the server stops sending data for longer than this period, the connection is considered idle and is terminated. Setting this to -1 disables the idle timeout, which is useful for long-running streaming connections.</p> </li> <li> <p>poolTimeout: Maximum time to wait for all requests in a pool to complete. If any requests in the pool haven't completed within this time, they're terminated.</p> </li> </ul>"},{"location":"http/8-changing-client-config/#runtime-configuration","title":"Runtime Configuration","text":"<p>While configuration files provide a static way to configure clients, you often need to change configuration at runtime based on the specific requirements of a request or operation.</p>"},{"location":"http/8-changing-client-config/#using-withclient","title":"Using withClient","text":"<p>The simplest way to switch configurations at runtime is to use the <code>withClient</code> method to select a different pre-configured client:</p> <pre><code>// Start with default client\n$client = new HttpClient();\n\n// Switch to a client with longer timeouts\n$client-&gt;withClient('guzzle-long-timeout');\n\n// Switch to a client optimized for streaming\n$client-&gt;withClient('guzzle-streaming');\n</code></pre>"},{"location":"http/8-changing-client-config/#using-withconfig","title":"Using withConfig","text":"<p>For more dynamic configuration, you can create a custom <code>HttpClientConfig</code> object and apply it using the <code>withConfig</code> method:</p> <pre><code>use Cognesy\\Http\\Config\\HttpClientConfig;\n\n// Create a custom configuration\n$config = new HttpClientConfig(\n    driver: 'guzzle',\n    connectTimeout: 5,\n    requestTimeout: 60,\n    idleTimeout: 30,\n    maxConcurrent: 10,\n    poolTimeout: 120,\n    failOnError: false\n);\n\n// Use the custom configuration\n$client-&gt;withConfig($config);\n</code></pre> <p>This method gives you complete control over the configuration at runtime.</p>"},{"location":"http/8-changing-client-config/#creating-configuration-from-an-array","title":"Creating Configuration from an Array","text":"<p>You can also create a configuration from an associative array:</p> <pre><code>$configArray = [\n    'httpClientType' =&gt; 'symfony',\n    'connectTimeout' =&gt; 2,\n    'requestTimeout' =&gt; 45,\n    'idleTimeout' =&gt; -1,\n    'maxConcurrent' =&gt; 8,\n    'poolTimeout' =&gt; 90,\n    'failOnError' =&gt; true,\n];\n\n$config = HttpClientConfig::fromArray($configArray);\n$client-&gt;withConfig($config);\n</code></pre> <p>This approach is useful when loading configuration from external sources like environment variables or configuration files.</p>"},{"location":"http/8-changing-client-config/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>You can enable debug mode to see detailed information about requests and responses:</p> <pre><code>// Enable debug mode\n$client-&gt;withDebugPreset('on');\n\n// Make a request\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Disable debug mode when done\n$client-&gt;withDebugPreset('off');\n</code></pre> <p>When debug mode is enabled, detailed information about requests and responses is output to the console or log.</p>"},{"location":"http/8-changing-client-config/#example-dynamic-configuration-based-on-request-type","title":"Example: Dynamic Configuration Based on Request Type","text":"<p>Here's an example of dynamically adjusting configuration based on the type of request:</p> <pre><code>function configureClientForRequest(HttpClient $client, HttpRequest $request): HttpClient {\n    // Get the current configuration\n    $config = HttpClientConfig::load($client-&gt;getClientName());\n\n    // Adjust timeouts based on the request URL\n    if (strpos($request-&gt;url(), 'large-file') !== false) {\n        // For large file downloads, use longer timeouts\n        $config = new HttpClientConfig(\n            httpClientType: $config-&gt;httpClientType,\n            connectTimeout: $config-&gt;connectTimeout,\n            requestTimeout: 300, // 5 minutes\n            idleTimeout: 60,     // 1 minute\n            maxConcurrent: $config-&gt;maxConcurrent,\n            poolTimeout: $config-&gt;poolTimeout,\n            failOnError: $config-&gt;failOnError\n        );\n\n        $client-&gt;withConfig($config);\n    }\n\n    // Enable streaming for specific endpoints\n    if (strpos($request-&gt;url(), '/stream') !== false || strpos($request-&gt;url(), '/events') !== false) {\n        // Make sure the request is set to stream\n        $request = $request-&gt;withStreaming(true);\n    }\n\n    // Enable debug for development environment\n    if (getenv('APP_ENV') === 'development') {\n        $client-&gt;withDebugPreset('on');\n    }\n\n    return $client;\n}\n\n// Usage\n$client = new HttpClient();\n$request = new HttpRequest(...);\n\n// Configure the client based on the request\n$client = configureClientForRequest($client, $request);\n\n// Send the request\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>This approach allows for highly dynamic and contextual configuration adjustments.</p>"},{"location":"http/8-changing-client-config/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li> <p>Define Base Configurations in Files: Keep your common configurations in the <code>config/http.php</code> file for easy reference and maintenance.</p> </li> <li> <p>Use Named Configurations: Create named configurations for different scenarios (e.g., <code>'guzzle-short-timeout'</code>, <code>'guzzle-streaming'</code>) to make your code more readable and maintainable.</p> </li> <li> <p>Adjust Timeouts Appropriately: Set timeouts based on the expected response time of the API or service you're calling. Shorter for quick operations, longer for file uploads/downloads or streaming.</p> </li> <li> <p>Consider Error Handling Strategy: Set <code>failOnError</code> based on how you want to handle errors. For critical operations, set it to <code>true</code> to catch errors immediately. For bulk operations or request pools, set it to <code>false</code> to handle errors individually.</p> </li> <li> <p>Use Debug Mode Judiciously: Enable debug mode only when needed, as it can generate a lot of output and potentially impact performance.</p> </li> <li> <p>Test Different Configurations: Experiment with different settings to find the optimal configuration for your specific use cases.</p> </li> </ol>"},{"location":"http/8-changing-client-config/#adapting-to-different-environments","title":"Adapting to Different Environments","text":"<p>Different environments often require different configurations. Here's how you might handle this:</p> <pre><code>// In your application bootstrap or service provider\nfunction configureHttpClient() {\n    $env = getenv('APP_ENV') ?: 'production';\n\n    // Load base configuration\n    $config = HttpClientConfig::load('guzzle');\n\n    // Adjust based on environment\n    switch ($env) {\n        case 'development':\n            // Shorter timeouts for faster feedback during development\n            $config = new HttpClientConfig(\n                httpClientType: $config-&gt;httpClientType,\n                connectTimeout: 1,\n                requestTimeout: 10,\n                idleTimeout: $config-&gt;idleTimeout,\n                maxConcurrent: $config-&gt;maxConcurrent,\n                poolTimeout: $config-&gt;poolTimeout,\n                failOnError: true // Throw errors for immediate feedback\n            );\n            break;\n\n        case 'testing':\n            // Use mock driver for tests\n            $config = new HttpClientConfig(\n                httpClientType: 'custom',\n                connectTimeout: 1,\n                requestTimeout: 1,\n                idleTimeout: 1,\n                maxConcurrent: 1,\n                poolTimeout: 5,\n                failOnError: true\n            );\n\n            // Create a mock client\n            $mockDriver = new MockHttpDriver();\n            // Configure mock responses...\n\n            return (new HttpClient())-&gt;withConfig($config)-&gt;withDriver($mockDriver);\n\n        case 'production':\n            // More conservative timeouts for production\n            $config = new HttpClientConfig(\n                httpClientType: $config-&gt;httpClientType,\n                connectTimeout: 5,\n                requestTimeout: 60,\n                idleTimeout: $config-&gt;idleTimeout,\n                maxConcurrent: 10,\n                poolTimeout: 120,\n                failOnError: false // Handle errors gracefully in production\n            );\n            break;\n    }\n\n    return (new HttpClient())-&gt;withConfig($config);\n}\n\n// Get a properly configured client\n$client = configureHttpClient();\n</code></pre> <p>This approach allows you to adapt your HTTP client configuration to different environments while maintaining a consistent API.</p> <p>In the next chapter, we'll explore how to create and use custom HTTP client implementations for specialized needs.</p>"},{"location":"http/9-1-custom-clients/","title":"Using Custom HTTP Clients","text":"<p>While the Instructor HTTP client API provides built-in support for popular HTTP client libraries (Guzzle, Symfony, and Laravel), there may be cases where you need to integrate with other HTTP client libraries or create specialized implementations. This chapter covers how to create and use custom HTTP client drivers.</p>"},{"location":"http/9-1-custom-clients/#creating-custom-http-client-drivers","title":"Creating Custom HTTP Client Drivers","text":"<p>Creating a custom HTTP client driver involves implementing the <code>CanHandleHttpRequest</code> interface and optionally the <code>CanHandleRequestPool</code> interface for pool support.</p>"},{"location":"http/9-1-custom-clients/#implementing-the-canhandlehttprequest-interface","title":"Implementing the CanHandleHttpRequest Interface","text":"<p>The <code>CanHandleHttpRequest</code> interface requires implementing a single method:</p> <pre><code>interface CanHandleHttpRequest\n{\n    public function handle(HttpClientRequest $request): HttpResponse;\n}\n</code></pre> <p>Here's a template for creating a custom HTTP client driver:</p> <pre><code>namespace YourNamespace\\Http\\Drivers;\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse Exception;\n\nclass CustomHttpDriver implements CanHandleHttpRequest\n{\n    /**\n     * Your custom HTTP client instance\n     */\n    private $yourHttpClient;\n\n    /**\n     * Constructor\n     */\n    public function __construct(\n        protected HttpClientConfig $config,\n        protected ?EventDispatcher $events = null,\n    ) {\n        $this-&gt;events = $events ?? new EventDispatcher();\n\n        // Initialize your HTTP client with the configuration\n        $this-&gt;yourHttpClient = $this-&gt;createYourHttpClient();\n    }\n\n    /**\n     * Handle an HTTP request\n     */\n    public function handle(HttpRequest $request): HttpResponse\n    {\n        $url = $request-&gt;url();\n        $headers = $request-&gt;headers();\n        $body = $request-&gt;body()-&gt;toString();\n        $method = $request-&gt;method();\n        $streaming = $request-&gt;isStreamed();\n\n        // Dispatch event before sending request\n        $this-&gt;events-&gt;dispatch(new HttpRequestSent([\n            'url' =&gt; $url,\n            'method' =&gt; $method,\n            'headers' =&gt; $headers,\n            'body' =&gt; $request-&gt;body()-&gt;toArray(),\n        ]));\n\n        try {\n            // Use your HTTP client to make the request\n            $response = $this-&gt;yourHttpClient-&gt;send($method, $url, [\n                'headers' =&gt; $headers,\n                'body' =&gt; $body,\n                'timeout' =&gt; $this-&gt;config-&gt;requestTimeout,\n                'connect_timeout' =&gt; $this-&gt;config-&gt;connectTimeout,\n                'stream' =&gt; $streaming,\n                // Other options relevant to your client...\n            ]);\n\n            // Dispatch event for successful response\n            $this-&gt;events-&gt;dispatch(new HttpResponseReceived([\n                'statusCode' =&gt; $response-&gt;statusCode()\n            ]));\n\n            // Return the response wrapped in your adapter\n            return new YourHttpResponse($response, $streaming);\n\n        } catch (Exception $e) {\n            // Dispatch event for failed request\n            $this-&gt;events-&gt;dispatch(new HttpRequestFailed([\n                'url' =&gt; $url,\n                'method' =&gt; $method,\n                'headers' =&gt; $headers,\n                'body' =&gt; $request-&gt;body()-&gt;toArray(),\n                'errors' =&gt; $e-&gt;getMessage(),\n            ]));\n\n            // Wrap the exception\n            throw new HttpRequestException($e);\n        }\n    }\n\n    /**\n     * Create your HTTP client instance\n     */\n    private function createYourHttpClient()\n    {\n        // Initialize your HTTP client with appropriate configuration\n        return new YourHttpClient([\n            'connect_timeout' =&gt; $this-&gt;config-&gt;connectTimeout,\n            'timeout' =&gt; $this-&gt;config-&gt;requestTimeout,\n            'idle_timeout' =&gt; $this-&gt;config-&gt;idleTimeout,\n            // Other options...\n        ]);\n    }\n}\n</code></pre>"},{"location":"http/9-1-custom-clients/#creating-a-response-adapter","title":"Creating a Response Adapter","text":"<p>You also need to create a response adapter that implements the <code>HttpResponse</code> interface:</p> <pre><code>namespace YourNamespace\\Http\\Adapters;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Generator;\n\nclass YourHttpResponse implements HttpResponse\n{\n    /**\n     * Constructor\n     */\n    public function __construct(\n        private $yourResponse,\n        private bool $streaming = false\n    ) {}\n\n    /**\n     * Get the response status code\n     */\n    public function statusCode(): int\n    {\n        return $this-&gt;yourResponse-&gt;getStatusCode();\n    }\n\n    /**\n     * Get the response headers\n     */\n    public function headers(): array\n    {\n        return $this-&gt;yourResponse-&gt;getHeaders();\n    }\n\n    /**\n     * Get the response body\n     */\n    public function body(): string\n    {\n        return $this-&gt;yourResponse-&gt;getBody();\n    }\n\n    /**\n     * Stream the response body\n     */\n    public function stream(int $chunkSize = 1): Generator\n    {\n        if (!$this-&gt;streaming) {\n            // For non-streaming responses, just yield the entire body\n            yield $this-&gt;body();\n            return;\n        }\n\n        // For streaming responses, yield chunks\n        $stream = $this-&gt;yourResponse-&gt;getStream();\n\n        while (!$stream-&gt;eof()) {\n            yield $stream-&gt;read($chunkSize);\n        }\n    }\n}\n</code></pre>"},{"location":"http/9-1-custom-clients/#using-your-custom-http-client-driver","title":"Using Your Custom HTTP Client Driver","text":"<p>Once you've implemented your custom driver, you can use it with the <code>HttpClient</code>:</p> <pre><code>use Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\HttpClient;\nuse YourNamespace\\Http\\Drivers\\CustomHttpDriver;\n\n// Create a configuration for your custom driver\n$config = new HttpClientConfig(\n    driver: 'custom',\n    connectTimeout: 3,\n    requestTimeout: 30,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true\n);\n\n// Create your custom driver\n$customDriver = new CustomHttpDriver($config);\n\n// Create a client with your driver using the builder\n$client = (new HttpClientBuilder())-&gt;withDriver($customDriver)-&gt;create();\n\n// Use the client as usual\n$response = $client-&gt;withRequest(new HttpRequest(/* ... */))-&gt;get();\n</code></pre>"},{"location":"http/9-1-custom-clients/#real-world-example-creating-a-curl-driver","title":"Real-World Example: Creating a cURL Driver","text":"<p>Here's a practical example of implementing a custom driver using PHP's cURL extension directly:</p> <pre><code>namespace YourNamespace\\Http\\Drivers;\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse YourNamespace\\Http\\Adapters\\CurlHttpResponse;\n\nclass CurlHttpDriver implements CanHandleHttpRequest\n{\n    /**\n     * Constructor\n     */\n    public function __construct(\n        protected HttpClientConfig $config,\n        protected ?EventDispatcher $events = null,\n    ) {\n        $this-&gt;events = $events ?? new EventDispatcher();\n    }\n\n    /**\n     * Handle an HTTP request\n     */\n    public function handle(HttpRequest $request): HttpResponse\n    {\n        $url = $request-&gt;url();\n        $headers = $request-&gt;headers();\n        $body = $request-&gt;body()-&gt;toString();\n        $method = $request-&gt;method();\n        $streaming = $request-&gt;isStreamed();\n\n        // Dispatch event before sending request\n        $this-&gt;events-&gt;dispatch(new HttpRequestSent(\n            $url,\n            $method,\n            $headers,\n            $request-&gt;body()-&gt;toArray()\n        ));\n\n        try {\n            // Initialize cURL\n            $ch = curl_init();\n\n            // Format headers for cURL\n            $curlHeaders = [];\n            foreach ($headers as $name =&gt; $value) {\n                if (is_array($value)) {\n                    foreach ($value as $v) {\n                        $curlHeaders[] = \"{$name}: {$v}\";\n                    }\n                } else {\n                    $curlHeaders[] = \"{$name}: {$value}\";\n                }\n            }\n\n            // Set cURL options\n            curl_setopt_array($ch, [\n                CURLOPT_URL =&gt; $url,\n                CURLOPT_RETURNTRANSFER =&gt; true,\n                CURLOPT_HTTPHEADER =&gt; $curlHeaders,\n                CURLOPT_CONNECTTIMEOUT =&gt; $this-&gt;config-&gt;connectTimeout,\n                CURLOPT_TIMEOUT =&gt; $this-&gt;config-&gt;requestTimeout,\n                CURLOPT_HEADER =&gt; true, // Include headers in output\n                CURLOPT_FOLLOWLOCATION =&gt; true,\n                CURLOPT_MAXREDIRS =&gt; 5,\n            ]);\n\n            // Set method-specific options\n            switch ($method) {\n                case 'POST':\n                    curl_setopt($ch, CURLOPT_POST, true);\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'PUT':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PUT');\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'PATCH':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PATCH');\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'DELETE':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'DELETE');\n                    if (!empty($body)) {\n                        curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    }\n                    break;\n                case 'HEAD':\n                    curl_setopt($ch, CURLOPT_NOBODY, true);\n                    break;\n                case 'OPTIONS':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'OPTIONS');\n                    break;\n                case 'GET':\n                default:\n                    // GET is the default in cURL\n                    break;\n            }\n\n            // Handle streaming if requested\n            $responseBody = '';\n            $responseHeaders = [];\n\n            if ($streaming) {\n                $tempHandle = null;\n                $tempFile = tempnam(sys_get_temp_dir(), 'curl_stream_');\n                $tempHandle = fopen($tempFile, 'w+');\n\n                curl_setopt($ch, CURLOPT_FILE, $tempHandle);\n                curl_setopt($ch, CURLOPT_WRITEFUNCTION, function($ch, $data) use ($tempHandle) {\n                    return fwrite($tempHandle, $data);\n                });\n\n                curl_setopt($ch, CURLOPT_HEADERFUNCTION, function($ch, $header) use (&amp;$responseHeaders) {\n                    $len = strlen($header);\n                    $header = explode(':', $header, 2);\n                    if (count($header) &lt; 2) {\n                        return $len;\n                    }\n\n                    $name = trim($header[0]);\n                    $value = trim($header[1]);\n\n                    $responseHeaders[$name][] = $value;\n                    return $len;\n                });\n\n                $result = curl_exec($ch);\n                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n\n                if ($result === false) {\n                    throw new \\RuntimeException('cURL error: ' . curl_error($ch));\n                }\n\n                // Rewind the temp file so it can be read\n                rewind($tempHandle);\n\n                // Create streaming response\n                $response = new CurlHttpResponse(\n                    statusCode: $statusCode,\n                    headers: $responseHeaders,\n                    body: '',\n                    stream: $tempHandle,\n                    isStreaming: true,\n                    tempFile: $tempFile\n                );\n            } else {\n                // For non-streaming responses, get the full response\n                $result = curl_exec($ch);\n\n                if ($result === false) {\n                    throw new \\RuntimeException('cURL error: ' . curl_error($ch));\n                }\n\n                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n                $headerSize = curl_getinfo($ch, CURLINFO_HEADER_SIZE);\n\n                // Extract headers and body\n                $headerText = substr($result, 0, $headerSize);\n                $responseBody = substr($result, $headerSize);\n\n                // Parse headers\n                $headers = explode(\"\\r\\n\", $headerText);\n                foreach ($headers as $header) {\n                    $parts = explode(':', $header, 2);\n                    if (count($parts) === 2) {\n                        $name = trim($parts[0]);\n                        $value = trim($parts[1]);\n                        $responseHeaders[$name][] = $value;\n                    }\n                }\n\n                // Create regular response\n                $response = new CurlHttpResponse(\n                    statusCode: $statusCode,\n                    headers: $responseHeaders,\n                    body: $responseBody\n                );\n            }\n\n            // Clean up cURL\n            curl_close($ch);\n\n            // Dispatch event for successful response\n            $this-&gt;events-&gt;dispatch(new HttpResponseReceived([\n                'statusCode' =&gt; $response-&gt;statusCode()\n            ]));\n\n            return $response;\n\n        } catch (\\Exception $e) {\n            // Clean up if needed\n            if (isset($ch) &amp;&amp; is_resource($ch)) {\n                curl_close($ch);\n            }\n\n            // Dispatch event for failed request\n            $this-&gt;events-&gt;dispatch(new HttpRequestFailed([\n                'url' =&gt; $url,\n                'methods' =&gt; $method,\n                'headers' =&gt; $headers,\n                'body' =&gt; $request-&gt;body()-&gt;toArray(),\n                'errors' =&gt; $e-&gt;getMessage()\n            ]));\n\n            // Wrap the exception\n            throw new HttpRequestException($e);\n        }\n    }\n}\n</code></pre> <p>And here's the corresponding response adapter:</p> <pre><code>namespace YourNamespace\\Http\\Adapters;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;\nuse Generator;\n\nclass CurlHttpResponse implements HttpResponse\n{\n    private $stream;\n    private $tempFile;\n    private $isStreaming;\n\n    /**\n     * Constructor\n     */\n    public function __construct(\n        private int $statusCode,\n        private array $headers,\n        private string $body,\n        $stream = null,\n        bool $isStreaming = false,\n        ?string $tempFile = null\n    ) {\n        $this-&gt;stream = $stream;\n        $this-&gt;isStreaming = $isStreaming;\n        $this-&gt;tempFile = $tempFile;\n    }\n\n    /**\n     * Destructor - clean up temp files\n     */\n    public function __destruct()\n    {\n        if ($this-&gt;stream &amp;&amp; is_resource($this-&gt;stream)) {\n            fclose($this-&gt;stream);\n        }\n\n        if ($this-&gt;tempFile &amp;&amp; file_exists($this-&gt;tempFile)) {\n            unlink($this-&gt;tempFile);\n        }\n    }\n\n    /**\n     * Get the response status code\n     */\n    public function statusCode(): int\n    {\n        return $this-&gt;statusCode;\n    }\n\n    /**\n     * Get the response headers\n     */\n    public function headers(): array\n    {\n        return $this-&gt;headers;\n    }\n\n    /**\n     * Get the response body\n     */\n    public function body(): string\n    {\n        if ($this-&gt;isStreaming &amp;&amp; $this-&gt;stream) {\n            // For streaming responses, read the entire file\n            rewind($this-&gt;stream);\n            $contents = stream_get_contents($this-&gt;stream);\n            rewind($this-&gt;stream);\n            return $contents;\n        }\n\n        return $this-&gt;body;\n    }\n\n    /**\n     * Stream the response body\n     */\n    public function stream(int $chunkSize = 1): Generator\n    {\n        if ($this-&gt;isStreaming &amp;&amp; $this-&gt;stream) {\n            // For streaming responses, yield chunks from the file\n            rewind($this-&gt;stream);\n\n            while (!feof($this-&gt;stream)) {\n                yield fread($this-&gt;stream, $chunkSize);\n            }\n        } else {\n            // For non-streaming responses, yield the entire body\n            yield $this-&gt;body;\n        }\n    }\n}\n</code></pre>"},{"location":"http/9-1-custom-clients/#registering-custom-http-client-drivers","title":"Registering Custom HTTP Client Drivers","text":"<p>To use your custom driver, you can register it with the driver factory or use it directly with the HttpClientBuilder:</p> <pre><code>use Cognesy\\Http\\HttpClientBuilder;\nuse YourNamespace\\Http\\Drivers\\CustomHttpDriver;\n\n// Use your custom driver directly with the builder\n$customDriver = new CustomHttpDriver($config);\n$client = (new HttpClientBuilder())\n    -&gt;withDriver($customDriver)\n    -&gt;create();\n</code></pre>"},{"location":"http/9-1-custom-clients/#adding-custom-http-client-to-configuration","title":"Adding Custom HTTP Client to Configuration","text":"<p>Use your custom driver in the HTTP client configuration file (<code>config/http-client.php</code>):</p> <pre><code>// http-client.php configuration file\n// ...\n    'clients' =&gt; [\n        'my-custom-client' =&gt; [\n            // Our custom driver type\n            'httpDriverType' =&gt; 'my-custom-driver',\n            // Other driver-specific options...\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n// ...\n</code></pre>"},{"location":"http/9-1-custom-clients/#using-your-custom-http-client","title":"Using Your Custom HTTP Client","text":"<p>After adding the driver to the configuration, you can use it in your code:</p> <pre><code>use Cognesy\\Http\\HttpClient;\n\n// Create a client with your custom driver\n$client = HttpClient::using('my-custom-client');\n</code></pre>"},{"location":"http/9-1-custom-clients/#using-custom-http-clients-in-configuration-files","title":"Using Custom HTTP Clients in Configuration Files","text":"<p>Or you can refer to it in your LLM connections configuration:</p> <pre><code>    // llm-connections.php configuration file\n    // ...\n        'a21' =&gt; [\n            'providerType' =&gt; 'a21',\n            'apiUrl' =&gt; 'https://api.ai21.com/studio/v1',\n            'apiKey' =&gt; Env::get('A21_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'jamba-1.5-mini',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 256_000,\n            'maxOutputLength' =&gt; 4096,\n            // Our custom HTTP client\n            // Select your HTTP configuration via HttpClientBuilder\n        ],\n    // ...\n</code></pre>"},{"location":"instructor/cli_tools/","title":"CLI Tools","text":"<p>Instructor comes with command line tools:  - <code>./vendor/bin/instructor-setup publish</code>: Publishes configuration files and prompt templates to your project directory  - <code>./vendor/bin/instructor-hub</code>: Displays and executes Instructor examples</p> <p>Additional tool included with Instructor (under development):  - <code>./vendor/bin/tell \"&lt;prompt to LLM&gt;\"</code>: Interacts with LLMs from the command line</p>"},{"location":"instructor/introduction/","title":"Introduction","text":""},{"location":"instructor/introduction/#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to get structured, validated data from multiple types of inputs: text, chat messages, or images. It is powered by Large Language Models (LLMs).</p> <p>The library is inspired by the Instructor for Python created by Jason Liu.</p>"},{"location":"instructor/introduction/#learn-more","title":"Learn More...","text":"<p>          Check how to set up Instructor in your project and start processing data with LLMs      <pre><code>&lt;Card\n    title=\"Concepts\"\n    icon=\"shapes\"\n    href=\"/instructor/concepts/overview\"\n&gt;\n    Read more about basic concepts behind Instructor\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Essentials\"\n    icon=\"hammer\"\n    href=\"/instructor/essentials\"\n&gt;\n    Learn Instructor features and capabilities\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Cookbooks\"\n    icon=\"book\"\n    href=\"/cookbook/introduction\"\n&gt;\n    Browse examples to see Instructor in action and find out how to use it in your projects\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Internals\"\n    icon=\"gears\"\n    href=\"/instructor/internals\"\n&gt;\n    Deep dive into Instructor internals and low level mechanisms\n&lt;/Card&gt;\n</code></pre> <p></p>"},{"location":"instructor/introduction/#feature-highlights","title":"Feature Highlights","text":"<p>Instructor is designed to make it easy to process data with LLMs in PHP. Here are some of the key features of the library:</p>"},{"location":"instructor/introduction/#core-features","title":"Core features","text":"<ul> <li>Get structured responses from LLM inference</li> <li>Validation of returned data</li> <li>Automated retries in case of errors when LLM responds with invalid data</li> </ul>"},{"location":"instructor/introduction/#flexible-inputs","title":"Flexible inputs","text":"<ul> <li>Process various types of input data: text, series of chat messages or images</li> <li>'Structured-to-structured' processing - provide object or array as an input and get object with the results of inference back</li> <li>Demonstrate examples to improve the quality of inference</li> </ul>"},{"location":"instructor/introduction/#customizable-outputs","title":"Customizable outputs","text":"<ul> <li>Define response data model the way to need: type-hinted classes, JSON Schema arrays, or dynamically define your data shapes with Structures</li> <li>Customize prompts and retry prompts</li> <li>Use attributes or PHP DocBlocks to provide additional instructions for LLM</li> <li>Customize response model processing by providing your own implementation of schema, deserialization, validation and transformation interfaces</li> </ul>"},{"location":"instructor/introduction/#sync-and-streaming-support","title":"Sync and streaming support","text":"<ul> <li>Receive synchronous or streaming responses</li> <li>Get partial updates &amp; stream completed sequence items</li> </ul>"},{"location":"instructor/introduction/#observability","title":"Observability","text":"<ul> <li>Get detailed insight into internal processing via events</li> </ul>"},{"location":"instructor/introduction/#support-for-multiple-llms-api-providers","title":"Support for multiple LLMs / API providers","text":"<ul> <li>Use multiple LLM API providers (incl. OpenAI,  Anthropic, Cohere, Azure, Groq, Mistral, Fireworks AI, Ollama, OpenRouter, Together AI)</li> <li>Use local models with Ollama</li> </ul>"},{"location":"instructor/introduction/#documentation-and-examples","title":"Documentation and examples","text":"<ul> <li>Learn more from growing documentation and 50+ cookbooks</li> </ul>"},{"location":"instructor/introduction/#instructor-in-other-languages","title":"Instructor in Other Languages","text":"<p>Instructor has been implemented in various technology stacks. Check out implementations in other languages below:</p> <ul> <li>Python (original)</li> <li>Javascript (port)</li> <li>Elixir (port)</li> <li>Ruby (port)</li> <li>Go (port)</li> </ul> <p>If you want to port Instructor to another language, please reach out to us on Twitter we'd love to help you get started!</p>"},{"location":"instructor/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with Instructor in your PHP project in under 5 minutes.</p> <p>For detailed setup instructions, see Setup.</p>"},{"location":"instructor/quickstart/#install-instructor-with-composer","title":"Install Instructor with Composer","text":"<p>Run following command in your terminal:</p> <pre><code>composer require cognesy/instructor-php\n</code></pre>"},{"location":"instructor/quickstart/#create-and-run-example","title":"Create and Run Example","text":""},{"location":"instructor/quickstart/#step-1-prepare-your-openai-api-key","title":"Step 1: Prepare your OpenAI API Key","text":"<p>In this example, we'll use OpenAI as the LLM provider. You can get it from the OpenAI dashboard.</p>"},{"location":"instructor/quickstart/#step-2-create-a-new-php-file","title":"Step 2: Create a New PHP File","text":"<p>In your project directory, create a new PHP file <code>test-instructor.php</code>:</p> <pre><code>&lt;?php\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Set up OpenAI API key\n$apiKey = 'your-openai-api-key';\nputenv(\"OPENAI_API_KEY=\" . $apiKey);\n// WARNING: In real project you should set up API key in .env file.\n\n// Step 1: Define target data structure(s)\nclass City {\n    public string $name;\n    public string $country;\n    public int $population;\n}\n\n// Step 2: Use Instructor to run LLM inference\n$city = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withResponseClass(City::class)\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\nvar_dump($city);\n</code></pre> <p> You should never put your API keys directly in your real project code to avoid getting them compromised. Set them up in your .env file. </p>"},{"location":"instructor/quickstart/#step-3-run-the-example","title":"Step 3: Run the Example","text":"<p>Now, you can run the example:</p> <pre><code>php test-instructor.php\n\n# Output:\n# object(City)#1 (3) {\n#   [\"name\"]=&gt;\n#   string(5) \"Paris\"\n#   [\"country\"]=&gt;\n#   string(6) \"France\"\n#   [\"population\"]=&gt;\n#   int(2148000)\n# }\n</code></pre>"},{"location":"instructor/quickstart/#next-steps","title":"Next Steps","text":"<p>You can start using Instructor in your project right away after installation.</p> <p>But it's recommended to publish configuration files and prompt templates to your project directory, so you can customize the library's behavior and use your own prompt templates.</p> <p>You should also set up LLM provider API keys in your <code>.env</code> file instead of putting them directly in your code.</p> <p>See Setup Instructions for more details.</p>"},{"location":"instructor/setup/","title":"Setup","text":""},{"location":"instructor/setup/#overview","title":"Overview","text":"<p>Full Instructor setup consists of following steps:  - Step 1: Install Instructor via Composer  - Step 2: Publish Instructor assets (configurations and prompts) to your project directory  - Step 3: Set up LLM provider API key(s) in your .env file  - Step 4: Set configuration location in your .env file (optional)</p>"},{"location":"instructor/setup/#step-1-install-instructor-via-composer","title":"Step 1: Install Instructor via Composer","text":"<p>You can install Instructor via Composer by running:</p> <pre><code>composer require cognesy/instructor-php\n</code></pre>"},{"location":"instructor/setup/#step-2-publish-instructor-files-to-your-project","title":"Step 2: Publish Instructor Files to Your Project","text":"<p>Instructor comes with a set of configuration files and prompt templates that you can publish to your project directory.</p> <p>This will allow you to customize the library's behavior and use different prompt templates.</p> <p>These files can be found in the <code>vendor/cognesy/instructor-php</code> directory: - <code>.env-dist</code> - Environment variables for API keys and configuration paths - <code>/config/*.php</code> - Configurations of Instructor modules - <code>/prompts/*</code> - Prompt templates for generating structured data from text</p> <p>You can publish these files to your project directory by running following command:</p> <pre><code>./vendor/bin/instructor-setup publish \\n\n  --target-config-dir=&lt;target config dir location&gt;\n  --target-prompts-dir=&lt;target prompts dir location&gt;\n  --target-env-file=&lt;target .env file location&gt;\n</code></pre> <p>You can also manually copy the required files to your project directory.</p> <p> Read more: - Framework Integration     - Laravel Projects     - Symfony Projects     - Custom Framework Location - Use CLI Tool to publish Instructor assets - Manual Setup </p>"},{"location":"instructor/setup/#step-3-set-up-llm-provider-api-keys","title":"Step 3: Set Up LLM Provider API Key(s)","text":"<p>If you're using commercial LLM providers like OpenAI, you'll need to set up API keys in your project's <code>.env</code> file.</p> <p>Open the <code>.env</code> file in your project directory and set up API keys for the LLM providers you plan to use. You can find the keys in the respective provider's dashboard.</p> <p>```ini .env</p>"},{"location":"instructor/setup/#openai-default-provider","title":"OpenAI (default provider)","text":"<p>OPENAI_API_KEY='your-openai-api-key' <pre><code>Check `.env-dist` for other API keys Instructor uses in its default configuration files.\n\n\n### Step 4: Set Configuration Location (optional)\n\nInstructor uses a configuration directory to store its settings, e.g. LLM provider configurations.\n\nYou can set the path to this directory via `Settings::setPath('/path/to/config')` in your code.\n\nBut to make it easier you can just set the value in your `.env` file. `Settings` will pick it up automatically\nfrom there. This way you don't have to set it in every script.\n\n```ini .env\nINSTRUCTOR_CONFIG_PATHS='/path/to/your/config/dir/,another/path'\n</code></pre></p> <p> <code>INSTRUCTOR_CONFIG_PATHS</code> is set automatically if you use the Instructor CLI tool to publish assets. </p>"},{"location":"instructor/setup/#framework-integration","title":"Framework Integration","text":""},{"location":"instructor/setup/#laravel-projects","title":"Laravel Projects","text":"<p>For Laravel applications, it's recommended to align with the framework's directory structure:</p> <pre><code>./vendor/bin/instructor-setup publish \\\n    --target-config-dir=config/instructor \\\n    --target-prompts-dir=resources/prompts \\\n    --target-env-file=.env\n</code></pre> <p>This will: - Place configuration files in Laravel's <code>config</code> directory - Store prompts in Laravel's <code>resources</code> directory - Use Laravel's default <code>.env</code> file location</p> <p>After publishing, you can load Instructor configuration in your <code>config/app.php</code> or create a dedicated service provider.</p>"},{"location":"instructor/setup/#symfony-projects","title":"Symfony Projects","text":"<p>For Symfony applications, use the standard Symfony directory structure:</p> <pre><code>./vendor/bin/instructor-setup publish \\\n    --target-config-dir=config/packages/instructor \\\n    --target-prompts-dir=resources/instructor/prompts \\\n    --target-env-file=.env\n</code></pre> <p>This will: - Place configuration in Symfony's package configuration directory - Store prompts in Symfony's <code>resources</code> directory - Use Symfony's default <code>.env</code> file location</p> <p>For Symfony Flex applications, you may want to create a recipe to automate this setup process.</p>"},{"location":"instructor/setup/#custom-framework-location","title":"Custom Framework Location","text":"<p>You can use environment variables to set the location of configuration files: <pre><code>INSTRUCTOR_CONFIG_PATHS=/path/to/config,another/path\n</code></pre></p> <p>This allows you to maintain consistent paths across your application without specifying them in each command.</p>"},{"location":"instructor/setup/#using-cli-tool","title":"Using CLI Tool","text":"<p>After installing Instructor via Composer, you may want to publish the library's configuration files and resources to your project, so you can modify them according to your needs. You can do this either manually or automatically using the provided CLI tool.</p> <pre><code>./vendor/bin/instructor-setup publish\n</code></pre> <p>By default, this command will: 1. Copy configuration files from <code>vendor/cognesy/instructor-php/config</code> to <code>config/instructor/</code> 2. Copy prompt templates from <code>vendor/cognesy/instructor-php/prompts</code> to <code>resources/prompts/</code> 3. Merge (or copy) <code>vendor/cognesy/instructor-php/.env-dist</code> file to <code>.env</code> with environment variables</p>"},{"location":"instructor/setup/#command-options","title":"Command Options","text":"<ul> <li><code>-c, --target-config-dir=DIR</code> - Custom directory for configuration files (default: <code>config/instructor</code>)</li> <li><code>-p, --target-prompts-dir=DIR</code> - Custom directory for prompt templates (default: <code>resources/prompts</code>)</li> <li><code>-e, --target-env-file=FILE</code> - Custom location for .env file (default: <code>.env</code>)</li> <li><code>-l, --log-file=FILE</code> - Optional log file path to track the publishing process</li> <li><code>--no-op</code> - Dry run mode - shows what would be copied without making changes</li> </ul>"},{"location":"instructor/setup/#example-usage","title":"Example Usage","text":"<pre><code>./vendor/bin/instructor-setup publish \\\n    --target-config-dir=./config/instructor \\\n    --target-prompts-dir=./resources/prompts \\\n    --target-env-file=.env\n</code></pre> <p> When merging <code>.env</code> files, the tool will only add missing variables, preserving your existing file content, formatting and comments. </p>"},{"location":"instructor/setup/#manual-setup","title":"Manual Setup","text":"<p>If you prefer to set up Instructor manually or need more control over the process, you can copy the required files directly:</p>"},{"location":"instructor/setup/#configuration-files","title":"Configuration Files","text":"<p><pre><code># Create config in your preferred directory\nmkdir -p config/instructor\n\n# Copy configuration files\ncp -r vendor/cognesy/instructor-php/config/* config/instructor/\n</code></pre> These files contain LLM API connection settings and Instructor's behavior configuration.</p>"},{"location":"instructor/setup/#prompt-templates","title":"Prompt Templates","text":"<p><pre><code># Create prompts in your preferred directory\nmkdir -p resources/prompts\n\n# Copy prompt templates\ncp -r vendor/cognesy/instructor-php/prompts/* resources/prompts/\n</code></pre> Prompt templates define how Instructor communicates with LLMs for different tasks.</p>"},{"location":"instructor/setup/#environment-configuration","title":"Environment Configuration","text":"<p>If .env doesn't exist, copy the environment template:</p> <pre><code>[ ! -f .env ] &amp;&amp; cp vendor/cognesy/instructor-php/config/.env-dist .env\n</code></pre> <p>Add key values to your .env: <pre><code># OpenAI API key\nOPENAI_API_KEY=your_api_key\n# Other API keys (if you use other LLM providers)\n# ...\n\n# Set up Instructor configuration path (optional)\nINSTRUCTOR_CONFIG_PATHS='&lt;path/to/config&gt;,&lt;another/path&gt;'\n</code></pre></p>"},{"location":"instructor/upgrade/","title":"Upgrading Instructor","text":"<p>Recent changes to the Instructor package may require some manual fixes in your codebase.</p>"},{"location":"instructor/upgrade/#step-1-update-the-package","title":"Step 1: Update the package","text":"<p>Run the following command in your CLI:</p> <pre><code>composer update cognesy/instructor\n</code></pre>"},{"location":"instructor/upgrade/#step-2-config-files","title":"Step 2: Config files","text":"<p>Correct your config files to use new namespaces.</p>"},{"location":"instructor/upgrade/#step-3-instructor-config-path","title":"Step 3: Instructor config path","text":"<p>Correct INSTRUCTOR_CONFIG_PATHS in .env file to <code>config/instructor</code> (or your custom path).</p>"},{"location":"instructor/upgrade/#step-4-codebase","title":"Step 4: Codebase","text":"<p>Make sure that your code follows new namespaces.</p> <p>Suggestion: use IDE search and replace to find and replace old namespaces with new ones.</p>"},{"location":"instructor/advanced/function_calls/","title":"Function calls","text":""},{"location":"instructor/advanced/function_calls/#functioncall-helper-class","title":"FunctionCall helper class","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"instructor/advanced/function_calls/#extracting-arguments-for-a-function","title":"Extracting arguments for a function","text":"<pre><code>&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Save user data to storage */\nfunction saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromFunctionName('saveUser'),\n)-&gt;get();\n\n// call the function with the extracted arguments\nsaveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/function_calls/#extracting-arguments-for-a-method-call","title":"Extracting arguments for a method call","text":"<pre><code>&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass DataStore {\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) {\n        // ...\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromMethodName(Datastore::class, 'saveUser'),\n)-&gt;get();\n\n// call the function with the extracted arguments\n(new DataStore)-&gt;saveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/function_calls/#extracting-arguments-for-a-callable","title":"Extracting arguments for a callable","text":"<pre><code>&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Save user data to storage */\n$callable = function saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromCallable($callable),\n)-&gt;get();\n\n// call the function with the extracted arguments\n$callable(...$args);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/model_options/","title":"Model options","text":""},{"location":"instructor/advanced/model_options/#changing-llm-model-and-options","title":"Changing LLM model and options","text":"<p>You can specify model and other options that will be passed to LLM endpoint.</p> <p>Commonly used option supported by many providers is <code>temperature</code>, which controls randomness of the output.</p> <p>Lower values make the output more deterministic, while higher values make it more random.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: [\n        // custom temperature setting\n        'temperature' =&gt; 0.0\n        // ... other options - e.g. provider or model specific\n    ],\n)-&gt;get();\n</code></pre> <p>NOTE: Please note that many options might be specific to the provider or even some model that you are using.</p>"},{"location":"instructor/advanced/model_options/#customizing-configuration","title":"Customizing configuration","text":"<p>You can pass a custom LLM configuration to the Instructor.</p> <p>This allows you to specify your own API key, base URI or, which might be helpful in the case you are using OpenAI - organization.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;use Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\n\n// Create instance of OpenAI client initialized with custom parameters\n$config = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: $yourApiKey,\n    endpoint: '/chat/completions',\n    metadata: ['organization' =&gt; ''],\n    model: 'gpt-4o-mini',\n    maxTokens: 128,\n    // configure HTTP via HttpClientBuilder or facade-level methods\n    driver: 'openai',\n));\n\n/// Get Instructor with the default configuration overridden with your own\n$structuredOutput = (new StructuredOutput)-&gt;withLLMConfig($driver);\n\n$person = $structuredOutput-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    options: ['temperature' =&gt; 0.0],\n)-&gt;get();\n</code></pre>"},{"location":"instructor/advanced/partials/","title":"Partials","text":""},{"location":"instructor/advanced/partials/#partial-updates","title":"Partial updates","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated.</p> <p>You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p>This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive partial results define <code>onPartialUpdate()</code> callback that will be called on every update of the deserializad object.</p> <p>Instructor is smart about updates, it calculates and compares hashes of the previous and newly deserialized version of the model, so you won't get them on every token received, but only when any property of the object is updated.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nfunction updateUI($person) {\n    // Here you get partially completed Person object update UI with the partial result\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        options: ['stream' =&gt; true]\n    )\n    -&gt;onPartialUpdate(\n        fn($partial) =&gt; updateUI($partial)\n    )\n    -&gt;get();\n\n// Here you get completed and validated Person object\n$this-&gt;db-&gt;save($person); // ...for example: save to DB\n</code></pre> <p>Partially updated data is not validated while it is received and deserialized.</p> <p>The object returned from <code>get()</code> call is fully validated, so you can safely work with it, e.g. save it to the database.</p>"},{"location":"instructor/advanced/partials/#streaming-responses","title":"Streaming responses","text":"<p>You can get a stream of responses by calling the <code>stream()</code> method instead of <code>get()</code>. The <code>stream()</code> method is available on both <code>StructuredOutput</code> and <code>PendingStructuredOutput</code> instances.</p> <pre><code>// Direct streaming\n$stream = $structuredOutput-&gt;stream();\n\n// Or via create() method\n$pending = $structuredOutput-&gt;create();\n$stream = $pending-&gt;stream();\n</code></pre> <p>Both approaches return a <code>StructuredOutputStream</code> object, which gives you access to the response streamed from LLM and processed by Instructor into structured data.</p>"},{"location":"instructor/advanced/partials/#structuredoutputstream-methods","title":"StructuredOutputStream Methods","text":"<p>The <code>StructuredOutputStream</code> class provides comprehensive methods for processing streaming responses:</p>"},{"location":"instructor/advanced/partials/#core-streaming-methods","title":"Core Streaming Methods","text":"<ul> <li><code>partials()</code>: Returns an iterable of partial updates from the stream. Only final update is validated, partial updates are only deserialized and transformed.</li> <li><code>sequence()</code>: Dedicated to processing <code>Sequence</code> response models - returns only completed items in the sequence.</li> <li><code>responses()</code>: Generator of partial LLM responses as they are received.</li> </ul>"},{"location":"instructor/advanced/partials/#result-access-methods","title":"Result Access Methods","text":"<ul> <li><code>finalValue()</code>: Get the final parsed result (blocks until completion).</li> <li><code>finalResponse()</code>: Get the final LLM response (blocks until completion).</li> <li><code>lastUpdate()</code>: Returns the last object received and processed by Instructor.</li> <li><code>lastResponse()</code>: Returns the last received LLM response.</li> </ul>"},{"location":"instructor/advanced/partials/#utility-methods","title":"Utility Methods","text":"<ul> <li><code>usage()</code>: Get token usage statistics from the streaming response.</li> </ul>"},{"location":"instructor/advanced/partials/#example-streaming-partial-responses","title":"Example: streaming partial responses","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $update) {\n    // render updated person view\n    // for example:\n    $view-&gt;updateView($update); // render the updated person view\n}\n\n// now you can get final, fully processed person object\n$person = $stream-&gt;finalValue();\n// ...and for example save it to the database\n$db-&gt;savePerson($person);\n</code></pre>"},{"location":"instructor/advanced/partials/#example-streaming-sequence-items","title":"Example: streaming sequence items","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)\n    -&gt;with(\n        messages: \"Jason is 28 years old, Amanda is 26 and John (CEO) is 40.\",\n        responseModel: Sequence::of(Participant::class),\n    )\n    -&gt;stream();\n\nforeach ($stream-&gt;sequence() as $update) {\n    // append last completed item from the sequence\n    // for example:\n    $view-&gt;appendParticipant($update-&gt;last());\n}\n\n// now you can get final, fully processed sequence of participants\n$participants = $stream-&gt;finalValue();\n// ...and for example save it to the database\n$db-&gt;saveParticipants($participants-&gt;toArray());\n</code></pre>"},{"location":"instructor/advanced/prompts/","title":"Prompt Templates","text":""},{"location":"instructor/advanced/prompts/#overview","title":"Overview","text":"<p>As your applications grow in complexity, your prompts may become large and complex, with multiple variables and metadata. Managing these prompts can become a challenge, especially when you need to reuse them across different parts of your application. Large prompts are also hard to maintain if they are part of your codebase.</p> <p><code>Prompt</code> addon provides a powerful and flexible way to manage your prompts. It supports multiple template engines (Twig, Blade), prompt metadata, variable injection, and validation.</p>"},{"location":"instructor/advanced/prompts/#what-are-prompts","title":"What are Prompts","text":"<p>Prompts in Instructor are based on structured text templates that can be rendered to text or series of chat messages. As many of your prompts will be dynamically generated based on input data, you can use syntax of one of the supported template engines (Twig, Blade) to define your prompts.</p> <p>This document will be using Twig syntax for prompt templates for simplicity and consistency, but you can use Blade syntax in your prompts if you prefer it.</p> <ul> <li>For more information on Twig syntax see Twig documentation.</li> <li>For more information on Blade syntax see Blade documentation.</li> </ul>"},{"location":"instructor/advanced/prompts/#basic-prompt-template","title":"Basic Prompt Template","text":"<p>Example prompt template in Twig: <pre><code>Hello, world.\n</code></pre></p>"},{"location":"instructor/advanced/prompts/#prompt-template-with-variables","title":"Prompt Template with Variables","text":"<p>You can define variables in your prompt templates and inject values when rendering the prompt.</p> <pre><code>Hello, {{ name }}!\n</code></pre>"},{"location":"instructor/advanced/prompts/#chat-messages","title":"Chat Messages","text":"<p>You can define chat messages in your prompts, which can be used to generate a sequence of messages for LLM chat APIs.</p> <pre><code>&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;What is the capital of {{ country }}?&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#prompt-template-metadata","title":"Prompt Template Metadata","text":"<p>We recommend to preface each prompt with front matter - a block of metadata that describes the prompt and its variables. This metadata can be used for validation, documentation, and schema generation.</p> <pre><code>{#---\ndescription: Capital finder template\nvariables:\n    country:\n        description: Country name\n        type: string\n        default: France\nschema:\n    name: capital\n    properties:\n        name:\n            description: Capital city name\n            type: string\n    required: [name]\n---#}\n&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;What is the capital of {{ country }}?&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#template-libraries","title":"Template Libraries","text":"<p>Instructor allows you to define multiple <code>template libraries</code> in your app. Library is just a collection of prompt templates which is stored under a specific directory. Library can have a nested structure, which allows you to organize your prompts in a way that makes sense for your application.</p> <p>Library properties are specified in <code>config/prompt.php</code> configuration file.</p> <p>where you can define:  - <code>templateEngine</code> - template engine used for prompts in this library,  - <code>resourcePath</code> - path to prompt templates,  - <code>cachePath</code> - path to compiled templates,  - <code>extension</code> - file extension for prompt templates,  - <code>frontMatterTags</code> - start and end tags for front matter,  - <code>frontMatterFormat</code> - format of front matter (yaml, json, toml),  - <code>metadata</code> - engine-specific configuration.</p> <p>Instructor comes with 3 default prompt libraries:  - <code>system</code> - prompt templates used by Instructor itself,  - <code>demo-twig</code> - demo prompt templates using Twig template engine,  - <code>demo-blade</code> - demo prompt templates using Blade template engine.</p> <p>Instructor's does not specify how you should organize or manage your prompt templates, but it provides a flexible way to do it in a way that suits your application.</p>"},{"location":"instructor/advanced/prompts/#using-prompt-templates","title":"Using Prompt Templates","text":""},{"location":"instructor/advanced/prompts/#rendering-a-simple-prompt","title":"Rendering a Simple Prompt","text":"<p>To get started, you can create and render a simple prompt defined in the bundled library using the <code>Prompt::using</code> or <code>Prompt::make</code> methods. Here's how you can use them:</p> <p><pre><code>&lt;?php\nuse Cognesy\\Template\\Template;\n\n// Basic example using \"using-&gt;get-&gt;with\" syntax\n$prompt = Template::using('demo-twig')-&gt;get('hello')-&gt;with(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre> Or, using the shorthand <code>make()</code> syntax:</p> <pre><code>&lt;?php\n$prompt = Template::make('demo-twig:hello')-&gt;with(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#rendering-a-chat-template","title":"Rendering a Chat Template","text":"<p>The Prompt class can also render prompts directly as chat-style messages:</p> <pre><code>&lt;?php\n$messages = Template::messages('demo-twig:hello', ['name' =&gt; 'World']);\n\nprint_r($messages-&gt;toArray());\n// Outputs:\n// [\n//     ['role' =&gt; 'user', 'content' =&gt; 'Hello, World!']\n// ]\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#custom-configuration-and-template-content","title":"Custom Configuration and Template Content","text":"<p>If you need to customize the configuration or set the template content directly, you can do so with additional methods:</p> <pre><code>&lt;?php\nuse Cognesy\\Template\\Config\\TemplateEngineConfig;use Cognesy\\Template\\Enums\\TemplateEngineType;\n\n// Setting custom configuration\n$config = new TemplateEngineConfig(\n    templateEngine: TemplateEngineType::Twig,\n    resourcePath: '',\n    cachePath: '/tmp/cache',\n    extension: '.twig',\n);\n\n$prompt = new Template();\n$prompt-&gt;withConfig($config)\n       -&gt;withTemplateContent('Hello, {{ name }}!')\n       -&gt;withValues(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#in-memory-templates","title":"In Memory Templates","text":"<p>If you need to create an inline prompt (without saving it to a file), you can use following syntax:</p> <pre><code>&lt;?php\n$prompt = Template::twig() // or Template::blade() for Blade syntax\n    -&gt;withTemplateContent('Hello, {{ name }}!')\n    -&gt;withValues(['name' =&gt; 'World'])\n    -&gt;toText();\n?&gt;\n</code></pre> <p>There's shorter syntax for creating in-memory prompts:</p> <pre><code>&lt;?php\n$prompt = Template::twig() // or Template::blade() for Blade syntax\n    -&gt;from('Hello, {{ name }}!')\n    -&gt;with(['name' =&gt; 'World'])\n    -&gt;toText();\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#handling-template-variables","title":"Handling Template Variables","text":"<p>To check which variables are available in a prompt template:</p> <pre><code>&lt;?php\n$prompt = Template::using('demo-twig')\n    -&gt;withTemplateContent('Hello, {{ name }}!')\n    -&gt;withValues(['name' =&gt; 'World']);\n\n$variables = $prompt-&gt;variables();\n\nprint_r($variables); // Outputs: ['name']\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#loading-templates-by-name-and-using-dsns","title":"Loading Templates by Name and Using DSNs","text":"<p>For more flexible template loading, you can load templates by name or use a 'DSN-like' (Data Source Name) syntax:</p> <pre><code>&lt;?php\n// Load a template by name using specified library 'demo-blade'\n$prompt = Template::using('demo-blade')-&gt;withTemplate('hello');\necho $prompt-&gt;template();\n\n// Load a template from specified library using DSN syntax\n$prompt = Template::fromDsn('demo-blade:hello')-&gt;with(['name' =&gt; 'World']);\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/prompts/#converting-to-messages-with-markup","title":"Converting to Messages with Markup","text":"<p>The Prompt class also supports converting templates containing chat-specific markup into structured messages:</p> <p>Here is an example XML that can be used to generate a sequence of chat messages: <pre><code>&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;Hello, {{ name }}&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre></p> <p>And here is how you use <code>Prompt</code> class to convert XML template into a sequence of messages:</p> <pre><code>&lt;?php\n\n$prompt = Template::using('demo-blade')\n    -&gt;withTemplateContent('&lt;chat&gt;&lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;&lt;message role=\"user\"&gt;Hello, {{ $name }}&lt;/message&gt;&lt;/chat&gt;')\n    -&gt;withValues(['name' =&gt; 'assistant']);\n\n$messages = $prompt-&gt;toMessages();\n\necho $messages-&gt;toArray();\n// Outputs:\n// [\n//     ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant.'],\n//     ['role' =&gt; 'user', 'content' =&gt; 'Hello, assistant']\n// ]\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/sequences/","title":"Sequences","text":""},{"location":"instructor/advanced/sequences/#extracting-sequences-of-objects","title":"Extracting Sequences of Objects","text":"<p>Sequence is a wrapper class that can be used to represent a list of objects to be extracted by Instructor from provided context.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <pre><code>&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new StructuredOutput)\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    )-&gt;get();\n</code></pre>"},{"location":"instructor/advanced/sequences/#streaming-sequences","title":"Streaming Sequences","text":"<p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <p>NOTE This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive sequence updates provide a callback via Instructor's <code>onSequenceUpdate()</code> that will be called each  time a new item is received from LLM.</p> <p>The callback provided a full sequence that has been retrieved so far. You can get the last added object from the sequence via <code>$sequence-&gt;last()</code>.</p> <p>Remember that while the sequence is being updated, the data is not validated - only when the sequence is fully extracted, the objects are validated and a full sequence is returned (see example below).</p> <pre><code>&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\nfunction updateUI(Person $person) {\n    // add newly extracted person to the UI list\n    $this-&gt;ui-&gt;appendToList($person);\n    // remember those objects are not validated yet\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(\n        fn($sequence) =&gt; updateUI($sequence-&gt;last()) // get last added object\n    )\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\n// now the list is fully extracted and validated\nforeach ($list as $person) {\n    // do something with each person\n    $this-&gt;db-&gt;save($person);\n}\n</code></pre>"},{"location":"instructor/advanced/sequences/#working-with-sequences","title":"Working with Sequences","text":"<p>Sequences offer array access (via ArrayAccess) and convenience methods to work with the list of extracted objects.</p> <pre><code>&lt;?php\n$sequence-&gt;count();   // returns the number of extracted items\n$sequence-&gt;first();   // returns the first extracted item\n$sequence-&gt;last();    // returns the last extracted item\n$sequence-&gt;get(1);    // returns the second extracted item\n$sequence-&gt;toArray(); // returns the list of extracted items as an array\n</code></pre>"},{"location":"instructor/advanced/sequences/#streaming-sequence-updates","title":"Streaming sequence updates","text":"<p>See: Streaming and partial updates for more information on how to get partial updates and streaming of sequences.</p>"},{"location":"instructor/advanced/structure-to-structure/","title":"Structure to structure","text":""},{"location":"instructor/advanced/structure-to-structure/#structured-to-structured-processing","title":"Structured-to-structured processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>create()</code> method can be an object, but also an array or just a string.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new StructuredOutput)-&gt;with(\n    input: $email,\n    responseModel: Email::class,\n    prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n)-&gt;get();\n\nassert($translation instanceof Email); // true\ndump($translation);\n// Email {\n//     address: \"joe@gmail\",\n//     subject: \"Actualizaci\u00f3n de estado\",\n//     body: \"Su cuenta ha sido actualizada.\"\n// }\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/","title":"Structures","text":""},{"location":"instructor/advanced/structures/#handling-dynamic-data-models","title":"Handling dynamic data models","text":"<p>If you want to define the shape of data during runtime, you can use <code>Structure</code> class.</p> <p>Structures allow you to define and modify arbitrary shape of data to be extracted by LLM. Classes may not be the best fit for this purpose, as declaring or changing them during execution is not possible.</p> <p>With structures, you can define custom data shapes dynamically, for example based on the user input or context of the processing, to specify the information you need LLM to infer from the provided text or chat messages.</p>"},{"location":"instructor/advanced/structures/#defining-a-shape-of-data","title":"Defining a shape of data","text":"<p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns an array in the shape you defined.</p> <p><code>Structure::define()</code> accepts array of <code>Field</code> objects.</p> <p>Let's first define the structure, which is a shape of the data we want to extract from the message.</p> <pre><code>&lt;?php\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::enum('role', Role::class),\n]);\n?&gt;\n</code></pre> <p>Following types of fields are currently supported:</p> <ul> <li><code>Field::bool()</code> - boolean value</li> <li><code>Field::int()</code> - int value</li> <li><code>Field::string()</code> - string value</li> <li><code>Field::float()</code> - float value</li> <li><code>Field::enum()</code> - enum value</li> <li><code>Field::structure()</code> - for nesting structures</li> </ul>"},{"location":"instructor/advanced/structures/#optional-fields","title":"Optional fields","text":"<p>Fields can be marked as optional with <code>$field-&gt;optional()</code>.  By default, all defined fields are required.</p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    //...\n    Field::int('age')-&gt;optional(),\n    //...\n]);\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#descriptions-for-guiding-llm-inference","title":"Descriptions for guiding LLM inference","text":"<p>Instructor includes field descriptions in the content of instructions for LLM, so you can use them to provide explanations, detailed specifications or requirements for each field.</p> <p>You can also provide extra inference instructions for LLM at the structure level with <code>$structure-&gt;description(string $description)</code></p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name', 'Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;optional(),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#nesting-structures","title":"Nesting structures","text":"<p>You can use <code>Field::structure()</code> to nest structures in case you want to define more complex data shapes.</p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#validation-of-structure-data","title":"Validation of structure data","text":"<p>Instructor supports validation of structures.</p> <p>You can define field validator with:</p> <ul> <li><code>$field-&gt;validator(callable $validator)</code> - $validator has to return an instance of <code>ValidationResult</code></li> <li><code>$field-&gt;validIf(callable $condition, string $message)</code> - $condition has to return false if validation has not succeeded, $message with be provided to LLM as explanation for self-correction of the next extraction attempt</li> </ul> <p>Let's add a simple field validation to the example above: </p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    // ...\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    // ...\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#extracting-data","title":"Extracting data","text":"<p>Now, let's extract the data from the message.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield. She is 25 years old and works as a line worker. \n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345.\n    TEXT;\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;get();\n\ndump($person-&gt;toArray());\n// array [\n//   \"name\" =&gt; \"Jane Doe\"\n//   \"age\" =&gt; 25\n//   \"address\" =&gt; array [\n//     \"city\" =&gt; \"Springfield\"\n//   ]\n//   \"role\" =&gt; \"line\"\n// ]\n?&gt;\n</code></pre>"},{"location":"instructor/advanced/structures/#working-with-structure-objects","title":"Working with <code>Structure</code> objects","text":"<p>Structure object properties can be accessed using <code>get()</code> and <code>set()</code> methods, but also directly as properties.</p> <pre><code>&lt;?php\n$person = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::structure('role', [\n        Field::string('name'),\n        Field::int('level'),\n    ])\n]);\n\n// Setting properties via set()\n$person-&gt;set('name', 'John Doe');\n$person-&gt;set('age', 30);\n$person-&gt;get('role')-&gt;set('name', 'Manager');\n$person-&gt;get('role')-&gt;set('level', 1);\n\n// Setting properties directly \n$person-&gt;name = 'John Doe';\n$person-&gt;age = 30;\n$person-&gt;role-&gt;name = 'Manager';\n$person-&gt;role-&gt;level = 1;\n\n// Getting properties via get()\n$name = $person-&gt;get('name');\n$age = $person-&gt;get('age');\n$role = $person-&gt;get('role')-&gt;get('name');\n$level = $person-&gt;get('role')-&gt;get('level');\n\n// Getting properties directly\n$name = $person-&gt;name;\n$age = $person-&gt;age;\n$role = $person-&gt;role-&gt;name;\n$level = $person-&gt;role-&gt;level;\n?&gt;\n</code></pre>"},{"location":"instructor/concepts/overview/","title":"Overview","text":""},{"location":"instructor/concepts/overview/#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to get structured, validated data from multiple types of inputs: text,  chat messages, or images. It is powered by Large Language Models (LLMs).</p> <p>The library is inspired by the Instructor for Python created by Jason Liu.</p>"},{"location":"instructor/concepts/overview/#how-it-works","title":"How it works","text":"<p>Instructor uses Large Language Models (LLMs) to process data and return structured information you can easily use in your code.</p> <p></p>"},{"location":"instructor/concepts/overview/#instructor-in-action","title":"Instructor in action","text":"<p>Here's a simple CLI demo app using Instructor to extract structured data from text:</p> <p></p>"},{"location":"instructor/concepts/overview/#how-instructor-enhances-your-workflow","title":"How Instructor Enhances Your Workflow","text":"<p>Instructor introduces three key enhancements compared to direct API usage.</p>"},{"location":"instructor/concepts/overview/#response-model","title":"Response Model","text":"<p>You just specify a PHP class to extract data into via the 'magic' of LLM chat completion. And that's it.</p> <p>Instructor reduces brittleness of the code extracting the information from textual data by leveraging structured LLM responses.</p> <p>Instructor helps you write simpler, easier to understand code - you no longer have to define lengthy function call definitions or write code for assigning returned JSON into target data objects.</p>"},{"location":"instructor/concepts/overview/#validation","title":"Validation","text":"<p>Response model generated by LLM can be automatically validated, following set of rules. Currently, Instructor supports only Symfony validation.</p> <p>You can also provide a context object to use enhanced validator capabilities.</p>"},{"location":"instructor/concepts/overview/#max-retries","title":"Max Retries","text":"<p>You can set the number of retry attempts for requests.</p> <p>Instructor will repeat requests in case of validation or deserialization error up to the specified number of times, trying to get a valid response from LLM.</p>"},{"location":"instructor/concepts/why/","title":"Why use Instructor?","text":"<p>Our library introduces three key enhancements:</p> <ul> <li>Response Model: Specify a data model to be returned by LLM to simplify your code.</li> <li>Validation: Automatically validate response generated by LLM before you start using it.</li> <li>Max Retries: Automated retry attempts for invalid responses.</li> </ul>"},{"location":"instructor/concepts/why/#a-glimpse-into-instructors-capabilities","title":"A Glimpse into Instructor's Capabilities","text":"<p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek.</p>"},{"location":"instructor/concepts/why/#understanding-the-workflow","title":"Understanding the workflow","text":"<p>Let's see how we can leverage it to make use of instructor</p>"},{"location":"instructor/concepts/why/#step-1-define-the-data-model","title":"Step 1: Define the data model","text":"<p>Create a data model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>&lt;?php\nclass UserDetail {\n    public string $name;\n    public int $age;\n}\n</code></pre>"},{"location":"instructor/concepts/why/#step-2-extract","title":"Step 2: Extract","text":"<p>Use the <code>StructuredOutput::create()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies the model to use for extraction.</p> <pre><code>/** @var UserDetail */\n$user = (new StructuredOutput)-&gt;with(\n    messages: [[\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"]],\n    responseModel: UserDetail::class,\n    model: \"gpt-3.5-turbo\",\n)-&gt;get();\n\nassert($user-&gt;name == \"Jason\")\nassert($user-&gt;age == 25)\n</code></pre> <p>It's helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p>"},{"location":"instructor/concepts/why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same data model. If the response triggers any validation rules Instructor will raise a validation error.</p>"},{"location":"instructor/concepts/why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>LeadReport</code> model is passed as the <code>$responseModel</code>, and <code>$maxRetries</code> is set to 2. It means that if the extracted data does not match the model, Instructor will re-ask the model 2 times before giving up.</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via jason@gmailcom -- Jason\"]],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;email === \"jason@gmail.com\");\n</code></pre> <p>More about Validation</p> <p>Check out Jason's blog post Good LLM validation is just good validation</p>"},{"location":"instructor/concepts/why/#custom-validators","title":"Custom Validators","text":"<p>Instructor uses Symfony validation component to validate extracted data. You can use #[Assert/Callback] annotation to build fully customized validation logic.</p> <p>See Symfony docs for more details on how to use Callback constraint.</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre>"},{"location":"instructor/essentials/configuration/","title":"Configuration Options","text":"<p>Instructor provides extensive configuration options through fluent API methods to customize its behavior, processing, and integration with various LLM providers.</p>"},{"location":"instructor/essentials/configuration/#request-configuration","title":"Request Configuration","text":"<p>Configure how Instructor processes your input and builds requests:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMessages($messages)           // Set chat messages\n    -&gt;withInput($input)                 // Set input (converted to messages)\n    -&gt;withSystem($systemPrompt)         // Set system prompt\n    -&gt;withPrompt($prompt)               // Set additional prompt\n    -&gt;withExamples($examples)           // Set example data for context\n    -&gt;withModel($modelName)             // Set LLM model name\n    -&gt;withOptions($options)             // Set LLM-specific options\n    -&gt;withOption($key, $value)          // Set individual LLM option\n    -&gt;withStreaming(true)               // Enable streaming responses\n    -&gt;withCachedContext($messages, $system, $prompt, $examples) // Use cached context\n</code></pre>"},{"location":"instructor/essentials/configuration/#response-configuration","title":"Response Configuration","text":"<p>Define how Instructor should process and validate responses:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMaxRetries(3)                 // Set retry count for failed validations\n    -&gt;withOutputMode(OutputMode::Tools) // Set output mode (Tools, Json, JsonSchema, MdJson)\n    -&gt;withRetryPrompt($prompt)          // Set custom retry prompt for validation failures\n    -&gt;withSchemaName($name)             // Set schema name for documentation\n    -&gt;withToolName($name)               // Set tool name for Tools mode\n    -&gt;withToolDescription($description) // Set tool description for Tools mode\n</code></pre>"},{"location":"instructor/essentials/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<p>Fine-tune Instructor's internal processing:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withConfig($configObject)         // Use custom StructuredOutputConfig instance\n    -&gt;withConfigPreset($presetName)     // Use predefined configuration preset\n    -&gt;withConfigProvider($provider)     // Use custom configuration provider\n    -&gt;withObjectReferences(true)        // Enable object reference handling\n    -&gt;withDefaultToStdClass(true)       // Default to stdClass for unknown types\n    -&gt;withDeserializationErrorPrompt($prompt) // Custom deserialization error prompt\n    -&gt;withThrowOnTransformationFailure(true)  // Throw on transformation failures\n</code></pre>"},{"location":"instructor/essentials/configuration/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<p>Configure connection and communication with LLM providers:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;using($preset)                    // Use LLM preset (e.g., 'openai', 'anthropic')\n    -&gt;withDsn($dsn)                     // Set connection DSN\n    -&gt;withLLMProvider($provider)        // Set custom LLM provider instance\n    -&gt;withLLMConfig($config)            // Set LLM configuration object\n    -&gt;withLLMConfigOverrides($overrides) // Override specific LLM config values\n    -&gt;withDriver($driver)               // Set custom inference driver\n    -&gt;withHttpClient($client)           // Set custom HTTP client\n    -&gt;withHttpClientPreset($preset)     // Use HTTP client preset\n    -&gt;withDebugPreset($preset)          // Enable debug preset\n    -&gt;withClientInstance($driverName, $instance) // Set client instance for specific driver\n</code></pre>"},{"location":"instructor/essentials/configuration/#processing-pipeline-overrides","title":"Processing Pipeline Overrides","text":"<p>Customize validation, transformation, and deserialization:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withValidators(...$validators)    // Override response validators\n    -&gt;withTransformers(...$transformers) // Override response transformers  \n    -&gt;withDeserializers(...$deserializers) // Override response deserializers\n</code></pre>"},{"location":"instructor/essentials/configuration/#event-handling","title":"Event Handling","text":"<p>Configure real-time processing callbacks:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;onPartialUpdate($callback)        // Handle partial response updates during streaming\n    -&gt;onSequenceUpdate($callback)       // Handle sequence item completion during streaming\n</code></pre>"},{"location":"instructor/essentials/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"instructor/essentials/configuration/#basic-openai-configuration","title":"Basic OpenAI Configuration","text":"<pre><code>$result = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4')\n    -&gt;withMaxRetries(3)\n    -&gt;withMessages(\"Extract person data from: John is 25 years old\")\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/configuration/#streaming-with-callbacks","title":"Streaming with Callbacks","text":"<pre><code>$result = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withStreaming(true)\n    -&gt;onPartialUpdate(fn($partial) =&gt; updateUI($partial))\n    -&gt;withMessages(\"Generate a list of tasks\")\n    -&gt;withResponseClass(Sequence::of(Task::class))\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/configuration/#custom-configuration-object","title":"Custom Configuration Object","text":"<pre><code>$config = new StructuredOutputConfig(\n    maxRetries: 5,\n    outputMode: OutputMode::JsonSchema,\n    retryPrompt: \"Please fix the validation errors and try again.\"\n);\n\n$result = (new StructuredOutput)\n    -&gt;withConfig($config)\n    -&gt;withMessages($input)\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/customize_prompts/","title":"Customize prompts","text":""},{"location":"instructor/essentials/customize_prompts/#customizing-prompts","title":"Customizing prompts","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>create()</code> method.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"instructor/essentials/customize_prompts/#prompting-models-with-tool-calling-support","title":"Prompting models with tool calling support","text":"<p><code>OutputMode::Tools</code> is usually most reliable way to get structured outputs following provided response schema.</p> <p><code>OutputMode::Tools</code> can make use of <code>$toolName</code> and <code>$toolDescription</code> parameters to provide additional semantic context to the LLM, describing the tool to be used for processing the input. <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> ignore these parameters, as tools are not used in these modes.</p> <pre><code>&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        toolName: 'extract',\n        toolDescription: 'Extract information from provided content',\n        mode: OutputMode::Tools)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/customize_prompts/#prompting-models-supporting-json-output","title":"Prompting models supporting JSON output","text":"<p>Aside from tool calling Instructor supports two other modes for getting structured outputs from LLM: <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code>.</p> <p><code>OutputMode::Json</code> uses JSON mode offered by some models and API providers to get LLM respond in JSON format rather than plain text.</p> <p><pre><code>&lt;?php\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    prompt: \"\\nYour task is to respond correctly with JSON object.\",\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre> Note that various models and API providers have specific requirements on the input format, e.g. for OpenAI JSON mode you are required to include <code>JSON</code> string in the prompt.</p>"},{"location":"instructor/essentials/customize_prompts/#including-json-schema-in-the-prompt","title":"Including JSON Schema in the prompt","text":"<p>Instructor takes care of automatically setting the <code>response_format</code> parameter, but this may not be sufficient for some models or providers - some of them require specifying JSON response format as part of the prompt, rather than just as <code>response_format</code> parameter in the request (e.g. OpenAI).</p> <p>For this reason, when using Instructor's <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> you should include the expected JSON Schema in the prompt. Otherwise, the response is unlikely to match your target model, making it impossible for Instructor to deserialize it correctly.</p> <pre><code>&lt;?php\n// NOTE: You don't have to create JSON Schema manually, you can use\n// schema automatically generated by Instructor for your response model.\n// See the next example.\n\n$jsonSchema = json_encode([\n    \"type\" =&gt; \"object\",\n    \"properties\" =&gt; [\n        \"name\" =&gt; [\"type\" =&gt; \"string\"],\n        \"age\" =&gt; [\"type\" =&gt; \"integer\"]\n    ],\n    \"required\" =&gt; [\"name\", \"age\"]\n]);\n\n$user = $structuredOutput\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema: $jsonSchema\\n\",\n        mode: OutputMode::Json)\n    -&gt;get();\n</code></pre> <p>The example above demonstrates how to manually create JSON Schema, but with Instructor you do not have to build the schema manually - you can use prompt template placeholder syntax to use Instructor-generated JSON Schema.</p>"},{"location":"instructor/essentials/customize_prompts/#prompt-as-template","title":"Prompt as template","text":"<p>Instructor allows you to use a template string as a prompt. You can use <code>&lt;|variable|&gt;</code> placeholders in the template string, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>&lt;|json_schema|&gt;</code> - replaced with the JSON Schema for current response model</p> <p>Example below demonstrates how to use a template string as a prompt:</p> <pre><code>&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/customize_prompts/#prompting-the-models-with-no-support-for-tool-calling-or-json-output","title":"Prompting the models with no support for tool calling or JSON output","text":"<p><code>OutputMode::MdJson</code> is the most basic (and least reliable) way to get structured outputs from LLM. Still, you may want to use it with the models which do not support tool calling or JSON output.</p> <p><code>OutputMode::MdJson</code> relies on the prompting to get LLM response in JSON formatted data.</p> <p>Many models prompted in this mode will respond with a mixture of plain text and JSON data. Instructor will try to find JSON data fragment in the response and ignore the rest of the text.</p> <p>This approach is most prone to deserialization and validation errors and needs providing JSON Schema in the prompt to increase the probability that the response is correctly structured and contains the expected data.</p> <pre><code>&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::MdJson)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/data_model/","title":"Data model","text":"<p>Instructor provides several ways the data model of LLM response.</p>"},{"location":"instructor/essentials/data_model/#using-classes","title":"Using classes","text":"<p>The default way is to use PHP classes to define the data model. You can also use PHPDoc comments to specify the types of fields of the response. Additionally, you can use attributes to provide more context to the language model or to provide additional instructions to the model.</p>"},{"location":"instructor/essentials/data_model/#type-hints","title":"Type Hints","text":"<p>Use PHP type hints to specify the type of extracted data.</p> <p>Use nullable types to indicate that given field is optional.</p> <pre><code>&lt;?php\n\nclass Person {\n    public string $name;\n    public ?int $age;\n    public Address $address;\n}\n</code></pre> <p>Instructor will only fill in the fields that are public. Private and protected fields are ignored and their values are not going to be extracted (they will be left empty, with default values set as defined in your class).</p>"},{"location":"instructor/essentials/data_model/#private-vs-public-object-field","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM.</p> <p>Private and protected fields are left unchanged, unless the class has setter methods defined or there are parameters in the constructor that match the field names.</p> <p>Provide default values for the fields that are not set by Instructor, to avoid unexpected behavior when accessing those fields.</p> <p>See:  - <code>examples/A01_Basics/BasicPrivateVsPublicFields/run.php</code> to check the details on the behavior of extraction for classes with private and public fields,  - <code>examples/A01_Basics/BasicGetSet/run.php</code> to see how Instructor uses getter and setter methods,  - <code>examples/A01_Basics/BasicConstructor/run.php</code> to see how Instructor uses constructor parameters.</p>"},{"location":"instructor/essentials/data_model/#docblock-type-hints","title":"DocBlock type hints","text":"<p>You can also use PHP DocBlock style comments to specify the type of extracted data. This is useful when you want to specify property types for LLM, but can't or don't want to enforce type at the code level.</p> <pre><code>&lt;?php\n\nclass Person {\n    /** @var string */\n    public $name;\n    /** @var int */\n    public $age;\n    /** @var Address $address person's address */\n    public $address;\n}\n</code></pre> <p>See PHPDoc documentation for more details on DocBlock: https://docs.phpdoc.org/3.0/guide/getting-started/what-is-a-docblock.html#what-is-a-docblock</p>"},{"location":"instructor/essentials/data_model/#using-docblocks-as-additional-instructions-for-llm","title":"Using DocBlocks as Additional Instructions for LLM","text":"<p>You can use PHP DocBlocks (/** */) to provide additional instructions for LLM at class or field level, for example to clarify what you expect or how LLM should process your data.</p> <p>Instructor extracts PHP DocBlocks comments from class and property defined and includes them in specification of response model sent to LLM.</p> <p>Using PHP DocBlocks instructions is not required, but sometimes you may want to clarify your intentions to improve LLM's inference results.</p> <pre><code>    /**\n     * Represents a skill of a person and context in which it was mentioned. \n     */\n    class Skill {\n        public string $name;\n        /** @var SkillType $type type of the skill, derived from the description and context */\n        public SkillType $type;\n        /** Directly quoted, full sentence mentioning person's skill */\n        public string $context;\n    }\n</code></pre>"},{"location":"instructor/essentials/data_model/#attributes-for-data-model-descriptions-and-instructions","title":"Attributes for data model descriptions and instructions","text":"<p>Instructor supports <code>#[Description]</code> and <code>#[Instructions]</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p> <p><code>#[Description]</code> attribute is used to describe a class or property in your data model. Instructor will use this text to provide more context to the language model.</p> <p><code>#[Instructions]</code> attribute is used to provide additional instructions to the language model, such as how to process the data.</p> <p>You can add multiple attributes to a class or property - Instructor will merge them into a single block of text.</p> <p>Instructor will still include any PHPDoc comments provided in the class, but using attributes might be more convenient and easier to read.</p> <pre><code>&lt;?php\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make it ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    public string $job;\n}\n</code></pre> <p>NOTE: Technically both <code>#[Description]</code> and <code>#[Instructions]</code> attributes do the same thing - they provide additional context to the language model. Yet, providing them in separate attributes allows you to better organize your code and make it more readable. In the future, we may extend the functionality of these attributes to provide more specific instructions to the language model, so it is a good idea to use them now.</p>"},{"location":"instructor/essentials/data_model/#typed-collections-arrays","title":"Typed Collections / Arrays","text":"<p>PHP currently does not support generics or typehints to specify array element types.</p> <p>Use PHP DocBlock style comments to specify the type of array elements.</p> <pre><code>&lt;?php\nclass Person {\n    // ...\n}\n\nclass Event {\n    // ...\n    /** @var Person[] list of extracted event participants */\n    public array $participants;\n    // ...\n}\n</code></pre>"},{"location":"instructor/essentials/data_model/#example-of-complex-data-extraction","title":"Example of complex data extraction","text":"<p>Instructor can retrieve complex data structures from text. Your response model can contain nested objects, arrays, and enums.</p> <pre><code>&lt;?php\nuse Cognesy/Instructor/Instructor;\n\n// define a data structures to extract data into\nclass Person {\n    public string $name;\n    public int $age;\n    public string $profession;\n    /** @var Skill[] */\n    public array $skills;\n}\n\nclass Skill {\n    public string $name;\n    public SkillType $type;\n}\n\nenum SkillType : string {\n    case Technical = 'technical';\n    case Other = 'other';\n}\n\n$text = \"Alex is 25 years old software engineer, who knows PHP, Python and can play the guitar.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n)-&gt;get(); // client is passed explicitly, can specify e.g. different base URL\n\n// data is extracted into an object of given class\nassert($person instanceof Person); // true\n\n// you can access object's extracted property values\necho $person-&gt;name; // Alex\necho $person-&gt;age; // 25\necho $person-&gt;profession; // software engineer\necho $person-&gt;skills[0]-&gt;name; // PHP\necho $person-&gt;skills[0]-&gt;type; // SkillType::Technical\n// ...\n\nvar_dump($person);\n// Person {\n//     name: \"Alex\",\n//     age: 25,\n//     profession: \"software engineer\",\n//     skills: [\n//         Skill {\n//              name: \"PHP\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"Python\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"guitar\",\n//              type: SkillType::Other\n//         },\n//     ]\n// }\n</code></pre>"},{"location":"instructor/essentials/data_model/#dynamic-data-schemas-with-structure-class","title":"Dynamic data schemas with <code>Structure</code> class","text":"<p>In case you work with dynamic data schemas, you can use <code>Structure</code> class to define the data model.</p> <p>See Structures for more details on how to work with dynamic data schemas.</p>"},{"location":"instructor/essentials/data_model/#optional-data-with-maybe-class","title":"Optional data with <code>Maybe</code> class","text":"<p>The <code>Maybe</code> class provides a way to handle optional data that may or may not be present in the input text. It wraps a value type and indicates whether the data was found or not, along with an error message when the data is missing.</p>"},{"location":"instructor/essentials/data_model/#basic-usage","title":"Basic Usage","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\n\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n$maybe = Maybe::is(Person::class, 'person', 'Person data if found in the text');\n\n$result = (new StructuredOutput)\n    -&gt;with(\n        messages: \"The document mentions some information but no person details.\",\n        responseModel: $maybe,\n    )\n    -&gt;get();\n\nif ($result-&gt;hasValue()) {\n    $person = $result-&gt;get();\n    echo \"Found person: \" . $person-&gt;name;\n} else {\n    echo \"No person found. Error: \" . $result-&gt;error();\n}\n</code></pre>"},{"location":"instructor/essentials/data_model/#maybe-methods","title":"Maybe Methods","text":"<ul> <li><code>Maybe::is(class, name?, description?)</code> - Static factory method to create a Maybe instance</li> <li><code>get()</code> - Get the value if present, or null if not found</li> <li><code>error()</code> - Get the error message explaining why the value wasn't found</li> <li><code>hasValue()</code> - Check if a value was successfully extracted</li> <li><code>toJsonSchema()</code> - Generate JSON schema for the Maybe wrapper</li> </ul>"},{"location":"instructor/essentials/demonstrations/","title":"Demonstrations","text":""},{"location":"instructor/essentials/demonstrations/#providing-examples-to-llm","title":"Providing examples to LLM","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> modes, where the output is expected to be a JSON object.</p> <p>Instructor's <code>request()</code> method accepts an array of examples as the <code>examples</code> parameter, where each example is an instance of the <code>Example</code> class.</p>"},{"location":"instructor/essentials/demonstrations/#example-class","title":"<code>Example</code> class","text":"<p><code>Example</code> constructor have two main arguments: <code>input</code> and <code>output</code>.</p> <p>The <code>input</code> property is  a string which describes the input message, while he <code>output</code> property is an array which represents the expected output.</p> <p>Instructor will append the list of examples to the prompt sent to LLM, with output array data rendered as JSON text.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n        ),\n    ],\n    mode: OutputMode::Json\n)-&gt;get();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#modifying-the-example-template","title":"Modifying the example template","text":"<p>You can use a template string as an input for the Example class. The template string may contain placeholders for the input data, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>{input}</code> - replaced with the actual input message  - <code>{output}</code> - replaced with the actual output data</p> <p>In case input or output data is an array, Instructor will automatically convert it to a JSON string before replacing the placeholders.</p> <pre><code>$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50],\n            template: \"EXAMPLE:\\n{input} =&gt; {output}\\n\",\n        ),\n    ],\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#convenience-factory-methods","title":"Convenience factory methods","text":"<p>You can also create Example instances using the <code>fromText()</code>, <code>fromChat()</code>, <code>fromData()</code> helper static methods. All of them accept $output as an array of the expected output data and differ in the way the input data is provided.</p>"},{"location":"instructor/essentials/demonstrations/#make-example-from-text","title":"Make example from text","text":"<p><code>Example::fromText()</code> method accepts a string as an input. It is equivalent to creating an instance of Example using the constructor.</p> <pre><code>$example = Example::fromText(\n    input: 'Ian is 27 yo',\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#make-example-from-chat","title":"Make example from chat","text":"<p><code>Example::fromChat()</code> method accepts an array of messages, which may be useful when you want to use a chat or chat fragment as a demonstration of the input.</p> <pre><code>$example = Example::fromChat(\n    input: [['role' =&gt; 'user', 'content' =&gt; 'Ian is 27 yo']],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"instructor/essentials/demonstrations/#make-example-from-data","title":"Make example from data","text":"<p><code>Example::fromData()</code> method accepts any data type and uses the <code>Json::encode()</code> method to convert it to a string. It may be useful to provide a complex data structure as an example input.</p> <pre><code>$example = Example::fromData(\n    input: ['firstName' =&gt; 'Ian', 'lastName' =&gt; 'Brown', 'birthData' =&gt; '1994-01-01'],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"instructor/essentials/modes/","title":"Modes","text":""},{"location":"instructor/essentials/modes/#extraction-modes","title":"Extraction modes","text":"<p>Instructor supports several ways to extract data from the response.</p>"},{"location":"instructor/essentials/modes/#output-modes","title":"Output Modes","text":"<p>Instructor supports multiple output modes to allow working with various models depending on their capabilities. - <code>OutputMode::Json</code> - generate structured output via LLM's native JSON generation - <code>OutputMode::JsonSchema</code> - use LLM's strict JSON Schema mode to enforce JSON Schema - <code>OutputMode::Tools</code> - use tool calling API to get LLM follow provided schema - <code>OutputMode::MdJson</code> - use prompting to generate structured output; fallback for the models that do not support JSON generation or tool calling</p> <p>Additionally, you can use <code>Text</code> and <code>Unrestricted</code> modes to get LLM to generate text output without any structured data extraction.</p> <p>Those modes are not useful for <code>StructuredOutput</code> class (as it is focused on structured output generation) but can be used with <code>Inference</code> class.</p> <ul> <li><code>OutputMode::Text</code> - generate text output</li> <li><code>OutputMode::Unrestricted</code> - generate unrestricted output based on inputs provided by the user (with no enforcement of specific output format)</li> </ul>"},{"location":"instructor/essentials/modes/#example-of-using-modes","title":"Example of Using Modes","text":"<p>Mode can be set via parameter of <code>StructuredOutput::create()</code> method.</p> <p>The default mode is <code>OutputMode::Tools</code>, which leverages OpenAI-style tool calls.</p> <p><pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$structuredOutput = new StructuredOutput();\n\n$response = $structuredOutput-&gt;with(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre> Mode, like other parameters can also be set via fluent API methods.</p> <pre><code>&lt;?php\n$response = $structuredOutput\n    -&gt;withMessages(\"...\")\n    -&gt;withResponseModel(...)\n    //...\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"instructor/essentials/modes/#modes","title":"Modes","text":""},{"location":"instructor/essentials/modes/#outputmodetools","title":"<code>OutputMode::Tools</code>","text":"<p>This mode is the default one. It uses OpenAI tools to extract data from the response.</p> <p>It is the most reliable mode, but not all models and API providers support it - check their documentation for more information.</p> <ul> <li>https://platform.openai.com/docs/guides/function-calling</li> <li>https://docs.anthropic.com/en/docs/build-with-claude/tool-use</li> <li>https://docs.mistral.ai/capabilities/function_calling/</li> </ul>"},{"location":"instructor/essentials/modes/#outputmodejson","title":"<code>OutputMode::Json</code>","text":"<p>In this mode Instructor provides response format as JSONSchema and asks LLM to respond with JSON object following provided schema.</p> <p>It is supported by many open source models and API providers - check their documentation.</p> <p>See more about JSON mode in:</p> <ul> <li>https://platform.openai.com/docs/guides/text-generation/json-mode</li> <li>https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency</li> <li>https://docs.mistral.ai/capabilities/json_mode/</li> </ul>"},{"location":"instructor/essentials/modes/#outputmodejsonschema","title":"<code>OutputMode::JsonSchema</code>","text":"<p>In contrast to <code>OutputMode::Json</code> which may not always manage to meet the schema requirements, <code>OutputMode::JsonSchema</code> is strict and guarantees the response to be a valid JSON object that matches the provided schema.</p> <p>It is currently supported only by new OpenAI models (check their docs for details).</p> <p>NOTE: OpenAI JsonSchema mode does not support optional properties. If you need to have optional properties in your schema, use <code>OutputMode::Tools</code> or <code>OutputMode::Json</code>.</p> <p>See more about JSONSchema mode in:</p> <ul> <li>https://platform.openai.com/docs/guides/structured-outputs</li> </ul>"},{"location":"instructor/essentials/modes/#outputmodemdjson","title":"<code>OutputMode::MdJson</code>","text":"<p>In this mode Instructor asks LLM to answer with JSON object following provided schema and return answer as Markdown codeblock.</p> <p>It may improve the results for LLMs that have not been finetuned to respond with JSON as they are likely to be already trained on large amounts of programming docs and have seen a lot of properly formatted JSON objects within MD codeblocks.</p>"},{"location":"instructor/essentials/scalars/","title":"Scalars","text":""},{"location":"instructor/essentials/scalars/#extracting-scalar-values","title":"Extracting Scalar Values","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::integer('age'),\n    )\n    -&gt;get();\n\nvar_dump($value);\n// int(28)\n</code></pre> <p>In this example, we're extracting a single integer value from the text. You can also use <code>Scalar::string()</code>, <code>Scalar::boolean()</code> and <code>Scalar::float()</code> to extract other types of values.</p> <p>Additionally, you can use Scalar adapter to extract enums via <code>Scalar::enum()</code>.</p>"},{"location":"instructor/essentials/scalars/#examples","title":"Examples","text":""},{"location":"instructor/essentials/scalars/#string-result","title":"String result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::string(name: 'firstName'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe(\"Jason\");\n</code></pre>"},{"location":"instructor/essentials/scalars/#integer-result","title":"Integer result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::integer('age'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeInt();\n// expect($value)-&gt;toBe(28);\n</code></pre>"},{"location":"instructor/essentials/scalars/#boolean-result","title":"Boolean result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::boolean(name: 'isAdult'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeBool();\n// expect($value)-&gt;toBe(true);\n</code></pre>"},{"location":"instructor/essentials/scalars/#float-result","title":"Float result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old and his 100m sprint record is 11.6 seconds.\",\n        responseModel: Scalar::float(name: 'recordTime'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeFloat();\n// expect($value)-&gt;toBe(11.6);\n</code></pre>"},{"location":"instructor/essentials/scalars/#enum-result-select-one-of-the-options","title":"Enum result / select one of the options","text":"<pre><code>&lt;?php\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'system', 'content' =&gt; $text],\n            ['role' =&gt; 'user', 'content' =&gt; 'What is Jason\\'s citizenship?'],\n        ],\n        responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n    )-&gt;get();\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe('other');\n</code></pre>"},{"location":"instructor/essentials/usage/","title":"Usage","text":""},{"location":"instructor/essentials/usage/#basic-usage","title":"Basic usage","text":"<p>This is a simple example demonstrating how Instructor retrieves structured information from provided text (or chat message sequence).</p> <p>Response model class is a plain PHP class with typehints specifying the types of fields of the object.</p> <p>NOTE: By default, Instructor looks for OPENAI_API_KEY environment variable to get your API key. You can also provide the API key explicitly when creating the Instructor instance.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Step 0: Create .env file in your project root:\n// OPENAI_API_KEY=your_api_key\n\n// Step 1: Define target data structure(s)\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n// Step 2: Provide content to process\n$text = \"His name is Jason and he is 28 years old.\";\n\n// Step 3: Use Instructor to run LLM inference\n$person = (new StructuredOutput)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        responseModel: Person::class,\n    )\n    -&gt;get();\n\n// Step 4: Work with structured response data\nassert($person instanceof Person); // true\nassert($person-&gt;name === 'Jason'); // true\nassert($person-&gt;age === 28); // true\n\necho $person-&gt;name; // Jason\necho $person-&gt;age; // 28\n\nvar_dump($person);\n// Person {\n//     name: \"Jason\",\n//     age: 28\n// }\n?&gt;\n</code></pre> <p>Note</p> <p>Instructor supports classes/objects as response models, as well as specialized helper classes like <code>Scalar</code> for simple values, <code>Maybe</code> for optional data, <code>Sequence</code> for arrays, and <code>Structure</code> for dynamically defined schemas.</p>"},{"location":"instructor/essentials/usage/#fluent-api-methods","title":"Fluent API Methods","text":"<p>StructuredOutput provides a comprehensive fluent API for configuring requests:</p>"},{"location":"instructor/essentials/usage/#request-configuration","title":"Request Configuration","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMessages($messages)           // Set chat messages\n    -&gt;withInput($input)                 // Set input (converted to messages)\n    -&gt;withSystem($systemPrompt)         // Set system prompt\n    -&gt;withPrompt($prompt)               // Set additional prompt\n    -&gt;withExamples($examples)           // Set example data\n    -&gt;withModel($modelName)             // Set LLM model\n    -&gt;withOptions($options)             // Set LLM options\n    -&gt;withStreaming(true)               // Enable streaming\n</code></pre>"},{"location":"instructor/essentials/usage/#response-model-configuration","title":"Response Model Configuration","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withResponseModel($model)         // Set response model (class/object/array)\n    -&gt;withResponseClass($className)     // Set response class specifically\n    -&gt;withResponseObject($object)       // Set response object instance\n    -&gt;withResponseJsonSchema($schema)   // Set JSON schema directly\n</code></pre>"},{"location":"instructor/essentials/usage/#configuration-and-behavior","title":"Configuration and Behavior","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMaxRetries(3)                 // Set retry count\n    -&gt;withOutputMode($mode)             // Set output mode\n    -&gt;withToolName($name)               // Set tool name for Tools mode\n    -&gt;withToolDescription($desc)        // Set tool description\n    -&gt;withRetryPrompt($prompt)          // Set retry prompt\n    -&gt;withConfig($config)               // Set configuration object\n    -&gt;withConfigPreset($preset)         // Use configuration preset\n</code></pre>"},{"location":"instructor/essentials/usage/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;using($preset)                    // Use LLM preset (e.g., 'openai')\n    -&gt;withDsn($dsn)                     // Set connection DSN\n    -&gt;withLLMProvider($provider)        // Set custom LLM provider\n    -&gt;withLLMConfig($config)            // Set LLM configuration\n    -&gt;withDriver($driver)               // Set inference driver\n    -&gt;withHttpClient($client)           // Set HTTP client\n</code></pre>"},{"location":"instructor/essentials/usage/#processing-overrides","title":"Processing Overrides","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withValidators(...$validators)    // Override validators\n    -&gt;withTransformers(...$transformers) // Override transformers\n    -&gt;withDeserializers(...$deserializers) // Override deserializers\n</code></pre>"},{"location":"instructor/essentials/usage/#request-execution-methods","title":"Request Execution Methods","text":"<p>After configuring your <code>StructuredOutput</code> instance, you have several ways to execute the request and access different types of responses:</p>"},{"location":"instructor/essentials/usage/#direct-execution-methods","title":"Direct Execution Methods","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$structuredOutput = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n\n// Get structured result directly\n$person = $structuredOutput-&gt;get();\n\n// Get raw LLM response\n$llmResponse = $structuredOutput-&gt;response();\n\n// Get streaming interface\n$stream = $structuredOutput-&gt;stream();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#pending-execution-with-create","title":"Pending Execution with <code>create()</code>","text":"<p>The <code>create()</code> method returns a <code>PendingStructuredOutput</code> instance, which acts as an execution handler that provides the same access methods:</p> <pre><code>&lt;?php\n$pending = $structuredOutput-&gt;create();\n\n// Execute and get structured result\n$person = $pending-&gt;get();\n\n// Execute and get raw LLM response\n$llmResponse = $pending-&gt;response();\n\n// Execute and get streaming interface\n$stream = $pending-&gt;stream();\n\n// Additional utility methods\n$json = $pending-&gt;toJson();      // Convert result to JSON string\n$array = $pending-&gt;toArray();    // Convert result to array\n$jsonObj = $pending-&gt;toJsonObject(); // Convert result to Json object\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#response-types-explained","title":"Response Types Explained","text":"<ul> <li><code>get()</code>: Returns the parsed and validated structured result (e.g., <code>Person</code> object)</li> <li><code>response()</code>: Returns the raw LLM response object with metadata like tokens, model info, etc.</li> <li><code>stream()</code>: Returns <code>StructuredOutputStream</code> for real-time processing of streaming responses</li> </ul> <p>The <code>PendingStructuredOutput</code> class serves as a flexible execution interface that lets you choose how to process the LLM response based on your specific needs.</p>"},{"location":"instructor/essentials/usage/#string-as-input","title":"String as Input","text":"<p>You can provide a string instead of an array of messages. This is useful when you want to extract data from a single block of text and want to keep your code simple.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Person::class,\n    )\n    -&gt;get();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#structured-to-structured-data-processing","title":"Structured-to-structured data processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>with()</code> method can be an object, but also an array or just a string.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new StructuredOutput)-&gt;with(\n        input: $email,\n        responseModel: Email::class,\n        prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n    )\n    -&gt;get();\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#streaming-support","title":"Streaming support","text":"<p>Instructor supports streaming of partial results, allowing you to start processing the data as soon as it is available.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n    options: ['stream' =&gt; true]\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $partialPerson) {\n    // process partial person data\n    echo \"Name: \" $partialPerson-&gt;name ?? '...';\n    echo \"Age: \" $partialPerson-&gt;age ?? '...';\n}\n\n// after streaming is done you can get the final, fully processed person object...\n$person = $stream-&gt;lastUpdate()\n// ...to, for example, save it to the database\n$db-&gt;save($person);\n?&gt;\n</code></pre>"},{"location":"instructor/essentials/usage/#scalar-responses","title":"Scalar responses","text":"<p>See Scalar responses for more information on how to generate scalar responses with <code>Scalar</code> adapter class.</p>"},{"location":"instructor/essentials/usage/#partial-responses-and-streaming","title":"Partial responses and streaming","text":"<p>See Streaming and partial updates for more information on how to work with partial updates and streaming.</p>"},{"location":"instructor/essentials/usage/#extracting-arguments-for-function-call","title":"Extracting arguments for function call","text":"<p>See FunctionCall helper class for more information on how to extract arguments for callable objects.</p>"},{"location":"instructor/essentials/usage/#execution-methods-summary","title":"Execution Methods Summary","text":"<p>Once configured, you can execute your request using different methods depending on your needs:</p> <pre><code>// Direct execution methods\n$result = $structuredOutput-&gt;get();       // Get structured result\n$response = $structuredOutput-&gt;response(); // Get raw LLM response  \n$stream = $structuredOutput-&gt;stream();     // Get streaming interface\n\n// Or use create() to get PendingStructuredOutput for flexible execution\n$pending = $structuredOutput-&gt;create();\n$result = $pending-&gt;get();                 // Same methods available\n$json = $pending-&gt;toJson();               // Plus utility methods\n</code></pre> <ul> <li><code>get()</code>: Returns the parsed and validated structured result</li> <li><code>response()</code>: Returns the raw LLM response with metadata</li> <li><code>stream()</code>: Returns <code>StructuredOutputStream</code> for real-time processing</li> <li><code>create()</code>: Returns <code>PendingStructuredOutput</code> for flexible execution control</li> </ul>"},{"location":"instructor/essentials/validation/","title":"Validation","text":""},{"location":"instructor/essentials/validation/#basic-validation","title":"Basic validation","text":"<p>Instructor validates results of LLM response against validation rules specified in your data model.</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is Jason, he is -28 years old.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: Person::class,\n)-&gt;get();\n\n// if the resulting object does not validate, Instructor throws an exception\n</code></pre> <p>NOTE: For further details on available validation rules, check Symfony Validation constraints.</p>"},{"location":"instructor/essentials/validation/#max-retries","title":"Max Retries","text":"<p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    #[Assert\\Length(min: 3)]\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is JX, aka Jason, he is -28 years old.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: Person::class,\n    maxRetries: 3,\n)-&gt;get();\n\n// if all LLM's attempts to self-correct the results fail, Instructor throws an exception\n</code></pre>"},{"location":"instructor/essentials/validation/#custom-validation","title":"Custom Validation","text":"<p>You can easily add custom validation code to your response model by using <code>ValidationTrait</code> and defining validation logic in <code>validate()</code> method.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $age;\n\n    public function validate() : array {\n        if ($this-&gt;name === strtoupper($this-&gt;name)) {\n            return [];\n        }\n        return [[\n            'message' =&gt; \"Name must be in uppercase.\",\n            'path' =&gt; 'name',\n            'value' =&gt; $this-&gt;name\n        ]];\n    }\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note that method <code>validate()</code> has to return:  * an empty array if the object is valid,  * or an array of validation violations.</p> <p>This information will be used by LLM to make subsequent attempts to correct the response.</p> <pre><code>$violations = [\n    [\n        'message' =&gt; \"Error message with violation details.\",\n        'path' =&gt; 'path.to.property',\n        'value' =&gt; '' // invalid value\n    ],\n    // ...other violations\n];\n</code></pre>"},{"location":"instructor/essentials/validation/#custom-validation-via-symfony-assertcallback","title":"Custom Validation via Symfony <code>#[Assert/Callback]</code>","text":"<p>Instructor uses Symfony validation component to validate extracted data.</p> <p>You can use <code>#[Assert/Callback]</code> annotation to build fully customized validation logic.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )\n    -&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>NOTE: See Symfony docs for more details on how to use Callback constraint.</p>"},{"location":"instructor/internals/config_files/","title":"Config files","text":""},{"location":"instructor/internals/config_files/#configuration-groups","title":"Configuration Groups","text":"<p>Instructor's configuration is organized into groups. Each group contains a set of settings that are related to a specific aspect of Instructor's functionality.</p> <p>Instructor comes with the following default settings groups: - <code>debug</code>: Debugging settings - <code>embed</code>: Embedding provider connections - <code>http</code>: HTTP client configurations - <code>llm</code>: LLM provider connections - <code>prompt</code>: Prompt libraries and their settings - <code>web</code>: Web service providers (e.g. scraper API configurations)</p> <p>Each group is stored in a separate file in the configuration directory. The file name corresponds to the group name.</p>"},{"location":"instructor/internals/configuration_path/","title":"Configuration Path","text":"<p>Instructor comes with a set of configuration files and prompt templates that you can publish to your project directory.</p> <p>There are 2 ways to set up the location of Instructor's configuration directory: - Using <code>Settings</code> class method <code>setPath()</code> - Using environment variable (recommended)</p> <p> To check how to publish configuration files to your project see Setup section. </p>"},{"location":"instructor/internals/configuration_path/#setting-configuration-path-via-settings-class","title":"Setting Configuration Path via <code>Settings</code> Class","text":"<p>You can set Instructor configuration path using the <code>Settings::setPath()</code> method:</p> <pre><code>&lt;?php\nuse Cognesy\\Config\\Settings;\n\nSettings::setPath('/path/to/config');\n?&gt;\n</code></pre>"},{"location":"instructor/internals/configuration_path/#setting-configuration-path-via-environment-variable","title":"Setting Configuration Path via Environment Variable","text":"<p>You can set the path to Instructor's configuration directory in your <code>.env</code> file:</p> <pre><code>INSTRUCTOR_CONFIG_PATHS='/path/to/config/,another/path'\n</code></pre>"},{"location":"instructor/internals/configuration_path/#configuration-location-resolution","title":"Configuration Location Resolution","text":"<p>Instructor uses a configuration directory with a set of <code>.php</code> files to store its settings, e.g. LLM provider configurations.</p> <p>Instructor will look for its configuration location in the following order: - If static variable value <code>$path</code> in <code>Settings</code> class is set, it will use it, - If <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable is set, it will use its value, - Finally, it will default to the directory, which is bundled with Instructor package (under <code>/config</code>) and contains default set of configuration files.</p>"},{"location":"instructor/internals/debugging/","title":"Debugging","text":"<p>Instructor offers several ways to debug it's internal state and execution flow.</p>"},{"location":"instructor/internals/debugging/#events","title":"Events","text":"<p>Instructor emits events at various points in its lifecycle, which you can listen to and react to. You can use these events to debug execution flow and to inspect data at various stages of processing.</p> <p>For more details see the Events section.</p>"},{"location":"instructor/internals/debugging/#http-debugging","title":"HTTP Debugging","text":"<p>The <code>StructuredOutput</code> class has a <code>withDebug()</code> method that can be used to debug the request and response.</p> <pre><code>$result = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: \"Jason is 25 years old\",\n        responseModel: User:class,\n    )\n    -&gt;get();\n</code></pre> <p>It displays detailed information about the request being sent to LLM API and response received from it, including:</p> <ul> <li>request headers, URI, method and body,</li> <li>response status, headers, and body.</li> </ul> <p>This is useful for debugging the request and response when you are not getting the expected results.</p>"},{"location":"instructor/internals/environment/","title":"Environment","text":""},{"location":"instructor/internals/environment/#environment-configuration","title":"Environment Configuration","text":"<p>Instructor uses environment variables for configuration settings and API keys. The library comes with a <code>.env-dist</code> template file that lists all supported variables.</p> <p>You can copy it to <code>.env</code> (or merge with your <code>.env</code> file) and fill in your values.</p> <p>Alternatively, you can set the variables directly in your environment</p> <p>Check setup instructions for more details on how to set up your environment and how it can be done automatically with the Instructor's CLI tool.</p> <p>     Keep your .env file secure and never commit it to version control.     For production, consider using your environment's secrets management system. </p>"},{"location":"instructor/internals/environment/#llm-provider-api-keys","title":"LLM Provider API Keys","text":"<p>Instructor supports multiple LLM providers.</p> <p>Configure the ones you plan to use:</p> <pre><code># OpenAI (default provider)\nOPENAI_API_KEY=''\n\n# Alternative providers\nANTHROPIC_API_KEY=''\nANYSCALE_API_KEY=''\nAZURE_OPENAI_API_KEY=''\nAZURE_OPENAI_EMBED_API_KEY=''\nCOHERE_API_KEY=''\nFIREWORKS_API_KEY=''\nGEMINI_API_KEY=''\nGROK_API_KEY=''\nGROQ_API_KEY=''\nMISTRAL_API_KEY=''\nOLLAMA_API_KEY=''\nOPENROUTER_API_KEY=''\nTOGETHER_API_KEY=''\nJINA_API_KEY=''\n</code></pre> <p>Only configure the services you plan to use; others can remain empty.</p>"},{"location":"instructor/internals/environment/#other-api-keys-in-env-dist","title":"Other API Keys in <code>.env-dist</code>","text":"<p>Instructor comes with bundled add-on capabilities that use other APIs for various purposes. You can find them in the <code>.env-dist</code> file. They are not required for the core functionality of Instructor.</p>"},{"location":"instructor/internals/environment/#instructor-configuration-directory-path","title":"Instructor Configuration Directory Path","text":"<p>Instructor uses a configuration directory to store its settings, e.g. LLM provider configurations.</p> <p>You can set the path to this directory in your <code>.env</code> file: <pre><code>INSTRUCTOR_CONFIG_PATHS='/../../config/,another/path'\n</code></pre></p> <p>This tells Instructor where to find its configuration files, if it has not been configured manually via <code>Settings</code> class. The path is relative to the vendor directory where Instructor is installed.</p> <p> <code>INSTRUCTOR_CONFIG_PATHS</code> is set automatically if you use the Instructor CLI tool to publish assets. </p>"},{"location":"instructor/internals/events/","title":"Events","text":""},{"location":"instructor/internals/events/#event-classes","title":"Event classes","text":"<p>Instructor dispatches multiple classes of events during its execution. All of them are descendants of <code>Event</code> class.</p> <p>You can listen to these events and react to them in your application, for example to log information or to monitor the execution process.</p> <p>Check the list of available event classes in the <code>Cognesy\\Instructor\\Events</code> namespace.</p>"},{"location":"instructor/internals/events/#event-methods","title":"Event methods","text":"<p>Each Instructor event offers following methods, which make interacting with them more convenient:</p> <ul> <li><code>print()</code> - prints a string representation of the event to console output</li> <li><code>printDebug()</code> - prints a string representation of the event to console output, with additional debug information</li> <li><code>asConsole()</code> - returns the event in a format suitable for console output</li> <li><code>asLog()</code> - returns the event in a format suitable for logging</li> </ul>"},{"location":"instructor/internals/events/#receiving-notification-on-internal-events","title":"Receiving notification on internal events","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <ul> <li><code>(new StructuredOutput)-&gt;onEvent(string $class, callable $callback)</code> method - receive callback when specified type of event is dispatched</li> <li><code>(new StructuredOutput)-&gt;wiretap(callable $callback)</code> method - receive any event dispatched by Instructor, may be useful for debugging or performance analysis</li> </ul> <p>Receiving events can help you to monitor the execution process and makes it easier for a developer to understand and resolve any processing issues.</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    // see requests to LLM\n    -&gt;onEvent(HttpRequestSent::class, fn($e) =&gt; dump($e))\n    // see responses from LLM\n    -&gt;onEvent(HttpResponseReceived::class, fn($event) =&gt; dump($event))\n    // see all events in console-friendly format\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    // log all events in log-friendly format\n    -&gt;wiretap(fn($event) =&gt; YourLogger::log($event-&gt;asLog()))\n\n$structuredOutput-&gt;with(\n    messages: \"What is the population of Paris?\",\n    responseModel: Scalar::integer(),\n)-&gt;get();\n// check your console for the details on the Instructor execution\n</code></pre>"},{"location":"instructor/internals/events/#convenience-methods-for-get-streamed-model-updates","title":"Convenience methods for get streamed model updates","text":"<p><code>StructuredOutput</code> class provides convenience methods allowing client code to receive model updates  when streaming is enabled:</p> <ul> <li><code>onPartialUpdate(callable $callback)</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate(callable $callback)</code> - to handle partial sequence updates of the response</li> </ul> <p>In both cases your callback will receive updated model, so you don't have to extract it from the event.</p>"},{"location":"instructor/internals/instructor/","title":"Instructor","text":""},{"location":"instructor/internals/instructor/#structuredoutput-class","title":"<code>StructuredOutput</code> class","text":"<p><code>StructuredOutput</code> class is the main entry point to the library. It is responsible for handling all interactions with the client code and internal Instructor components.</p>"},{"location":"instructor/internals/instructor/#request-handlers","title":"Request handlers","text":"<p>One of the essential tasks of the <code>StructuredOutput</code> class is to read the configuration and use it to retrieve a component implementing <code>CanHandleRequest</code> interface (specified in the configuration) to process the request and return the response.</p>"},{"location":"instructor/internals/instructor/#dispatched-events","title":"Dispatched events","text":"<p><code>StructuredOutput</code> class dispatches several high level events during initialization and processing of the request and response:</p> <ul> <li><code>StructuredOutputStarted</code> - dispatched when the structured output processes starts</li> <li><code>StructuredOutputRequestReceived</code> - dispatched when the request is received</li> <li><code>StructuredOutputResponseGenerated</code> - dispatched when the response is generated</li> <li><code>StructuredOutputResponseUpdated</code> - dispatched when the response update is streamed</li> </ul>"},{"location":"instructor/internals/instructor/#event-listeners","title":"Event listeners","text":"<p><code>StructuredOutput</code> class provides several methods allowing client code to plug into Instructor event system, including:  - <code>onEvent()</code> - to receive a callback when specified type of event is dispatched  - <code>wiretap()</code> - to receive any event dispatched by Instructor</p>"},{"location":"instructor/internals/instructor/#response-model-updates","title":"Response model updates","text":"<p>Additionally, <code>StructuredOutput</code> class provides convenience methods allowing client code to receive model updates when streaming is enabled:</p> <ul> <li><code>onPartialUpdate()</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate()</code> - to handle partial sequence updates of the response</li> </ul>"},{"location":"instructor/internals/instructor/#error-handling","title":"Error handling","text":"<p><code>StructuredOutput</code> class contains top level try-catch block to let user handle any uncaught errors before throwing them back to the client code. It allows you to register a handler which will log the error or notify your monitoring system about a problem.</p>"},{"location":"instructor/internals/lifecycle/","title":"Lifecycle","text":""},{"location":"instructor/internals/lifecycle/#instructors-request-lifecycle","title":"Instructor's request lifecycle","text":"<p>As Instructor for PHP processes your request, it goes through several stages:</p> <ol> <li>Initialize and self-configure (with possible overrides defined by developer).</li> <li>Analyze classes and properties of the response data model specified by developer.</li> <li>Translate data model into a schema that can be provided to LLM.</li> <li>Execute request to LLM using specified messages (content) and response model metadata.</li> <li>Receive a response from LLM or multiple partial responses (if streaming is enabled).</li> <li>Deserialize response received from LLM into originally requested classes and their properties.</li> <li>In case response contained unserializable data - create feedback message for LLM and request regeneration of the response.</li> <li>Execute validations defined by developer on the deserialized data - if any of them fail, create feedback message for LLM and requests regeneration of the response.</li> <li>Repeat the steps 4-8, unless specified limit of retries has been reached or response passes validation.</li> </ol>"},{"location":"instructor/internals/response_models/","title":"Response Models","text":"<p>Instructor's request parameter <code>responseModel</code> allows you to specify shape of the response you expect from LLM .</p> <p>Instructor translates the <code>responseModel</code> parameter into actual schema based on the type and value of the parameter.</p>"},{"location":"instructor/internals/response_models/#handling-string-responsemodel-value","title":"Handling string $responseModel value","text":"<p>If <code>string</code> value is provided, it is used as a name of the class of the response model.</p> <p>Instructor checks if the class exists and analyzes the class &amp; properties type information &amp; doc comments to generate a schema needed to specify LLM response constraints.</p> <p>The best way to provide the name of the response model class is to use <code>NameOfTheClass::class</code>, making it easy for IDE to check the type, handle refactorings, etc.</p>"},{"location":"instructor/internals/response_models/#handling-object-responsemodel-value","title":"Handling object $responseModel value","text":"<p>If <code>object</code> value is provided, it is considered an instance of the response model. Instructor checks the class of the instance, then analyzes it and its property type data to specify LLM response constraints.</p>"},{"location":"instructor/internals/response_models/#handling-array-responsemodel-value","title":"Handling array $responseModel value","text":"<p>If <code>array</code> value is provided, it is considered a raw JSON Schema, therefore allowing Instructor to use it directly in LLM requests (after wrapping in appropriate context - e.g. function call).</p> <p>Instructor requires information on the class of each nested object in your JSON Schema, so it can correctly deserialize the data into appropriate type.</p> <p>This information is available to Instructor when you are passing $responseModel as a class name or an instance, but it is missing from raw JSON Schema. Lack of the information on target class makes it impossible for Instructor to deserialize the data into appropriate, expected type.</p> <p>Current design uses JSON Schema <code>$comment</code> field on property to overcome this information gap. Instructor expects developer to use <code>$comment</code> field to provide fully qualified name of the target class to be used to deserialize property data of object or enum type.</p>"},{"location":"instructor/internals/response_models/#custom-response-handling-strategy","title":"Custom response handling strategy","text":"<p>Instructor allows you to customize processing of <code>$responseModel</code> value also by looking at the interfaces the class or instance implements:</p> <ul> <li><code>CanProvideJsonSchema</code> - implement to be able to provide raw JSON Schema (as an array) of the response model, overriding the default approach of Instructor, which is analyzing $responseModel value class information,</li> <li><code>CanProvideSchema</code> - implement to be able to provide <code>Schema</code> object of the response model, overriding class analysis stage; can be useful in building object wrappers (see: <code>Sequence</code> class),</li> <li><code>CanDeserializeSelf</code> - implement to customize the way the response from LLM is deserialized from JSON into PHP object,</li> <li><code>CanValidateSelf</code> - implement to customize the way the deserialized object is validated - it fully replaces the default validation process for given response model,</li> <li><code>CanTransformSelf</code> - implement to transform the validated object into any target value that will be then passed back to the caller (e.g. unwrap simple type from a class to scalar value)</li> </ul> <p>Methods implemented by those interfaces are executed as following:</p> <ul> <li><code>CanProvideJsonSchema</code> - executed during the schema generation phase,</li> <li><code>CanDeserializeSelf</code> - executed during the deserialization phase,</li> <li><code>CanValidateSelf</code> - executed during the validation phase,</li> <li><code>CanTransformSelf</code> - executed during the transformation phase.</li> </ul> <p>When implementing custom response handling strategy, avoid doing all transformations in a single block of code. Split the logic between relevant methods implemented by your class for clarity and easier code maintenance.</p>"},{"location":"instructor/internals/response_models/#example-implementations","title":"Example implementations","text":"<p>For a practical example of using those contracts to customize Instructor processing flow see:</p> <ul> <li>src/Extras/Scalar/</li> <li>src/Extras/Sequence/</li> </ul> <p>Examples contain an implementation of custom response model handling strategies, e.g. providing scalar value support via a wrapper class implementing:  - custom schema provider,  - deserialization,  - validation  - and transformation</p> <p>into requested value type.</p>"},{"location":"instructor/internals/settings_class/","title":"Settings class","text":""},{"location":"instructor/internals/settings_class/#settings-class","title":"<code>Settings</code> Class","text":"<p><code>Settings</code> class is the main entry point for telling Instructor where to look for its configuration. It allows you to set up path of Instructor's configuration directory and access Instructor settings.</p> <p><code>Settings</code> class provides the following methods: - <code>setPath(string $path)</code>: Set the path to Instructor's configuration directory - <code>getPath(): string</code>: Get current path to Instructor's configuration directory - <code>has($group, $key): bool</code>: Check if a specific setting exists in Instructor's configuration - <code>get($group, $key, $default): mixed</code>: Get a specific setting from Instructor's configuration - <code>set($group, $key, $value)</code>: Set a specific setting in Instructor's configuration</p> <p>You won't usually need to use these methods directly, but they are used internally by Instructor to access configuration settings.</p>"},{"location":"instructor/misc/contributing/","title":"Contributing","text":""},{"location":"instructor/misc/contributing/#contributing","title":"Contributing","text":"<p>If you want to help, check out some of the issues. They could be anything from code improvements, a guest blog post, or a new cookbook.</p>"},{"location":"instructor/misc/help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p> <p>          The concepts section explains the core concepts of Instructor and how to prompt with models.               The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.      <pre><code>&lt;Card\n    title=\"GitHub Discussions\"\n    icon=\"github\"\n    href=\"https://github.com/cognesy/instructor-php/discussions\"\n&gt;\n    [GitHub discussions](https://github.com/cognesy/instructor-php/discussions) are useful for asking questions, your question and the answer will help everyone.\n&lt;/Card&gt;\n&lt;Card\n    title=\"GitHub Issues\"\n    icon=\"bug\"\n    href=\"https://github.com/cognesy/instructor-php/issues\"\n&gt;\n    [GitHub issues](https://github.com/cognesy/instructor-php/issues) are useful for reporting bugs or requesting new features.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Discord\"\n    icon=\"discord\"\n    href=\"https://discord.gg/CV8sPM5k5Y\"\n&gt;\n    The [Discord](https://discord.gg/CV8sPM5k5Y) is a great place to ask questions and get help from the community.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Blog\"\n    icon=\"newspaper\"\n    href=\"https://cognesy.com/blog\"\n&gt;\n    The [blog](blog/index.md) contains articles that explain how to use Instructor in different scenarios.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Twitter\"\n    icon=\"twitter\"\n    href=\"https://twitter.com/ddebowczyk\"\n&gt;\n    You can also reach out to me [@ddebowczyk](https://twitter.com/ddebowczyk) if you have any questions or ideas.\n&lt;/Card&gt;\n</code></pre> <p></p>"},{"location":"instructor/misc/llm_providers/","title":"LLM API Providers","text":"<p>Following table lists the supported providers:</p> <ul> <li>A21</li> <li>Anthropic</li> <li>Azure</li> <li>Cerebras</li> <li>Cohere</li> <li>Cohere (OpenAI compatible API)</li> <li>DeepSeek</li> <li>Fireworks</li> <li>Gemini</li> <li>Gemini (OpenAI compatible API)</li> <li>Groq</li> <li>MiniMaxi</li> <li>Mistral</li> <li>Moonshot</li> <li>Ollama</li> <li>OpenAI</li> <li>OpenRouter</li> <li>Perplexity</li> <li>SambaNova</li> <li>Together</li> <li>XAI</li> </ul> <p>Check the cookbook for examples of how to use them.</p>"},{"location":"instructor/misc/philosophy/","title":"Philosophy","text":""},{"location":"instructor/misc/philosophy/#philosophy-of-instructor","title":"Philosophy of Instructor","text":"<p>NOTE: Philosophy behind Instructor was formulated by Jason Liu, the creator of original version of Instructor in Python and adapted for the PHP port.</p> <p>Instructor values simplicity and flexibility in leveraging language models. It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions.</p> <p>\u201cSimplicity is a great virtue, but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"instructor/misc/philosophy/#simplicity","title":"Simplicity","text":"<ol> <li>Most users will only need to learn <code>responseModel</code> and <code>StructuredOutput::create()</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"instructor/misc/philosophy/#transparency","title":"Transparency","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We give you config over the prompts we do write ('reasking' and in the future - JSON_MODE prompts).</li> </ol>"},{"location":"instructor/misc/philosophy/#flexibility","title":"Flexibility","text":"<ol> <li>If you build a system with OpenAI directly, it is easy to incrementally adopt Instructor by just adding <code>StructuredOutput::create()</code> with data schemas fed in via <code>responseModel</code>.</li> <li>Use any class to define your data schema (no need to inherit from some base class).</li> </ol>"},{"location":"instructor/misc/philosophy/#the-zen-of-instructor","title":"The zen of Instructor","text":"<p>Maintain the flexibility and power of PHP classes, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define data schema <code>class SomeStructuredData { ... }</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>function extract($input) : SomeStructuredData</code></li> <li>Define typed computations against your data with <code>function compute(SomeStructuredData $data):</code> or call methods on your schema <code>$data-&gt;compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"instructor/misc/philosophy/#our-goals","title":"Our Goals","text":"<p>The goal for the library and documentation is to help you be a better programmer and, as a result, a better AI engineer.</p> <ul> <li>The library is a result of our desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>We won't try to write prompts for you,</li> <li>We don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit us up @jnxlco or @ddebowczyk</p> <p>Cheers!</p>"},{"location":"instructor/techniques/classification/","title":"Classification","text":""},{"location":"instructor/techniques/classification/#text-classification-using-llm","title":"Text Classification using LLM","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using LLM (via OpenAI API), PHP's <code>enums</code> and classes.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using language models in combination with PHP data structures.</p>"},{"location":"instructor/techniques/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"instructor/techniques/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>&lt;?php\n// Enumeration for single-label text classification. \nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction. \nclass SinglePrediction {\n    public Label $classLabel;\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Perform single-label classification on the input text. \n */\nfunction classify(string $data) : SinglePrediction {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n        model: \"gpt-3.5-turbo-0613\",\n    )-&gt;get();\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\nassert($prediction-&gt;classLabel == Label::SPAM);\n</code></pre>"},{"location":"instructor/techniques/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"instructor/techniques/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass Ticket {\n    /** @var Label[] */\n    public array $ticketLabels = [];\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : Ticket {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify following support ticket: {$data}\",\n        ]],\n        responseModel: Ticket::class,\n        model: \"gpt-3.5-turbo-0613\",\n    )-&gt;get();\n}\n</code></pre>"},{"location":"instructor/techniques/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;classLabels));\nassert(in_array(Label::BILLING, $prediction-&gt;classLabels));\n</code></pre>"},{"location":"instructor/techniques/prompting/","title":"Prompting","text":""},{"location":"instructor/techniques/prompting/#general-tips-for-prompt-engineering","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor for function calling is to make the models self-descriptive, modular, and flexible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use PHPDoc comments or #[Description('')] attribute for clear field descriptions.</li> <li>Optionality: Use PHP's nullable types (e.g. ?int) for optional fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"instructor/techniques/prompting/#utilize-nullable-attribute","title":"Utilize Nullable Attribute","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?Role $role = null; \n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?string $role = null;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $result = null;\n    public ?string $errorMessage = '';\n    public bool $error = false;\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;error ? null : $this-&gt;result;\n    }\n}\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in 'errorMessage'.</p> <p>Original Instructor implementation in Python provides utility class Maybe making this pattern even easier. Such mechanism is not yet available in PHP version of Instructor.</p>"},{"location":"instructor/techniques/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>&lt;?php\nenum Role : string {\n    case Principal = 'principal'\n    case Teacher = 'teacher'\n    case Student = 'student'\n    case Other = 'other'\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /**  Correctly assign one of the predefined roles to the user. */\n    public Role $role;\n}\n</code></pre> <p>If you'd like to improve LLM inference performance, try reiterating the requirements in the field descriptions (in the docstrings).</p>"},{"location":"instructor/techniques/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>&lt;?php\n/** Extract the role based on the following rules: &lt;your rules go here&gt; */\nclass Role\n{\n    /** Restate the instructions and rules to correctly determine the title. */\n    public string $instructions;\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>&lt;?php\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>&lt;?php\nclass Property\n{\n    /**  Monotonically increasing ID */\n    public string $index; \n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age\n    public string $name;\n    /** @var Property[] Numbered list of arbitrary extracted properties, should be less than 3 */\n    public array $properties;\n}\n</code></pre> <p>To be 100% certain the list does not exceed the limit add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"instructor/techniques/prompting/#consistent-arbitrary-properties","title":"Consistent Arbitrary Properties","text":"<p>For multiple records containing arbitrary properties, instruct LLM to use consistent key names when extracting properties.</p> <pre><code>&lt;?php\nclass Property {\n    public int $id;\n    public string $key;\n    public string $name;\n}\n\nclass UserDetails\n{\n    /** @var UserDetail[] Extract information for multiple users. Use consistent key names for properties across users. */\n    public array $users;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> field:</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /** @var int[] Correct and complete list of coworker IDs, representing collaboration between users. */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /** @var UserDetail[] Collection of users, correctly capturing the relationships among them. */\n    public array $users;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>&lt;?php\nclass Role\n{\n    /** Think step by step to determine the correct title. */\n    public string $chainOfThought = '';\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>&lt;?php\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public int $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n</code></pre>"},{"location":"instructor/techniques/prompting/#adding-context-to-components","title":"Adding Context to Components","text":"<p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>&lt;?php\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n</code></pre>"},{"location":"instructor/techniques/search/","title":"Search","text":""},{"location":"instructor/techniques/search/#expanding-search-queries","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p>"},{"location":"instructor/techniques/search/#motivation","title":"Motivation","text":"<p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"instructor/techniques/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>&lt;?php\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n</code></pre>"},{"location":"instructor/techniques/search/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>StructuredOutput::create()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nfunction segment(string $data) : Search {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    )-&gt;get();\n}\n\nforeach (segment(\"Search for a picture of a cat and a video of a dog\")-&gt;queries as $query) {\n    $query-&gt;execute();\n    // dump($query);\n}\n</code></pre>"},{"location":"polyglot/overview/","title":"Overview","text":"<p>Polyglot is a PHP library that provides a unified API for interacting with various Large Language Model (LLM) providers. It allows developers to build applications that use LLMs without being locked into a specific provider or having to rewrite code when switching between providers.</p> <p>The core philosophy behind Polyglot is to create a consistent, provider-agnostic interface that abstracts away the differences between LLM APIs, while still allowing access to provider-specific features when needed. This enables developers to:</p> <ul> <li>Write code once and use it with any supported LLM provider</li> <li>Easily switch between providers without changing application code</li> <li>Use different providers in different environments (development, testing, production)</li> <li>Fall back to alternative providers if one becomes unavailable</li> </ul> <p>Polyglot was developed as part of the Instructor for PHP library, which focuses on structured outputs from LLMs, but can also be used as a standalone library for general LLM interactions.</p>"},{"location":"polyglot/overview/#key-features","title":"Key Features","text":""},{"location":"polyglot/overview/#unified-llm-api","title":"Unified LLM API","text":"<p>Polyglot's primary feature is its unified API that works across multiple LLM providers:</p> <ul> <li>Consistent interface for making inference or embedding requests</li> <li>Common message format across all providers</li> <li>Standardized response handling</li> <li>Unified error handling</li> </ul>"},{"location":"polyglot/overview/#framework-agnostic","title":"Framework-Agnostic","text":"<p>Polyglot is designed to work with any PHP framework or even in plain PHP applications. It does not depend on any specific framework, making it easy to integrate into existing projects.</p> <ul> <li>Compatible with Laravel, Symfony, CodeIgniter, and others</li> <li>Can be used in CLI scripts or web applications</li> <li>Lightweight and easy to install</li> </ul>"},{"location":"polyglot/overview/#comprehensive-provider-support","title":"Comprehensive Provider Support","text":"<p>Polyglot supports a wide range of LLM providers, including:</p> <ul> <li>OpenAI (GPT models)</li> <li>Anthropic (Claude models)</li> <li>Google Gemini (native and OpenAI compatible)</li> <li>Mistral AI</li> <li>Azure OpenAI</li> <li>Cohere</li> <li>And many others (see full list below)</li> </ul>"},{"location":"polyglot/overview/#multiple-interaction-modes","title":"Multiple Interaction Modes","text":"<p>Polyglot supports various modes of interaction with LLMs:</p> <ul> <li>Text mode: Simple text completion/chat</li> <li>JSON mode: Structured JSON responses</li> <li>JSON Schema mode: Responses validated against a schema</li> <li>Tools mode: Function/tool calling for task execution</li> </ul>"},{"location":"polyglot/overview/#streaming-support","title":"Streaming Support","text":"<p>Real-time streaming of responses is supported across compatible providers:</p> <ul> <li>Token-by-token streaming</li> <li>Progress handling</li> <li>Partial response accumulation</li> </ul>"},{"location":"polyglot/overview/#embeddings-generation","title":"Embeddings Generation","text":"<p>Beyond text generation, Polyglot includes support for vector embeddings:</p> <ul> <li>Generate embeddings from text</li> <li>Support for multiple embedding providers</li> <li>Utilities for finding similar documents</li> </ul>"},{"location":"polyglot/overview/#configuration-flexibility","title":"Configuration Flexibility","text":"<p>Polyglot offers a flexible configuration system:</p> <ul> <li>Configure multiple providers simultaneously</li> <li>Environment-based configuration</li> <li>Runtime provider switching</li> <li>Per-request customization</li> </ul>"},{"location":"polyglot/overview/#middleware-and-extensibility","title":"Middleware and Extensibility","text":"<p>The library is built with extensibility in mind:</p> <ul> <li>HTTP client middleware for customization</li> <li>Event system for request/response monitoring</li> <li>Ability to add custom providers</li> </ul>"},{"location":"polyglot/overview/#use-cases","title":"Use Cases","text":"<p>Polyglot is a good choice for a variety of use cases:</p> <ul> <li>Applications requiring LLM provider flexibility: Switch between providers based on cost, performance, or feature needs</li> <li>Multi-environment deployments: Use different LLM providers in development, staging, and production</li> <li>Redundancy and fallback: Implement fallback strategies when a provider is unavailable</li> <li>Hybrid approaches: Combine different providers for different tasks based on their strengths</li> <li>Local + cloud development: Use local models (via Ollama) for development and cloud providers for production</li> </ul>"},{"location":"polyglot/overview/#supported-providers","title":"Supported Providers","text":""},{"location":"polyglot/overview/#inference-providers","title":"Inference Providers","text":"<p>Polyglot currently supports the following LLM providers for chat completion:</p> <ul> <li>A21: API access to Jamba models</li> <li>Anthropic: Claude family of models</li> <li>Microsoft Azure: Azure-hosted OpenAI models</li> <li>Cerebras: Cerebras LLMs</li> <li>Cohere: Command models (both native and OpenAI compatible interfaces)</li> <li>Deepseek: Deepseek models including reasoning capabilities</li> <li>Google Gemini: Google's Gemini models (both native and OpenAI compatible)</li> <li>Groq: High-performance inference platform</li> <li>Hugging Face: Hugging Face hosted models</li> <li>Meta: Jina AI models</li> <li>Minimaxi: MiniMax models</li> <li>Mistral: Mistral AI models</li> <li>Moonshot: Kimi models</li> <li>Ollama: Self-hosted open source models</li> <li>OpenAI: GPT models family</li> <li>OpenRouter: Multi-provider routing service</li> <li>Perplexity: Perplexity models</li> <li>SambaNova: SambaNova hosted models</li> <li>Together: Together AI hosted models</li> <li>xAI: xAI's Grok models</li> </ul>"},{"location":"polyglot/overview/#embeddings-providers","title":"Embeddings Providers","text":"<p>For embeddings generation, Polyglot supports:</p> <ul> <li>Microsoft Azure: Azure-hosted OpenAI embeddings</li> <li>Cohere: Cohere embeddings models</li> <li>Google Gemini: Google's embedding models</li> <li>Jina: Jina embeddings</li> <li>Mistral: Mistral embedding models</li> <li>Ollama: Self-hosted embedding models</li> <li>OpenAI: OpenAI embeddings</li> </ul>"},{"location":"polyglot/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with Polyglot in your PHP project in under 5 minutes.</p> <p>For detailed setup instructions, see Setup.</p>"},{"location":"polyglot/quickstart/#install-polyglot-with-composer","title":"Install Polyglot with Composer","text":"<p>To install Polyglot in your project, run following command in your terminal:</p> <pre><code>composer require cognesy/instructor-polyglot\n</code></pre> <p>NOTE: Polyglot is already included in Instructor for PHP package, so if you have it installed, you don't need to install Polyglot separately.</p>"},{"location":"polyglot/quickstart/#create-and-run-example","title":"Create and Run Example","text":""},{"location":"polyglot/quickstart/#step-1-prepare-your-openai-api-key","title":"Step 1: Prepare your OpenAI API Key","text":"<p>In this example, we'll use OpenAI as the LLM provider. You can get it from the OpenAI dashboard.</p>"},{"location":"polyglot/quickstart/#step-2-create-a-new-php-file","title":"Step 2: Create a New PHP File","text":"<p>In your project directory, create a new PHP file <code>test-polyglot.php</code>:</p> <pre><code>&lt;?php\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Set up OpenAI API key\n$apiKey = 'your-openai-api-key';\nputenv(\"OPENAI_API_KEY=\" . $apiKey);\n// WARNING: In real project you should set up API key in .env file.\n\n$answer = (new Inference)\n    -&gt;withMessages('What is capital of Germany')\n    -&gt;get();\n\necho \"USER: What is capital of Germany\\n\";\necho \"ASSISTANT: $answer\\n\";\n</code></pre> <p>     You should never put your API keys directly in your real project code to avoid getting them compromised. Set them up in your .env file. </p>"},{"location":"polyglot/quickstart/#step-3-run-the-example","title":"Step 3: Run the Example","text":"<p>Now, you can run the example:</p> <pre><code>php test-polyglot.php\n\n# Output:\n# USER: What is capital of Germany\n# ASSISTANT: Berlin\n</code></pre>"},{"location":"polyglot/quickstart/#next-steps","title":"Next Steps","text":"<p>You can start using Polyglot in your project right away after installation.</p> <p>But it's recommended to publish configuration files and prompt templates to your project directory, so you can customize the library's behavior and use your own prompt templates.</p> <p>You should also set up LLM provider API keys in your <code>.env</code> file instead of putting them directly in your code.</p> <p>See setup instructions for more details.</p>"},{"location":"polyglot/setup/","title":"Setup","text":"<p>This chapter will guide you through the initial steps of setting up and using Polyglot in your PHP project. We'll cover installation and configuration to get you up and running quickly.</p>"},{"location":"polyglot/setup/#installation","title":"Installation","text":"<p>You can install it using Composer:</p> <pre><code>composer require cognesy/instructor-polyglot\n</code></pre> <p>This will install Polyglot along with its dependencies.</p> <p>NOTE: Polyglot is distributed as part of the Instructor PHP package, so if you have it installed, you don't need to install Polyglot separately.</p>"},{"location":"polyglot/setup/#requirements","title":"Requirements","text":"<ul> <li>PHP 8.2 or higher</li> <li>Composer</li> <li>Valid API keys for at least one supported LLM provider</li> </ul>"},{"location":"polyglot/setup/#configuration","title":"Configuration","text":""},{"location":"polyglot/setup/#setting-up-api-keys","title":"Setting Up API Keys","text":"<p>Polyglot requires API keys to authenticate with LLM providers. The recommended approach is to use environment variables:</p> <ol> <li>Create a <code>.env</code> file in your project root (or use your existing one)</li> <li>Add your API keys:</li> </ol> <pre><code># OpenAI\nOPENAI_API_KEY=sk-your-openai-key\n\n# Anthropic\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\n\n# Other providers as needed\nMISTRAL_API_KEY=your-mistral-key\nGEMINI_API_KEY=your-gemini-key\n# etc.\n</code></pre>"},{"location":"polyglot/setup/#configuration-files","title":"Configuration Files","text":"<p>Polyglot loads its configuration from PHP files.</p> <p>The default configuration files are located in the Instructor package, but you can publish and customize them:</p> <ol> <li>Create a <code>config</code> directory in your project if it doesn't exist</li> <li>Copy the configuration files from the Instructor package:</li> </ol> <pre><code># Create config directory if it doesn't exist\nmkdir -p config\n\n# Copy configuration files\ncp vendor/cognesy/instructor-polyglot/config/* config/\n</code></pre> <ol> <li>Customize the configuration files as needed</li> </ol>"},{"location":"polyglot/setup/#llm-configuration","title":"LLM Configuration","text":"<p>The <code>llm.php</code> configuration file contains settings for LLM providers:</p> <pre><code>&lt;?php\n// Example of a simplified config/llm.php\n\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',  // Default connection to use\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        'anthropic' =&gt; [\n            'driver' =&gt; 'anthropic',\n            'apiUrl' =&gt; 'https://api.anthropic.com/v1',\n            'apiKey' =&gt; Env::get('ANTHROPIC_API_KEY', ''),\n            'endpoint' =&gt; '/messages',\n            'metadata' =&gt; [\n                'apiVersion' =&gt; '2023-06-01',\n            ],\n            'model' =&gt; 'claude-3-haiku-20240307',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/setup/#embeddings-configuration","title":"Embeddings Configuration","text":"<p>The <code>embed.php</code> configuration file contains settings for embeddings providers:</p> <pre><code>&lt;?php\n// Example of a simplified config/embed.php\n\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/embeddings',\n            'model' =&gt; 'text-embedding-3-small',\n            'dimensions' =&gt; 1536,\n            'maxInputs' =&gt; 16,\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/setup/#custom-configuration-location","title":"Custom Configuration Location","text":"<p>By default, Polyglot looks for custom configuration files in the <code>config</code> directory relative to your project root. You can specify a different location by setting the <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable:</p> <pre><code>INSTRUCTOR_CONFIG_PATHS='/path/to/your/config,alternative/path'\n</code></pre>"},{"location":"polyglot/setup/#overriding-configuration-location","title":"Overriding Configuration Location","text":"<p>You can use <code>Settings</code> class static <code>setPath()</code> method to override the value of config path set in environment variable with your own value.</p> <pre><code>use Cognesy\\Config\\Settings;\n\nSettings::setPath('/your/path/to/config');\n</code></pre>"},{"location":"polyglot/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"polyglot/setup/#common-installation-issues","title":"Common Installation Issues","text":"<ul> <li>Composer Dependencies: Make sure you have PHP 8.2+ installed and Composer correctly configured.</li> <li>API Keys: Verify that your API keys are correctly set in your environment variables.</li> <li>Configuration Files: Check that your configuration files are properly formatted and accessible.</li> </ul>"},{"location":"polyglot/setup/#testing-your-installation","title":"Testing Your Installation","text":"<p>A simple way to test if everything is working correctly is to run a small script:</p> <pre><code>&lt;?php\nrequire 'vendor/autoload.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$result = (new Inference)\n    -&gt;withMessages('Say hello.')\n    -&gt;get();\n</code></pre> <p>If you see a friendly greeting, your installation is working correctly!</p>"},{"location":"polyglot/advanced/connection-mgmt/","title":"Preset Management","text":"<p>One of Polyglot's strengths is the ability to easily switch between different LLM providers, which is made easy by using connection presets.</p> <p>More complex applications may need to manage multiple LLM provider connections and switch between them dynamically to implement fallback strategies or leverage the strengths of different models and providers for various tasks.</p>"},{"location":"polyglot/advanced/connection-mgmt/#switching-providers","title":"Switching Providers","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n// Use OpenAI\n$openaiResponse = $inference\n    -&gt;using('openai')\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\necho \"OpenAI response: $openaiResponse\\n\";\n\n// Switch to Anthropic\n$anthropicResponse = $inference\n    -&gt;using('anthropic')\n    -&gt;withMessages('What is the capital of Germany?')\n    -&gt;get();\n\necho \"Anthropic response: $anthropicResponse\\n\";\n</code></pre>"},{"location":"polyglot/advanced/connection-mgmt/#implementing-fallbacks","title":"Implementing Fallbacks","text":"<p>You can implement a fallback mechanism to try alternative providers if one fails:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withFallback(array $providers, callable $requestFn) {\n    $lastException = null;\n\n    foreach ($providers as $provider) {\n        try {\n            $inference = (new Inference)-&gt;using($provider);\n            return $requestFn($inference);\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            echo \"Provider '$provider' failed: {$e-&gt;getMessage()}. Trying next provider...\\n\";\n        }\n    }\n\n    throw new \\Exception(\"All providers failed. Last error: \" .\n        ($lastException ? $lastException-&gt;getMessage() : \"Unknown error\"));\n}\n\n// Usage\ntry {\n    $providers = ['openai', 'anthropic', 'gemini'];\n\n    $response = withFallback($providers, function($inference) {\n        return $inference-&gt;with(\n            messages: 'What is the capital of France?'\n        )-&gt;toText();\n    });\n\n    echo \"Response: $response\\n\";\n} catch (\\Exception $e) {\n    echo \"Error: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/advanced/connection-mgmt/#cost-aware-provider-selection","title":"Cost-Aware Provider Selection","text":"<p>You might want to select providers based on cost considerations:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nclass CostAwareLLM {\n    private $inference;\n    private $providers = [\n        'low' =&gt; [\n            'preset' =&gt; 'ollama',\n            'model' =&gt; 'llama2',\n        ],\n        'medium' =&gt; [\n            'preset' =&gt; 'mistral',\n            'model' =&gt; 'mistral-small-latest',\n        ],\n        'high' =&gt; [\n            'preset' =&gt; 'openai',\n            'model' =&gt; 'gpt-4o',\n        ],\n    ];\n\n    public function __construct() {\n        $this-&gt;inference = new Inference();\n    }\n\n    public function ask(string $question, string $tier = 'medium'): string {\n        $provider = $this-&gt;providers[$tier] ?? $this-&gt;providers['medium'];\n\n        return $this-&gt;inference-&gt;using($provider['preset'])\n            -&gt;with(\n                messages: $question,\n                model: $provider['model']\n            )\n            -&gt;get();\n    }\n}\n\n// Usage\n$costAwareLLM = new CostAwareLLM();\n\n// Simple question - use low-cost tier\n$simpleQuestion = \"What is the capital of France?\";\necho \"Simple question (low cost): $simpleQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($simpleQuestion, 'low') . \"\\n\\n\";\n\n// More complex question - use medium-cost tier\n$mediumQuestion = \"Explain the concept of deep learning in simple terms.\";\necho \"Medium question (medium cost): $mediumQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($mediumQuestion, 'medium') . \"\\n\\n\";\n\n// Critical question - use high-cost tier\n$complexQuestion = \"Analyze the ethical implications of AI in healthcare.\";\necho \"Complex question (high cost): $complexQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($complexQuestion, 'high') . \"\\n\\n\";\n</code></pre>"},{"location":"polyglot/advanced/connection-mgmt/#provider-selection-strategy","title":"Provider Selection Strategy","text":"<p>You can implement a strategy to select the most appropriate provider for each request:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nclass GroupOfExperts {\n    private $inference;\n    private $providerStrategies = [\n        'creative' =&gt; 'anthropic',\n        'factual' =&gt; 'openai',\n        'code' =&gt; 'gemini',\n        'default' =&gt; 'openai',\n    ];\n\n    public function __construct() {\n        $this-&gt;inference = new Inference();\n    }\n\n    public function ask(string $question, string $taskType = 'default'): string {\n        // Select the appropriate provider based on the task type\n        $preset = $this-&gt;providerStrategies[$taskType] ?? $this-&gt;providerStrategies['default'];\n\n        // Use the selected provider\n        return $this-&gt;inference-&gt;using($preset)\n            -&gt;with(messages: $question)\n            -&gt;get();\n    }\n}\n\n// Usage\n$experts = new GroupOfExperts();\n\n$tasks = [\n    [\"Write a short poem about the ocean.\", 'creative'],\n    [\"Create a brief story about a robot discovering emotions.\", 'creative'],\n    [\"What is the capital of France?\", 'factual'],\n    [\"Who wrote 'Pride and Prejudice'?\", 'factual'],\n    [\"Write a PHP function to check if a string is a palindrome.\", 'code'],\n    [\"Create a simple JavaScript function to sort an array of objects by a property.\", 'code'],\n];\n\nforeach ($tasks as $task) {\n    echo \"Task: $task\\n\";\n    echo \"Response: \" . $experts-&gt;ask($task[0], $task[1]) . \"\\n\\n\";\n}\n</code></pre>"},{"location":"polyglot/advanced/context-caching/","title":"Context Caching","text":"<p>Context caching improves performance by reusing parts of a conversation, reducing token usage and API costs. This is particularly useful for multi-turn conversations or when processing large documents.</p>"},{"location":"polyglot/advanced/context-caching/#using-cached-context","title":"Using Cached Context","text":"<p>Polyglot supports context caching through the <code>withCachedContext()</code> method:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference()-&gt;using('anthropic');\n\n// Set up a conversation with cached context\n$inference-&gt;withCachedContext(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant who provides concise answers.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'I want to discuss machine learning concepts.'],\n        ['role' =&gt; 'assistant', 'content' =&gt; 'Great! I\\'d be happy to discuss machine learning concepts with you. What specific aspect would you like to explore?'],\n    ]\n);\n\n// First query using the cached context\n$response1 = $inference-&gt;with(\n    messages: 'What is supervised learning?'\n)-&gt;response();\n\necho \"Response 1: \" . $response1-&gt;content() . \"\\n\";\necho \"Tokens from cache: \" . $response1-&gt;usage()-&gt;cacheReadTokens . \"\\n\\n\";\n\n// Second query, still using the same cached context\n$response2 = $inference-&gt;with(\n    messages: 'And what about unsupervised learning?'\n)-&gt;response();\n\necho \"Response 2: \" . $response2-&gt;content() . \"\\n\";\necho \"Tokens from cache: \" . $response2-&gt;usage()-&gt;cacheReadTokens . \"\\n\";\n</code></pre>"},{"location":"polyglot/advanced/context-caching/#provider-support-for-context-caching","title":"Provider Support for Context Caching","text":"<p>Different providers have varying levels of support for context caching:</p> <ul> <li>Anthropic: Supports native context caching with explicit cache markers</li> <li>OpenAI: Provides automatic caching for optimization, but not as explicit as Anthropic</li> <li>Other providers: May not support native caching, but Polyglot still helps manage conversation state</li> </ul>"},{"location":"polyglot/advanced/context-caching/#processing-large-documents-with-cached-context","title":"Processing Large Documents with Cached Context","text":"<p>Context caching is particularly valuable when working with large documents:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Load a large document\n$documentContent = file_get_contents('large_document.txt');\n\n// Set up cached context with the document\n$inference = new Inference()-&gt;using('anthropic');\n$inference-&gt;withCachedContext(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You will help analyze and summarize documents.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'Here is the document to analyze:'],\n        ['role' =&gt; 'user', 'content' =&gt; $documentContent],\n    ]\n);\n\n// Ask multiple questions about the document without resending it each time\n$questions = [\n    'Summarize the key points of this document in 3 bullets.',\n    'What are the main arguments presented?',\n    'Are there any contradictions or inconsistencies in the text?',\n    'What conclusions can be drawn from this document?',\n];\n\nforeach ($questions as $index =&gt; $question) {\n    $response = $inference-&gt;with(messages: $question)-&gt;response();\n\n    echo \"Question \" . ($index + 1) . \": $question\\n\";\n    echo \"Answer: \" . $response-&gt;content() . \"\\n\";\n    echo \"Tokens from cache: \" . $response-&gt;usage()-&gt;cacheReadTokens . \"\\n\\n\";\n}\n</code></pre>"},{"location":"polyglot/advanced/custom-config/","title":"Configuration Deep Dive","text":"<p>One of Polyglot's core strengths is its ability to work with multiple LLM providers through a unified API. This chapter covers how to configure, manage, and switch between different providers and models to get the most out of the library.</p>"},{"location":"polyglot/advanced/custom-config/#understanding-provider-configuration","title":"Understanding Provider Configuration","text":"<p>Polyglot organizes provider settings through connection presets - named configurations that include the details needed to communicate with a specific LLM provider. These connection presets are defined in the configuration files and can be selected at runtime.</p>"},{"location":"polyglot/advanced/custom-config/#the-configuration-files","title":"The Configuration Files","text":"<p>The primary configuration files for Polyglot are:</p> <ol> <li><code>config/llm.php</code>: Contains configurations for LLM providers (chat/completion)</li> <li><code>config/embed.php</code>: Contains configurations for embedding providers</li> </ol> <p>Let's focus on the structure of these configuration files.</p>"},{"location":"polyglot/advanced/custom-config/#llm-configuration-structure","title":"LLM Configuration Structure","text":"<p>The <code>llm.php</code> configuration file has the following structure:</p> <pre><code>&lt;?php\nuse Cognesy\\Config\\Env;\n\nreturn [\n    // Default connection to use when none is specified\n    'defaultPreset' =&gt; 'openai',\n\n    // Connection preset definitions\n    'presets' =&gt; [\n        // OpenAI connection\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'metadata' =&gt; [\n                'organization' =&gt; '',\n                'project' =&gt; '',\n            ],\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 128_000,\n            'maxOutputLength' =&gt; 16384,\n        ],\n\n        // Anthropic connection\n        'anthropic' =&gt; [\n            'providerType' =&gt; 'anthropic',\n            'apiUrl' =&gt; 'https://api.anthropic.com/v1',\n            'apiKey' =&gt; Env::get('ANTHROPIC_API_KEY', ''),\n            'endpoint' =&gt; '/messages',\n            'metadata' =&gt; [\n                'apiVersion' =&gt; '2023-06-01',\n                'beta' =&gt; 'prompt-caching-2024-07-31',\n            ],\n            'model' =&gt; 'claude-3-haiku-20240307',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 200_000,\n            'maxOutputLength' =&gt; 8192,\n        ],\n\n        // Additional connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#embedding-configuration-structure","title":"Embedding Configuration Structure","text":"<p>The <code>embed.php</code> configuration file follows a similar pattern:</p> <pre><code>&lt;?php\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/embeddings',\n            'metadata' =&gt; [\n                'organization' =&gt; ''\n            ],\n            'model' =&gt; 'text-embedding-3-small',\n            'defaultDimensions' =&gt; 1536,\n            'maxInputs' =&gt; 2048,\n        ],\n\n        // Additional embedding connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#connection-parameters","title":"Connection Parameters","text":"<p>Each connection includes several parameters:</p> <ul> <li><code>providerType</code>: The type of provider (OpenAI, Anthropic, etc.)</li> <li><code>apiUrl</code>: The base URL for the provider's API</li> <li><code>apiKey</code>: The API key for authentication</li> <li><code>endpoint</code>: The specific API endpoint for chat completions or embeddings</li> <li><code>metadata</code>: Additional provider-specific settings</li> <li><code>model</code>: The default model to use</li> <li><code>maxTokens</code>: Default maximum tokens for responses</li> <li><code>contextLength</code>: Maximum context length supported by the model</li> <li><code>maxOutputLength</code>: Maximum output length supported by the model</li> <li><code>httpClient</code>: (Optional) Custom HTTP client to use</li> </ul> <p>For embedding connections, the parameters are:</p> <ul> <li><code>providerType</code>: The type of provider (OpenAI, Anthropic, etc.)</li> <li><code>apiUrl</code>: The base URL for the provider's API</li> <li><code>apiKey</code>: The API key for authentication</li> <li><code>endpoint</code>: The specific API endpoint for chat completions or embeddings</li> <li><code>metadata</code>: Additional provider-specific settings</li> <li><code>model</code>: The default model to use</li> <li><code>defaultDimensions</code>: The default dimensions of embedding vectors</li> <li><code>maxInputs</code>: Maximum number of inputs that can be processed in a single request</li> </ul>"},{"location":"polyglot/advanced/custom-config/#connection-preset-name-vs-provider-type","title":"Connection preset name vs provider type","text":"<p>Configuration file <code>llm.php</code> contains a list of connection presets with the default names that might resemble provider type names, but those are separate entities.</p> <p>Provider type name refers to one of the supported LLM API providers and its underlying driver implementation, either specific to this provider or a generic one - for example compatible with OpenAI ('openai-compatible').</p> <p>Connection preset name refers to LLM API provider endpoint configuration with specific provider type, but also URL, credentials, default model name, and default model parameter values.</p>"},{"location":"polyglot/advanced/custom-config/#managing-api-keys","title":"Managing API Keys","text":"<p>API keys should be stored securely and never committed to your codebase. Polyglot uses environment variables for API keys.</p>"},{"location":"polyglot/advanced/custom-config/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># OpenAI\nOPENAI_API_KEY=sk-your-key-here\n\n# Anthropic\nANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# Other providers\nGEMINI_API_KEY=your-key-here\nMISTRAL_API_KEY=your-key-here\nCOHERE_API_KEY=your-key-here\n# etc.\n</code></pre> <p>Then load these environment variables using a package like <code>vlucas/phpdotenv</code>:</p> <pre><code>require_once __DIR__ . '/vendor/autoload.php';\n\n$dotenv = Dotenv\\Dotenv::createImmutable(__DIR__);\n$dotenv-&gt;load();\n</code></pre> <p>Or in frameworks like Laravel, environment variables are automatically loaded.</p>"},{"location":"polyglot/advanced/custom-config/#rotating-api-keys","title":"Rotating API Keys","text":"<p>For better security, consider rotating your API keys regularly. You can update the environment variables without changing your code.</p>"},{"location":"polyglot/advanced/custom-config/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>Different providers may support unique parameters and features. You can pass these as options to the <code>create()</code> method.</p>"},{"location":"polyglot/advanced/custom-config/#openai-specific-parameters","title":"OpenAI-Specific Parameters","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference('openai');\n\n$response = $inference-&gt;with(\n    messages: 'Generate a creative story.',\n    options: [\n        'temperature' =&gt; 0.8,         // Controls randomness (0.0 to 1.0)\n        'top_p' =&gt; 0.95,              // Nucleus sampling parameter\n        'frequency_penalty' =&gt; 0.5,   // Penalize repeated tokens\n        'presence_penalty' =&gt; 0.5,    // Penalize repeated topics\n        'stop' =&gt; [\"\\n\\n\", \"THE END\"],// Stop sequences\n        'logit_bias' =&gt; [             // Adjust token probabilities\n            // Token ID =&gt; bias value (-100 to +100)\n            15043 =&gt; -100,  // Discourage a specific token\n        ],\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#anthropic-specific-parameters","title":"Anthropic-Specific Parameters","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference('anthropic');\n\n$response = $inference-&gt;with(\n    messages: 'Generate a creative story.',\n    options: [\n        'temperature' =&gt; 0.7,\n        'top_p' =&gt; 0.9,\n        'top_k' =&gt; 40,               // Consider only the top 40 tokens\n        'max_tokens' =&gt; 1000,\n        'stop_sequences' =&gt; [\"\\n\\nHuman:\"],\n        'system' =&gt; 'You are a creative storyteller who specializes in magical realism.',\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#creating-custom-provider-configurations","title":"Creating Custom Provider Configurations","text":"<p>You can create custom configurations for providers that aren't included in the default settings or to modify existing ones.</p>"},{"location":"polyglot/advanced/custom-config/#modifying-configuration-files","title":"Modifying Configuration Files","text":"<p>You can edit the <code>config/llm.php</code> and <code>config/embed.php</code> files directly:</p> <pre><code>// In config/llm.php\nreturn [\n    'defaultPreset' =&gt; 'custom_openai',\n\n    'presets' =&gt; [\n        'custom_openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://custom.openai-proxy.com/v1',\n            'apiKey' =&gt; Env::get('CUSTOM_OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4-turbo',\n            'maxTokens' =&gt; 2048,\n            'contextLength' =&gt; 128_000,\n            'maxOutputLength' =&gt; 16384,\n            // HTTP client configuration is defined via HttpClientBuilder or facade-level methods\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also create custom configurations at runtime using the <code>LLMConfig</code> class:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom configuration\n$customConfig = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'gpt-4-turbo',\n    maxTokens: 2048,\n    contextLength: 128000,\n    driver: 'openai'\n);\n\n// Use the custom configuration\n$inference = (new Inference)-&gt;withConfig($customConfig);\n\n$response = $inference-&gt;with(\n    messages: 'What are the benefits of using custom configurations?'\n)-&gt;get();\n\necho $response;\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>You might want to use different providers in different environments:</p> <pre><code>&lt;?php\n// config/llm.php\n\nuse Cognesy\\Config\\Env;\n\n$environment = Env::get('APP_ENV', 'production');\n\nreturn [\n    'defaultPreset' =&gt; $environment === 'production' ? 'openai' : 'ollama',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        'ollama' =&gt; [\n            'providerType' =&gt; 'ollama',\n            'apiUrl' =&gt; 'http://localhost:11434/v1',\n            'apiKey' =&gt; '',\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'llama2',\n            'maxTokens' =&gt; 1024,\n            // Select appropriate HTTP client preset via HttpClientBuilder if needed\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"polyglot/advanced/custom-config/#creating-custom-inference-drivers","title":"Creating Custom Inference Drivers","text":"<p>In this example we will use an existing driver bundled with Polyglot (OpenAIDriver) as a base class for our custom driver.</p> <p>The driver can be any class that implements <code>CanHandleInference</code> interface.</p> <pre><code>// we register new provider type - 'custom-driver'\nLLM::registerDriver(\n    'custom-driver',\n    fn($config, $httpClient) =&gt; new class($config, $httpClient) extends OpenAIDriver {\n        public function handle(InferenceRequest $request): HttpResponse {\n            // some extra functionality to demonstrate our driver is being used\n            echo \"&gt;&gt;&gt; Handling request...\\n\";\n            return parent::handle($request);\n        }\n    }\n);\n\n// in configuration we use newly defined provider type - 'custom-driver'\n$config = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: Env::get('OPENAI_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'gpt-4o-mini',\n    maxTokens: 128,\n    httpClient: 'guzzle',\n    providerType: 'custom-driver',\n);\n\n// now we're calling inference using our configuration\n$answer = (new Inference)\n    -&gt;withConfig($config)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;toText();\n</code></pre> <p>An alternative way of providing driver definition is via class-string:</p> <pre><code>LLM::registerDriver('another-driver', AnotherDriver::class);\n</code></pre>"},{"location":"polyglot/advanced/custom-http-client/","title":"Customizing HTTP Client","text":"<p>Polyglot allows you to use custom HTTP clients for specific connection requirements:</p> <pre><code>&lt;?php\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;use Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom HTTP client configuration\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 5,      // 5 seconds connection timeout\n    requestTimeout: 60,     // 60 seconds request timeout\n    idleTimeout: 120,       // 120 seconds idle timeout for streaming\n    maxConcurrent: 10,      // Maximum 10 concurrent requests\n    failOnError: true,      // Throw exceptions on HTTP errors\n);\n\n// Create a custom HTTP client\n$httpClient = new HttpClient('guzzle', $httpConfig);\n\n// Use the custom HTTP client with Inference\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Make a request with the custom HTTP client\n$response = $inference-&gt;with(\n    messages: 'This request uses a custom HTTP client.'\n)-&gt;get();\n\necho $response;\n</code></pre>"},{"location":"polyglot/advanced/extending/","title":"Extending Polyglot","text":"<p>Understanding Polyglot's architecture makes it easier to extend the library to support new providers or add new functionality.</p>"},{"location":"polyglot/advanced/extending/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<p>To add support for a new LLM provider, you need to implement several components:</p> <ol> <li>Message Format Adapter: Implements <code>CanMapMessages</code> to convert Polyglot's message format to the provider's format</li> <li>Body Format Adapter: Implements <code>CanMapRequestBody</code> to structure the request body according to the provider's API</li> <li>Request Adapter: Implements <code>ProviderRequestAdapter</code> to build HTTP requests for the provider</li> <li>Response Adapter: Implements <code>ProviderResponseAdapter</code> to parse responses from the provider</li> <li>Usage Format Adapter: Implements <code>CanMapUsage</code> to extract token usage information</li> </ol> <p>Then, you need to modify the <code>InferenceDriverFactory</code> to create the appropriate driver for your provider:</p> <pre><code>// In InferenceDriverFactory\npublic function newProvider(LLMConfig $config, CanHandleHttpRequest $httpClient, EventDispatcher $events): CanHandleInference {\n    return new ModularLLMDriver(\n        $config,\n        new NewProviderRequestAdapter(\n            $config,\n            new NewProviderBodyFormat($config, new NewProviderMessageFormat())\n        ),\n        new NewProviderResponseAdapter(new NewProviderUsageFormat()),\n        $httpClient,\n        $events\n    );\n}\n</code></pre> <p>Finally, add your provider to the <code>make</code> method's match statement.</p>"},{"location":"polyglot/advanced/extending/#adding-a-new-embeddings-provider","title":"Adding a New Embeddings Provider","text":"<p>Similarly, to add a new embeddings provider, implement the <code>CanVectorize</code> interface:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Drivers;\n\nclass NewEmbeddingsDriver implements CanVectorize {\n    public function __construct(\n        protected EmbeddingsConfig $config,\n        protected ?CanHandleHttpRequest $httpClient = null,\n        protected ?EventDispatcher $events = null\n    ) { ... }\n\n    public function vectorize(array $input, array $options = []): EmbeddingsResponse { ... }\n\n    protected function getEndpointUrl(): string { ... }\n    protected function getRequestHeaders(): array { ... }\n    protected function getRequestBody(array $input, array $options): array { ... }\n    protected function toResponse(array $response): EmbeddingsResponse { ... }\n    protected function makeUsage(array $response): Usage { ... }\n}\n</code></pre> <p>Then, modify the <code>Embeddings</code> class to create your driver:</p> <pre><code>// In Embeddings::getDriver\nprotected function getDriver(EmbeddingsConfig $config, CanHandleHttpRequest $httpClient): CanVectorize {\n    return match ($config-&gt;providerType) {\n        // Existing providers...\n        'new-provider' =&gt; new NewEmbeddingsDriver($config, $httpClient, $this-&gt;events),\n        default =&gt; throw new InvalidArgumentException(\"Unknown client: {$config-&gt;providerType}\"),\n    };\n}\n</code></pre>"},{"location":"polyglot/advanced/extending/#adding-custom-middleware","title":"Adding Custom Middleware","text":"<p>You can extend Polyglot's HTTP layer by creating custom middleware:</p> <pre><code>namespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\HttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass YourCustomMiddleware extends BaseMiddleware {\n    protected function beforeRequest(HttpRequest $request): void {\n        // Modify the request before it's sent\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        // Modify the response after it's received\n        return $response;\n    }\n}\n</code></pre> <p>Then, add your middleware to the HTTP client:</p> <pre><code>$httpClient = new HttpClient();\n$httpClient-&gt;withMiddleware(new YourCustomMiddleware());\n\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/","title":"Structured outputs with JsonSchema class","text":"<p>JsonSchema is a powerful utility in the Polyglot library that enables developers to define structured data schemas for LLM interactions. This guide explains how to use JsonSchema to shape your LLM outputs and ensure consistent, typed responses from language models.</p>"},{"location":"polyglot/advanced/json-schema/#quick-start","title":"Quick Start","text":"<p>Here's a simple example of how to use JsonSchema with Polyglot's Inference API:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define your schema\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', description: 'City name'),\n        JsonSchema::integer('population', description: 'City population'),\n        JsonSchema::integer('founded', description: 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n// Use the schema with Inference\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with JSON data.']\n        ],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; $schema-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#why-use-jsonschema","title":"Why Use JsonSchema?","text":"<p>JsonSchema provides several benefits when working with LLMs:</p> <ol> <li>Type Safety: Ensure LLM outputs conform to your expected data structure</li> <li>Data Validation: Specify required fields and data types</li> <li>Structured Responses: Get consistent, well-formatted data instead of raw text</li> <li>Complex Nesting: Define deeply nested structures for sophisticated applications</li> <li>Better LLM Guidance: Help the LLM understand exactly what format you need</li> </ol>"},{"location":"polyglot/advanced/json-schema/#available-types","title":"Available Types","text":""},{"location":"polyglot/advanced/json-schema/#string","title":"String","text":"<p>For text values of any length:</p> <pre><code>use Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$nameSchema = JsonSchema::string(\n    name: 'full_name',\n    description: 'The user\\'s full name'\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#number-integer","title":"Number &amp; Integer","text":"<p>For numeric values:</p> <pre><code>$ageSchema = JsonSchema::integer(\n    name: 'age',\n    description: 'The user\\'s age in years'\n);\n\n$priceSchema = JsonSchema::number(\n    name: 'price',\n    description: 'Product price'\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#boolean","title":"Boolean","text":"<p>For true/false values:</p> <pre><code>$activeSchema = JsonSchema::boolean(\n    name: 'is_active',\n    description: 'Whether the user account is active'\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#array","title":"Array","text":"<p>For lists of items:</p> <pre><code>$tagsSchema = JsonSchema::array(\n    name: 'tags',\n    description: 'List of tags associated with the post',\n    itemSchema: JsonSchema::string()\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#enum","title":"Enum","text":"<p>For values from a specific set of options:</p> <pre><code>$statusSchema = JsonSchema::enum(\n    name: 'status',\n    description: 'The current status of the post',\n    enumValues: ['draft', 'published', 'archived']\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#object","title":"Object","text":"<p>For complex, nested data structures:</p> <pre><code>$profileSchema = JsonSchema::object(\n    name: 'profile',\n    description: 'A user\\'s public profile information',\n    properties: [\n        JsonSchema::string('username', 'The unique username'),\n        JsonSchema::string('bio', 'A short biography'),\n        JsonSchema::integer('joined_year', 'Year the user joined'),\n    ],\n    requiredProperties: ['username']\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#working-with-required-and-nullable-fields","title":"Working with Required and Nullable Fields","text":""},{"location":"polyglot/advanced/json-schema/#required-fields","title":"Required Fields","text":"<p>Required fields are specified at the object level using the <code>requiredProperties</code> parameter:</p> <pre><code>$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('email', 'Primary email address'),\n        JsonSchema::string('name', 'User\\'s full name'),\n        JsonSchema::string('bio', 'User biography'),\n    ],\n    requiredProperties: ['email', 'name'] // email and name must be present\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#nullable-fields","title":"Nullable Fields","text":"<p>Nullable fields are specified at the individual field level:</p> <pre><code>$bioSchema = JsonSchema::string(\n    name: 'bio',\n    description: 'Optional user biography',\n    nullable: true\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#understanding-required-vs-nullable","title":"Understanding Required vs. Nullable","text":"<ul> <li>Required: The field must be present in the data structure</li> <li>Nullable: The field can contain a null value</li> <li>A field can be both required and nullable (must be present, can be null)</li> <li>A field can be non-required and non-nullable (when present, cannot be null)</li> </ul>"},{"location":"polyglot/advanced/json-schema/#common-patterns","title":"Common Patterns","text":"<pre><code>// Required and Non-nullable (most strict)\nJsonSchema::string('email', 'Primary email', nullable: false);\n// requiredProperties: ['email']\n\n// Required but Nullable (must be present, can be null)\nJsonSchema::string('bio', 'User bio', nullable: true);\n// requiredProperties: ['bio']\n\n// Optional and Non-nullable (can be omitted, but if present cannot be null)\nJsonSchema::string('phone', 'Phone number', nullable: false);\n// requiredProperties: [] (doesn't include 'phone')\n\n// Optional and Nullable (most permissive)\nJsonSchema::string('website', 'Personal website', nullable: true);\n// requiredProperties: [] (doesn't include 'website')\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#working-with-openai-and-other-providers","title":"Working with OpenAI and Other Providers","text":"<p>When working with OpenAI in strict mode, follow these guidelines:</p> <pre><code>// For OpenAI strict mode: \n// - All fields should be required\n// - Use nullable: true for optional fields\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('email', 'Required email address'),\n        JsonSchema::string('bio', 'Optional biography', nullable: true),\n    ],\n    requiredProperties: ['email', 'bio'] // Note: bio is required but nullable\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#building-complex-schemas","title":"Building Complex Schemas","text":"<p>For more complex data structures, you can nest schemas:</p> <pre><code>// Define child schemas first\n$addressSchema = JsonSchema::object(\n    name: 'address',\n    properties: [\n        JsonSchema::string('street', 'Street address'),\n        JsonSchema::string('city', 'City name'),\n        JsonSchema::string('country', 'Country name'),\n    ],\n    requiredProperties: ['street', 'city', 'country']\n);\n\n// Use them in parent schemas\n$userSchema = JsonSchema::object(\n    name: 'user',\n    properties: [\n        JsonSchema::string('name', 'User name'),\n        $addressSchema, // embed the address schema\n    ],\n    requiredProperties: ['name', 'address']\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#fluent-api-for-schema-creation","title":"Fluent API for Schema Creation","text":"<p>JsonSchema supports method chaining for a more fluent API:</p> <pre><code>$schema = JsonSchema::array('tags')\n    -&gt;withItemSchema(JsonSchema::string())\n    -&gt;withDescription('A list of tags')\n    -&gt;withNullable(true);\n</code></pre> <p>Available methods include: - <code>withName(string $name)</code> - <code>withDescription(string $description)</code> - <code>withTitle(string $title)</code> - <code>withNullable(bool $nullable = true)</code> - <code>withMeta(array $meta = [])</code> - <code>withEnumValues(?array $enum = null)</code> - <code>withProperties(?array $properties = null)</code> - <code>withItemSchema(JsonSchema $itemSchema = null)</code> - <code>withRequiredProperties(?array $required = null)</code> - <code>withAdditionalProperties(bool $additionalProperties = false)</code></p>"},{"location":"polyglot/advanced/json-schema/#accessing-schema-properties","title":"Accessing Schema Properties","text":"<p>JsonSchema provides various methods to access schema properties:</p> <pre><code>$schema-&gt;type();                // Get schema type (e.g., 'object')\n$schema-&gt;name();                // Get schema name\n$schema-&gt;isNullable();          // Check if schema is nullable\n$schema-&gt;requiredProperties();  // Get array of required properties\n$schema-&gt;properties();          // Get array of all properties\n$schema-&gt;property('name');      // Get specific property\n$schema-&gt;itemSchema();          // Get item schema for array schemas\n$schema-&gt;enumValues();          // Get enum values\n$schema-&gt;hasAdditionalProperties(); // Check if additional properties are allowed\n$schema-&gt;description();         // Get schema description\n$schema-&gt;title();               // Get schema title\n$schema-&gt;meta();                // Get all meta fields\n$schema-&gt;meta('key');           // Get specific meta field\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#converting-schemas-to-arrays-and-function-calls","title":"Converting Schemas to Arrays and Function Calls","text":"<p>JsonSchema can be converted to arrays and function calls:</p> <pre><code>// Convert to array\n$schemaArray = $schema-&gt;toArray();\n\n// Convert to JSON schema\n$jsonSchema = $schema-&gt;toJsonSchema();\n\n// Convert to function call (for tools/functions)\n$functionCall = $schema-&gt;toFunctionCall(\n    functionName: 'getUserProfile',\n    functionDescription: 'Gets the user profile information',\n    strict: true\n);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#meta-fields","title":"Meta Fields","text":"<p>You can add custom meta fields to your schemas:</p> <pre><code>$schema = JsonSchema::string(\n    name: 'username',\n    description: 'The username',\n    meta: [\n        'min_length' =&gt; 3,\n        'max_length' =&gt; 50,\n        'pattern' =&gt; '^[a-zA-Z0-9_]+$',\n    ]\n);\n</code></pre> <p>Meta fields will be transformed to include the <code>x-</code> prefix when converted to arrays (e.g., <code>x-min_length</code>).</p>"},{"location":"polyglot/advanced/json-schema/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Descriptions: Write clear, concise descriptions for each field.</li> </ol> <pre><code>// \u274c Not helpful\nJsonSchema::string('name', 'the name');\n\n// \u2705 Much better\nJsonSchema::string('name', 'The user\\'s display name (2-50 characters)');\n</code></pre> <ol> <li> <p>Only Mark Required Fields: Only mark fields as required if they're truly necessary.</p> </li> <li> <p>Organize Nested Schemas: Keep your schemas organized when dealing with complex structures.</p> </li> </ol> <pre><code>// Define child schemas first for clarity\n$addressSchema = JsonSchema::object(/*...*/);\n$contactSchema = JsonSchema::object(/*...*/);\n\n// Then use them in your parent schema\n$userSchema = JsonSchema::object(\n    properties: [$addressSchema, $contactSchema]\n);\n</code></pre> <ol> <li>Be Explicit About Requirements: Specify both the nullable status and required fields for clarity.</li> </ol>"},{"location":"polyglot/advanced/json-schema/#full-example-creating-user-profile-schema","title":"Full Example: Creating User Profile Schema","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define address schema\n$addressSchema = JsonSchema::object(\n    name: 'address',\n    properties: [\n        JsonSchema::string('street', 'Street address'),\n        JsonSchema::string('city', 'City name'),\n        JsonSchema::string('postal_code', 'Postal/ZIP code'),\n        JsonSchema::string('country', 'Country name'),\n    ],\n    requiredProperties: ['city', 'country'],\n);\n\n// Define contact schema\n$contactSchema = JsonSchema::object(\n    name: 'contact',\n    properties: [\n        JsonSchema::string('email', 'Email address'),\n        JsonSchema::string('phone', 'Phone number', nullable: true),\n    ],\n    requiredProperties: ['email', 'phone'],\n);\n\n// Define hobbies schema\n$hobbiesSchema = JsonSchema::array(\n    name: 'hobbies',\n    description: 'List of user hobbies',\n    itemSchema: JsonSchema::object(\n        properties: [\n            JsonSchema::string('name', 'Hobby name'),\n            JsonSchema::string('description', 'Hobby description', nullable: true),\n            JsonSchema::integer('years_experience', 'Years of experience', nullable: true),\n        ],\n        requiredProperties: ['name'],\n    ),\n);\n\n// Define main user schema\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'User\\'s full name'),\n        JsonSchema::integer('age', 'User\\'s age'),\n        $addressSchema,\n        $contactSchema,\n        $hobbiesSchema,\n        JsonSchema::enum(\n            'status',\n            'Account status',\n            enumValues: ['active', 'inactive', 'pending'],\n        ),\n    ],\n    requiredProperties: ['name', 'age', 'address', 'contact', 'status'],\n);\n\n// Use the schema with Inference\n$userData = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a profile for John Doe who lives in New York.']\n        ],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'User profile data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'user_profile',\n                'schema' =&gt; $userSchema-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\nprint_r($userData);\n</code></pre>"},{"location":"polyglot/advanced/json-schema/#advanced-creating-function-calls","title":"Advanced: Creating Function Calls","text":"<p>JsonSchema can be used to define function/tool parameters for LLMs:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define the schema for the function parameters\n$weatherParamsSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('location', 'City and country name'),\n        JsonSchema::enum(\n            'unit', \n            'Temperature unit', \n            enumValues: ['celsius', 'fahrenheit'],\n            nullable: true\n        ),\n    ],\n    requiredProperties: ['location'],\n);\n\n// Convert schema to function call format\n$functionDefinition = $weatherParamsSchema-&gt;toFunctionCall(\n    functionName: 'getWeather',\n    functionDescription: 'Get the current weather for a location',\n    strict: true\n);\n\n// Use with Polyglot's Inference API\n$result = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What\\'s the weather like in Tokyo?']\n        ],\n        tools: [$functionDefinition],\n        // Additional configuration...\n    )\n    -&gt;create();\n</code></pre>"},{"location":"polyglot/embeddings/optimization/","title":"Optimization","text":""},{"location":"polyglot/embeddings/optimization/#optimization","title":"Optimization","text":""},{"location":"polyglot/embeddings/optimization/#batch-processing-for-efficiency","title":"Batch Processing for Efficiency","text":"<p>When processing many documents, it's more efficient to batch them:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$allDocuments = [/* large array of documents */];\n\n// Process in batches of 25 (check provider-specific limits)\n$batchSize = 25;\n$vectors = [];\n\nfor ($i = 0; $i &lt; count($allDocuments); $i += $batchSize) {\n    $batch = array_slice($allDocuments, $i, $batchSize);\n\n    try {\n        $response = $embeddings-&gt;with($batch)-&gt;get();\n        $batchVectors = $response-&gt;toValuesArray();\n\n        // Add to our vectors array\n        $vectors = array_merge($vectors, $batchVectors);\n\n        echo \"Processed batch \" . (floor($i / $batchSize) + 1) . \" of \" . ceil(count($allDocuments) / $batchSize) . \"\\n\";\n    } catch (\\Exception $e) {\n        echo \"Error processing batch: \" . $e-&gt;getMessage() . \"\\n\";\n    }\n\n    // Optional: Add a small delay to avoid hitting rate limits\n    usleep(100000); // 100ms\n}\n\necho \"Processed \" . count($vectors) . \" embeddings in total.\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/optimization/#caching-embeddings","title":"Caching Embeddings","text":"<p>For better performance, you can cache embeddings to avoid regenerating them:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass CachedEmbeddings {\n    private $embeddings;\n    private $cache = [];\n\n    public function __construct(?Embeddings $embeddings = null) {\n        $this-&gt;embeddings = $embeddings ?? new Embeddings();\n    }\n\n    public function create($input, array $options = []): array {\n        if (is_string($input)) {\n            // Single string input\n            $cacheKey = $this-&gt;getCacheKey($input, $options);\n\n            if (isset($this-&gt;cache[$cacheKey])) {\n                return $this-&gt;cache[$cacheKey];\n            }\n\n            $response = $this-&gt;embeddings-&gt;with($input, $options)-&gt;get();\n            $vector = $response-&gt;first()-&gt;values();\n\n            $this-&gt;cache[$cacheKey] = $vector;\n            return $vector;\n        } else {\n            // Array of strings\n            $results = [];\n            $uncachedInputs = [];\n            $uncachedIndices = [];\n\n            // Check cache for each input\n            foreach ($input as $i =&gt; $text) {\n                $cacheKey = $this-&gt;getCacheKey($text, $options);\n\n                if (isset($this-&gt;cache[$cacheKey])) {\n                    $results[$i] = $this-&gt;cache[$cacheKey];\n                } else {\n                    $uncachedInputs[] = $text;\n                    $uncachedIndices[] = $i;\n                }\n            }\n\n            // Generate embeddings for uncached inputs\n            if (!empty($uncachedInputs)) {\n                $response = $this-&gt;embeddings-&gt;with($uncachedInputs, $options)-&gt;get();\n                $vectors = $response-&gt;toValuesArray();\n\n                foreach ($vectors as $j =&gt; $vector) {\n                    $i = $uncachedIndices[$j];\n                    $results[$i] = $vector;\n\n                    // Update cache\n                    $cacheKey = $this-&gt;getCacheKey($input[$i], $options);\n                    $this-&gt;cache[$cacheKey] = $vector;\n                }\n            }\n\n            // Sort by original indices\n            ksort($results);\n            return $results;\n        }\n    }\n\n    private function getCacheKey(string $input, array $options): string {\n        $model = $options['model'] ?? '';\n        return md5($input . serialize($options) . $model);\n    }\n}\n\n// Usage\n$cachedEmbeddings = new CachedEmbeddings(new Embeddings('openai'));\n\n// First call will generate embeddings\n$vector1 = $cachedEmbeddings-&gt;create(\"This is a test\");\necho \"First call completed, generated vector with \" . count($vector1) . \" dimensions.\\n\";\n\n// Second call will use the cache\n$vector2 = $cachedEmbeddings-&gt;create(\"This is a test\");\necho \"Second call completed (from cache).\\n\";\n\n// Compare vectors to verify they're the same\n$equal = (serialize($vector1) === serialize($vector2));\necho \"Vectors are \" . ($equal ? \"identical\" : \"different\") . \".\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/","title":"Overview of Embeddings","text":"<p>Embeddings are a key component of many LLM-based solutions and are used to represent text (or multimodal data) with numbers capturing their meaning and relationships.</p> <p>Embeddings are numerical representations of text or other data that capture semantic meaning in a way that computers can process efficiently. They enable powerful applications like semantic search, document clustering, recommendation systems, and more. This chapter explores how to use Polyglot's Embeddings API to work with vector embeddings across multiple providers.</p>"},{"location":"polyglot/embeddings/overview/#understanding-embeddings","title":"Understanding Embeddings","text":"<p>Before diving into code, it's helpful to understand what embeddings are and how they work:</p> <ul> <li>Embeddings represent words, phrases, or documents as vectors of floating-point numbers in a high-dimensional space</li> <li>Similar items (semantically related) have vectors that are closer together in this space</li> <li>The \"distance\" between vectors can be measured using metrics like cosine similarity or Euclidean distance</li> <li>Modern embedding models are trained on massive corpora of text to capture nuanced relationships</li> </ul> <p>Common use cases for embeddings include:</p> <ul> <li>Semantic search: Finding documents similar to a query based on meaning, not just keywords</li> <li>Clustering: Grouping similar documents together</li> <li>Classification: Assigning categories to documents based on their content</li> <li>Recommendations: Suggesting related items</li> <li>Information retrieval: Finding relevant information in large datasets</li> </ul>"},{"location":"polyglot/embeddings/overview/#embeddings-class","title":"<code>Embeddings</code> class","text":"<p>The <code>Embeddings</code> class is a facade that provides access to embeddings APIs across multiple providers. It combines functionality through traits for provider configuration, request building, and result handling.</p>"},{"location":"polyglot/embeddings/overview/#architecture-overview","title":"Architecture Overview","text":"<p>The <code>Embeddings</code> class combines functionality through traits: - HandlesInitMethods: Provider configuration and setup - HandlesFluentMethods: Request parameter configuration - HandlesInvocation: Request execution and PendingEmbeddings creation - HandlesShortcuts: Convenient methods for common result formats</p>"},{"location":"polyglot/embeddings/overview/#supported-providers","title":"Supported providers","text":"<p><code>Embeddings</code> class supports the following embeddings providers: - Azure OpenAI: Azure-hosted OpenAI embedding models - Cohere: Cohere's embedding models - Gemini: Google's Gemini embedding models - Jina: Jina AI's embedding models - OpenAI: OpenAI's embedding models (text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large)</p> <p>Provider configurations are managed through the configuration system.</p>"},{"location":"polyglot/embeddings/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Simple embedding generation\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('The quick brown fox jumps over the lazy dog.')-&gt;get();\n\n// Get the vector values from the first result\n$vector = $result-&gt;first()-&gt;values();\necho \"Generated a vector with \" . count($vector) . \" dimensions.\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/#provider-configuration-methods","title":"Provider Configuration Methods","text":"<p>Configure the underlying embeddings provider:</p> <pre><code>// Provider selection and configuration\n$embeddings-&gt;using('openai');                          // Use preset configuration\n$embeddings-&gt;withPreset('openai');                     // Alternative preset method\n$embeddings-&gt;withDsn('openai://model=text-embedding-3-large'); // Configure via DSN\n$embeddings-&gt;withConfig($customConfig);                // Explicit configuration\n$embeddings-&gt;withConfigProvider($configProvider);     // Custom config provider\n\n// HTTP and debugging\n$embeddings-&gt;withHttpClient($customHttpClient);       // Custom HTTP client\n$embeddings-&gt;withDebugPreset('verbose');              // Debug configuration\n\n// Driver management\n$embeddings-&gt;withDriver($customDriver);               // Custom vectorization driver\n$embeddings-&gt;withProvider($customProvider);           // Custom provider instance\n</code></pre>"},{"location":"polyglot/embeddings/overview/#request-configuration-methods","title":"Request Configuration Methods","text":"<p>Configure the embedding request:</p> <pre><code>// Input configuration\n$embeddings-&gt;withInputs('Single text input');         // Single string\n$embeddings-&gt;withInputs(['Text 1', 'Text 2']);       // Multiple strings\n$embeddings-&gt;with('Input text');                      // Shorthand input method\n\n// Model and options\n$embeddings-&gt;withModel('text-embedding-3-large');    // Specific model\n$embeddings-&gt;withOptions(['dimensions' =&gt; 1536]);     // Provider-specific options\n\n// Complete configuration\n$embeddings-&gt;with(\n    input: ['Text 1', 'Text 2'],\n    options: ['dimensions' =&gt; 1536],\n    model: 'text-embedding-3-large'\n);\n</code></pre>"},{"location":"polyglot/embeddings/overview/#response-methods","title":"Response Methods","text":"<p>Get embeddings in different formats:</p> <pre><code>// Full response object\n$response = $embeddings-&gt;get();                       // EmbeddingsResponse object\n\n// Vector extraction\n$vectors = $embeddings-&gt;vectors();                    // Array of Vector objects\n$firstVector = $embeddings-&gt;first();                 // First Vector object\n$values = $embeddings-&gt;first()-&gt;values();           // Array of floats\n\n// Advanced response handling\n$pending = $embeddings-&gt;create();                    // PendingEmbeddings for custom handling\n$response = $pending-&gt;get();                         // Execute and get response\n</code></pre>"},{"location":"polyglot/embeddings/overview/#working-with-multiple-providers","title":"Working with Multiple Providers","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// OpenAI embeddings\n$openaiVectors = (new Embeddings())\n    -&gt;using('openai')\n    -&gt;withModel('text-embedding-3-large')\n    -&gt;with(['Document 1', 'Document 2'])\n    -&gt;vectors();\n\n// Cohere embeddings  \n$cohereVectors = (new Embeddings())\n    -&gt;using('cohere')\n    -&gt;withModel('embed-english-v3.0')\n    -&gt;with(['Document 1', 'Document 2'])\n    -&gt;vectors();\n\necho \"OpenAI dimensions: \" . count($openaiVectors[0]-&gt;values()) . \"\\n\";\necho \"Cohere dimensions: \" . count($cohereVectors[0]-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/#custom-configuration","title":"Custom Configuration","text":"<p>Create custom configurations for specific use cases:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create custom configuration\n$config = new EmbeddingsConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/embeddings',\n    model: 'text-embedding-3-large',\n    dimensions: 3072,\n    maxInputs: 100,\n    driver: 'openai'\n);\n\n// Use custom configuration\n$embeddings = (new Embeddings())\n    -&gt;withConfig($config)\n    -&gt;with('Custom configuration example');\n\n$vector = $embeddings-&gt;first()-&gt;values();\necho \"Generated embedding with \" . count($vector) . \" dimensions\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/overview/#driver-registration","title":"Driver Registration","text":"<p>Register custom drivers for new providers:</p> <pre><code>// Register with class name\nEmbeddings::registerDriver('custom-provider', CustomEmbeddingsDriver::class);\n\n// Register with factory callable\nEmbeddings::registerDriver('custom-provider', function($config, $httpClient) {\n    return new CustomEmbeddingsDriver($config, $httpClient);\n});\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/","title":"Working with Embeddings","text":""},{"location":"polyglot/embeddings/work-with-embeddings/#the-embeddings-class","title":"The Embeddings Class","text":"<p>Polyglot provides the <code>Embeddings</code> class as the primary interface for generating and working with vector embeddings.</p>"},{"location":"polyglot/embeddings/work-with-embeddings/#creating-an-embeddings-instance","title":"Creating an Embeddings Instance","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create a basic embeddings instance with default settings\n$embeddings = new Embeddings();\n\n// Create an embeddings instance with a specific connection\n$embeddings = new Embeddings('openai');\n\n// Alternative method to specify connection\n$embeddings = (new Embeddings())-&gt;using('openai');\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#key-methods","title":"Key Methods","text":"<p>The <code>Embeddings</code> class provides several important methods:</p> <ul> <li><code>create()</code>: Generates embeddings for input text</li> <li><code>using()</code>: Specifies which connection preset to use</li> <li><code>withConfig()</code>: Sets a custom configuration</li> <li><code>withHttpClient()</code>: Specifies a custom HTTP client</li> <li><code>withModel()</code>: Overrides the default model</li> <li><code>findSimilar()</code>: Finds documents similar to a query</li> </ul>"},{"location":"polyglot/embeddings/work-with-embeddings/#generating-embeddings","title":"Generating Embeddings","text":"<p>The core functionality of the <code>Embeddings</code> class is to transform text into vector representations.</p>"},{"location":"polyglot/embeddings/work-with-embeddings/#basic-embedding-generation","title":"Basic Embedding Generation","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('The quick brown fox jumps over the lazy dog.')-&gt;get();\n\n// Get the vector values from the first (and only) result\n$vector = $result-&gt;first()?-&gt;values();\n\necho \"Generated a vector with \" . count($vector) . \" dimensions.\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#embedding-multiple-texts","title":"Embedding Multiple Texts","text":"<p>You can generate embeddings for multiple texts in a single request, which is more efficient than making separate requests:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n\n$documents = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning models can process text into vector representations.\",\n    \"Embeddings capture semantic relationships between words and documents.\"\n];\n\n$result = $embeddings-&gt;with($documents)-&gt;get();\n\n// Get all vectors\n$vectors = $result-&gt;vectors();\n\nforeach ($vectors as $index =&gt; $vector) {\n    echo \"Document \" . ($index + 1) . \" has a vector with \" . count($vector-&gt;values()) . \" dimensions.\\n\";\n}\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#accessing-embedding-results","title":"Accessing Embedding Results","text":"<p>The <code>create()</code> method returns an <code>EmbeddingsResponse</code> object with several useful methods:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('Sample text for embedding')-&gt;get();\n\n// Get the first vector\n$firstVector = $result-&gt;first();\n\n// Get the last vector (useful when processing multiple inputs)\n$lastVector = $result-&gt;last();\n\n// Get all vectors\n$allVectors = $result-&gt;vectors();\n\n// Get all vector values as a simple array of arrays\n$valuesArray = $result-&gt;toValuesArray();\n\n// Get usage information\n$usage = $result-&gt;usage();\necho \"Input tokens: \" . $usage-&gt;input() . \"\\n\";\necho \"Output tokens: \" . $usage-&gt;output() . \"\\n\";\necho \"Total tokens: \" . $usage-&gt;total() . \"\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#working-with-vector-objects","title":"Working with Vector Objects","text":"<p>Each vector in the response is represented by a <code>Vector</code> object with its own methods:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('Sample text for embedding')-&gt;get();\n$vector = $result-&gt;first();\n\n// Get vector values\n$values = $vector-&gt;values();\n\n// Get vector ID (index)\n$id = $vector-&gt;id();\n\n// Compare with another vector\n$otherVector = $result-&gt;with('Another text for comparison')-&gt;first();\n$similarity = $vector-&gt;compareTo($otherVector, 'cosine');\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#working-with-different-providers","title":"Working with Different Providers","text":"<p>Polyglot supports multiple embedding providers, each with their own strengths and characteristics.</p>"},{"location":"polyglot/embeddings/work-with-embeddings/#switching-between-providers","title":"Switching Between Providers","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Compare embeddings from different providers\n$text = \"Artificial intelligence is transforming industries worldwide.\";\n\n// OpenAI embeddings\n$openaiEmbeddings = new Embeddings('openai');\n$openaiResult = $openaiEmbeddings-&gt;with($text)-&gt;get();\necho \"OpenAI embedding dimensions: \" . count($openaiResult-&gt;first()?-&gt;values()) . \"\\n\";\n\n// Cohere embeddings\n$cohereEmbeddings = new Embeddings('cohere');\n$cohereResult = $cohereEmbeddings-&gt;with($text)-&gt;get();\necho \"Cohere embedding dimensions: \" . count($cohereResult-&gt;first()?-&gt;values()) . \"\\n\";\n\n// Mistral embeddings\n$mistralEmbeddings = new Embeddings('mistral');\n$mistralResult = $mistralEmbeddings-&gt;with($text)-&gt;get();\necho \"Mistral embedding dimensions: \" . count($mistralResult-&gt;first()?-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#provider-specific-options","title":"Provider-Specific Options","text":"<p>Different providers may support additional options for embedding generation:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Example with OpenAI-specific options\n$openaiEmbeddings = new Embeddings('openai');\n$response = $openaiEmbeddings-&gt;with(\n    input: [\"Sample text for embedding\"],\n    options: [\n        'encoding_format' =&gt; 'float',  // Get float values instead of base64\n        'dimensions' =&gt; 512,           // Request a specific vector size (if supported)\n    ]\n)-&gt;get();\n\n// Example with Cohere-specific options\n$cohereEmbeddings = new Embeddings('cohere');\n$response = $cohereEmbeddings-&gt;with(\n    input: [\"Sample text for embedding\"],\n    options: [\n        'input_type' =&gt; 'classification',  // Cohere-specific option\n        'truncate' =&gt; 'END',               // How to handle texts that exceed the token limit\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/embeddings/work-with-embeddings/#models-and-dimensions","title":"Models and Dimensions","text":"<p>Different embedding models produce vectors of different dimensions:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;use Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create custom configuration with a specific model\n$config = new EmbeddingsConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/embeddings',\n    model: 'text-embedding-3-large',  // Use the larger model\n    dimensions: 3072,                 // Specify expected dimensions\n);\n\n$embeddings = new Embeddings();\n$embeddings-&gt;withConfig($config);\n\n$response = $embeddings-&gt;with(\"Test text for large embedding model\")-&gt;get();\necho \"Vector dimensions: \" . count($response-&gt;first()?-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/","title":"Creating Requests","text":"<p>This section covers how to create requests to LLM providers using the Polyglot library. It includes examples of basic requests, handling multiple messages, and using different message formats.</p>"},{"location":"polyglot/essentials/creating-requests/#creating-requests","title":"Creating Requests","text":"<p>The <code>with()</code> method is the main way to set the parameters of the requests to LLM providers.</p> <p>It accepts several parameters:</p> <pre><code>public function with(\n    string|array $messages = [],    // The messages to send to the LLM\n    string $model = '',             // The model to use (overrides default)\n    array $tools = [],              // Tools/functions for the model to use\n    string|array $toolChoice = [],  // Tool selection preference\n    array $responseFormat = [],     // Response format specification\n    array $options = [],            // Additional request options\n    Mode $mode = OutputMode::Text   // Output mode (Text, JSON, etc.)\n) : self\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#basic-request-example","title":"Basic Request Example","text":"<p>Here's a simple example of creating a request:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;with(\n        messages: 'What is the capital of France?'\n    )\n    -&gt;create() // create pending inference\n    -&gt;get();   // get the data - here it executes the request\n\necho \"Response: $response\";\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#request-with-multiple-messages","title":"Request with Multiple Messages","text":"<p>For chat-based interactions, you can pass an array of messages:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages([\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant who provides concise answers.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?'],\n        ['role' =&gt; 'assistant', 'content' =&gt; 'Paris.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'And what about Germany?']\n    ])\n    -&gt;get();\n\necho \"Response: $response\";\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#using-messages-class","title":"Using <code>Messages</code> Class","text":"<p>You can also use the <code>Messages</code> class to create message sequences more conveniently:</p> <pre><code>&lt;?php\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$messages = (new Messages)\n    -&gt;asSystem('You are a senior PHP8 backend developer.')\n    -&gt;asDeveloper('Be concise and use modern PHP8.2+ features.') // OpenAI developer role is supported and normalized for other providers\n    -&gt;asUser([\n        'What is the best way to handle errors in PHP8?',\n        'Provide a code example.',\n    ]); // you can pass array of strings to create multiple content parts\n\n$response = (new Inference)\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4o')\n    -&gt;withMessages($messages)\n    -&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/creating-requests/#message-formats","title":"Message Formats","text":"<p>Polyglot supports different message formats depending on the provider:</p> <ul> <li>String: A simple string will be converted to a user message</li> <li>Array of messages: Each message should have a <code>role</code> and <code>content</code> field</li> <li>Multimodal content: Some providers support images in messages</li> </ul> <p>Example with image (for providers that support it):</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$imageData = base64_encode(file_get_contents('image.jpg'));\n$messages = [\n    [\n        'role' =&gt; 'user',\n        'content' =&gt; [\n            [\n                'type' =&gt; 'text',\n                'text' =&gt; 'What\\'s in this image?'\n            ],\n            [\n                'type' =&gt; 'image_url',\n                'image_url' =&gt; [\n                    'url' =&gt; \"data:image/jpeg;base64,$imageData\"\n                ]\n            ]\n        ]\n    ]\n];\n\n$response = (new Inference())\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4o') // use multimodal model\n    -&gt;with(messages: $messages)\n    -&gt;get();\n</code></pre> <p>Instructor library offers <code>Cognesy\\Messages\\Utils\\Image</code> class for easier conversion of image files to the message format.</p>"},{"location":"polyglot/essentials/inference-class/","title":"Inference Class","text":"<p>The <code>Inference</code> class is the primary facade for making requests to LLM providers in Polyglot. It provides a unified interface for configuring providers, building requests, and executing inference operations.</p>"},{"location":"polyglot/essentials/inference-class/#architecture-overview","title":"Architecture Overview","text":"<p>The <code>Inference</code> class combines functionality through traits: - HandlesLLMProvider: Provider configuration and driver management - HandlesRequestBuilder: Request construction and configuration - HandlesInvocation: Request execution and PendingInference creation - HandlesShortcuts: Convenient methods for common response formats</p>"},{"location":"polyglot/essentials/inference-class/#basic-usage","title":"Basic Usage","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Simple text completion\n$response = (new Inference())\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\n// Using a specific provider\n$response = (new Inference())\n    -&gt;using('openai')\n    -&gt;withMessages('Explain quantum physics')\n    -&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#llm-provider-configuration-methods","title":"LLM Provider Configuration Methods","text":"<p>Configure the underlying LLM provider:</p> <pre><code>// Provider selection and configuration\n$inference-&gt;using('openai');                           // Use preset configuration\n$inference-&gt;withDsn('openai://model=gpt-4');          // Configure via DSN\n$inference-&gt;withConfig($customConfig);                 // Explicit configuration\n$inference-&gt;withConfigProvider($configProvider);      // Custom config provider\n$inference-&gt;withLLMConfigOverrides(['temperature' =&gt; 0.7]);\n\n// HTTP client configuration\n$inference-&gt;withHttpClient($customHttpClient);        // Custom HTTP client\n$inference-&gt;withHttpClientPreset('debug');           // HTTP client preset\n$inference-&gt;withDebugPreset('verbose');              // Debug configuration\n\n// Driver management\n$inference-&gt;withDriver($customDriver);               // Custom inference driver\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#request-building-methods","title":"Request Building Methods","text":"<p>Configure the inference request:</p> <pre><code>// Message configuration\n$inference-&gt;withMessages('Hello, world!');           // String message\n$inference-&gt;withMessages(['user' =&gt; 'Hello']);       // Array format\n$inference-&gt;withMessages($messageObject);            // Message object\n\n// Model and generation parameters\n$inference-&gt;withModel('gpt-4');                      // Specific model\n$inference-&gt;withMaxTokens(100);                      // Token limit\n$inference-&gt;withOutputMode($outputMode);             // Response format mode\n\n// Tool usage\n$inference-&gt;withTools($toolDefinitions);             // Available tools\n$inference-&gt;withToolChoice('auto');                  // Tool selection strategy\n\n// Response formatting\n$inference-&gt;withResponseFormat(['type' =&gt; 'json']); // Response format\n$inference-&gt;withOptions(['temperature' =&gt; 0.5]);    // Additional options\n\n// Advanced features\n$inference-&gt;withStreaming(true);                     // Enable streaming\n$inference-&gt;withCachedContext($messages, $tools);   // Context caching\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#invocation-methods","title":"Invocation Methods","text":"<p>Execute inference requests:</p> <pre><code>// Flexible configuration method\n$inference-&gt;with(\n    messages: 'Hello',\n    model: 'gpt-4',\n    tools: [],\n    toolChoice: 'auto',\n    responseFormat: ['type' =&gt; 'text'],\n    options: ['temperature' =&gt; 0.7],\n    mode: OutputMode::Text\n);\n\n// Create pending inference for advanced handling\n$pending = $inference-&gt;create();\n\n// Direct request execution\n$inference-&gt;withRequest($existingRequest);\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#response-shortcuts","title":"Response Shortcuts","text":"<p>Get responses in different formats:</p> <pre><code>// Text responses\n$text = $inference-&gt;get();                           // Plain text\n$response = $inference-&gt;response();                  // Full InferenceResponse object\n\n// JSON responses  \n$json = $inference-&gt;asJson();                        // JSON string\n$data = $inference-&gt;asJsonData();                    // Parsed array\n\n// Streaming\n$stream = $inference-&gt;stream();                      // InferenceStream object\n</code></pre>"},{"location":"polyglot/essentials/inference-class/#driver-registration","title":"Driver Registration","text":"<p>Register custom drivers for new providers:</p> <pre><code>// Register with class name\nInference::registerDriver('custom-provider', CustomDriver::class);\n\n// Register with factory callable\nInference::registerDriver('custom-provider', function($config, $httpClient) {\n    return new CustomDriver($config, $httpClient);\n});\n</code></pre>"},{"location":"polyglot/essentials/overview/","title":"Overview of Inference","text":"<p><code>Inference</code> class offers access to LLM APIs and convenient methods to execute model inference, incl. chat completions, tool calling or JSON output generation.</p> <p>LLM providers access details can be found and modified via <code>/config/llm.php</code>.</p>"},{"location":"polyglot/essentials/overview/#simple-text-generation","title":"Simple Text Generation","text":"<p>The simplest way to use Polyglot is to generate text using static <code>Inference::text()</code> method. Simplified inference API uses the default connection for convenient ad-hoc calls.</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Generate text using the default connection\n$answer = (new Inference)-&gt;with(messages: 'What is the capital of France?')-&gt;get();\n\necho \"Answer: $answer\";\n\n// Output: Answer: The capital of France is Paris.\n</code></pre> <p>This static method uses the default connection specified in your configuration. Default LLM connection can be configured via config/llm.php.</p>"},{"location":"polyglot/essentials/overview/#creating-an-inference-object","title":"Creating an Inference Object","text":"<p>For more control, you can create an instance of the <code>Inference</code> class:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference();\n\n// Generate text using the default connection\n$answer = $inference-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']]\n)-&gt;get();\n\necho \"Answer: $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#specifying-a-connection","title":"Specifying a Connection","text":"<p>You can specify which connection preset to use:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object with a specific connection\n$inference = new Inference();\n$answer = $inference-&gt;using('anthropic')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']]\n    )-&gt;get();\n\necho \"Answer (using Anthropic): $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#creating-chat-conversations","title":"Creating Chat Conversations","text":"<p>For multi-turn conversations, provide an array of messages:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a chat conversation\n$messages = [\n    ['role' =&gt; 'user', 'content' =&gt; 'Hello, can you help me with a math problem?'],\n    ['role' =&gt; 'assistant', 'content' =&gt; 'Of course! I\\'d be happy to help with a math problem. What would you like to solve?'],\n    ['role' =&gt; 'user', 'content' =&gt; 'What is the square root of 144?'],\n];\n\n$inference = new Inference();\n$answer = $inference-&gt;with(\n    messages: $messages\n)-&gt;get();\n\necho \"Answer: $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#customizing-request-parameters","title":"Customizing Request Parameters","text":"<p>You can customize various parameters for your requests:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference with custom options\n$inference = new Inference();\n$answer = $inference-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'Write a short poem about coding.']],\n    model: 'gpt-4', // Override the default model\n    options: [\n        'temperature' =&gt; 0.7,\n        'max_tokens' =&gt; 100,\n    ]\n)-&gt;get();\n\necho \"Poem: $answer\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#fluent-api","title":"Fluent API","text":"<p>Regular inference API allows you to customize inference options, letting you set values specific for a given LLM provider.</p> <p>Most of the provider options are compatible with OpenAI API.</p> <p>This example shows how to create an inference object, specify a connection and generate text using the <code>create()</code> method. The <code>toText()</code> method returns text completion from the LLM response.</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$answer = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#streaming-inference-results","title":"Streaming inference results","text":"<p>Inference API allows streaming responses, which is useful for building more responsive UX as you can display partial responses from LLM as soon as they arrive, without waiting until the whole response is ready.</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$stream = (new Inference)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'Describe capital of Brasil']])\n    -&gt;withOptions(['max_tokens' =&gt; 512])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: Describe capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\n</code></pre>"},{"location":"polyglot/essentials/overview/#connecting-to-a-specific-llm-api-provider","title":"Connecting to a specific LLM API provider","text":"<p>Instructor allows you to define multiple API connections in <code>llm.php</code> file. This is useful when you want to use different LLMs or API providers in your application.</p> <p>Default configuration is located in <code>/config/llm.php</code> in the root directory of Instructor codebase. It contains a set of predefined connections to all LLM APIs supported out-of-the-box by Instructor.</p> <p>Config file defines connections to LLM APIs and their parameters. It also specifies the default connection to be used when calling Instructor without specifying the client connection.</p> <p><pre><code>    // This is fragment of /config/llm.php file\n    'defaultPreset' =&gt; 'openai',\n    //...\n    'presets' =&gt; [\n        'anthropic' =&gt; [ ... ],\n        'azure' =&gt; [ ... ],\n        'cohere' =&gt; [ ... ],\n        'fireworks' =&gt; [ ... ],\n        'gemini' =&gt; [ ... ],\n        'xai' =&gt; [ ... ],\n        'groq' =&gt; [ ... ],\n        'mistral' =&gt; [ ... ],\n        'ollama' =&gt; [\n            'driver' =&gt; 'ollama',\n            'apiUrl' =&gt; 'http://localhost:11434/v1',\n            'apiKey' =&gt; Env::get('OLLAMA_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'qwen2.5:0.5b',\n            'maxTokens' =&gt; 1024,\n            // select HTTP behavior via HttpClientBuilder or facade-level methods\n        ],\n        'openai' =&gt; [ ... ],\n        'openrouter' =&gt; [ ... ],\n        'together' =&gt; [ ... ],\n    // ...\n</code></pre> To customize the available connections you can either modify existing entries or add your own.</p> <p>Connecting to LLM API via predefined connection is as simple as calling <code>withPreset</code> method with the connection preset name.</p> <pre><code>&lt;?php\n// ...\n$answer = (new Inference)\n    -&gt;using('ollama') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n// ...\n</code></pre> <p>You can change the location of the configuration files for Instructor to use via <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable. You can use copies of the default configuration files as a starting point.</p>"},{"location":"polyglot/essentials/overview/#switching-between-providers","title":"Switching Between Providers","text":"<p>Polyglot makes it easy to switch between different LLM providers at runtime.</p>"},{"location":"polyglot/essentials/overview/#using-different-providers-for-llm-requests","title":"Using Different Providers for LLM Requests","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference();\n\n// Use the default provider (set in config)\n$defaultResponse = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n\necho \"Default provider response: $defaultResponse\\n\\n\";\n\n// Switch to Anthropic\n$anthropicResponse = $inference-&gt;using('anthropic')\n    -&gt;with(\n        messages: 'What is the capital of Germany?'\n    )-&gt;get();\n\necho \"Anthropic response: $anthropicResponse\\n\\n\";\n\n// Switch to Mistral\n$mistralResponse = $inference-&gt;using('mistral')\n    -&gt;with(\n        messages: 'What is the capital of Italy?'\n    )-&gt;get();\n\necho \"Mistral response: $mistralResponse\\n\\n\";\n\n// You can create a new instance for each provider\n$openAI = new Inference('openai');\n$anthropic = new Inference('anthropic');\n$mistral = new Inference('mistral');\n\n// And use them independently\n$responses = [\n    'openai' =&gt; $openAI-&gt;with(messages: 'What is the capital of Spain?')-&gt;get(),\n    'anthropic' =&gt; $anthropic-&gt;with(messages: 'What is the capital of Portugal?')-&gt;get(),\n    'mistral' =&gt; $mistral-&gt;with(messages: 'What is the capital of Greece?')-&gt;get(),\n];\n\nforeach ($responses as $provider =&gt; $response) {\n    echo \"$provider response: $response\\n\";\n}\n</code></pre>"},{"location":"polyglot/essentials/overview/#selecting-different-models","title":"Selecting Different Models","text":"<p>Each provider offers multiple models with different capabilities, context lengths, and pricing. Polyglot lets you override the default model for each request.</p>"},{"location":"polyglot/essentials/overview/#specifying-models-for-llm-requests","title":"Specifying Models for LLM Requests","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference('openai');\n\n// Use the default model (set in config)\n$defaultModelResponse = $inference-&gt;with(\n    messages: 'What is machine learning?'\n)-&gt;get();\n\n// Use a specific model\n$specificModelResponse = $inference-&gt;with(\n    messages: 'What is machine learning?',\n    model: 'gpt-4o'  // Override the default model\n)-&gt;get();\n\n// You can also set the model and other options\n$customResponse = $inference-&gt;with(\n    messages: 'What is machine learning?',\n    model: 'gpt-4-turbo',\n    options: [\n        'temperature' =&gt; 0.7,\n        'max_tokens' =&gt; 500,\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/request-options/","title":"Request Options","text":"<p>The <code>options</code> parameter allows you to customize various aspects of the request.</p> <p>NOTE: Except for <code>max_tokens</code>, all option parameters are provider-specific and may not be available or compatible with all providers. Check the provider's API documentation for details.</p>"},{"location":"polyglot/essentials/request-options/#common-options","title":"Common Options","text":"<pre><code>$options = [\n    // Generation parameters\n    'temperature' =&gt; 0.7,         // Controls randomness (0.0 to 1.0)\n    'max_tokens' =&gt; 1000,         // Maximum tokens to generate\n    'top_p' =&gt; 0.95,              // Nucleus sampling parameter\n    'frequency_penalty' =&gt; 0.0,   // Penalize repeated tokens\n    'presence_penalty' =&gt; 0.0,    // Penalize repeated topics\n    'stream' =&gt; false,            // Enable streaming responses\n    'stop' =&gt; [\"\\n\\n\", \"User:\"],  // Stop sequences\n\n    // Provider-specific options\n    'top_k' =&gt; 40,                // For some providers\n    'response_format' =&gt; [        // OpenAI-specific format control\n        'type' =&gt; 'json_object'\n    ],\n    // Additional provider-specific options...\n];\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short poem about programming.',\n    options: $options\n)-&gt;toText();\n</code></pre>"},{"location":"polyglot/essentials/request-options/#provider-specific-options","title":"Provider-Specific Options","text":"<p>Different providers may support additional options. Consult the provider's documentation for details:</p> <pre><code>// Anthropic-specific options\n$anthropicOptions = [\n    'temperature' =&gt; 0.7,\n    'max_tokens' =&gt; 1000,\n    'top_p' =&gt; 0.9,\n    'stop_sequences' =&gt; [\"\\n\\nHuman:\"],\n    'stream' =&gt; true,\n];\n\n$inference = new Inference()-&gt;using('anthropic');\n$response = $inference-&gt;with(\n    messages: 'Write a short poem about programming.',\n    options: $anthropicOptions\n)-&gt;toText();\n</code></pre>"},{"location":"polyglot/essentials/response-handling/","title":"Response Handling","text":"<p>Polyglot's <code>PendingInference</code> class represents pending inference execution. It provides methods to access the response in different formats, but also provides access to streaming responses. It does not execute the request to underlying LLM until you actually access the response data.</p> <p>It is returned by the <code>Inference</code> class when you call the <code>create()</code> method.</p>"},{"location":"polyglot/essentials/response-handling/#basic-response-handling","title":"Basic Response Handling","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;create();\n\n// Get the response as plain text\n$text = $response-&gt;get();\necho \"Text response: $text\\n\";\n\n// Get the response as a JSON object (for JSON responses)\n$json = $response-&gt;asJsonData();\necho \"JSON response: \" . json_encode($json) . \"\\n\";\n\n// Get the full response object\n$fullResponse = $response-&gt;response();\n\n// Access specific information\necho \"Content: \" . $fullResponse-&gt;content() . \"\\n\";\necho \"Finish reason: \" . $fullResponse-&gt;finishReason() . \"\\n\";\necho \"Usage - Total tokens: \" . $fullResponse-&gt;usage()-&gt;total() . \"\\n\";\necho \"Usage - Input tokens: \" . $fullResponse-&gt;usage()-&gt;input() . \"\\n\";\necho \"Usage - Output tokens: \" . $fullResponse-&gt;usage()-&gt;output() . \"\\n\";\n</code></pre>"},{"location":"polyglot/essentials/response-handling/#working-with-streaming-responses","title":"Working with Streaming Responses","text":"<p>For streaming responses, use the <code>stream()</code> method:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages('Write a short story about a robot.')\n    -&gt;withStreaming()\n    -&gt;create();\n\n// Get a generator that yields partial responses\n$stream = $response-&gt;stream()-&gt;responses();\n\necho \"Story: \";\nforeach ($stream as $partialResponse) {\n    // Output each chunk as it arrives\n    echo $partialResponse-&gt;contentDelta;\n\n    // Flush the output buffer to show progress in real-time\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n        flush();\n    }\n}\n\necho \"\\n\\nComplete response: \" . $response-&gt;get();\n</code></pre>"},{"location":"polyglot/essentials/response-handling/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>For models that support function calling or tools:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$tools = [\n    [\n        'type' =&gt; 'function',\n        'function' =&gt; [\n            'name' =&gt; 'get_weather',\n            'description' =&gt; 'Get the current weather in a location',\n            'parameters' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'location' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'The city and state, e.g. San Francisco, CA',\n                    ],\n                    'unit' =&gt; [\n                        'type' =&gt; 'string',\n                        'enum' =&gt; ['celsius', 'fahrenheit'],\n                        'description' =&gt; 'The temperature unit to use',\n                    ],\n                ],\n                'required' =&gt; ['location'],\n            ],\n        ],\n    ],\n];\n\n$inference = new Inference()-&gt;using('openai');\n$response = $inference-&gt;with(\n    messages: 'What is the weather in Paris?',\n    tools: $tools,\n    toolChoice: 'auto',  // Let the model decide when to use tools\n    mode: OutputMode::Tools    // Enable tools mode\n)-&gt;response();\n\n// Check if there are tool calls\nif ($response-&gt;hasToolCalls()) {\n    $toolCalls = $response-&gt;toolCalls();\n    foreach ($toolCalls-&gt;all() as $call) {\n        echo \"Tool called: \" . $call-&gt;name() . \"\\n\";\n        echo \"Arguments: \" . $call-&gt;argsAsJson() . \"\\n\";\n\n        // In a real application, you would call the actual function here\n        // and then send the result back to the model\n        $result = ['temperature' =&gt; 22, 'unit' =&gt; 'celsius', 'condition' =&gt; 'sunny'];\n\n        // Continue the conversation with the tool result\n        $newMessages = [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is the weather in Paris?'],\n            [\n                'role' =&gt; 'assistant',\n                'content' =&gt; '',\n                '_metadata' =&gt; [\n                    'tool_calls' =&gt; [\n                        [\n                            'id' =&gt; $call-&gt;id(),\n                            'function' =&gt; [\n                                'name' =&gt; $call-&gt;name(),\n                                'arguments' =&gt; $call-&gt;argsAsJson(),\n                            ],\n                        ],\n                    ],\n                ],\n            ],\n            [\n                'role' =&gt; 'tool',\n                'content' =&gt; json_encode($result),\n                '_metadata' =&gt; [\n                    'tool_call_id' =&gt; $call-&gt;id(),\n                    'tool_name' =&gt; $call-&gt;name(),\n                ],\n            ],\n        ];\n\n        $finalResponse = $inference-&gt;with(\n            messages: $newMessages\n        )-&gt;get();\n\n        echo \"Final response: $finalResponse\\n\";\n    }\n} else {\n    echo \"Response: \" . $response-&gt;content() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/","title":"Adapters","text":"<p>Each provider has a set of adapters that handle its specific format requirements:</p>"},{"location":"polyglot/internals/adapters/#request-adapters","title":"Request Adapters","text":"<p>Request adapters convert Polyglot's unified request format to provider-specific HTTP requests:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIRequestAdapter implements ProviderRequestAdapter {\n    public function __construct(\n        protected LLMConfig $config,\n        protected CanMapRequestBody $bodyFormat\n    ) { ... }\n\n    public function toHttpClientRequest(\n        array $messages,\n        string $model,\n        array $tools,\n        array|string $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): HttpRequest { ... }\n\n    protected function toHeaders(): array { ... }\n    protected function toUrl(string $model = '', bool $stream = false): string { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#message-formatters","title":"Message Formatters","text":"<p>Message formatters handle the conversion of messages to provider-specific formats:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIMessageFormat implements CanMapMessages {\n    public function map(array $messages): array { ... }\n\n    protected function mapMessage(array $message): array { ... }\n    protected function toNativeToolCall(array $message): array { ... }\n    protected function toNativeToolResult(array $message): array { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#body-formatters","title":"Body Formatters","text":"<p>Body formatters handle the conversion of request bodies to provider-specific formats:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIBodyFormat implements CanMapRequestBody {\n    public function __construct(\n        protected LLMConfig $config,\n        protected CanMapMessages $messageFormat\n    ) { ... }\n\n    public function map(\n        array $messages,\n        string $model,\n        array $tools,\n        array|string $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): array { ... }\n\n    private function applyMode(\n        array $request,\n        Mode $mode,\n        array $tools,\n        string|array $toolChoice,\n        array $responseFormat\n    ): array { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#response-adapters","title":"Response Adapters","text":"<p>Response adapters convert provider-specific responses to Polyglot's unified format:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIResponseAdapter implements ProviderResponseAdapter {\n    public function __construct(\n        protected CanMapUsage $usageFormat\n    ) { ... }\n\n    public function fromResponse(HttpResponse $response): ?InferenceResponse { ... }\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse { ... }\n    public function toEventBody(string $data): string|bool { ... }\n\n    protected function makeToolCalls(array $data): ToolCalls { ... }\n    protected function makeContent(array $data): string { ... }\n    protected function makeContentDelta(array $data): string { ... }\n    protected function makeToolId(array $data): string { ... }\n    protected function makeToolNameDelta(array $data): string { ... }\n    protected function makeToolArgsDelta(array $data): string { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/adapters/#usage-formatters","title":"Usage Formatters","text":"<p>Usage formatters extract token usage information from provider responses:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIUsageFormat implements CanMapUsage {\n    public function fromData(array $data): Usage { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/configuration/","title":"Configuration Layer","text":"<p>Polyglot's configuration layer manages settings for different providers.</p>"},{"location":"polyglot/internals/configuration/#llm-configuration","title":"LLM Configuration","text":"<pre><code>namespace Cognesy\\Polyglot\\Inference\\Data;\n\nclass LLMConfig {\n    public function __construct(\n        public string $apiUrl = '',\n        public string $apiKey = '',\n        public string $endpoint = '',\n        public array $queryParams = [],\n        public array $metadata = [],\n        public string $model = '',\n        public int $maxTokens = 1024,\n        public int $contextLength = 8000,\n        public int $maxOutputLength = 4096,\n        public string $httpClient = '',\n        public string $providerType = 'openai-compatible'\n    ) { ... }\n\n    public static function load(string $connection): \\Cognesy\\Polyglot\\Inference\\Config\\LLMConfig { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/configuration/#embeddings-configuration","title":"Embeddings Configuration","text":"<pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Data;\n\nclass EmbeddingsConfig {\n    public function __construct(\n        public string $apiUrl = '',\n        public string $apiKey = '',\n        public string $endpoint = '',\n        public string $model = '',\n        public int $dimensions = 0,\n        public int $maxInputs = 0,\n        public array $metadata = [],\n        public string $httpClient = '',\n        public string $providerType = 'openai'\n    ) { ... }\n\n    public static function load(string $connection): \\Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/events/","title":"Events System","text":"<p>Polyglot uses an event system to generate internal notifications at the various stages of the execution process.</p> <p>It has been built primarily to ensure observability of the internal components of the library.</p> <pre><code>namespace Cognesy\\Utils\\Events;\n\nuse Cognesy\\Events\\Event;\n\nclass EventDispatcher {\n    public function dispatch(Event $event): void { ... }\n    public function wiretap(callable $listener): self { ... }\n    public function addListener(string $eventClass, callable $listener): self { ... }\n}\n\nnamespace Cognesy\\Polyglot\\Inference\\Events;\n\nclass InferenceResponseReceived extends Event {}\n\nclass InferenceRequested extends Event {}\n\nclass PartialInferenceResponseReceived extends Event {}\n</code></pre>"},{"location":"polyglot/internals/http-client/","title":"HTTP Client Layer","text":"<p>At the lowest level, Polyglot uses an HTTP client layer to communicate with provider APIs. This layer includes:</p> <ol> <li>A unified <code>HttpClient</code> interface</li> <li>Implementations for different HTTP libraries (Guzzle, Symfony, Laravel)</li> <li>A middleware system for extending functionality</li> </ol>"},{"location":"polyglot/internals/http-client/#httpclient","title":"HttpClient","text":"<p>The <code>HttpClient</code> class provides a unified interface for HTTP requests:</p> <pre><code>namespace Cognesy\\Http;\n\nclass HttpClient implements CanHandleHttpRequest {\n    public function __construct(\n        string $client = '',\n        ?HttpClientConfig $config = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    public static function make(\n        string $client = '',\n        ?HttpClientConfig $config = null,\n        ?EventDispatcher $events = null\n    ): self { ... }\n\n    public function withClient(string $client): self { ... }\n    public function withConfig(HttpClientConfig $config): self { ... }\n    public function withMiddleware(...$middleware): self { ... }\n    public function withDebugPreset(?string $preset): self { ... }\n\n    public function handle(HttpClientRequest $request): HttpResponse { ... }\n    public function middleware(): MiddlewareStack { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/http-client/#httprequest-and-httpresponse","title":"HttpRequest and HttpResponse","text":"<p>These classes represent HTTP requests and responses:</p> <pre><code>namespace Cognesy\\Http\\Data;\n\nclass HttpRequest {\n    public function __construct(\n        private string $url,\n        private string $method,\n        private array $headers,\n        private mixed $body,\n        private array $options\n    ) { ... }\n\n    public function url(): string { ... }\n    public function method(): string { ... }\n    public function headers(): array { ... }\n    public function body(): HttpRequestBody { ... }\n    public function options(): array { ... }\n    public function isStreamed(): bool { ... }\n\n    public function withStreaming(bool $streaming): self { ... }\n}\n\ninterface HttpResponse {\n    public function statusCode(): int;\n    public function headers(): array;\n    public function body(): string;\n    public function stream(int $chunkSize = 1): Generator;\n    public function original(): mixed;\n}\n</code></pre>"},{"location":"polyglot/internals/http-client/#middleware-system","title":"Middleware System","text":"<p>The HTTP client layer includes a middleware system that allows extending functionality:</p> <pre><code>namespace Cognesy\\Http;\n\ninterface HttpMiddleware {\n    public function handle(\n        HttpRequest $request,\n        CanHandleHttpRequest $next\n    ): HttpResponse;\n}\n\nabstract class BaseMiddleware implements HttpMiddleware {\n    public function handle(\n        HttpRequest $request,\n        CanHandleHttpRequest $next\n    ): HttpResponse { ... }\n\n    protected function beforeRequest(HttpClientRequest $request): void {}\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return $response;\n    }\n\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        return false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return $response;\n    }\n}\n\nclass MiddlewareStack {\n    public function append(HttpMiddleware $middleware, string $name = ''): self { ... }\n    public function prepend(HttpMiddleware $middleware, string $name = ''): self { ... }\n    public function remove(string $name): self { ... }\n    public function replace(string $name, HttpMiddleware $middleware): self { ... }\n    public function clear(): self { ... }\n    public function has(string $name): bool { ... }\n    public function get(string|int $nameOrIndex): ?HttpMiddleware { ... }\n    public function all(): array { ... }\n    public function process(\n        HttpRequest $request,\n        CanHandleHttpRequest $handler\n    ): HttpResponse { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/lifecycle/","title":"Request/Response Lifecycle","text":"<p>Let's follow the complete flow of a request through Polyglot:</p>"},{"location":"polyglot/internals/lifecycle/#request-processing","title":"Request Processing","text":"<ol> <li>Application creates an <code>Inference</code> object</li> <li>Application calls <code>create()</code> with parameters</li> <li><code>Inference</code> creates an <code>InferenceRequest</code>.</li> <li><code>Inference</code> creates a <code>PendingInference</code> object with the instances of request, driver and event dispatcher.</li> <li><code>Inference</code> returns a <code>PendingInference</code> object to the application.</li> </ol>"},{"location":"polyglot/internals/lifecycle/#response-processing","title":"Response Processing","text":"<ol> <li>Application accesses the <code>PendingInference</code> object content, e.g. via <code>response()</code> method.</li> <li><code>PendingInference</code> checks if HTTP request has been already executed.</li> <li>If already sent, it returns the cached response.</li> <li><code>PendingInference</code> dispatches the <code>InferenceRequested</code> event</li> <li><code>PendingInference</code> passes the request to the driver.</li> <li>Driver uses request adapter to create HTTP request</li> <li>Request adapter uses request body formatter and message formatter.</li> <li>Driver sends the HTTP request and returns it to <code>PendingInference</code>.</li> <li><code>PendingInference</code> calls the driver to read and parse the response.</li> <li>Driver uses a response adapter to extract content into appropriate fields of <code>InferenceResponse</code> object</li> <li><code>PendingInference</code> dispatches the <code>InferenceResponseReceived</code> event</li> <li>Result <code>InferenceResponse</code> object is returned to the application</li> </ol>"},{"location":"polyglot/internals/overview/","title":"Overview of Architecture","text":"<p>This section provides a detailed look at Polyglot's internal architecture.</p> <p>Understanding the core components, interfaces, and design patterns will help you extend the library, contribute to its development, or build your own integrations with new LLM providers.</p>"},{"location":"polyglot/internals/overview/#core-architecture","title":"Core Architecture","text":"<p>Polyglot is built on a modular, layered architecture that separates concerns and promotes extensibility. The high-level architecture consists of:</p> <ol> <li>Public API Layer: Classes like <code>Inference</code> and <code>Embeddings</code> that provide a unified interface for applications</li> <li>Provider Abstraction Layer: Adapters, drivers, and formatters that translate between the unified API and provider-specific formats</li> <li>HTTP Client Layer: A flexible HTTP client with middleware support for communicating with LLM APIs</li> <li>Configuration Layer: Configuration management for different providers and models</li> </ol> <pre><code>+---------------------+    +---------------------+\n|      Inference      |    |     Embeddings      |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|  InferenceRequest   |    | EmbeddingsRequest   |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|   PendingInference  |    |  PendingEmbeddings  |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|  InferenceDrivers   |    |  EmbeddingsDrivers  |\n+---------------------+    +---------------------+\n            |                        |\n+------------------------------------------------+\n|               HTTP Client Layer                |\n+------------------------------------------------+\n                         |\n+------------------------------------------------+\n|           Provider-specific API Calls          |\n+------------------------------------------------+\n</code></pre>"},{"location":"polyglot/internals/providers/","title":"Provider Abstraction Layer","text":"<p>The provider abstraction layer is where Polyglot handles the differences between LLM and embedding providers. This layer includes:</p> <ol> <li>Provider Classes: <code>LLMProvider</code> and <code>EmbeddingsProvider</code> - Builder classes for configuring and resolving driver configurations</li> <li>Drivers: Classes that implement provider-specific logic for inference and embeddings</li> <li>Adapters: Classes that convert between unified and provider-specific formats</li> <li>Factories: Classes that create appropriate drivers based on configuration</li> </ol>"},{"location":"polyglot/internals/providers/#provider-builder-classes","title":"Provider Builder Classes","text":""},{"location":"polyglot/internals/providers/#llmprovider","title":"LLMProvider","text":"<p>The <code>LLMProvider</code> class is a builder that configures inference settings and resolves configuration from various sources. It provides a fluent interface for setting up LLM configurations:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n// Create with preset\n$provider = LLMProvider::using('openai');\n\n// Create with DSN\n$provider = LLMProvider::dsn('openai://model=gpt-4&amp;temperature=0.7');\n\n// Fluent configuration\n$provider = LLMProvider::new()\n    -&gt;withLLMPreset('openai')\n    -&gt;withConfig($customConfig);\n\n// Create the final driver using a factory and injected HTTP client\n$httpClient = (new \\Cognesy\\Http\\HttpClientBuilder())-&gt;create();\n$config = $provider-&gt;resolveConfig();\n$driver = (new \\Cognesy\\Polyglot\\Inference\\Creation\\InferenceDriverFactory($events))\n    -&gt;makeDriver($config, $httpClient);\n</code></pre> <p>Key methods: - <code>withLLMPreset(string $preset)</code>: Set configuration preset - <code>withConfig(LLMConfig $config)</code>: Set explicit configuration - <code>withConfigOverrides(array $overrides)</code>: Override specific config values - <code>withDsn(string $dsn)</code>: Configure via DSN string - <code>withDriver(CanHandleInference $driver)</code>: Set explicit driver - Use <code>resolveConfig()</code> + <code>InferenceDriverFactory::makeDriver()</code> to create drivers</p>"},{"location":"polyglot/internals/providers/#embeddingsprovider","title":"EmbeddingsProvider","text":"<p>The <code>EmbeddingsProvider</code> class builds and configures embeddings settings, resolving configuration from various sources:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\n\n// Create with preset\n$provider = EmbeddingsProvider::using('openai');\n\n// Create with DSN\n$provider = EmbeddingsProvider::dsn('openai://model=text-embedding-3-large');\n\n// Fluent configuration\n$provider = EmbeddingsProvider::new()\n    -&gt;withPreset('openai')\n    -&gt;withConfig($customConfig);\n\n// Create the final driver using a factory and injected HTTP client\n$httpClient = (new \\Cognesy\\Http\\HttpClientBuilder())-&gt;create();\n$config = $provider-&gt;resolveConfig();\n$driver = (new \\Cognesy\\Polyglot\\Embeddings\\Drivers\\EmbeddingsDriverFactory($events))\n    -&gt;makeDriver($config, $httpClient);\n</code></pre> <p>Key methods: - <code>withPreset(string $preset)</code>: Set configuration preset - <code>withConfig(EmbeddingsConfig $config)</code>: Set explicit configuration - <code>withDsn(string $dsn)</code>: Configure via DSN string - <code>withDriver(CanHandleVectorization $driver)</code>: Set explicit driver - Use <code>resolveConfig()</code> + <code>EmbeddingsDriverFactory::makeDriver()</code> to create drivers</p>"},{"location":"polyglot/internals/providers/#key-interfaces-for-llm","title":"Key Interfaces for LLM","text":"<p>Several interfaces define the contract for LLM drivers and adapters:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Contracts;\n\ninterface CanHandleInference {\n    public function handle(InferenceRequest $request): HttpResponse;\n    public function fromResponse(HttpResponse $response): ?InferenceResponse;\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse;\n    public function toEventBody(string $data): string|bool;\n}\n\ninterface ProviderRequestAdapter {\n    public function toHttpClientRequest(\n        array $messages,\n        string $model,\n        array $tools,\n        string|array $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): HttpRequest;\n}\n\ninterface ProviderResponseAdapter {\n    public function fromResponse(HttpResponse $response): ?InferenceResponse;\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse;\n    public function toEventBody(string $data): string|bool;\n}\n\ninterface CanMapMessages {\n    public function map(array $messages): array;\n}\n\ninterface CanMapRequestBody {\n    public function map(\n        array $messages,\n        string $model,\n        array $tools,\n        array|string $toolChoice,\n        array $responseFormat,\n        array $options,\n        Mode $mode\n    ): array;\n}\n\ninterface CanMapUsage {\n    public function fromData(array $data): Usage;\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#key-interfaces-for-embeddings","title":"Key Interfaces for Embeddings","text":"<p>The embeddings functionality uses these key interfaces:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Contracts;\n\n// Main driver interface\ninterface CanHandleVectorization {\n    public function vectorize(EmbeddingsRequest $request): EmbeddingsResponse;\n}\n\n// Request and response mapping interfaces\ninterface CanMapRequestBody {\n    public function map(EmbeddingsRequest $request): array;\n}\n\ninterface EmbedRequestAdapter {\n    public function toHttpRequest(EmbeddingsRequest $request): HttpRequest;\n}\n\ninterface EmbedResponseAdapter {\n    public function fromHttpResponse(HttpResponse $response): EmbeddingsResponse;\n}\n\ninterface CanMapUsage {\n    public function fromData(array $data): Usage;\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#modularllmdriver","title":"ModularLLMDriver","text":"<p>The <code>ModularLLMDriver</code> is a central component that implements the <code>CanHandleInference</code> interface using adapters:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers;\n\nclass ModularLLMDriver implements CanHandleInference {\n    public function __construct(\n        protected LLMConfig $config,\n        protected ProviderRequestAdapter $requestAdapter,\n        protected ProviderResponseAdapter $responseAdapter,\n        protected ?CanHandleHttpRequest $httpClient = null,\n        protected ?EventDispatcher $events = null\n    ) { ... }\n\n    public function handle(InferenceRequest $request): HttpResponse { ... }\n    public function fromResponse(HttpResponse $response): ?InferenceResponse { ... }\n    public function fromStreamResponse(string $eventBody): ?PartialInferenceResponse { ... }\n    public function toEventBody(string $data): string|bool { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#driver-factories","title":"Driver Factories","text":""},{"location":"polyglot/internals/providers/#inferencedriverfactory","title":"InferenceDriverFactory","text":"<p>The <code>InferenceDriverFactory</code> creates the appropriate driver for each LLM provider:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers;\n\nclass InferenceDriverFactory {\n    public function makeDriver(\n        LLMConfig $config,\n        HttpClient $httpClient\n    ): CanHandleInference { ... }\n\n    // Provider-specific factory methods\n    public function openAI(...): CanHandleInference { ... }\n    public function anthropic(...): CanHandleInference { ... }\n    public function mistral(...): CanHandleInference { ... }\n    // Other providers...\n\n    // Driver registration\n    public static function registerDriver(string $name, string|callable $driver): void { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/providers/#embeddingsdriverfactory","title":"EmbeddingsDriverFactory","text":"<p>The <code>EmbeddingsDriverFactory</code> creates embeddings drivers:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Drivers;\n\nclass EmbeddingsDriverFactory {\n    public function makeDriver(\n        EmbeddingsConfig $config,\n        HttpClient $httpClient\n    ): CanHandleVectorization { ... }\n\n    // Provider-specific factory methods  \n    public function openAI(...): CanHandleVectorization { ... }\n    public function cohere(...): CanHandleVectorization { ... }\n    public function gemini(...): CanHandleVectorization { ... }\n    // Other providers...\n\n    // Driver registration\n    public static function registerDriver(string $name, string|callable $driver): void { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/public-api/","title":"Public API Layer","text":""},{"location":"polyglot/internals/public-api/#the-inference-class","title":"The Inference Class","text":"<p>The <code>Inference</code> class is the main entry point for LLM interactions. It encapsulates the complexities of different providers behind a unified interface.</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nclass Inference {\n    // Create and manage the LLM instance\n    public function __construct(\n        string $connection = '',\n        ?LLMConfig $config = null,\n        ?CanHandleHttpRequest $httpClient = null,\n        ?CanHandleInference $driver = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    // Configure the instance\n    public function using(string $preset): self { ... }\n    public function withConfig(LLMConfig $config): self { ... }\n    public function withHttpClient(CanHandleHttpRequest $httpClient): self { ... }\n    public function withDriver(CanHandleInference $driver): self { ... }\n    public function withDebugPreset(?string $preset): self { ... }\n    public function withCachedContext(...): self { ... }\n\n    // Main method for creating inference requests\n    public function create(\n        string|array $messages = [],\n        string $model = '',\n        array $tools = [],\n        string|array $toolChoice = [],\n        array $responseFormat = [],\n        array $options = [],\n        Mode $mode = OutputMode::Text\n    ): PendingInference { ... }\n\n    // Static convenience method for simple text generation\n    public static function text(\n        string|array $messages,\n        string $connection = '',\n        string $model = '',\n        array $options = []\n    ): string { ... }\n}\n</code></pre> <p>The <code>Inference</code> class follows a fluent interface pattern, allowing method chaining for configuration.</p>"},{"location":"polyglot/internals/public-api/#the-embeddings-class","title":"The Embeddings Class","text":"<p>Similarly, the <code>Embeddings</code> class provides a unified interface for generating embeddings:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings;\n\nuse Cognesy\\Polyglot\\Embeddings\\Data\\EmbeddingsResponse;class Embeddings {\n    public function __construct(\n        string $connection = '',\n        ?EmbeddingsConfig $config = null,\n        ?CanHandleHttpRequest $httpClient = null,\n        ?CanVectorize $driver = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    // Configuration methods\n    public function using(string $preset): self { ... }\n    public function withConfig(EmbeddingsConfig $config): self { ... }\n    public function withModel(string $model): self { ... }\n    public function withHttpClient(CanHandleHttpRequest $httpClient): self { ... }\n    public function withDriver(CanVectorize $driver): self { ... }\n\n    // Main method for generating embeddings\n    public function create(string|array $input, array $options = []): EmbeddingsResponse { ... }\n\n    // Utility methods for finding similar content\n    public function findSimilar(string $query, array $documents, int $topK = 5): array { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/request-response/","title":"Request and Response Objects","text":""},{"location":"polyglot/internals/request-response/#inferencerequest","title":"InferenceRequest","text":"<p>The <code>InferenceRequest</code> class encapsulates all the parameters needed for an LLM request:</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nclass InferenceRequest {\n    public array $messages = [];\n    public string $model = '';\n    public array $tools = [];\n    public string|array $toolChoice = [];\n    public array $responseFormat = [];\n    public array $options = [];\n    public Mode $mode = OutputMode::Text;\n    public ?CachedContext $cachedContext;\n\n    public function __construct(...) { ... }\n\n    // Getters\n    public function messages(): array { ... }\n    public function model(): string { ... }\n    public function isStreamed(): bool { ... }\n    public function tools(): array { ... }\n    public function toolChoice(): string|array { ... }\n    public function responseFormat(): array { ... }\n    public function options(): array { ... }\n    public function mode(): Mode { ... }\n    public function cachedContext(): ?CachedContext { ... }\n\n    // Fluent setters\n    public function withMessages(string|array $messages): self { ... }\n    public function withModel(string $model): self { ... }\n    public function withStreaming(bool $streaming): self { ... }\n    public function withTools(array $tools): self { ... }\n    public function withToolChoice(string|array $toolChoice): self { ... }\n    public function withResponseFormat(array $responseFormat): self { ... }\n    public function withOptions(array $options): self { ... }\n    public function withMode(Mode $mode): self { ... }\n    public function withCachedContext(?CachedContext $cachedContext): self { ... }\n\n    // Utility methods\n    public function toArray(): array { ... }\n    public function withCacheApplied(): self { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/request-response/#pendinginference","title":"PendingInference","text":"<p>The <code>PendingInference</code> class handles the response from an LLM API:</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;class PendingInference {\n    public function __construct(\n        InferenceRequest $request,\n        CanHandleInference $driver,\n        EventDispatcherInterface $events,\n    ) { ... }\n\n    // Access methods\n    public function isStreamed(): bool { ... }\n    public function toText(): string { ... }\n    public function toArray(): array { ... }\n    public function stream(): InferenceStream { ... }\n    public function response(): InferenceResponse { ... }\n}\n</code></pre> <p>For streaming responses, the <code>InferenceStream</code> class provides methods to process the stream:</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nclass InferenceStream {\n    public function __construct(\n        HttpResponse        $httpResponse,\n        CanHandleInference        $driver,\n        EventDispatcherInterface  $events,\n    ) { ... }\n\n    // Stream processing methods\n    public function responses(): Generator { ... }\n    public function all(): array { ... }\n    public function final(): ?InferenceResponse { ... }\n    public function onPartialResponse(callable $callback): self { ... }\n}\n</code></pre>"},{"location":"polyglot/internals/request-response/#embeddingsresponse","title":"EmbeddingsResponse","text":"<p>The <code>EmbeddingsResponse</code> class encapsulates the result of an embeddings request:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings;\n\nclass EmbeddingsResponse {\n    public function __construct(\n        public array $vectors,\n        public ?Usage $usage\n    ) { ... }\n\n    // Access methods\n    public function first(): Vector { ... }\n    public function last(): Vector { ... }\n    public function all(): array { ... }\n    public function usage(): Usage { ... }\n    public function toValuesArray(): array { ... }\n    public function totalTokens(): int { ... }\n    public function split(int $index): array { ... }\n}\n</code></pre>"},{"location":"polyglot/modes/json-schema/","title":"JSON Schema Mode","text":"<p>JSON Schema mode takes JSON generation a step further by validating the response against a predefined schema. This ensures the response has exactly the structure your application expects.</p>"},{"location":"polyglot/modes/json-schema/#defining-and-using-a-json-schema","title":"Defining and Using a JSON Schema","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');  // Currently best supported by OpenAI\n\n// Define a schema for a weather report\n$schema = [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'location' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'The city and country'\n        ],\n        'current_temperature' =&gt; [\n            'type' =&gt; 'number',\n            'description' =&gt; 'Current temperature in Celsius'\n        ],\n        'conditions' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'Current weather conditions (e.g., sunny, rainy)'\n        ],\n        'forecast' =&gt; [\n            'type' =&gt; 'array',\n            'items' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'day' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'Day of the week'\n                    ],\n                    'temperature_high' =&gt; [\n                        'type' =&gt; 'number',\n                        'description' =&gt; 'Expected high temperature in Celsius'\n                    ],\n                    'temperature_low' =&gt; [\n                        'type' =&gt; 'number',\n                        'description' =&gt; 'Expected low temperature in Celsius'\n                    ],\n                    'conditions' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'Expected weather conditions'\n                    ]\n                ],\n                'required' =&gt; ['day', 'temperature_high', 'temperature_low', 'conditions']\n            ],\n            'description' =&gt; 'Three-day weather forecast'\n        ]\n    ],\n    'required' =&gt; ['location', 'current_temperature', 'conditions', 'forecast']\n];\n\n// Request a weather report\n$response = $inference-&gt;with(\n    messages: 'Provide a weather report for Paris, France.',\n    responseFormat: [\n        'type' =&gt; 'json_schema',\n        'json_schema' =&gt; [\n            'name' =&gt; 'weather_report',\n            'schema' =&gt; $schema,\n            'strict' =&gt; true,\n        ],\n    ],\n    mode: OutputMode::JsonSchema\n)-&gt;asJsonData();\n\n// The response will match the schema's structure exactly\necho \"Weather in {$response['location']}:\\n\";\necho \"Currently {$response['conditions']} and {$response['current_temperature']}\u00b0C\\n\\n\";\n\necho \"Forecast:\\n\";\nforeach ($response['forecast'] as $day) {\n    echo \"{$day['day']}: {$day['conditions']}, {$day['temperature_low']}\u00b0C to {$day['temperature_high']}\u00b0C\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/json-schema/#schema-validation","title":"Schema Validation","text":"<p>With JSON Schema mode, Polyglot ensures the LLM's response adheres to your schema:</p> <ol> <li>The schema is sent to the model as part of the request</li> <li>The model structures its response to match the schema</li> <li>For providers with native schema support, validation happens at the API level</li> <li>For other providers, Polyglot helps guide the model to produce correctly formatted output</li> </ol>"},{"location":"polyglot/modes/json-schema/#provider-support-for-json-schema","title":"Provider Support for JSON Schema","text":"<p>Provider support for JSON Schema varies:</p> <ul> <li>OpenAI (GPT-4 and newer): Native support with <code>json_schema</code> response format</li> <li>Anthropic (Claude 3 and newer): Partial support via prompt engineering</li> <li>Other providers: May require more explicit instructions in the prompt</li> </ul> <p>For best compatibility, use OpenAI for schema-validated responses.</p>"},{"location":"polyglot/modes/json-schema/#when-to-use-json-schema-mode","title":"When to Use JSON Schema Mode","text":"<p>JSON Schema mode is ideal for: - Applications requiring strictly typed data - Integration with databases or APIs that expect specific structures - Data extraction with complex nested structures - Ensuring consistent response formats across multiple requests</p>"},{"location":"polyglot/modes/json/","title":"JSON Mode","text":"<p>JSON mode instructs the model to return responses formatted as valid JSON objects. This is useful when you need structured data that can be easily processed by your application.</p>"},{"location":"polyglot/modes/json/#basic-json-generation","title":"Basic JSON Generation","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n$response = $inference-&gt;with(\n    messages: 'List the top 3 most populous cities in the world with their populations.',\n    mode: OutputMode::Json\n)-&gt;asJsonData();\n\n// $response is now a PHP array parsed from the JSON\necho \"Top cities:\\n\";\nforeach ($response['cities'] as $city) {\n    echo \"- {$city['name']}: {$city['population']} million\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/json/#structuring-json-responses-with-instructions","title":"Structuring JSON Responses with Instructions","text":"<p>For best results, include clear instructions about the expected JSON structure:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// Include the expected structure in the prompt\n$prompt = &lt;&lt;&lt;EOT\nList the top 3 most populous cities in the world.\nReturn your answer as a JSON object with the following structure:\n{\n  \"cities\": [\n    {\n      \"name\": \"City name\",\n      \"country\": \"Country name\",\n      \"population\": population in millions (number)\n    },\n    ...\n  ]\n}\nEOT;\n\n$response = $inference-&gt;with(\n    messages: $prompt,\n    mode: OutputMode::Json\n)-&gt;asJsonData();\n\n// Process the response\necho \"Top cities by population:\\n\";\nforeach ($response['cities'] as $index =&gt; $city) {\n    echo ($index + 1) . \". {$city['name']}, {$city['country']}: {$city['population']} million\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/json/#provider-specific-json-options","title":"Provider-Specific JSON Options","text":"<p>Some providers offer additional options for JSON mode:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// OpenAI example\n$inference = new Inference()-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'List the top 3 most populous cities in the world.',\n    mode: OutputMode::Json,\n    options: [\n        'response_format' =&gt; ['type' =&gt; 'json_object'],\n        // Other OpenAI-specific options...\n    ]\n)-&gt;asJsonData();\n\n// The response will be a JSON object\n</code></pre>"},{"location":"polyglot/modes/json/#when-to-use-json-mode","title":"When to Use JSON Mode","text":"<p>JSON mode is ideal for: - Extracting structured data (lists, records, etc.) - API responses that need to be machine-readable - Generating data for web applications - Creating datasets</p>"},{"location":"polyglot/modes/md-json/","title":"MdJSON Mode","text":"<p>Markdown JSON mode is a special mode that requests the model to format its response as JSON within a Markdown code block. This is particularly useful for models or providers that don't have native JSON output support.</p>"},{"location":"polyglot/modes/md-json/#using-markdown-json-mode","title":"Using Markdown JSON Mode","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// This works with virtually any provider\n$response = $inference-&gt;with(\n    messages: 'List three programming languages and their key features.',\n    mode: OutputMode::MdJson\n)-&gt;asJsonData();\n\n// The model will return JSON wrapped in Markdown, which Polyglot processes for you\nforeach ($response['languages'] as $language) {\n    echo \"{$language['name']} - {$language['paradigm']}\\n\";\n    echo \"Key features: \" . implode(', ', $language['key_features']) . \"\\n\\n\";\n}\n</code></pre>"},{"location":"polyglot/modes/md-json/#how-mdjson-mode-works","title":"How MdJson Mode Works","text":"<ol> <li>Polyglot instructs the model to respond with a JSON object wrapped in a Markdown code block</li> <li>The model formats its response accordingly (<code>json {...}</code>)</li> <li>Polyglot extracts the JSON content from the Markdown code block</li> <li>The JSON is parsed and returned to your application</li> </ol>"},{"location":"polyglot/modes/md-json/#providing-guidance-for-mdjson","title":"Providing Guidance for MdJson","text":"<p>While MdJson is more flexible across providers, you still need to provide clear instructions:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// Include expected format in the prompt\n$prompt = &lt;&lt;&lt;EOT\nList three programming languages with their key features.\nRespond with a JSON object following this structure:\n{\n  \"languages\": [\n    {\n      \"name\": \"Language name\",\n      \"paradigm\": \"Programming paradigm\",\n      \"year_created\": year as number,\n      \"key_features\": [\"feature1\", \"feature2\", \"feature3\"]\n    },\n  ]\n}\nEOT;\n\n$response = $inference-&gt;with(\nmessages: $prompt,\nmode: OutputMode::MdJson\n)-&gt;toJson();\n\n// Process as normal JSON\n</code></pre>"},{"location":"polyglot/modes/md-json/#when-to-use-mdjson-mode","title":"When to Use MdJson Mode","text":"<p>MdJson mode is ideal for: - Working with providers that don't have native JSON output - Ensuring portability across different providers - Getting structured responses from older model versions - Fallback option when JSON Schema mode isn't available</p>"},{"location":"polyglot/modes/overview/","title":"Overview of Output Modes","text":"<p>One of Polyglot's key strengths is its ability to support various output formats from LLM providers. This flexibility allows you to structure responses in the format that best suits your application, whether you need plain text, structured JSON data, or function/tool calls. This chapter explores the different output modes supported by Polyglot and how to implement them effectively.</p> <p>Polyglot's support for different output formats gives you the flexibility to work with LLM responses in the way that best suits your application's needs. Whether you need simple text, structured JSON, or interactive tool calls, you can configure the output format to match your requirements.</p>"},{"location":"polyglot/modes/overview/#understanding-output-modes","title":"Understanding Output Modes","text":"<p>Polyglot supports multiple output modes through the <code>Mode</code> enum:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// Available modes\n// OutputMode::Text       - Plain text output (default)\n// OutputMode::Json       - JSON output\n// OutputMode::JsonSchema - JSON output validated against a schema\n// OutputMode::MdJson     - JSON wrapped in Markdown code blocks\n// OutputMode::Tools      - Function/tool calling\n</code></pre> <p>Each mode influences: 1. How the request is formatted and sent to the provider 2. How the provider's response is processed 3. What extraction or validation is applied to the response</p>"},{"location":"polyglot/modes/overview/#output-modes-overview","title":"Output Modes Overview","text":"Mode Description Best For <code>OutputMode::Text</code> Default mode, returns unstructured text Simple text generation <code>OutputMode::Json</code> Returns structured JSON data Structured data processing <code>OutputMode::JsonSchema</code> Returns JSON data validated against a schema Strictly typed data <code>OutputMode::MdJson</code> Returns JSON wrapped in Markdown code blocks Compatibility across providers <code>OutputMode::Tools</code> Returns function/tool calls Function calling/external actions"},{"location":"polyglot/modes/overview/#choosing-the-right-format","title":"Choosing the Right Format","text":"<p>Consider these factors when selecting an output format:</p> <ol> <li>Complexity of the data: More complex data structures benefit from JSON Schema</li> <li>Provider support: Check which formats are natively supported by your provider</li> <li>Consistency requirements: Stricter format requirements favor JSON Schema or Tools</li> <li>Application needs: Consider how the data will be used in your application</li> </ol>"},{"location":"polyglot/modes/overview/#improving-format-reliability","title":"Improving Format Reliability","text":"<p>For better results:</p> <ol> <li>Be explicit in prompts: Clearly describe the expected format</li> <li>Provide examples: Show what good responses look like</li> <li>Use constraints: Specify limits and requirements</li> <li>Test across providers: Verify formats work with all providers you use</li> <li>Implement fallbacks: Have backup strategies for format failures</li> </ol>"},{"location":"polyglot/modes/text/","title":"Text Mode","text":"<p>Text mode is the default and simplest output format, returning unstructured text from the model.</p>"},{"location":"polyglot/modes/text/#basic-text-generation","title":"Basic Text Generation","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// OutputMode::Text is the default, so you don't need to specify it\n$response = $inference\n    -&gt;with(\n        messages: 'What is the capital of France?',\n        mode: OutputMode::Text  // Optional, this is the default\n    )\n    -&gt;get();\n\necho \"Response: $response\\n\";\n// Output: Response: The capital of France is Paris.\n</code></pre>"},{"location":"polyglot/modes/text/#when-to-use-text-mode","title":"When to Use Text Mode","text":"<p>Text mode is ideal for: - Simple question answering - Creative content generation - Conversational interactions - Summaries and paraphrasing - Any use case where structured data is not required</p>"},{"location":"polyglot/modes/text/#text-mode-across-providers","title":"Text Mode Across Providers","text":"<p>Text mode works consistently across all providers, making it the most portable option:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n// Using OpenAI\n$openAIResponse = $inference\n    -&gt;using('openai')\n    -&gt;withMessages('Write a short poem about the ocean.')\n    -&gt;get();\n\necho \"OpenAI response:\\n$openAIResponse\\n\\n\";\n\n// Using Anthropic\n$anthropicResponse = $inference\n    -&gt;using('anthropic')\n    -&gt;with('Write a short poem about the ocean.')\n    -&gt;get();\n\necho \"Anthropic response:\\n$anthropicResponse\\n\\n\";\n\n// Using any other provider\n// ...\n</code></pre>"},{"location":"polyglot/modes/tools/","title":"Tools Mode","text":"<p>Tools mode enables function calling, allowing the model to request specific actions to be performed by your application. This is powerful for creating LLM-powered applications that can interact with external systems or perform specific tasks.</p>"},{"location":"polyglot/modes/tools/#setting-up-tools","title":"Setting Up Tools","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');  // Tools are best supported by OpenAI\n\n// Define a tool for weather information\n$weatherTool = [\n    'type' =&gt; 'function',\n    'function' =&gt; [\n        'name' =&gt; 'get_weather',\n        'description' =&gt; 'Get the current weather for a location',\n        'parameters' =&gt; [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'location' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The city and state or country (e.g., \"San Francisco, CA\")',\n                ],\n                'unit' =&gt; [\n                    'type' =&gt; 'string',\n                    'enum' =&gt; ['celsius', 'fahrenheit'],\n                    'description' =&gt; 'The temperature unit to use',\n                ],\n            ],\n            'required' =&gt; ['location'],\n        ],\n    ],\n];\n\n// Define a tool for flight information\n$flightTool = [\n    'type' =&gt; 'function',\n    'function' =&gt; [\n        'name' =&gt; 'get_flight_info',\n        'description' =&gt; 'Get information about a flight',\n        'parameters' =&gt; [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'flight_number' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The flight number (e.g., \"AA123\")',\n                ],\n                'date' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The date of the flight in YYYY-MM-DD format',\n                ],\n            ],\n            'required' =&gt; ['flight_number'],\n        ],\n    ],\n];\n\n// Create an array of tools\n$tools = [$weatherTool, $flightTool];\n\n// Make a request that might require tools\n$response = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris today?',\n    tools: $tools,\n    toolChoice: 'auto',  // Let the model decide which tool to use\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Check if the model called a tool\nif ($response-&gt;hasToolCalls()) {\n    $toolCalls = $response-&gt;toolCalls();\n\n    foreach ($toolCalls-&gt;all() as $call) {\n        $toolName = $call-&gt;name();\n        $args = $call-&gt;args();\n\n        echo \"Tool called: $toolName\\n\";\n        echo \"Arguments: \" . json_encode($args, JSON_PRETTY_PRINT) . \"\\n\";\n\n        // Handle the tool call\n        if ($toolName === 'get_weather') {\n            // In a real application, you would call a weather API here\n            $weatherData = simulateWeatherApi($args['location'], $args['unit'] ?? 'celsius');\n\n            // Send the tool result back to the model\n            $withToolResult = $inference-&gt;with(\n                messages: [\n                    ['role' =&gt; 'user', 'content' =&gt; 'What\\'s the weather like in Paris today?'],\n                    [\n                        'role' =&gt; 'assistant',\n                        'content' =&gt; '',\n                        '_metadata' =&gt; [\n                            'tool_calls' =&gt; [\n                                [\n                                    'id' =&gt; $call-&gt;id(),\n                                    'function' =&gt; [\n                                        'name' =&gt; $call-&gt;name(),\n                                        'arguments' =&gt; $call-&gt;argsAsJson(),\n                                    ],\n                                ],\n                            ],\n                        ],\n                    ],\n                    [\n                        'role' =&gt; 'tool',\n                        'content' =&gt; json_encode($weatherData),\n                        '_metadata' =&gt; [\n                            'tool_call_id' =&gt; $call-&gt;id(),\n                            'tool_name' =&gt; $call-&gt;name(),\n                        ],\n                    ],\n                ]\n            )-&gt;get();\n\n            echo \"Final response: $withToolResult\\n\";\n        }\n    }\n} else {\n    // Model responded directly\n    echo \"Response: \" . $response-&gt;content() . \"\\n\";\n}\n\n// Simulate a weather API call\nfunction simulateWeatherApi(string $location, string $unit): array {\n    return [\n        'location' =&gt; $location,\n        'temperature' =&gt; 22,\n        'unit' =&gt; $unit,\n        'conditions' =&gt; 'Partly cloudy',\n        'humidity' =&gt; 65,\n    ];\n}\n</code></pre>"},{"location":"polyglot/modes/tools/#controlling-tool-usage","title":"Controlling Tool Usage","text":"<p>You can control how tools are used with the <code>toolChoice</code> parameter:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');\n\n// Let the model decide whether to use tools\n$autoResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: 'auto',\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Always require the model to use a specific tool\n$requiredToolResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: [\n        'type' =&gt; 'function',\n        'function' =&gt; [\n            'name' =&gt; 'get_weather'\n        ]\n    ],\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Prevent tool usage\n$noToolResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: 'none',\n    mode: OutputMode::Tools\n)-&gt;response();\n</code></pre>"},{"location":"polyglot/modes/tools/#streaming-tool-calls","title":"Streaming Tool Calls","text":"<p>You can also stream tool calls to provide real-time feedback:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris today?',\n    tools: $tools,\n    toolChoice: 'auto',\n    mode: OutputMode::Tools,\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n\n$toolName = '';\n$toolId = '';\n$toolArgs = '';\n\nforeach ($stream as $partialResponse) {\n    // Tool name is being generated\n    if (!empty($partialResponse-&gt;toolName)) {\n        if (empty($toolName)) {\n            $toolName = $partialResponse-&gt;toolName;\n            echo \"Tool being called: $toolName\\n\";\n        }\n    }\n\n    // Tool ID is received\n    if (!empty($partialResponse-&gt;toolId) &amp;&amp; empty($toolId)) {\n        $toolId = $partialResponse-&gt;toolId;\n    }\n\n    // Tool arguments are being generated\n    if (!empty($partialResponse-&gt;toolArgs)) {\n        $toolArgs .= $partialResponse-&gt;toolArgs;\n        echo \"Receiving tool arguments...\\n\";\n    }\n\n    // Regular content is being generated\n    if (!empty($partialResponse-&gt;contentDelta)) {\n        echo $partialResponse-&gt;contentDelta;\n        flush();\n    }\n\n    // Check for finish reason\n    if (!empty($partialResponse-&gt;finishReason)) {\n        echo \"\\nFinished with reason: {$partialResponse-&gt;finishReason}\\n\";\n    }\n}\n\n// Process the complete tool call\nif (!empty($toolName) &amp;&amp; !empty($toolArgs)) {\n    try {\n        $args = json_decode($toolArgs, true, 512, JSON_THROW_ON_ERROR);\n        echo \"\\nFinal tool arguments: \" . json_encode($args, JSON_PRETTY_PRINT) . \"\\n\";\n\n        // Process the tool call as in the previous example\n    } catch (\\JsonException $e) {\n        echo \"Error parsing tool arguments: \" . $e-&gt;getMessage() . \"\\n\";\n    }\n}\n</code></pre>"},{"location":"polyglot/modes/tools/#provider-support-for-tools","title":"Provider Support for Tools","text":"<p>Tool support varies across providers:</p> <ul> <li>OpenAI: Comprehensive support with <code>function_call</code>/<code>tool_call</code> features</li> <li>Anthropic: Growing support with <code>tool_use</code> feature in Claude 3 models</li> <li>Other providers: Implementation varies; check provider documentation</li> </ul>"},{"location":"polyglot/modes/tools/#when-to-use-tools-mode","title":"When to Use Tools Mode","text":"<p>Tools mode is ideal for: - Creating agents that can interact with external systems - Building assistants that need to retrieve real-time information - Implementing complex workflows that require multiple steps - Giving the model access to specific capabilities (calculations, API calls, etc.)</p>"},{"location":"polyglot/streaming/misc/","title":"Advanced Stream Processing","text":""},{"location":"polyglot/streaming/misc/#using-callbacks","title":"Using Callbacks","text":"<p>You can use the <code>onPartialResponse</code> method to register a callback that is called for each partial response:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n// Set up a callback for processing partial responses\n$stream = $response-&gt;stream()-&gt;onPartialResponse(function($partialResponse) {\n    echo $partialResponse-&gt;contentDelta;\n    flush();\n});\n\n// Process all responses\nforeach ($stream-&gt;responses() as $_) {\n    // The callback is called for each partial response\n    // We don't need to do anything here\n}\n</code></pre>"},{"location":"polyglot/streaming/misc/#transforming-stream-content","title":"Transforming Stream Content","text":"<p>You can process and transform the content as it streams:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Generate a list of 10 book titles.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$titleCount = 0;\n$currentTitle = '';\n\nforeach ($stream as $partialResponse) {\n    $content = $partialResponse-&gt;contentDelta;\n\n    // Check for new titles (assuming numbered list format)\n    if (preg_match('/(\\d+)\\.\\s+(.+?)(?=\\n\\d+\\.|\\Z)/s', $content, $matches)) {\n        $titleCount++;\n        $title = trim($matches[2]);\n        echo \"Title #{$matches[1]}: $title\\n\";\n    } elseif (!empty(trim($content))) {\n        echo $content;\n    }\n}\n</code></pre>"},{"location":"polyglot/streaming/misc/#processing-json-streams","title":"Processing JSON Streams","text":"<p>For streaming JSON responses, you need to accumulate content until you have valid JSON:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'List 5 countries and their capitals in JSON format.',\n    mode: OutputMode::Json,  // Request JSON response\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$jsonBuffer = '';\n\nforeach ($stream as $partialResponse) {\n    $jsonBuffer .= $partialResponse-&gt;contentDelta;\n\n    // Try to parse the accumulated JSON\n    $tempJson = $jsonBuffer;\n\n    // Attempt to complete any incomplete JSON\n    if (substr(trim($tempJson), -1) !== '}') {\n        $tempJson .= '}';\n    }\n\n    // Replace any trailing commas which would make the JSON invalid\n    $tempJson = preg_replace('/,\\s*}$/', '}', $tempJson);\n\n    try {\n        $data = json_decode($tempJson, true, 512, JSON_THROW_ON_ERROR);\n        echo \"Valid JSON received so far: \" . json_encode($data, JSON_PRETTY_PRINT) . \"\\n\";\n    } catch (\\JsonException $e) {\n        // Not a complete valid JSON yet\n        echo \"Accumulated content: $jsonBuffer\\n\";\n    }\n}\n\n// Process the final, complete JSON\ntry {\n    $finalData = json_decode($jsonBuffer, true, 512, JSON_THROW_ON_ERROR);\n    echo \"Final JSON: \" . json_encode($finalData, JSON_PRETTY_PRINT) . \"\\n\";\n} catch (\\JsonException $e) {\n    echo \"Error parsing final JSON: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/streaming/misc/#cancellation","title":"Cancellation","text":"<p>In some cases, you may want to stop the generation early:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a long story about space exploration.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$wordCount = 0;\n$maxWords = 100;  // Limit to 100 words\n\nforeach ($stream as $partialResponse) {\n    echo $partialResponse-&gt;contentDelta;\n    flush();\n\n    // Count words in the accumulated content\n    $words = str_word_count($partialResponse-&gt;content());\n\n    // Stop after reaching the word limit\n    if ($words &gt;= $maxWords) {\n        echo \"\\n\\n[Generation stopped after $maxWords words]\\n\";\n        break;  // Exit the loop early\n    }\n}\n</code></pre> <p>Note that when you break out of the loop, the request to the provider continues in the background, but your application stops processing the response.</p>"},{"location":"polyglot/streaming/misc/#performance-considerations","title":"Performance Considerations","text":"<p>When working with streaming responses, keep these performance considerations in mind:</p> <ol> <li>Memory Usage: Be careful with how you accumulate content, especially for very long responses</li> <li>Buffer Flushing: In web applications, make sure output buffers are properly flushed</li> <li>Connection Stability: Streaming connections can be more sensitive to network issues</li> <li>Timeouts: Adjust timeout settings for long-running streams</li> </ol> <p>Here's an example of memory-efficient processing for very long responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Generate a very long story.',\n    options: [\n        'stream' =&gt; true,\n        'max_tokens' =&gt; 10000  // Request a long response\n    ]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$outputFile = fopen('generated_story.txt', 'w');\n\nforeach ($stream as $partialResponse) {\n    // Write chunks directly to file instead of keeping them in memory\n    fwrite($outputFile, $partialResponse-&gt;contentDelta);\n\n    // Optional: Show a progress indicator\n    echo \".\";\n    flush();\n}\n\nfclose($outputFile);\necho \"\\nGeneration complete. Story saved to generated_story.txt\\n\";\n</code></pre>"},{"location":"polyglot/streaming/overview/","title":"Overview of Streaming","text":"<p>Streaming LLM responses may be preferred for user experience and system performance. Polyglot makes it easy to implement streaming with a consistent API across different providers.</p> <p>Streaming responses are a powerful feature of modern LLM APIs that allow you to receive and process model outputs incrementally as they're being generated, rather than waiting for the complete response. This chapter covers how to work with streaming responses in Polyglot, from basic setup to advanced processing techniques.</p>"},{"location":"polyglot/streaming/overview/#benefits-of-streaming","title":"Benefits of Streaming","text":"<p>Streaming responses offer several advantages:</p> <ol> <li>Improved User Experience: Display content to users as it's generated, creating a more responsive interface</li> <li>Reduced Latency Perception: Users see the beginning of a response almost immediately</li> <li>Progressive Processing: Begin processing early parts of the response while later parts are still being generated</li> <li>Handling Long Outputs: Efficiently process responses that may be very long without hitting timeout limits</li> <li>Early Termination: Stop generation early if needed, saving resources</li> </ol>"},{"location":"polyglot/streaming/overview/#enabling-streaming","title":"Enabling Streaming","text":"<p>Enabling streaming in Polyglot is straightforward - you need to set the <code>stream</code> option to <code>true</code> in your request:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]  // Enable streaming\n);\n</code></pre> <p>Once you have a streaming-enabled response, you can access the stream using the <code>stream()</code> method:</p> <pre><code>// Get the stream of partial responses\n$stream = $response-&gt;stream();\n</code></pre>"},{"location":"polyglot/streaming/overview/#basic-stream-processing","title":"Basic Stream Processing","text":"<p>The most common way to process a stream is to iterate through the partial responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n// Get a generator that yields partial responses\n$stream = $response-&gt;stream()-&gt;responses();\n\necho \"Story: \";\nforeach ($stream as $partialResponse) {\n    // Output each chunk as it arrives\n    echo $partialResponse-&gt;contentDelta;\n\n    // Flush the output buffer to show progress in real-time (for CLI or streaming HTTP responses)\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n        flush();\n    }\n}\necho \"\\n\";\n</code></pre>"},{"location":"polyglot/streaming/overview/#understanding-partial-responses","title":"Understanding Partial Responses","text":"<p>Each iteration of the stream yields a <code>PartialInferenceResponse</code> object with these key properties:</p> <ul> <li><code>contentDelta</code>: The new content received in this chunk</li> <li><code>content</code>: The accumulated content up to this point</li> <li><code>finishReason</code>: The reason why the response finished (empty until the final chunk)</li> <li><code>usage</code>: Token usage statistics</li> </ul> <pre><code>foreach ($stream as $partialResponse) {\n    // The new content in this chunk\n    echo \"New content: \" . $partialResponse-&gt;contentDelta . \"\\n\";\n\n    // The total content received so far\n    echo \"Total content so far: \" . $partialResponse-&gt;content() . \"\\n\";\n\n    // Check if this is the final chunk\n    if ($partialResponse-&gt;finishReason !== '') {\n        echo \"Response finished: \" . $partialResponse-&gt;finishReason . \"\\n\";\n    }\n}\n</code></pre>"},{"location":"polyglot/streaming/overview/#retrieving-the-final-response","title":"Retrieving the Final Response","text":"<p>After processing the stream, you can get the complete response:</p> <pre><code>// Method 1: Using the original response object's get() method\n$completeText = $response-&gt;get();\n\n// Method 2: Getting the final state from the stream\n$finalResponse = $response-&gt;stream()-&gt;final();\n$completeText = $finalResponse-&gt;content();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/","title":"Debugging Requests and Responses","text":"<p>Polyglot debug mode provides a simple way to enable debugging for LLM interactions. Debugging is essential for troubleshooting and optimizing your applications. It allows you to inspect the requests sent to the LLM and the responses received, helping you identify issues and improve performance.</p>"},{"location":"polyglot/troubleshooting/debugging/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>Polyglot provides a simple way to enable debug mode:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Enable debug mode when creating the inference object\n$inference = (new Inference())\n    -&gt;withDebugPreset('on');\n\n// Make a request - debug output will show the request and response details\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/#http-request-inspection-with-middleware","title":"HTTP Request Inspection with Middleware","text":"<p>You can manually add debugging middleware to inspect raw HTTP requests and responses.</p> <p>In this example we're using built-in middleware, but you can also create your own custom middleware.</p> <pre><code>&lt;?php\nuse Cognesy\\Http\\Middleware\\Debug\\DebugMiddleware;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom debug middleware with specific options\n$debugMiddleware = new DebugMiddleware([\n    'requestUrl' =&gt; true,\n    'requestHeaders' =&gt; true,\n    'requestBody' =&gt; true,\n    'responseHeaders' =&gt; true,\n    'responseBody' =&gt; true,\n]);\n\n// Create an HTTP client with the debug middleware\n$httpClient = new HttpClient();\n$httpClient-&gt;withMiddleware($debugMiddleware);\n\n// Use the HTTP client with Inference\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/#event-listeners","title":"Event Listeners","text":"<p>Use event listeners to trace the flow of requests and responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an event dispatcher\n$events = new EventDispatcher();\n\n// Add listeners\n$events-&gt;listen(InferenceRequested::class, function (InferenceRequested $event) {\n    echo \"Request sent: \" . json_encode($event-&gt;request-&gt;toArray()) . \"\\n\";\n});\n\n$events-&gt;listen(InferenceResponseCreated::class, function (InferenceResponseCreated $event) {\n    echo \"Response received: \" . substr($event-&gt;inferenceResponse-&gt;content(), 0, 50) . \"...\\n\";\n    echo \"Token usage: \" . $event-&gt;inferenceResponse-&gt;usage()-&gt;total() . \"\\n\";\n});\n\n// Create an inference object with the event dispatcher\n$inference = new Inference(events: $events);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/debugging/#logging-to-files","title":"Logging to Files","text":"<p>For more persistent debugging, you can log to files:</p> <pre><code>&lt;?php\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a function to log to file\nfunction logToFile(string $message, string $filename = 'llm_debug.log'): void {\n    $timestamp = date('Y-m-d H:i:s');\n    file_put_contents(\n        $filename,\n        \"[$timestamp] $message\" . PHP_EOL,\n        FILE_APPEND\n    );\n}\n\n// Create a custom event dispatcher\n$events = new EventDispatcher();\n\n// Listen for request events\n$events-&gt;listen(InferenceRequested::class, function (InferenceRequested $event) {\n    $request = $event-&gt;request;\n    logToFile(\"REQUEST: \" . json_encode($request-&gt;toArray()));\n});\n\n// Listen for response events\n$events-&gt;listen(InferenceResponseCreated::class, function (InferenceResponseCreated $event) {\n    $response = $event-&gt;inferenceResponse;\n    logToFile(\"RESPONSE: \" . json_encode($response-&gt;toArray()));\n});\n\n// Create an inference object with the custom event dispatcher\n$inference = new Inference(events: $events);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is artificial intelligence?'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-authentication/","title":"Authentication","text":"<p>One of the most common issues when working with LLM APIs is authentication problems.</p>"},{"location":"polyglot/troubleshooting/issues-authentication/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages containing terms like \"authentication failed,\" \"invalid API key,\" or \"unauthorized\"</li> <li>HTTP status codes 401 or 403</li> </ul>"},{"location":"polyglot/troubleshooting/issues-authentication/#solutions","title":"Solutions","text":"<ol> <li> <p>Verify API Key: Ensure your API key is correctly set in your environment variables <pre><code>// Check if API key is set\nif (empty(getenv('OPENAI_API_KEY'))) {\necho \"API key is not set in environment variables\\n\";\n}\n</code></pre></p> </li> <li> <p>Check API Key Format: Some providers require specific formats for API keys <pre><code>// OpenAI keys typically start with 'sk-'\nif (!str_starts_with(getenv('OPENAI_API_KEY'), 'sk-')) {\necho \"OpenAI API key format is incorrect\\n\";\n}\n\n// Anthropic keys typically start with 'sk-ant-'\nif (!str_starts_with(getenv('ANTHROPIC_API_KEY'), 'sk-ant-')) {\necho \"Anthropic API key format is incorrect\\n\";\n}\n</code></pre></p> </li> <li> <p>Test Keys Directly: Use a simple script to test your API keys</p> </li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction testApiKey(string $preset): bool {\n    try {\n        $llm = (new LLMFactory)-&gt;fromPreset($preset);\n        $inference = new Inference($llm);\n        $inference-&gt;with(\n            messages: 'Test message',\n            options: ['max_tokens' =&gt; 5]\n        )-&gt;get();\n\n        echo \"Connection using '$connection' is working correctly\\n\";\n        return true;\n    } catch (HttpRequestException $e) {\n        echo \"Error with connection '$connection': \" . $e-&gt;getMessage() . \"\\n\";\n        return false;\n    }\n}\n\n// Test major providers\ntestApiKey('openai');\ntestApiKey('anthropic');\ntestApiKey('mistral');\n?&gt;\n</code></pre> <ol> <li>Environment Variables: Ensure your environment variables are being loaded correctly <pre><code>&lt;?php\n// If using dotenv\n$dotenv = Dotenv\\Dotenv::createImmutable(__DIR__);\n$dotenv-&gt;load();\n$dotenv-&gt;required(['OPENAI_API_KEY'])-&gt;notEmpty();\n?&gt;\n</code></pre></li> </ol>"},{"location":"polyglot/troubleshooting/issues-configuration/","title":"Connection Configurations","text":""},{"location":"polyglot/troubleshooting/issues-configuration/#symptoms","title":"Symptoms","text":"<ul> <li>Errors like \"connection timeout,\" \"failed to connect,\" or \"network error\"</li> <li>Long delays before errors appear</li> <li>Issues with specific providers (e.g., OpenAI, Anthropic, Mistral)</li> <li>Incorrect API keys or permissions</li> <li>Missing or incorrect configuration parameters</li> </ul>"},{"location":"polyglot/troubleshooting/issues-configuration/#solutions","title":"Solutions","text":""},{"location":"polyglot/troubleshooting/issues-configuration/#1-verify-api-keys","title":"1. Verify API Keys","text":"<p>Make sure your API keys are correct and have the necessary permissions:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction testApiKey(string $preset): bool {\n    try {\n        $llm = LLMProvider::using($preset);\n        $inference = new Inference($preset);\n        $response = $inference-&gt;with(\n            messages: 'Test message',\n            options: ['max_tokens' =&gt; 5]\n        )-&gt;get();\n\n        echo \"Connection preset '$preset' is working.\\n\";\n        return true;\n    } catch (HttpRequestException $e) {\n        echo \"Error with connection '$preset': \" . $e-&gt;getMessage() . \"\\n\";\n        return false;\n    }\n}\n\n// Test each connection\n$presets = ['openai', 'anthropic', 'mistral'];\nforeach ($presets as $preset) {\n    testApiKey($preset);\n}\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-configuration/#2-enable-debug-mode","title":"2. Enable Debug Mode","text":"<p>Use debug mode to see the actual requests and responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Enable debug mode\n$inference = new Inference()\n    -&gt;using('openai')\n    -&gt;withDebugPreset('on');\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'Test message with debug enabled'\n)-&gt;get();\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-configuration/#3-check-provider-status","title":"3. Check Provider Status","text":"<p>Some issues might be related to the provider's service status. Check their status pages or documentation.</p>"},{"location":"polyglot/troubleshooting/issues-configuration/#4-verify-configuration-parameters","title":"4. Verify Configuration Parameters","text":"<p>Ensure all required configuration parameters are present and correctly formatted:</p> <pre><code>&lt;?php\n\nfunction verifyConfig(string $preset): void {\n    try {\n        $provider = new ConfigProvider();\n        $config = LLMConfig::fromArray($provider-&gt;getConfig($preset));\n\n        echo \"Configuration for '$preset':\\n\";\n        echo \"API URL: {$config-&gt;apiUrl}\\n\";\n        echo \"Endpoint: {$config-&gt;endpoint}\\n\";\n        echo \"Default Model: {$config-&gt;model}\\n\";\n        echo \"Provider Type: {$config-&gt;providerType}\\n\";\n\n        // Check for empty values\n        if (empty($config-&gt;apiKey)) {\n            echo \"Warning: API key is empty\\n\";\n        }\n\n        if (empty($config-&gt;model)) {\n            echo \"Warning: Default model is not set\\n\";\n        }\n    } catch (\\Exception $e) {\n        echo \"Error loading configuration for '$preset': \" . $e-&gt;getMessage() . \"\\n\";\n    }\n}\n\n// Verify configurations\n$presets = ['openai', 'anthropic', 'mistral'];\nforeach ($presets as $preset) {\n    verifyConfig($preset);\n    echo \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-connection/","title":"Connection Issues","text":"<p>Network connectivity problems can prevent successful API requests.</p>"},{"location":"polyglot/troubleshooting/issues-connection/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages like \"connection timeout,\" \"failed to connect,\" or \"network error\"</li> <li>Long delays before errors appear</li> </ul>"},{"location":"polyglot/troubleshooting/issues-connection/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Internet Connection: Ensure your server has a stable internet connection</p> </li> <li> <p>Verify API Endpoint: Make sure the API endpoint URL is correct <pre><code>// In your configuration file (config/llm.php)\n'apiUrl' =&gt; 'https://api.openai.com/v1', // Correct URL\n</code></pre></p> </li> <li> <p>Proxy Settings: If you're behind a proxy, configure it properly</p> </li> </ol> <pre><code>// Using custom HTTP client with proxy settings\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;\n\n$config = new HttpClientConfig(\n    requestTimeout: 30,\n    connectTimeout: 10,\n    additionalOptions: ['proxy' =&gt; 'http://proxy.example.com:8080']\n);\n\n$httpClient = new HttpClient('guzzle', $config);\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n</code></pre> <ol> <li> <p>Firewall Rules: Check if your firewall is blocking outgoing connections to API endpoints</p> </li> <li> <p>DNS Resolution: Ensure your DNS is resolving the API domains correctly</p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-model-specific/","title":"Model-Specific Issues","text":"<p>When working with different LLM models, you may encounter issues that are specific to the model you're using, as different models have different capabilities and limitations. This section covers common model-specific issues and how to resolve them.</p>"},{"location":"polyglot/troubleshooting/issues-model-specific/#symptoms","title":"Symptoms","text":"<ul> <li>Errors like \"model not found,\" \"parameter not supported,\" or \"context length exceeded\"</li> <li>Unexpected responses or performance from certain models</li> </ul>"},{"location":"polyglot/troubleshooting/issues-model-specific/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Model Availability: Ensure the model you're requesting is available from the provider <pre><code>// Check available models for each provider in their documentation\n// Example: For OpenAI 'gpt-4o-mini' is valid, but 'gpt5' is not\n</code></pre></p> </li> <li> <p>Context Length: Be aware of each model's maximum context length <pre><code>// In config/llm.php, check contextLength for each model\n// Example: OpenAI models have different context windows\n// - gpt-3.5-turbo: 16K tokens\n// - gpt-4-turbo: 128K tokens\n// - claude-3-opus: 200K tokens\n</code></pre></p> </li> <li> <p>Feature Support: Different models support different features <pre><code>// Some features may not work with all models\n// Example: Vision capabilities are only available in select models\n\n// Check for vision support before sending images\n$modelSupportsVision = in_array($model, [\n    'gpt-4-vision', 'gpt-4o', 'claude-3-opus', 'claude-3-sonnet'\n]);\n\nif (!$modelSupportsVision) {\n    echo \"Warning: The selected model doesn't support vision capabilities\\n\";\n}\n</code></pre></p> </li> <li> <p>Fallback Models: Implement fallbacks to other models when preferred models fail</p> </li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withModelFallback(array $models, string $prompt): string {\n    $inference = new Inference();\n    $lastException = null;\n\n    foreach ($models as $model) {\n        try {\n            return $inference-&gt;with(\n                messages: $prompt,\n                model: $model\n            )-&gt;get();\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            echo \"Model '$model' failed: \" . $e-&gt;getMessage() . \"\\n\";\n            echo \"Trying next model...\\n\";\n        }\n    }\n\n    throw new \\Exception(\"All models failed. Last error: \" .\n        ($lastException ? $lastException-&gt;getMessage() : \"Unknown error\"));\n}\n\n// Try advanced models first, then fall back to simpler ones\n$models = ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'];\n\ntry {\n    $response = withModelFallback($models, \"What is the capital of France?\");\n    echo \"Response: $response\\n\";\n} catch (\\Exception $e) {\n    echo \"Error: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"polyglot/troubleshooting/issues-provider-specific/","title":"Provider-Specific Issues","text":"<p>Each LLM provider has unique quirks and issues. This section covers common provider-specific issues and how to resolve them.</p>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#openai","title":"OpenAI","text":"<ol> <li> <p>Organization IDs: Set the organization ID if using a shared account <pre><code>// In config/llm.php\n'metadata' =&gt; [\n    'organization' =&gt; 'org-your-organization-id',\n],\n</code></pre></p> </li> <li> <p>API Versions: Pay attention to API version changes <pre><code>// Updates to OpenAI API may require changes to your code\n// Monitor OpenAI's release notes for changes\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#anthropic","title":"Anthropic","text":"<ol> <li> <p>Message Format: Anthropic uses a different message format <pre><code>// Polyglot handles this automatically, but be aware when debugging\n</code></pre></p> </li> <li> <p>Tool Support: Tool support has specific requirements <pre><code>// When using tools with Anthropic, check their latest documentation\n// for supported features and limitations\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#mistral","title":"Mistral","text":"<ol> <li>Rate Limits: Mistral has strict rate limits on free tier <pre><code>// Implement more aggressive rate limiting for Mistral\n</code></pre></li> </ol>"},{"location":"polyglot/troubleshooting/issues-provider-specific/#ollama","title":"Ollama","text":"<ol> <li> <p>Local Setup: Ensure Ollama is properly installed and running <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/version\n</code></pre></p> </li> <li> <p>Model Availability: Download models before using them <pre><code># Pull a model before using it\nollama pull llama2\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/issues-rate-limits/","title":"Rate Limits","text":"<p>Provider rate limits can cause request failures during high traffic periods.</p>"},{"location":"polyglot/troubleshooting/issues-rate-limits/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages containing \"rate limit exceeded,\" \"too many requests,\" or \"quota exceeded\"</li> <li>HTTP status code 429</li> </ul>"},{"location":"polyglot/troubleshooting/issues-rate-limits/#solutions","title":"Solutions","text":"<ol> <li>Implement Retry Logic: Add automatic retries with exponential backoff</li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withRetry(callable $fn, int $maxRetries = 3): mixed {\n    $attempt = 0;\n    $lastException = null;\n\n    while ($attempt &lt; $maxRetries) {\n        try {\n            return $fn();\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            $attempt++;\n\n            // Only retry on rate limit errors\n            if (strpos($e-&gt;getMessage(), 'rate limit') === false &amp;&amp;\n                $e-&gt;getCode() !== 429) {\n                throw $e;\n            }\n\n            if ($attempt &gt;= $maxRetries) {\n                break;\n            }\n\n            // Exponential backoff\n            $sleepTime = (2 ** $attempt);\n            echo \"Rate limit hit. Retrying in $sleepTime seconds...\\n\";\n            sleep($sleepTime);\n        }\n    }\n\n    throw $lastException;\n}\n\n// Usage\n$inference = new Inference();\n\ntry {\n    $response = withRetry(function() use ($inference) {\n        return $inference-&gt;with(\n            messages: 'What is the capital of France?'\n        )-&gt;get();\n    });\n\n    echo \"Response: $response\\n\";\n} catch (HttpRequestException $e) {\n    echo \"All retry attempts failed: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre> <ol> <li> <p>Request Throttling: Limit the rate of requests from your application <pre><code>&lt;?php\nclass RateLimiter {\n    private $lastRequestTime = 0;\n    private $requestsPerMinute;\n    private $minTimeBetweenRequests;\n\n    public function __construct(int $requestsPerMinute = 60) {\n        $this-&gt;requestsPerMinute = $requestsPerMinute;\n        $this-&gt;minTimeBetweenRequests = 60 / $requestsPerMinute;\n    }\n\n    public function waitIfNeeded(): void {\n        $currentTime = microtime(true);\n        $timeSinceLastRequest = $currentTime - $this-&gt;lastRequestTime;\n\n        if ($timeSinceLastRequest &lt; $this-&gt;minTimeBetweenRequests) {\n            $waitTime = $this-&gt;minTimeBetweenRequests - $timeSinceLastRequest;\n            usleep($waitTime * 1000000);\n        }\n\n        $this-&gt;lastRequestTime = microtime(true);\n    }\n}\n\n// Usage\n$limiter = new RateLimiter(30); // 30 requests per minute\n$inference = new Inference();\n\nfor ($i = 0; $i &lt; 10; $i++) {\n    $limiter-&gt;waitIfNeeded();\n    $response = $inference-&gt;with(\n        messages: \"This is request $i\"\n    )-&gt;toText();\n    echo \"Response $i: $response\\n\";\n}\n</code></pre></p> </li> <li> <p>Request Batching: Combine multiple requests into batches when possible</p> </li> </ol> <pre><code>&lt;?php\n// Instead of making many small requests\n$responses = [];\nforeach ($questions as $question) {\n    // This would hit rate limits quickly\n    $responses[] = $inference-&gt;with(messages: $question)-&gt;get();\n}\n\n// Better: Use a context-aware batch approach\n$batchedQuestions = \"Please answer the following questions:\\n\";\nforeach ($questions as $i =&gt; $question) {\n    $batchedQuestions .= ($i + 1) . \". $question\\n\";\n}\n\n$batchResponse = $inference-&gt;with(messages: $batchedQuestions)-&gt;get();\n// Then parse the batch response into individual answers\n</code></pre> <ol> <li>Upgrade API Plan: Consider upgrading to a higher tier with increased rate limits</li> </ol>"},{"location":"polyglot/troubleshooting/issues-streaming/","title":"Streaming","text":"<p>Streaming responses can encounter specific problems.</p>"},{"location":"polyglot/troubleshooting/issues-streaming/#symptoms","title":"Symptoms","text":"<ul> <li>Streams cutting off prematurely</li> <li>Errors during stream processing</li> <li>Partial or incomplete responses</li> </ul>"},{"location":"polyglot/troubleshooting/issues-streaming/#solutions","title":"Solutions","text":"<ol> <li>Connection Timeouts: Increase timeout settings for streaming responses</li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;\n\n// Create a custom HTTP client with longer timeouts\n$config = new HttpClientConfig(\n    requestTimeout: 180,  // 3 minutes for the entire request\n    connectTimeout: 10,   // 10 seconds to establish connection\n    idleTimeout: 60       // 60 seconds allowed between stream chunks\n);\n\n$httpClient = new HttpClient('guzzle', $config);\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Use streaming with the custom client\n$response = $inference-&gt;with(\n    messages: 'Write a long story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n    flush();\n}\n</code></pre> <ol> <li> <p>Buffer Flushing: Ensure output buffers are properly flushed during streaming <pre><code>foreach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n\n    // Flush output buffer to ensure content is sent immediately\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n    }\n    flush();\n}\n</code></pre></p> </li> <li> <p>Error Handling in Streams: Implement specific error handling for streams <pre><code>&lt;?php\ntry {\n    $response = $inference-&gt;with(\n        messages: 'Write a long story.',\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $stream = $response-&gt;stream()-&gt;responses();\n        $content = '';\n\n        foreach ($stream as $partial) {\n            $content .= $partial-&gt;contentDelta;\n            echo $partial-&gt;contentDelta;\n            flush();\n        }\n    } catch (\\Exception $streamException) {\n        echo \"\\nStream error: \" . $streamException-&gt;getMessage() . \"\\n\";\n\n        // If we got a partial response before the error, use it\n        if (!empty($content)) {\n            echo \"Partial content received: \" . strlen($content) . \" characters\\n\";\n        }\n    }\n} catch (RequestException $e) {\n    echo \"Request failed: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre></p> </li> <li> <p>Fallback to Non-streaming: Implement a fallback to non-streaming mode <pre><code>&lt;?php\nfunction getResponse(string $prompt, bool $preferStreaming = true): string {\n    $inference = new Inference();\n\n    try {\n        if ($preferStreaming) {\n            // Try streaming first\n            $response = $inference-&gt;with(\n                messages: $prompt,\n                options: ['stream' =&gt; true]\n            );\n\n            $content = '';\n            foreach ($response-&gt;stream()-&gt;responses() as $partial) {\n                $content .= $partial-&gt;contentDelta;\n                // Output can be done here if needed\n            }\n\n            return $content;\n        }\n    } catch (\\Exception $e) {\n        echo \"Streaming failed, falling back to non-streaming mode\\n\";\n    }\n\n    // Fallback to non-streaming\n    return $inference-&gt;with(messages: $prompt)-&gt;toText();\n}\n</code></pre></p> </li> </ol>"},{"location":"polyglot/troubleshooting/overview/","title":"Overview of Troubleshooting","text":"<p>This chapter covers common issues you might encounter when working with Polyglot, along with best practices for effectively using LLMs in your applications.</p>"},{"location":"polyglot/troubleshooting/overview/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li>Authentication Issues</li> <li>Connection Issues</li> <li>Rate Limiting</li> <li>Model-Specific Issues</li> <li>Streaming Issues</li> <li>Provider-Specific Issues</li> <li>Debugging and Logging</li> </ol>"},{"location":"release-notes/v0.12.0/","title":"v0.12.0","text":"<ul> <li>Redesigned directory structure to separate the essential code from the addons, aux tools, etc.</li> <li>Directory 'src' to hold Instructor structured outputs code</li> <li>New directory <code>src-llm</code> to hold the LLM connectivity code (required for Instructor)</li> <li>New directory <code>src-utils</code> to hold the utility classes (required for Instructor)</li> <li>Directory <code>src-setup</code> to hold the Instructor setup tool</li> <li>Directory <code>src-hub</code> to hold the CLI tool for executing examples and generating documentation</li> <li>Directory 'src-tell' to hold a simple tool for prompting LLMs from CLI</li> <li>New directory <code>src-addons</code> to hold the additional capabilities (optional)</li> <li>New directory <code>src-aux</code> to hold the auxiliary tools used, e.g. by examples (optional)</li> <li>New directory <code>src-experimental</code> to hold the not yet ready, experimental work (not distributed)</li> <li>Moved package-specific events to their respective directories</li> <li>Moved embeddings API support to 'src-llm'</li> <li>Added version sync script and Github release automation</li> </ul>"},{"location":"release-notes/v0.12.10/","title":"v0.12.10","text":"<ul> <li>Excluded examples, evals and some CLI tools (hub and tell) to minimize distribution size (they are still available in the source code repository)</li> </ul>"},{"location":"release-notes/v0.12.11/","title":"v0.12.11","text":"<ul> <li>Middleware support for HTTP client layer - unified across clients</li> <li>Request / response debugging is now unified across clients</li> <li>Response and stream buffering middleware to allow for multiple reads from LLM API responses across request lifecycle</li> <li>New tests added</li> </ul>"},{"location":"release-notes/v0.12.12/","title":"v0.12.12","text":"<ul> <li>Moved HTTP connectivity layer to a separate directory (src-http)</li> <li>Added HTTP connectivity layer docs</li> <li>Minor corrections in docs</li> </ul>"},{"location":"release-notes/v0.12.13/","title":"v0.12.13","text":"<ul> <li>Polyglot dev guide added</li> </ul>"},{"location":"release-notes/v0.12.2/","title":"v0.12.2","text":"<ul> <li>Corrected .gitignore to further limit the distribution size (excluded non-essential sources, e.g. /src-hub/ and /src-aux/)</li> <li>Renamed src-llm to src-polyglot</li> <li>Renamed Cognesy/LLM package to Cognesy/Polyglot</li> <li>Skipped obsolete tests for experimental Module code</li> </ul>"},{"location":"release-notes/v0.12.3/","title":"v0.12.3","text":"<ul> <li>Corrected names in composer.json (cognesy/llm &gt; cognesy/polyglot-llm)</li> </ul>"},{"location":"release-notes/v0.12.4/","title":"v0.12.4","text":"<ul> <li>Corrected composer.json and publish script to keep single repo model until everything is ready for the split</li> </ul>"},{"location":"release-notes/v0.12.5/","title":"v0.12.5","text":"<ul> <li>Added basic Polyglot docs, some moved from Instructor documentation</li> <li>Corrected mint.json to reflect new paths</li> <li>Corrected scrips and paths after file / dir location changes</li> <li>Renamed Cognesy\\Aux to Cognesy\\Auxiliary to avoid name conflicts</li> <li>Extracted class autoloading from examples and evals to separate files</li> </ul>"},{"location":"release-notes/v0.12.6/","title":"v0.12.6","text":"<ul> <li>Continued project files reorganization</li> <li>Created README.md, LICENSE.md, composer.json, .gitattributes for subprojects to be ready for future split</li> <li>Corrected composer.json, .gitignore and .gitattributes</li> <li>Docs directory structure cleanup to work with Mintlify</li> </ul>"},{"location":"release-notes/v0.12.7/","title":"v0.12.7","text":"<ul> <li>Moved Cognesy/Addons/Prompt to Cognesy/Utils/Template to clarify purpose and remove cyclic dependency between src-addons and src-utils</li> <li>Added Anthropic thinking traces support in LLM drivers (thinking data is now available directly in LLMResponse and LLMPartialResponse object properties)</li> </ul>"},{"location":"release-notes/v0.12.8/","title":"v0.12.8","text":"<ul> <li>composer.json cleanup</li> <li>MessageRole now supports OpenAI 'developer' role, other drivers recognize and properly convert it</li> <li>Removed missed dependency on experimental code in one of Structure traits</li> <li>Moved tests into subpackage-specific directories</li> <li>Added tests for EventDispatcher class</li> <li>Added tests for Message class</li> <li>Added tests for Messages class</li> </ul>"},{"location":"release-notes/v0.12.9/","title":"v0.12.9","text":"<ul> <li>Fixed composer.json</li> </ul>"},{"location":"release-notes/v0.13.0/","title":"v0.13.0","text":"<ul> <li>Moved source code of each component to the <code>packages</code> directory</li> <li>Added Github workflow to split the monorepo into multiple packages</li> <li>Corrected sync and publish scripts</li> <li>Templates use BasePath for better path handling</li> </ul>"},{"location":"release-notes/v0.14.0/","title":"v0.14.0","text":"<ul> <li>Added license files to subpackages</li> </ul>"},{"location":"release-notes/v0.14.1/","title":"v0.14.1","text":"<ul> <li>Better Mintlify docs structure and fixes in generation</li> </ul>"},{"location":"release-notes/v0.14.2/","title":"v0.14.2","text":"<ul> <li>(utils) Env class now uses BasePath for base path resolution</li> </ul>"},{"location":"release-notes/v0.14.3/","title":"v0.14.3","text":"<ul> <li>(all) Corrected dependencies in composer.json files for all packages</li> <li>(utils) Removed circular dependency on addons package (via Image class)</li> <li>(all) Tests moved under packages/*/tests directories</li> <li>(all) Added .gitattributes file to all packages</li> <li>(all) Corrected .gitignore files in all packages</li> <li>(main) Scripts added in ./bin to install &amp; update composer dependencies and run all tests</li> <li>(main) Corrected php.yml to use the new script ./bin/run-all-tests.sh</li> <li>(docs) Corrected docs references: old example viewer &amp; launcher script (./hub.sh) to new (./bin/instructor hub)</li> </ul>"},{"location":"release-notes/v0.14.4/","title":"v0.14.4","text":"<ul> <li>(docs) Package specific docs moved to subpackage docs/ directories</li> <li>(docs) Modified Mintlify group naming to avoid conflicts</li> <li>(docs) Modified docs building process to use subpackage docs</li> </ul>"},{"location":"release-notes/v0.14.5/","title":"v0.14.5","text":"<ul> <li>(all) Added missing license information to composer.json files</li> </ul>"},{"location":"release-notes/v0.14.6/","title":"v0.14.6","text":"<ul> <li>(addons) Moved Cognesy\\Addons\\Evals to a separate package under packages/evals and new   namespace Cognesy\\Evals</li> <li>(polyglot) Corrected warning on missing 'thinking' key in the non-reasoning responses</li> <li>(docs) Minor corrections in docs</li> </ul>"},{"location":"release-notes/v0.14.7/","title":"v0.14.7","text":"<ul> <li>(utils) Moved Cognesy\\Utils\\Template to a new, separate package cognesy/templates</li> <li>(utils) Moved Debug related files to cognesy/http-client package</li> <li>(utils) Moved chat template code (Script and related classes) to cognesy/templates</li> </ul>"},{"location":"release-notes/v0.15.0/","title":"v0.15.0","text":"<ul> <li>New packages extracted from instructor/utils and instructor/addons - evals and templates</li> </ul>"},{"location":"release-notes/v0.15.1/","title":"v0.15.1","text":"<ul> <li>Corrected dependencies after extraction of <code>evals</code> and <code>templates</code> packages</li> </ul>"},{"location":"release-notes/v0.15.2/","title":"v0.15.2","text":"<ul> <li>PHP8.4 compatibility fix across the codebase (warning: \"Implicitly marking parameter $parameters as nullable is deprecated, the explicit nullable type must be used instead\")</li> </ul>"},{"location":"release-notes/v0.16.0/","title":"v0.16.0","text":"<ul> <li>(http) Modified HTTP client layer to allow pluggable drivers</li> <li>(http) Minor additions to docs</li> <li>(http) Removed hard dependency of http config file on HttpClientType enum</li> <li>(all) Removed dependencies on HttpClientType enum</li> <li>(tests) Changes in tests to allow execution from both monorepo and individual packages</li> </ul>"},{"location":"release-notes/v0.17.0/","title":"v0.17.0","text":"<ul> <li>(polyglot) Renamed class <code>Mode</code> to <code>OutputMode</code></li> <li>(polyglot) Updates in driver mappings to catch up on the provider changes / improvements (e.g. support for JSON Schema / strict mode)</li> <li>(polyglot) Fixed token usage tracking for OpenAI/Azure and Gemini OpenAI-comptible drivers</li> <li>(http) Switching debug to true in config/debug.php turns on debugging globally</li> </ul>"},{"location":"release-notes/v0.17.1/","title":"v0.17.1","text":"<ul> <li>(polyglot) Corrected response format selection in Sambanova driver</li> </ul>"},{"location":"release-notes/v0.17.10/","title":"v0.17.10","text":"<ul> <li>(utils) <code>JsonSchema</code> class - simple API to build dynamic JSON schemas in Polyglot, lean alternative to <code>Structure</code> class (which is Instructor only)</li> <li>(docs) New and updated examples</li> </ul>"},{"location":"release-notes/v0.17.11/","title":"v0.17.11","text":"<ul> <li>(polyglot) Support for DSN string containing parameters of LLM provider connection - Inference, Embeddings classes and their configs</li> <li>(instructor) Support for DSN string containing parameters of LLM provider connection - Instructor class</li> <li>(utils) DSN class with DSN-like string parsing capabilities</li> <li>(docs) DSN examples for Inference and Instructor</li> <li>(instructor) Internal refactoring - moved some Instructor/Features/Core code to Instructor namespace</li> <li>(hub) Automated build of changelog section in docs</li> </ul>"},{"location":"release-notes/v0.17.12/","title":"v0.17.12","text":"<ul> <li>(hub) Moved Mintlify helpers to instructor-aux package to make it an integration available for other components</li> </ul>"},{"location":"release-notes/v0.17.3/","title":"v0.17.3","text":"<ul> <li>(build) Run package tests separately for each package after a new version release</li> </ul>"},{"location":"release-notes/v0.17.4/","title":"v0.17.4","text":"<ul> <li>(docs) Renamed <code>Mode</code> class references to <code>OutputMode</code> in docs and examples</li> <li>(docs) Slimmed down README.md - the removed sections are in the docs</li> <li>(docs) Minor corrections of obsolete or outdated code in docs and examples</li> </ul>"},{"location":"release-notes/v0.17.5/","title":"v0.17.5","text":"<ul> <li>(main) Added release script to distribute current versions of /bin/ins-setup, /bin/ins-hub, /bin/tell to subpackages</li> <li>(main) Release script now copies examples to hub subpackage, so they can be included in the distribution</li> <li>(main) Renamed /scripts/setup.php script to /bin/ins-setup</li> <li>(main) Renamed /scripts/hub.php script to /bin/ins-hub</li> <li>(main) Removed obsolete /bin/instructor script</li> <li>(all) Updated composer.json files in main and subpackages to include new package scripts</li> <li>(docs) Updated docs</li> </ul>"},{"location":"release-notes/v0.17.6/","title":"v0.17.6","text":"<ul> <li>(setup) Renamed bin/ins-setup to bin/instructor-setup for better clarity</li> <li>(hub) Renamed bin/ins-hub to bin/instructor-hub for better clarity</li> <li>(docs) Minor corrections related to setting default config path</li> </ul>"},{"location":"release-notes/v0.17.7/","title":"v0.17.7","text":"<ul> <li>Fixed script merge issues</li> </ul>"},{"location":"release-notes/v0.17.8/","title":"v0.17.8","text":"<ul> <li>(http) Replaced enums with strings in config files - config/http.conf</li> <li>(polyglot) Replaced enums with strings in config files - config/embeddings.php, config/llm.php</li> </ul>"},{"location":"release-notes/v0.17.9/","title":"v0.17.9","text":"<ul> <li>(polyglot) Refactoring - introduced driver factories for embeddings and inference</li> <li>(polyglot) Introduced provider specific driver classes (in addition to previous modular driver)</li> <li>(polyglot) Fixed issue with Deepseek reasoning model accepted message format (no support for successive user or assistant messages)</li> </ul>"},{"location":"release-notes/v0.8.0/","title":"v0.8.0","text":"<ul> <li>'Structured-to-structured' processing - provide objects or arrays to be processed with LLM, get object as a result</li> <li>Composite language programs with Module classes (inspired by DSPy)</li> <li><code>FunctionCall</code> helper class for extracting arguments for functions, methods or closures</li> <li>(experimental) Anthropic tool calls mode support</li> <li>(experimental) Cohere API support - MdJson only, other modes unstable</li> <li>(experimental) Gemini API client - MdJson &amp; sync only, streaming is unstable</li> <li>(internals) Simplified, cleaner API client code</li> <li>(internals) Consolidated message building logic to support formats required by different APIs</li> <li>(internals) Better control over complex chat message sequences with <code>Scripts</code> and <code>Sections</code></li> <li>(internals) Code cleanup and bug fixes</li> <li>Additions to docs and examples</li> </ul>"},{"location":"release-notes/v1.0.0-RC10/","title":"v1.0.0-RC10","text":"<ul> <li>(schema) Merged schema and schema-v6 code.</li> <li>(schema) Symfony version check and dynamic adapter selection (v6 for lowest, v7 for stable).</li> </ul>"},{"location":"release-notes/v1.0.0-RC11/","title":"v1.0.0-RC11","text":"<ul> <li>(schema) Corrections in Symfony version detection and adapter behavior.</li> </ul>"},{"location":"release-notes/v1.0.0-RC12/","title":"v1.0.0-RC12","text":"<ul> <li>(schema) Refactored schema handling, cleaned up the type handling code.</li> <li>(utils) Improvements in JsonSchema handling and integration with TypeDetails and schema handling.</li> </ul>"},{"location":"release-notes/v1.0.0-RC13/","title":"v1.0.0-RC13","text":"<ul> <li>(instructor) Improved JSON Schema handling with JsonSchemaType class</li> <li>(instructor) Improved date handling</li> <li>(schema) Support for parallel handling of PropertyInfo for Symfony v6 and v7 (auto-detected)</li> </ul>"},{"location":"release-notes/v1.0.0-RC14/","title":"v1.0.0-RC14","text":"<ul> <li>Fixed #43 - Call to undefined method Cognesy\\Http\\HttpClient::makeDefaultDriver()</li> </ul>"},{"location":"release-notes/v1.0.0-RC15/","title":"v1.0.0-RC15","text":"<ul> <li>(utils) Default config path is now taken from <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable. <code>INSTRUCTOR_CONFIG_PATH</code> is still recognized for backward compatibility, but it is recommended to use <code>INSTRUCTOR_CONFIG_PATHS</code> instead. Default config location is now your project root's <code>config</code> directory, with fallback to the bundled config directory if not set.</li> </ul>"},{"location":"release-notes/v1.0.0-RC16/","title":"v1.0.0-RC16","text":"<ul> <li>(utils) Settings class now check not just for the existence of config dir, but also config files when resolving config path.</li> </ul>"},{"location":"release-notes/v1.0.0-RC17/","title":"v1.0.0-RC17","text":"<ul> <li>(utils) New, simplified Settings class with bugs fixed.</li> </ul>"},{"location":"release-notes/v1.0.0-RC18/","title":"v1.0.0-RC18","text":"<ul> <li>(all) Strict types checking is now enabled for all packages.</li> </ul>"},{"location":"release-notes/v1.0.0-RC19/","title":"v1.0.0-RC19","text":"<ul> <li>(utils) Split message and message list classes into separate package (instructor-messages)</li> </ul>"},{"location":"release-notes/v1.0.0-RC20/","title":"v1.0.0-RC20","text":"<ul> <li>(messages) Fixing: instructor/messages not split into separate repo</li> </ul>"},{"location":"release-notes/v1.0.0-RC21/","title":"v1.0.0-RC21","text":"<ul> <li>(messages) Fixing: instructor/messages not split into separate repo</li> </ul>"},{"location":"release-notes/v1.0.0-RC22/","title":"v1.0.0-RC22","text":"<ul> <li>(instructor) Moved structure class to a separate package (instructor-dynamic)</li> <li>(hub) Fix: error in summary generation (due to strict types)</li> </ul>"},{"location":"release-notes/v1.0.0-RC6/","title":"v1.0.0-RC6","text":"<ul> <li>Corrected dependencies in composer.json files.</li> <li>Corrected naming scheme for the release candidate (rc -&gt; RC) to make Packagist happy.</li> </ul>"},{"location":"release-notes/v1.0.0-RC7/","title":"v1.0.0-RC7","text":"<ul> <li>(composer/packagist) Corrected minimum-stability value to \"RC\" (case sensitive).</li> </ul>"},{"location":"release-notes/v1.0.0-RC8/","title":"v1.0.0-RC8","text":"<ul> <li>(packagist) Removed 'version' from composer.json files.</li> </ul>"},{"location":"release-notes/v1.0.0-RC9/","title":"v1.0.0-RC9","text":"<ul> <li>(utils) Corrected tests and removed Messages dependency on Template</li> <li>(scripts) Moved build scripts to <code>scripts/</code> directory</li> <li>(scripts) Corrected publish script to NOT force add version field to composer.json</li> </ul>"},{"location":"release-notes/v1.0.0-rc1/","title":"v1.0.0-rc1","text":"<ul> <li>(all) Multiple breaking changes - proceed with caution</li> <li>(instructor) the <code>Instructor</code> class is being replaced with <code>StructuredOutput</code> class; the old class will be kept for some time to allow for a smooth transition.</li> <li>(all) Common conventions for working with StructuredOutput, Inference, and Embeddings classes</li> <li>(examples) All examples have been updated to use the new <code>StructuredOutput</code> class and recommended create(), generate() methods</li> <li>(docs) Updated documentation to reflect the new <code>StructuredOutput</code> class and its usage</li> <li>(instructor) Extracted structured output config into a separate file config/structured.php (and removed from config/llm.php)</li> <li>(instructor) Added structured output config object</li> <li>(instructor) Cleaned up ChatTemplate class</li> <li>(instructor) Response and response streams are now cached after the first call, so multiple calls to <code>StructuredOutputResponse::response()</code> or <code>StructuredOutputResponse::stream()</code> do not cause re-processing (deserialization, validation, transformation) of the response value</li> <li>(instructor) StructuredOutput class offers fluent API for creating structured output requests</li> <li>(instructor) StructuredOutputResponse now offers getXxx() methods for getting the response value as a specific type (e.g. getString(), getInt(), getFloat(), getBool(), getArray(), getObject())</li> <li>(instructor) Removed <code>input</code> argument from StructuredOutput methods and StructuredOutputRequest class - use <code>messages</code> instead - (examples) Added StructuredOutput fluent API example</li> <li>(polyglot) Added fluent API calls to Inference class</li> <li>(polyglot) Added fluent API calls to Embeddings class</li> <li>(polyglot) Corrections in inference drivers, fixed defects in JSON/JSON Schema modes</li> <li>(polyglot) Fixed error in selection of embeddings driver</li> <li>(polyglot) Added <code>withDebug()</code> support to Embeddings class</li> <li>(polyglot) Added experimental support for HuggingFace inference API</li> <li>(all) Multiple changes, improvements and refactorings in the codebase</li> <li>(all) Updated docs and examples to reflect the latest changes</li> </ul>"},{"location":"release-notes/v1.0.0-rc2/","title":"v1.0.0-rc2","text":"<ul> <li>(polyglot) Cleaned up the code and interfaces - split responsibilities between inference vs HTTP layers</li> <li>(polyglot) Heavily refactored API to improve integration with HTTP layer</li> <li>(instructor) Object hydration via constructor parameters with support for parameter nullability and default values.</li> <li>(instructor) Object hydration via getters and setters (recognizes nullable parameters and default values).</li> <li>(instructor) Replaced deprecated PropertyInfo Type class with TypeInfo one.</li> <li>(instructor) Support for mixed property type.</li> <li>(schema) Introduced 2 versions of the schema package - Symfony 7 (default) and Symfony 6 (compatibility).</li> <li>(inference) Added <code>withHttpClientPreset()</code> to <code>Inference</code> and <code>StructuredOutput</code> facades</li> <li>(http) Configurable stream chunk size to optimize performance</li> <li>(http) Replaced debugging middleware with EventSourceMiddleware - generates events for HTTP requests, added 2 built-in listeners (PrintToConsole, DispatchHttpEvents)</li> </ul>"},{"location":"release-notes/v1.0.0-rc3/","title":"v1.0.0-rc3","text":"<ul> <li>(bin) Initial version of local split-packages.sh</li> </ul>"},{"location":"release-notes/v1.0.0-rc4/","title":"v1.0.0-rc4","text":"<ul> <li>(dependencies) Corrected schema-v6 version</li> </ul>"},{"location":"release-notes/v1.0.0-rc5/","title":"v1.0.0-rc5","text":"<ul> <li>(schema), (instructor) Corrected dependencies in composer.json</li> <li>Corrected version naming scheme.</li> </ul>"},{"location":"release-notes/v1.0.0/","title":"v1.0.0","text":"<ul> <li>(inference) Corrected CanHandleInference contract to enable future integration with HTTP pool mechanism</li> </ul>"},{"location":"release-notes/v1.1.0/","title":"v1.1.0","text":"<ul> <li>(messages) Added nicer API for creating message sequences (via <code>asUser()</code>, <code>asSystem()</code>, <code>asAssistant()</code>)</li> <li>(inference) <code>Inference::with()</code>, <code>Inference::withMessages()</code> now support <code>Message</code>, <code>Messages</code> objects directly</li> <li>(instructor) <code>StructuredOutput::with()</code>, <code>StructuredOutput::withMessages()</code> now support <code>Message</code>, <code>Messages</code> objects directly</li> </ul>"},{"location":"release-notes/v1.10.0/","title":"v1.10.0","text":""},{"location":"release-notes/v1.10.0/#1100-summary","title":"1.10.0 Summary","text":"<p>This is a release focused mostly on type safety &amp; static analysis. We have made comprehensive improvements to PHPDoc annotations, generic types, null-safety, and PHP 8.3+ compatibility.</p>"},{"location":"release-notes/v1.10.0/#features-improvements","title":"\u2728 Features &amp; Improvements","text":""},{"location":"release-notes/v1.10.0/#type-safety-enhancements-all-packages","title":"Type Safety Enhancements (All Packages)","text":"<ul> <li>Generic Type Support: Added comprehensive <code>@template</code>, <code>@param</code>, and <code>@return</code> PHPDoc annotations across collections, operators, and state processors</li> <li>PHP 8.3+ Compatibility: Added <code>#[\\Override]</code> attributes to 300+ method implementations for compile-time verification</li> <li>Strict Type Checking: Applied <code>strict_types=1</code> declarations and strict comparison operators throughout</li> <li>Callable Signatures: Enhanced PHPDoc with precise callable type hints (e.g., <code>callable(Section): bool</code>)</li> </ul>"},{"location":"release-notes/v1.10.0/#null-safety-improvements","title":"Null-Safety Improvements","text":"<ul> <li>schema: Fixed nullable type handling in <code>TypeDetails</code>, <code>SignatureField</code>, and reflection classes</li> <li>instructor: Better null handling in deserialization layer (<code>CustomObjectNormalizer</code>, <code>FlexibleDateDenormalizer</code>)</li> <li>messages: Fixed null pointer issues in <code>Messages::firstRole()</code>, <code>Messages::lastRole()</code>, and filter operations</li> <li>auxiliary: Added type guards in <code>Codebase::getMethodParams()</code> and <code>RawHtml::convertToAbsoluteUrls()</code></li> <li>dynamic: Explicit null initialization for <code>Field::$value</code> and <code>Field::$validator</code> properties</li> </ul>"},{"location":"release-notes/v1.10.0/#collection-api-standardization-utils","title":"Collection API Standardization (utils)","text":"<ul> <li>Renamed <code>ArrayList::get()</code> \u2192 <code>ArrayList::itemAt()</code> for interface consistency</li> <li>Added comprehensive generic type annotations to <code>ArrayList</code>, <code>ArrayMap</code>, <code>ArraySet</code></li> <li>Improved <code>TagMap</code> type safety with proper tag storage annotations</li> </ul>"},{"location":"release-notes/v1.10.0/#result-monad-enhancements-utils","title":"Result Monad Enhancements (utils)","text":"<p>Helper methods for batch operations:</p> <pre><code>Result::tryAll(callable ...$operations): Result  // Execute all, aggregate errors\nResult::tryUntil(callable ...$operations): Result  // Execute until first success\n</code></pre> <p>Improved error aggregation via <code>CompositeException</code> for better debugging.</p>"},{"location":"release-notes/v1.10.0/#state-processing-addons","title":"State Processing (addons)","text":"<ul> <li>Added <code>forceThrowOnFailure</code> parameter to <code>Chat</code> class for graceful error handling</li> <li>Enhanced generic type constraints in <code>StateProcessors</code>, <code>ContinuationCriteria</code>, and middleware chains</li> <li>Added <code>ChatFactory::defaultProcessors()</code> helper method</li> </ul>"},{"location":"release-notes/v1.10.0/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":""},{"location":"release-notes/v1.10.0/#file-operations","title":"File Operations","text":"<ul> <li>doctor: Fixed <code>glob()</code> returning <code>false</code> instead of empty arrays (multiple files)</li> <li>doctor: Fixed unhandled <code>false</code> returns from <code>file_get_contents()</code>, <code>realpath()</code>, and <code>preg_replace()</code></li> <li>instructor: Fixed file operation error handling with runtime exceptions</li> </ul>"},{"location":"release-notes/v1.10.0/#type-safety","title":"Type Safety","text":"<ul> <li>auxiliary: Fixed null safety in parameter extraction and DOM element handling</li> <li>dynamic: Fixed schema immutability violations in <code>Field::withName()</code> and <code>Field::withDescription()</code></li> <li>messages: Added runtime exceptions for empty collection operations instead of returning null</li> <li>schema: Improved nullable type handling with proper null coalescing and type guards</li> <li>pipeline: Better null handling in <code>StepTimingTag::startDateTime()</code></li> </ul>"},{"location":"release-notes/v1.10.0/#array-operations","title":"Array Operations","text":"<ul> <li>evals: Fixed array key conflicts in <code>MakeObservations</code> and <code>SelectObservations</code> using <code>array_values()</code></li> <li>schema: Added strict type comparisons to all <code>in_array()</code> calls</li> </ul>"},{"location":"release-notes/v1.10.1/","title":"v1.10.1","text":""},{"location":"release-notes/v1.10.1/#changes","title":"Changes","text":""},{"location":"release-notes/v1.10.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed tests failing due to ResponseFormat refactoring</li> </ul>"},{"location":"release-notes/v1.10.2/","title":"v1.10.2","text":""},{"location":"release-notes/v1.10.2/#changes","title":"Changes","text":""},{"location":"release-notes/v1.10.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed Symfony PropertyInfo adapter auto-detection failing with <code>--prefer-lowest</code> dependencies. Detection now properly distinguishes between Symfony 6 (uses <code>PropertyInfoExtractor::getTypes()</code>) and Symfony 7 (uses <code>PropertyInfoExtractor::getType()</code> with TypeInfo component) by checking for <code>Symfony\\Component\\TypeInfo\\Type</code> class existence.</li> </ul>"},{"location":"release-notes/v1.10.3/","title":"v1.10.3","text":""},{"location":"release-notes/v1.10.3/#changes","title":"Changes","text":""},{"location":"release-notes/v1.10.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Removed <code>#[\\Override]</code> attribute from <code>getSupportedTypes()</code> in <code>FlexibleDateDenormalizer</code>, <code>BackedEnumNormalizer</code>, and <code>CustomObjectNormalizer</code> - method only exists in Symfony 7+ interfaces, causing fatal errors with Symfony 6.4.</li> </ul>"},{"location":"release-notes/v1.11.0/","title":"Release Notes","text":""},{"location":"release-notes/v1.11.0/#executive-summary","title":"Executive Summary","text":"<p>This release represents a major architectural milestone for the instructor-php monorepo, introducing the new stream package for functional data processing and significantly refactoring the instructor package's streaming architecture. The changes focus on modularity, composability, and maintainability while improving type safety across all packages.</p> <ul> <li>New <code>stream</code> package: Complete functional stream processing library with 24+ sources, 40+ transducers, and 20+ reducers</li> <li>Major instructor refactoring: Streaming architecture completely redesigned for modularity and testability</li> <li>Better code organization: Logical namespace restructuring across multiple packages</li> <li>Enhanced utilities: New data structures (Deque, Buffer, RingBuffer) and error collection system</li> <li>Type safety improvements: Better PHPDoc annotations and flexible type hints</li> </ul>"},{"location":"release-notes/v1.11.0/#instructor-package","title":"<code>instructor</code> Package","text":"<ul> <li>Replaced <code>PartialsGenerator</code> with modular components:</li> <li>Use <code>GeneratePartialsFromJson</code> for JSON streaming mode</li> <li>Use <code>GeneratePartialsFromToolCalls</code> for tool-based streaming mode</li> <li>Method rename: <code>CanGeneratePartials::getPartialResponses()</code> \u2192 <code>makePartialResponses()</code></li> <li>Moved classes to new namespaces (aliases may be provided for backward compatibility):</li> <li><code>Core/ResponseModelFactory</code> \u2192 <code>Creation/ResponseModelFactory</code></li> <li><code>Core/StructuredOutput*</code> \u2192 <code>Creation/StructuredOutput*</code></li> <li><code>Core/SequenceableHandler</code> \u2192 <code>Streaming/SequenceGen/SequenceableEmitter</code></li> </ul>"},{"location":"release-notes/v1.11.0/#stream-package","title":"<code>stream</code> Package","text":"<p>A complete functional stream processing library implementing transducers: - 24+ Stream Sources: Array, CSV, JSON, JSONL, HTTP, Filesystem, Text processing - 40+ Transducers: Transform, filter, flatten, group, limit, combine, deduplicate - 20+ Reducers: Terminal operations for stats, selection, side effects - Tee Splitting: Process same source through multiple parallel pipelines - Result Monad Support: Functional error handling with dedicated transducers/reducers - Progressive Rendering: Iterator-based execution for memory-efficient streaming</p>"},{"location":"release-notes/v1.11.0/#utils-package","title":"<code>utils</code> Package","text":"<ul> <li>Deque and Circular Buffer data structures</li> <li>Immutable error collection system with serialization</li> <li>New Arrays operations for merging multiple arrays</li> </ul>"},{"location":"release-notes/v1.11.0/#polyglot-package","title":"<code>polyglot</code> Package","text":"<ul> <li><code>Inference/Creation/</code>: Factory and builder classes</li> <li><code>Inference/Streaming/</code>: Stream processing with new <code>ContentAccumulation</code> class</li> <li>Optimized <code>InferenceExecution::errors()</code> using <code>Arrays::mergeMany()</code></li> <li>Refactored <code>InferenceStream::makePartialResponses()</code> with extracted methods</li> </ul>"},{"location":"release-notes/v1.11.0/#http-client-package","title":"<code>http-client</code> Package","text":"<ul> <li>Simplified Symfony driver</li> <li>Added annotations to streaming methods</li> </ul>"},{"location":"release-notes/v1.11.0/#addons-package","title":"<code>addons</code> Package","text":"<p>Type Flexibility: - Relaxed <code>StepByStep</code> return types from <code>Generator&lt;mixed, TState, mixed, mixed&gt;</code> to <code>iterable&lt;TState&gt;</code> - Allows implementations to return any iterable type while maintaining type safety</p>"},{"location":"release-notes/v1.2.0/","title":"v1.2.0","text":"<p>\ud83d\udd27 Centralized Monorepo Management: - packages.json - New centralized package configuration - Scripts modernized: - load-packages.sh - Loads centralized config - generate-split-matrix.sh - Generates GitHub Actions matrix - update-split-yml.sh - Updates split.yml automatically - sync-ver.sh &amp; publish-ver.sh - Now use centralized config</p> <p>\u2699\ufe0f GitHub Actions: - split.yml - Now triggers on main branch pushes + tags (was tags only) - Matrix generation - Auto-generated from packages.json</p> <p>\ud83d\udce6 New Package: - instructor-doctor - Added to monorepo</p> <p>\ud83d\udcda Documentation: - Doc generation - Split from hub to separate system - Codeblocks - Many new HTTP examples added - CONTENTS.md &amp; CONTRIBUTOR_GUIDE.md - Updated for new workflows</p> <p>Key Impact: Eliminates manual package list maintenance across scripts and GitHub Actions.</p>"},{"location":"release-notes/v1.3.0/","title":"v1.3.0","text":""},{"location":"release-notes/v1.3.0/#new-pipeline-package","title":"New Pipeline Package","text":"<ul> <li>Pipeline processing - New <code>packages/pipeline/</code> with pipeline components</li> <li>State and Result Aware Transformations - Apply transformations while maintaining computation integrity</li> <li>Conditional Processing - Execute steps based on runtime conditions</li> <li>Tap Operations - Inspect and modify data without affecting main flow</li> <li>ProcessingState Container - New immutable state wrapper with <code>Result&lt;T&gt;</code> monad and metadata via TagMap</li> <li>PipelineBuilder &amp; PendingExecution - Lazy evaluation with fluent builder pattern for pipeline construction</li> <li>Enhanced Operator System - Comprehensive set of operators: Call, ConditionalCall, Tap, Skip, Fail, and observability operators</li> <li>Advanced Error Handling - Sophisticated error strategies with <code>ErrorTag</code> and <code>CompositeException</code> support</li> </ul>"},{"location":"release-notes/v1.3.0/#utils-package-enhancements","title":"Utils Package Enhancements","text":"<ul> <li>Result Improvements - Extended Result type with better error handling and composition support</li> <li>Composite Exceptions - Better error aggregation with <code>CompositeException</code></li> <li>FrontMatter Parser - parsing front matter in documents uses Symfony YAML (replaced <code>webuni/front-matter</code>)</li> <li>Clock and Duration - time management components (<code>ClockInterface</code>, <code>SystemClock</code>, <code>VirtualClock</code>, <code>Duration</code>)</li> </ul>"},{"location":"release-notes/v1.3.0/#core-library-improvements","title":"Core Library Improvements","text":"<ul> <li>Response Generation - Enhanced <code>ResponseGenerator</code> with better partial response validation</li> <li>Partials Generator - Improved <code>PartialsGenerator</code> with unified naming and optimized processing</li> <li>Request Materialization - Streamlined request handling and validation flows</li> </ul>"},{"location":"release-notes/v1.3.0/#doc-generation-refactoring-in-progress","title":"Doc generation Refactoring (in progress)","text":"<ul> <li>Streamlined Architecture - Cleaner separation of concerns for documentation generation</li> <li>Enhanced Documentation Config - Better configuration management for docs generation</li> <li>Archived Legacy Components - Moved old doc generation components to archived folder</li> </ul>"},{"location":"release-notes/v1.3.0/#repository-structure","title":"Repository Structure","text":"<ul> <li>Script Modernization - Renamed <code>create-package.php</code> to <code>make-package</code> script</li> <li>Data Directory - Moved empty package templates to <code>data/empty-new/</code></li> </ul>"},{"location":"release-notes/v1.3.0/#codebase-cleanup","title":"Codebase Cleanup","text":"<ul> <li>Repository Organization - Better package structure with centralized configuration in packages.json</li> <li>Improved Gitignore - Enhanced ignore patterns across all packages</li> </ul> <p>Full Changelog: v1.2.0...v1.3.0</p>"},{"location":"release-notes/v1.4.0/","title":"v1.4.0","text":""},{"location":"release-notes/v1.4.0/#core-changes","title":"Core Changes","text":"<ul> <li>Migration to <code>Pipeline</code> - Library code migrated to use new <code>Pipeline</code> replacing legacy <code>RawChain</code> and <code>ResultChain</code></li> <li>Response Generation - Enhanced <code>ResponseGenerator</code> with improved error handling using <code>CanCarryState</code> interface</li> <li>Partial Response Validation - Updated validation flow to use new state interface</li> <li>Type Safety Improvements - Better type annotations and interface compliance across pipeline components</li> </ul>"},{"location":"release-notes/v1.4.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>JsonSchema factory methods - Parameter order changed in factory methods:</li> <li>Old: <code>JsonSchema::string($name, $nullable, $description)</code></li> <li>New: <code>JsonSchema::string($name, $description, $title, $nullable)</code></li> <li>Pipeline interfaces - Introduction of <code>CanCarryState</code> interface may affect custom pipeline processors (update type hints from <code>ProcessingState</code> to <code>CanCarryState</code>)</li> </ul>"},{"location":"release-notes/v1.4.0/#jsonschema-package","title":"JsonSchema Package","text":"<ul> <li>Fixed parameter ordering - Corrected <code>JsonSchema</code> factory method parameter order for consistent API across <code>string()</code>, <code>integer()</code>, <code>number()</code>, <code>boolean()</code>, <code>enum()</code>, <code>array()</code>, and <code>collection()</code> methods</li> <li>Enhanced nullable handling - Fixed bug where nullable property was incorrectly set when description was passed as second parameter</li> </ul>"},{"location":"release-notes/v1.4.0/#pipeline-package","title":"Pipeline Package","text":"<ul> <li>CanCarryState Interface - Introduced new <code>CanCarryState</code> interface for improved state management abstraction</li> <li>ProcessingState Optimizations - Slimmed down <code>ProcessingState</code> class for cleaner API</li> <li>Enhanced Error Handling - Improved error extraction and processing with better type safety</li> </ul>"},{"location":"release-notes/v1.4.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>JsonSchema nullable property - Fixed critical bug where <code>nullable: true</code> was incorrectly set in JSON schema output when description was passed as nullable parameter</li> <li>Parameter type coercion - Resolved issue where string descriptions were being cast to boolean for nullable property</li> </ul>"},{"location":"release-notes/v1.4.0/#documentation","title":"Documentation","text":"<ul> <li>Pipeline Documentation - New <code>CHEATSHEET.md</code> and <code>OVERVIEW.md</code> with current API patterns</li> </ul> <p>Full Changelog: v1.3.0...v1.4.0</p>"},{"location":"release-notes/v1.4.1/","title":"v1.4.1","text":""},{"location":"release-notes/v1.4.1/#documentation-system","title":"Documentation System","text":"<ul> <li>Dual Documentation Support - Separate Mintlify and MkDocs pipelines with independent command structure</li> <li>MkDocs Integration - Complete MkDocs documentation generation system with Material theme</li> <li>MkDocs Documentation Generator - New <code>MkDocsDocumentation</code> class for static site generation with YAML preprocessing</li> <li>Command Separation - Split documentation generation into separate commands:</li> <li><code>gen:mkdocs</code> - Complete MkDocs generation with initialization</li> <li><code>gen:mintlify</code> - Complete Mintlify generation with initialization  </li> <li><code>gen:packages</code> - Package docs only (no file clearing)</li> <li><code>gen:examples</code> - Example docs only (no file clearing)</li> <li>GitHub Pages Deployment - Automated MkDocs deployment via GitHub Actions to <code>docs-site/</code> output directory</li> </ul>"},{"location":"release-notes/v1.4.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Command Interference - Fixed Mintlify commands deleting each other's files by removing <code>initializeBaseFiles()</code> from individual commands</li> <li>Image Path Resolution - Fixed absolute image paths in MkDocs to use relative paths for GitHub Pages compatibility</li> </ul> <p>Full Changelog: v1.4.0...v1.4.1</p>"},{"location":"release-notes/v1.4.2/","title":"v1.4.2","text":""},{"location":"release-notes/v1.4.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Infinite Loop in LLMProvider - Fixed recursive call in <code>LLMProvider::using()</code> method that caused infinite loops when creating LLM providers</li> <li>Config Resolution Circular Dependency - Fixed circular dependency issues in <code>ConfigResolver</code> when handling <code>ConfigResolver</code> instances as providers</li> <li>Deferred Logic Error - Corrected inverted logic in <code>Deferred::isSet()</code> method that was causing incorrect state reporting</li> <li>OpenAI Token Parameter - Updated OpenAI driver to use <code>max_completion_tokens</code> instead of deprecated <code>max_tokens</code> for compatibility with newer models like o3-mini (thanks to @gewa24)</li> <li>Dynamic Structure Deserialization - Fixed bug where <code>Field::structure</code> was not deserializable when nested in structures used as collection item types (thanks to @gewa24)</li> </ul>"},{"location":"release-notes/v1.4.2/#core-changes","title":"Core Changes","text":"<ul> <li>Utils Package Reorganization - Moved core data structures to organized subdirectories:</li> <li><code>CachedMap</code> \u2192 <code>Data/CachedMap</code></li> <li><code>DataMap</code> \u2192 <code>Data/DataMap</code></li> <li><code>FrontMatter</code> \u2192 <code>Markdown/FrontMatter</code></li> <li>Context System - Added new <code>Context</code> and <code>Layer</code> classes to utils package for improved context management</li> <li>ImmutableDataMap - New immutable data structure for safer data handling</li> </ul>"},{"location":"release-notes/v1.4.2/#development-tools","title":"Development Tools","text":"<ul> <li>Code Validation Enhancement - Added comprehensive code block validation system in doctor package:</li> <li><code>ValidateCodeBlocks</code> command for automated validation</li> <li><code>ValidationService</code> with event-driven metrics collection</li> <li>Enhanced validation events and result tracking</li> <li>Cleanup - Removed unused clock-related classes and obsolete test files</li> </ul>"},{"location":"release-notes/v1.4.2/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Utils Package Structure - File locations changed for <code>CachedMap</code>, <code>DataMap</code>, and <code>FrontMatter</code> classes. Update import statements:   <pre><code>// Old\nuse Cognesy\\Utils\\Data\\CachedMap;\nuse Cognesy\\Utils\\DataMap;\nuse Cognesy\\Utils\\FrontMatter;\n\n// New\nuse Cognesy\\Utils\\Data\\CachedMap;\nuse Cognesy\\Utils\\Data\\DataMap;\nuse Cognesy\\Utils\\Markdown\\FrontMatter;\n</code></pre></li> </ul> <p>Full Changelog: v1.4.1...v1.4.2</p>"},{"location":"release-notes/v1.5.0/","title":"v1.5.0","text":""},{"location":"release-notes/v1.5.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Provider Architecture - LLMProvider and EmbeddingsProvider no longer create drivers directly. Custom implementations using these classes must be updated to work with the new configuration-only pattern</li> <li>Interface Requirements - Components relying on driver creation methods in providers need to implement new typed interfaces (<code>HasExplicitInferenceDriver</code>, <code>HasExplicitEmbeddingsDriver</code>)</li> </ul>"},{"location":"release-notes/v1.5.0/#architecture-refactoring","title":"Architecture Refactoring","text":"<ul> <li>Configuration Provider Pattern - Complete transformation of LLMProvider and EmbeddingsProvider into pure configuration resolvers, removing driver creation responsibilities for cleaner separation of concerns</li> <li>HTTP Client Initialization - Enhanced HTTP client handling with fallback creation in HandlesInvocation trait for improved reliability</li> <li>Type-Safe Interfaces - Added new typed interfaces: <code>CanResolveLLMConfig</code>, <code>CanResolveEmbeddingsConfig</code>, <code>HasExplicitInferenceDriver</code>, <code>HasExplicitEmbeddingsDriver</code> for better type safety</li> </ul>"},{"location":"release-notes/v1.5.0/#core-improvements","title":"Core Improvements","text":"<ul> <li>Immutable Data Structures - Enhanced message handling with immutable data structures for better thread safety and predictability</li> <li>Configuration Resolution - Improved configuration provider resolution in EmbeddingsProvider with proper events and error handling</li> <li>PHP 8.4 Support - Better support for PHP 8.4 with updated dependencies</li> </ul>"},{"location":"release-notes/v1.5.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>LLMProviderConfigTest - Fixed test configuration issues for improved reliability</li> <li>Import Statements - Corrected import statements and ensured consistent use of qualified class names</li> <li>Property Assignment - Fixed property assignment typo in HandlesFluentMethods trait</li> <li>Config Resolver Events - Added proper config resolution events and error handling throughout the system</li> </ul>"},{"location":"release-notes/v1.5.0/#development-tools-documentation","title":"Development Tools &amp; Documentation","text":"<ul> <li>Documentation Cleanup - Removed extensive outdated cookbook documentation, examples, and legacy configuration files for cleaner codebase maintenance</li> <li>Documentation Restructure - Reorganized package documentation files with consistent naming conventions (<code>OVERVIEW.md</code> \u2192 <code>CHEATSHEET.md</code>, <code>INTERNALS.md</code>)</li> <li>Configuration Files - Cleaned up deprecated <code>httpClientPreset</code> parameters from configuration presets</li> </ul> <p>Full Changelog: v1.4.2...v1.5.0</p>"},{"location":"release-notes/v1.6.0/","title":"v1.6.0","text":""},{"location":"release-notes/v1.6.0/#major-features","title":"Major Features","text":"<ul> <li>Chat Architecture - Multi-participant chat mechanism with conversation coordination, and automatic summaries</li> <li>ReAct Tool Use - New ReAct (Reasoning and Acting) driver for advanced tool-based reasoning patterns alongside existing tool calling capabilities</li> <li>Script System Refactoring - Script functionality moved from templates to dedicated messages package with enhanced rendering capabilities</li> </ul>"},{"location":"release-notes/v1.6.0/#chat-system","title":"Chat System","text":"<ul> <li>Multi-Participant Chats - Support for multiple LLM participants, human participants, and tool-equipped agents in conversation flows</li> <li>Chat Coordination - Built-in coordinators including LLM-based, tool-based, and round-robin participant selection</li> <li>Conversation Summaries - Automatic chat summarization with configurable buffering and context management</li> <li>Chat State Management - Comprehensive state tracking with steps, outcomes, and continuation criteria</li> <li>Chat Events - Complete event system for monitoring chat lifecycle, participant selection, and tool usage</li> </ul>"},{"location":"release-notes/v1.6.0/#tool-use-enhancements","title":"Tool Use Enhancements","text":"<ul> <li>ReAct Driver - New reasoning and acting pattern for more sophisticated tool use with decision tracking</li> <li>Tool Calling Refactoring - Enhanced tool calling driver with improved error handling and execution flow</li> <li>Tool Execution Formatting - Better formatting and display of tool execution results</li> <li>Multiple Driver Support - Clean separation between ReAct and tool calling approaches</li> </ul>"},{"location":"release-notes/v1.6.0/#architecture-improvements","title":"Architecture Improvements","text":"<ul> <li>Messages Package - Script system relocated from templates to messages package for better separation of concerns</li> <li>Rendering System - New arrow-pipe messages renderer and role-based rendering capabilities</li> <li>Time Management - Added clock interfaces for better testability and time-dependent operations</li> <li>Continuation Criteria - Enhanced continuation logic with token limits, execution time limits, and step limits</li> </ul>"},{"location":"release-notes/v1.6.0/#core-enhancements","title":"Core Enhancements","text":"<ul> <li>Structured Output - Improved deserialization and streaming capabilities for structured data</li> <li>HTTP Client - Enhanced HTTP client handling with better error management</li> <li>Documentation Testing - Major Doctest architecture refactoring with improved code block extraction and validation</li> <li>Cohere Integration - Fixed max_tokens parameter handling for Cohere API</li> </ul>"},{"location":"release-notes/v1.6.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Property Info Adapter - Enhanced compatibility with PropertyInfo v7 for better type introspection</li> <li>Test Infrastructure - Added comprehensive fake inference drivers and mock HTTP testing capabilities</li> <li>Memory Management - Improved memory handling in streaming and sequence processing</li> </ul>"},{"location":"release-notes/v1.6.0/#testing-quality","title":"Testing &amp; Quality","text":"<ul> <li>Test Coverage - New test suites for Chat, ToolUse, and Doctest functionality</li> <li>Mock Infrastructure - Enhanced testing infrastructure with fake drivers and HTTP mocking</li> <li>Static Analysis - Updated PHPStan and Psalm configurations for better code quality</li> </ul>"},{"location":"release-notes/v1.6.0/#documentation","title":"Documentation","text":"<ul> <li>Package Documentation - Updated CHEATSHEET.md files across packages with comprehensive usage examples</li> <li>Chat Documentation - New dedicated documentation for chat and tool use patterns</li> <li>Architecture Guides - Enhanced OVERVIEW.md files explaining package internals and design decisions</li> </ul> <p>Full Changelog: v1.5.0...v1.6.0</p>"},{"location":"release-notes/v1.7.0/","title":"v1.7.0","text":""},{"location":"release-notes/v1.7.0/#major-changes","title":"Major Changes","text":""},{"location":"release-notes/v1.7.0/#chat-module-refactoring","title":"Chat Module Refactoring","text":"<ul> <li>Refactoring of the Chat system - cleaner, simpler code</li> <li>Improved architecture supporting before, after, and before-and-after processing patterns</li> </ul>"},{"location":"release-notes/v1.7.0/#messagestore-improvements","title":"MessageStore Improvements","text":"<ul> <li>Migrated from Script to MessageStore for better clarity of purpose</li> <li>Replaced array-based section handling with dedicated Sections collection</li> <li>Enhanced message handling consistency across the system</li> </ul>"},{"location":"release-notes/v1.7.0/#http-client-enhancements","title":"HTTP Client Enhancements","text":"<ul> <li>Improved exception handling hierarchy with specific error types</li> <li>Better error categorization: client errors, server errors, network issues, timeouts</li> </ul>"},{"location":"release-notes/v1.7.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Chat API: <code>Chat::default()</code> method removed, use <code>ChatFactory::default()</code> instead</li> <li>Chat State Processing: Middleware pattern replaces simple processor loop</li> <li>Script Module: Replaced with MessageStore - update imports and usage patterns</li> <li>HTTP Exceptions: New exception hierarchy may require catch block updates</li> </ul>"},{"location":"release-notes/v1.7.0/#improvements","title":"Improvements","text":"<ul> <li>Enhanced type safety across Chat components</li> <li>Better separation of concerns in processing pipelines  </li> <li>More consistent API patterns across modules</li> <li>Improved test coverage and reliability</li> </ul>"},{"location":"release-notes/v1.8.0/","title":"v1.8.0","text":""},{"location":"release-notes/v1.8.0/#major-changes","title":"Major Changes","text":""},{"location":"release-notes/v1.8.0/#immutable-tooluse-system","title":"Immutable ToolUse System","text":"<ul> <li><code>ToolUseState</code> and <code>ToolUseStep</code> are now immutable readonly classes</li> <li>State transitions create new instances instead of modifying existing ones</li> <li>Functional composition replaces method chaining</li> </ul>"},{"location":"release-notes/v1.8.0/#enhanced-chat-events","title":"Enhanced Chat Events","text":"<ul> <li>Added <code>ChatResponseRequested</code> event for tracking participant responses</li> <li>Improved event dispatching with dedicated methods per event type</li> <li>Better participant selection and state update event handling</li> </ul>"},{"location":"release-notes/v1.8.0/#improved-message-compilation","title":"Improved Message Compilation","text":"<ul> <li>Chat participants now handle their own message compilation</li> <li><code>CanCompileMessages</code> interface used consistently across participants</li> <li>Default compiler (<code>AllSections</code>) applied where not specified</li> </ul>"},{"location":"release-notes/v1.8.0/#internal-improvements","title":"Internal Improvements","text":""},{"location":"release-notes/v1.8.0/#processing-pipeline","title":"Processing Pipeline","text":"<ul> <li><code>StepProcessors</code> rewritten using middleware pattern</li> <li>State processors use functional composition for better predictability</li> <li>Continuation criteria refactored as immutable collections</li> </ul>"},{"location":"release-notes/v1.8.0/#code-organization","title":"Code Organization","text":"<ul> <li>Event emission extracted to private methods for better maintainability (selected locations so far)</li> <li>Consistent error handling across participant implementations</li> <li>Reduced coupling between chat state and compilation logic</li> </ul>"},{"location":"release-notes/v1.8.0/#performance","title":"Performance","text":"<ul> <li>Immutable structures reduce side effects and improve memory safety</li> <li>Functional approach enables better optimization opportunities</li> <li>Cleaner separation of concerns reduces object graph complexity</li> </ul>"},{"location":"release-notes/v1.8.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Removed: <code>ToolUseOptions</code> class - configuration now handled directly in <code>ToolUse</code> constructor</li> <li>Changed: <code>CanUseTools::useTools()</code> signature now requires <code>Tools</code> parameter</li> <li>Removed: Observer interfaces (<code>ToolUseObserver</code>, <code>ToolsObserver</code>) - replaced with event system</li> <li><code>CanProcessToolStep::processStep()</code> \u2192 <code>CanProcessToolState::process()</code></li> <li><code>ToolUse</code> methods (<code>withOptions()</code>, <code>withTools()</code>, <code>withMessages()</code>) removed</li> <li><code>ChatState::compiledMessages()</code> removed - compilation moved to participants</li> </ul>"},{"location":"release-notes/v1.8.1/","title":"v1.8.1","text":""},{"location":"release-notes/v1.8.1/#bug-fixes","title":"Bug Fixes","text":""},{"location":"release-notes/v1.8.1/#test-suite-corrections","title":"Test Suite Corrections","text":"<ul> <li>Fixed test failures caused by removal of <code>Messages::middle()</code> method in v1.8.0</li> <li>Updated test assertions to use alternative message access patterns</li> <li>Restored test suite compatibility with new immutable message handling</li> </ul>"},{"location":"release-notes/v1.8.1/#notes","title":"Notes","text":"<p>This is a patch release addressing test suite issues introduced in v1.8.0. No functional changes to the library itself.</p>"},{"location":"release-notes/v1.9.0/","title":"v1.9.0","text":""},{"location":"release-notes/v1.9.0/#190-summary","title":"1.9.0 Summary","text":"<ul> <li>Unified execution model across Instructor and Polyglot</li> <li>Streaming and usage accounting made consistent and test\u2011covered</li> <li>Cleaner APIs and collections, clearer events</li> </ul>"},{"location":"release-notes/v1.9.0/#instructor","title":"Instructor","text":"<ul> <li>Execution/Attempts: <code>StructuredOutputExecution</code> + <code>StructuredOutputAttempt</code> track full lifecycle (final + partials, errors).</li> <li>Streaming: sequence updates yield immutable snapshots; final event payload standardized to <code>value</code> (+ <code>cached</code>).</li> <li>Usage: execution = finalized attempts + current partial tokens (until finalization). Attempt list usage counts only finalized.</li> <li>API cleanup: removed unused <code>$messages</code> from <code>withCurrentAttempt/withFailedAttempt/withSuccessfulAttempt</code> (retries come from execution history).</li> <li>Collections: <code>StructuredOutputAttemptList::of</code> variadic fix; <code>fromArray</code> supports wrapped and flat input.</li> </ul>"},{"location":"release-notes/v1.9.0/#polyglot","title":"Polyglot","text":"<ul> <li>Execution/Attempts: <code>InferenceExecution</code> + <code>InferenceAttempt</code> with immutable transitions and full state.</li> <li>Usage: attempt usage = finalized response + partial tokens; AttemptList = only finalized; Execution = finalized + current partials.</li> <li>Collections: variadic fixes (<code>InferenceAttemptList</code>, <code>InferenceResponseList</code>); <code>fromArray</code> uses <code>ArrayList::fromArray</code>.</li> <li>Correctness: <code>InferenceAttempt::hasErrors/isFailed</code> safety.</li> </ul>"},{"location":"release-notes/v1.9.0/#embeddings","title":"Embeddings","text":"<ul> <li>Facade: declared resolver property; removed unused field; avoid duplicate \u201crequested\u201d event (driver emits once).</li> <li>Shortcuts: <code>first()</code> returns <code>?Vector</code>. Cosine similarity guarded for zero vectors.</li> </ul>"},{"location":"release-notes/v1.9.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Instructor <code>StructuredOutputExecution</code>: removed <code>$messages</code> params from attempt helpers.</li> <li>Instructor <code>RequestHandler</code>: <code>responseFor</code> \u2192 <code>executionResultFor</code>; <code>streamResponseFor</code> \u2192 <code>streamUpdatesFor</code>.</li> <li>Usage semantics tightened to avoid double counting (see above).</li> <li>Polyglot driver/collection variadic behavior corrected.</li> </ul>"},{"location":"release-notes/v1.9.1/","title":"v1.9.1","text":""},{"location":"release-notes/v1.9.1/#changes","title":"Changes","text":""},{"location":"release-notes/v1.9.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Prevent null dereference when finalizing streaming responses with no current attempt</li> <li>packages/polyglot/src/Inference/Data/InferenceExecution.php<ul> <li>Null-safe access to <code>currentAttempt</code> in <code>withFinalizedPartialResponse()</code> and <code>withFailedFinalizedResponse()</code></li> </ul> </li> <li> <p>packages/polyglot/src/Inference/Data/InferenceAttempt.php</p> <ul> <li>Null-safe <code>withFinalizedPartialResponse()</code> using empty <code>PartialInferenceResponseList</code> when needed</li> </ul> </li> <li> <p>Console display stability in evals</p> </li> <li>packages/evals/src/Console/Display.php<ul> <li>Replaced invalid <code>ImmutableDataMap::except()</code> usage with safe filtering of <code>toArray()</code> output</li> <li>Fixed constant name <code>COLOR::BOLD</code> \u2192 <code>Color::BOLD</code></li> </ul> </li> </ul>"},{"location":"release-notes/v1.9.1/#behaviorcompatibility","title":"Behavior/Compatibility","text":"<ul> <li>Groq driver request body cleanup</li> <li>packages/polyglot/src/Inference/Drivers/Groq/GroqBodyFormat.php<ul> <li>Removed redundant <code>max_tokens</code> \u2192 <code>max_completion_tokens</code> conversion (already handled by parent)</li> </ul> </li> </ul>"},{"location":"release-notes/v1.9.1/#immutability-usage-accounting","title":"Immutability &amp; Usage Accounting","text":"<ul> <li>Immutable accumulation for usage data to avoid inadvertent mutation/double-counting</li> <li>packages/polyglot/src/Inference/InferenceResponseFactory.php<ul> <li>Use <code>withAccumulated(...)</code> when combining partial usage</li> </ul> </li> <li>packages/addons/src/StepByStep/State/Traits/HandlesUsage.php<ul> <li>Replace clone+mutate with <code>withAccumulated(...)</code></li> </ul> </li> <li>packages/evals/src/Traits/Experiment/HandlesAccess.php<ul> <li>Compute experiment usage on demand by accumulating execution usages immutably</li> </ul> </li> <li>packages/evals/src/Experiment.php<ul> <li>Removed cached <code>$usage</code> and post-run accumulator; observations/summary rely on computed usage</li> </ul> </li> </ul>"},{"location":"release-notes/v1.9.1/#notes","title":"Notes","text":"<ul> <li>This release focuses on correctness and immutability:</li> <li>Eliminates potential fatals in streaming finalization and console display</li> <li>Standardizes usage accumulation to immutable patterns across modules</li> </ul>"},{"location":"release-notes/versions/","title":"Changelog Overview","text":""},{"location":"release-notes/versions/#versioning","title":"Versioning","text":""},{"location":"release-notes/versions/#versioning-rules","title":"Versioning Rules","text":"<p>Starting from version 1.0.0 Instructor follows semantic versioning (SemVer) with version numbers in the format x.y.z (e.g., 1.2.3, where x is the major version, y is the minor version, and z is the patch version).</p> <p>Use these rules to plan updates for the Instructor library:</p> <ul> <li>Major version (x): Incremented for significant changes, such as extensive refactoring of the core library or breaking changes to public APIs that are incompatible with previous versions. Major version updates may not be fully incompatible, but compatibility depends on the specific changes. Always consult the upgrade guide for the corresponding major version to understand the impact.</li> <li>Minor version (y): Incremented for backward-compatible additions, such as new features or components, or for breaking changes to a limited subset of public APIs (e.g., modifying or removing specific APIs). While minor versions aim to maintain compatibility, breaking changes in these releases may affect some use cases. Refer to the upgrade guide for details.</li> <li>Patch version (z): Incremented for backward-compatible bug fixes, security patches, or minor enhancements that do not affect existing functionality. In rare cases, a patch release may include breaking changes to fix a completely unusable feature, but these changes are not treated as minor version updates since the affected functionality was already broken. New features or components introduced in patch releases are designed to be backward-compatible and should not impact existing code.</li> </ul>"},{"location":"release-notes/versions/#upgrading-instructor","title":"Upgrading Instructor","text":"<p>When upgrading the Instructor library, follow these guidelines:</p> <ul> <li>Major (x) or Minor (y) Upgrades: Review the upgrade guide for the specific version in the documentation. These upgrades may include breaking changes or new features that require code adjustments.</li> <li>Patch (z) Upgrades: These are backward-compatible and can typically be applied by running composer update instructor-php in your project\u2019s root directory to update the dependency. No additional changes are usually required.</li> </ul> <p>We recommend upgrading all Instructor components together to ensure a consistent development experience, rather than updating individual components separately.</p>"},{"location":"snippets/snippet-intro/","title":"Snippet intro","text":"<p>One of the core principles of software development is DRY (Don't Repeat Yourself). This is a principle that apply to documentation as well. If you find yourself repeating the same content in multiple places, you should consider creating a custom snippet to keep your content in sync.</p>"},{"location":"snippets/snippet-intro/#setting-up","title":"Setting up","text":"<p>The first step to data processing with LLMs is setting up your editing environments.</p> <p>          Setup your API keys in .env file to access LLM API provider               Run examples to see how Instructor in action      </p>"},{"location":"snippets/snippet-intro/#using-instructor","title":"Using Instructor","text":"<p>Learn how to use Instructor to process your data with LLMs.</p> <p>          Understand basic concepts behind Instructor               Learn how to use Instructor in your projects               Find out the ways to define response data models               Use validation to automatically retry for incorrect LLM responses      </p>"}]}