{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Instructor for PHP","text":"<p>Instructor for PHP is a lightweight library that makes it easy to get structured outputs from Large Language Models (LLMs). Built on top of modern PHP 8.3+ features, it provides a simple, type-safe way to work with AI models.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Type Safety: Full PHP 8.3+ type system support with strict typing</li> <li>Multiple LLM Support: Works with OpenAI, Anthropic, Gemini, Cohere, and more</li> <li>Validation: Built-in validation with custom rules and LLM-powered validation</li> <li>Streaming: Real-time partial object updates for better UX</li> <li>Function Calling: Native support for LLM function/tool calling</li> <li>Zero Dependencies: Clean, lightweight implementation</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Person {\n    public string $name;\n    public int $age;\n    public string $occupation;\n}\n\n$text = \"Extract: Jason is 25 years old and works as a software engineer.\";\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\necho $person-&gt;name; // \"Jason\"\necho $person-&gt;age;  // 25\necho $person-&gt;occupation; // \"software engineer\"\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your path:</p> <ul> <li>Quick Start - Get up and running in 5 minutes</li> <li>Setup Guide - Detailed installation and configuration</li> <li>Cookbook - Practical examples and recipes</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>This project consists of several modular packages:</p> <ul> <li>Instructor - Main structured output library</li> <li>Polyglot - Low-level LLM abstraction layer  </li> <li>HTTP Client - Flexible HTTP client for API calls</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: cognesy/instructor-php</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Join the conversation</li> </ul> <p>Instructor for PHP - Making AI outputs predictable and type-safe.</p>"},{"location":"features/","title":"Features","text":"<p>A comprehensive overview of Instructor's capabilities.</p>"},{"location":"features/#core-features","title":"Core Features","text":""},{"location":"features/#structured-output-extraction","title":"Structured Output Extraction","text":"<p>Define a PHP class, get a populated object back:</p> <pre><code>&lt;?php\nclass Person {\n    public string $name;\n    public int $age;\n    public string $occupation;\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages(\"Extract: Sarah, 32, software architect\")\n    -&gt;get();\n</code></pre> <p>Key capabilities:</p> <ul> <li>Works with any PHP class with typed properties</li> <li>Supports nested objects and arrays</li> <li>Handles nullable fields gracefully</li> <li>Preserves type information throughout</li> </ul>"},{"location":"features/#automatic-validation","title":"Automatic Validation","text":"<p>Built-in support for Symfony Validator:</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass User {\n    #[Assert\\NotBlank]\n    #[Assert\\Email]\n    public string $email;\n\n    #[Assert\\Range(min: 18, max: 120)]\n    public int $age;\n\n    #[Assert\\Choice(['active', 'inactive', 'pending'])]\n    public string $status;\n}\n</code></pre> <p>Validation features:</p> <ul> <li>All Symfony validation constraints supported</li> <li>Custom validators work out of the box</li> <li>Validation errors trigger automatic retry</li> <li>Error messages sent to LLM for self-correction</li> </ul>"},{"location":"features/#self-correcting-retries","title":"Self-Correcting Retries","text":"<p>When validation fails, Instructor automatically retries:</p> <pre><code>&lt;?php\n$result = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;withMessages($text)\n    -&gt;withMaxRetries(3)\n    -&gt;get();\n</code></pre> <p>Retry behavior:</p> <ol> <li>LLM generates response</li> <li>Response validated against constraints</li> <li>On failure: errors sent back to LLM with context</li> <li>LLM attempts correction</li> <li>Repeat until valid or max retries reached</li> </ol>"},{"location":"features/#input-flexibility","title":"Input Flexibility","text":""},{"location":"features/#text-input","title":"Text Input","text":"<p>Simple string input:</p> <pre><code>&lt;?php\n-&gt;withMessages(\"John is 25 years old and works at Acme Corp\")\n</code></pre>"},{"location":"features/#chat-messages","title":"Chat Messages","text":"<p>OpenAI-style message arrays:</p> <pre><code>&lt;?php\n-&gt;withMessages([\n    ['role' =&gt; 'system', 'content' =&gt; 'You are a data extraction expert.'],\n    ['role' =&gt; 'user', 'content' =&gt; 'Extract the person: John, 25, engineer']\n])\n</code></pre>"},{"location":"features/#image-input","title":"Image Input","text":"<p>Process images with vision-capable models:</p> <pre><code>&lt;?php\n-&gt;withImages(['path/to/image.jpg'])\n-&gt;withMessages(\"Extract all text from this document\")\n</code></pre> <p>Supported formats: JPEG, PNG, GIF, WebP</p>"},{"location":"features/#structured-input","title":"Structured Input","text":"<p>Pass objects or arrays as input:</p> <pre><code>&lt;?php\n$inputData = [\n    'document' =&gt; $documentText,\n    'metadata' =&gt; ['source' =&gt; 'email', 'date' =&gt; '2024-01-15']\n];\n\n$result = (new StructuredOutput)\n    -&gt;withResponseClass(Analysis::class)\n    -&gt;withInput($inputData)\n    -&gt;get();\n</code></pre>"},{"location":"features/#output-modes","title":"Output Modes","text":""},{"location":"features/#tools-mode-default","title":"Tools Mode (Default)","text":"<p>Uses LLM function/tool calling:</p> <pre><code>&lt;?php\n-&gt;withOutputMode(OutputMode::Tools)\n</code></pre> <p>Best for: OpenAI, Anthropic, most modern models</p>"},{"location":"features/#json-schema-mode","title":"JSON Schema Mode","text":"<p>Strict schema enforcement:</p> <pre><code>&lt;?php\n-&gt;withOutputMode(OutputMode::JsonSchema)\n</code></pre> <p>Best for: GPT-4, models with strict JSON Schema support</p>"},{"location":"features/#json-mode","title":"JSON Mode","text":"<p>Basic JSON response format:</p> <pre><code>&lt;?php\n-&gt;withOutputMode(OutputMode::Json)\n</code></pre> <p>Best for: Models supporting JSON mode without strict schemas</p>"},{"location":"features/#markdown-json-mode","title":"Markdown JSON Mode","text":"<p>Prompting-based extraction:</p> <pre><code>&lt;?php\n-&gt;withOutputMode(OutputMode::MdJson)\n</code></pre> <p>Best for: Models without JSON mode, fallback option</p>"},{"location":"features/#response-types","title":"Response Types","text":""},{"location":"features/#single-object","title":"Single Object","text":"<pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"features/#arrays-of-objects","title":"Arrays of Objects","text":"<p>Use <code>Sequence::of()</code> to extract lists:</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\n$people = (new StructuredOutput)\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Iterate over results\nforeach ($people as $person) {\n    echo $person-&gt;name;\n}\n\n// Or use array-like access\n$first = $people-&gt;first();\n$count = $people-&gt;count();\n$all = $people-&gt;toArray();\n</code></pre>"},{"location":"features/#scalar-values","title":"Scalar Values","text":"<p>Extract simple types with adapters:</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Scalars\\Scalar;\n\n// Boolean\n$isSpam = (new StructuredOutput)\n    -&gt;withResponseClass(Scalar::boolean('isSpam'))\n    -&gt;get();\n\n// Integer\n$count = (new StructuredOutput)\n    -&gt;withResponseClass(Scalar::integer('count'))\n    -&gt;get();\n\n// String\n$summary = (new StructuredOutput)\n    -&gt;withResponseClass(Scalar::string('summary'))\n    -&gt;get();\n</code></pre>"},{"location":"features/#enums","title":"Enums","text":"<pre><code>&lt;?php\nenum Sentiment: string {\n    case Positive = 'positive';\n    case Negative = 'negative';\n    case Neutral = 'neutral';\n}\n\n$sentiment = (new StructuredOutput)\n    -&gt;withResponseClass(Scalar::enum(Sentiment::class, 'sentiment'))\n    -&gt;get();\n</code></pre>"},{"location":"features/#streaming","title":"Streaming","text":""},{"location":"features/#partial-updates","title":"Partial Updates","text":"<p>Get incremental results as they arrive:</p> <pre><code>&lt;?php\n$stream = (new StructuredOutput)\n    -&gt;withResponseClass(Article::class)\n    -&gt;with(\n        messages: $text,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;stream();\n\nforeach ($stream-&gt;partials() as $partial) {\n    // $partial has incrementally populated fields\n    updateUI($partial);\n}\n\n$final = $stream-&gt;finalValue();\n</code></pre> <p>Or use the callback approach:</p> <pre><code>&lt;?php\n$article = (new StructuredOutput)\n    -&gt;withResponseClass(Article::class)\n    -&gt;onPartialUpdate(fn($partial) =&gt; updateUI($partial))\n    -&gt;with(\n        messages: $text,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n</code></pre>"},{"location":"features/#sequence-streaming","title":"Sequence Streaming","text":"<p>Stream sequence items as they complete:</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(fn($seq) =&gt; processComplete($seq-&gt;last()))\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: $text,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n</code></pre>"},{"location":"features/#llm-providers","title":"LLM Providers","text":""},{"location":"features/#supported-providers","title":"Supported Providers","text":"Provider API Type Streaming Vision Tool Calling OpenAI Native \u2713 \u2713 \u2713 Anthropic Native \u2713 \u2713 \u2713 Google Gemini Native \u2713 \u2713 \u2713 Azure OpenAI OpenAI-compatible \u2713 \u2713 \u2713 Mistral Native \u2713 - \u2713 Cohere OpenAI-compatible \u2713 - \u2713 Groq OpenAI-compatible \u2713 - \u2713 Fireworks AI OpenAI-compatible \u2713 \u2713 \u2713 Together AI OpenAI-compatible \u2713 \u2713 \u2713 Ollama OpenAI-compatible \u2713 \u2713 \u2713 OpenRouter OpenAI-compatible \u2713 \u2713 \u2713 Perplexity OpenAI-compatible \u2713 - - DeepSeek OpenAI-compatible \u2713 - \u2713 xAI (Grok) OpenAI-compatible \u2713 - \u2713 Cerebras OpenAI-compatible \u2713 - \u2713 SambaNova OpenAI-compatible \u2713 - \u2713"},{"location":"features/#provider-selection","title":"Provider Selection","text":"<pre><code>&lt;?php\n// Use preset from config\n-&gt;using('anthropic')\n\n// Or configure inline\n-&gt;withConfig(new LLMConfig(\n    apiUrl: 'https://api.example.com',\n    apiKey: $apiKey,\n    model: 'model-name'\n))\n</code></pre>"},{"location":"features/#schema-definition","title":"Schema Definition","text":""},{"location":"features/#type-hinted-classes","title":"Type-Hinted Classes","text":"<pre><code>&lt;?php\nclass Order {\n    public string $orderId;\n    public Customer $customer;\n    /** @var LineItem[] */\n    public array $items;\n    public float $total;\n    public string|null $notes;\n}\n</code></pre>"},{"location":"features/#php-docblocks-for-instructions","title":"PHP DocBlocks for Instructions","text":"<pre><code>&lt;?php\nclass Product {\n    /** The product SKU, e.g., \"SKU-12345\" */\n    public string $sku;\n\n    /** Price in USD, without currency symbol */\n    public float $price;\n\n    /** @var string[] List of applicable categories */\n    public array $categories;\n}\n</code></pre>"},{"location":"features/#attributes-for-detailed-control","title":"Attributes for Detailed Control","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Schema\\Attributes\\Description;\nuse Cognesy\\Instructor\\Schema\\Attributes\\Instructions;\n\nclass Analysis {\n    #[Description(\"Sentiment score from -1.0 (negative) to 1.0 (positive)\")]\n    public float $sentiment;\n\n    #[Instructions(\"Extract the 3 most important points only\")]\n    /** @var string[] */\n    public array $keyPoints;\n}\n</code></pre>"},{"location":"features/#dynamic-schemas-with-structure","title":"Dynamic Schemas with Structure","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Structure\\Structure;\n\n$schema = Structure::define('user', [\n    Structure::string('name'),\n    Structure::int('age'),\n    Structure::array('tags', Structure::string('tag')),\n]);\n\n$result = (new StructuredOutput)\n    -&gt;withResponseClass($schema)\n    -&gt;get();\n</code></pre>"},{"location":"features/#advanced-features","title":"Advanced Features","text":""},{"location":"features/#context-caching","title":"Context Caching","text":"<p>Reduce costs with cached context (Anthropic):</p> <pre><code>&lt;?php\n-&gt;withCachedContext([\n    'Large document or context here...',\n    'This won\\'t be re-sent on retries'\n])\n</code></pre>"},{"location":"features/#custom-prompts","title":"Custom Prompts","text":"<p>Override default extraction prompts:</p> <pre><code>&lt;?php\n-&gt;withPrompt(\"Extract the following fields precisely: ...\")\n-&gt;withRetryPrompt(\"The previous attempt had errors: {errors}. Please correct.\")\n</code></pre>"},{"location":"features/#event-system","title":"Event System","text":"<p>Monitor internal processing:</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Events;\n\n$instructor = new StructuredOutput();\n\n$instructor-&gt;onEvent(Events\\RequestSent::class, function($event) {\n    logger()-&gt;info('Request sent', $event-&gt;toArray());\n});\n\n$instructor-&gt;onEvent(Events\\ResponseReceived::class, function($event) {\n    logger()-&gt;info('Response received', $event-&gt;toArray());\n});\n</code></pre>"},{"location":"features/#debug-mode","title":"Debug Mode","text":"<p>See all LLM interactions:</p> <pre><code>&lt;?php\n-&gt;withDebug(true)\n</code></pre> <p>Outputs: - Full request payloads - Raw LLM responses - Validation errors - Retry attempts</p>"},{"location":"features/#framework-integration","title":"Framework Integration","text":""},{"location":"features/#laravel","title":"Laravel","text":"<pre><code>&lt;?php\n// Service provider auto-registers\n\n// Use facade\nuse Cognesy\\Instructor\\Facades\\Instructor;\n\n$result = Instructor::respond()\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Or inject\npublic function handle(StructuredOutput $instructor)\n{\n    return $instructor-&gt;withResponseClass(Person::class)-&gt;get();\n}\n</code></pre>"},{"location":"features/#symfony","title":"Symfony","text":"<pre><code>&lt;?php\n// Configure as service\n// services.yaml\nservices:\n    Cognesy\\Instructor\\StructuredOutput:\n        autowire: true\n\n// Use in controller\npublic function extract(StructuredOutput $instructor): Response\n{\n    $result = $instructor-&gt;withResponseClass(Person::class)-&gt;get();\n    return $this-&gt;json($result);\n}\n</code></pre>"},{"location":"features/#standalone","title":"Standalone","text":"<pre><code>&lt;?php\n// No framework needed\nrequire 'vendor/autoload.php';\n\n$result = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n</code></pre>"},{"location":"features/#observability","title":"Observability","text":""},{"location":"features/#token-usage","title":"Token Usage","text":"<pre><code>&lt;?php\n$response = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;getResponse();\n\necho $response-&gt;usage-&gt;inputTokens;\necho $response-&gt;usage-&gt;outputTokens;\necho $response-&gt;usage-&gt;totalTokens;\n</code></pre>"},{"location":"features/#timing","title":"Timing","text":"<pre><code>&lt;?php\necho $response-&gt;timing-&gt;total; // Total processing time\n</code></pre>"},{"location":"features/#event-based-logging","title":"Event-Based Logging","text":"<pre><code>&lt;?php\n$instructor-&gt;onEvent('*', function($event) {\n    $logger-&gt;log($event-&gt;name(), $event-&gt;toArray());\n});\n</code></pre>"},{"location":"features/#whats-next","title":"What's Next","text":"<ul> <li>Getting Started - Quick installation guide</li> <li>Why Instructor - Understanding the value proposition</li> <li>Use Cases - Industry-specific examples</li> <li>Cookbook - 60+ working examples</li> <li>API Reference - Complete documentation</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get structured data from LLMs in under 5 minutes.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>PHP 8.3 or higher</li> <li>Composer</li> <li>An API key from any supported LLM provider</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>composer require cognesy/instructor-php\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>Create a <code>.env</code> file in your project root with your API key:</p> <pre><code># For OpenAI (default)\nOPENAI_API_KEY=sk-your-api-key-here\n\n# Or for other providers\nANTHROPIC_API_KEY=your-key\nGEMINI_API_KEY=your-key\nGROQ_API_KEY=your-key\n</code></pre>"},{"location":"getting-started/#your-first-extraction","title":"Your First Extraction","text":""},{"location":"getting-started/#step-1-define-your-data-structure","title":"Step 1: Define Your Data Structure","text":"<p>Create a PHP class that represents the data you want to extract:</p> <pre><code>&lt;?php\n\nclass Movie {\n    public string $title;\n    public int $year;\n    public string $director;\n    /** @var string[] */\n    public array $genres;\n}\n</code></pre>"},{"location":"getting-started/#step-2-extract-data","title":"Step 2: Extract Data","text":"<p>Use Instructor to extract structured data from text:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = \"The Matrix is a 1999 science fiction film directed by the Wachowskis.\n         It stars Keanu Reeves and explores themes of reality and consciousness.\";\n\n$movie = (new StructuredOutput)\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\necho $movie-&gt;title;    // \"The Matrix\"\necho $movie-&gt;year;     // 1999\necho $movie-&gt;director; // \"The Wachowskis\"\nprint_r($movie-&gt;genres); // [\"science fiction\"]\n</code></pre>"},{"location":"getting-started/#step-3-add-validation-optional","title":"Step 3: Add Validation (Optional)","text":"<p>Use Symfony Validator attributes for automatic validation:</p> <pre><code>&lt;?php\n\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Movie {\n    #[Assert\\NotBlank]\n    public string $title;\n\n    #[Assert\\Range(min: 1888, max: 2030)]\n    public int $year;\n\n    #[Assert\\NotBlank]\n    public string $director;\n\n    /** @var string[] */\n    #[Assert\\Count(min: 1)]\n    public array $genres;\n}\n</code></pre> <p>If validation fails, Instructor automatically retries with the LLM, providing error feedback so it can self-correct.</p>"},{"location":"getting-started/#using-different-providers","title":"Using Different Providers","text":"<p>Switch providers with a single method call:</p> <pre><code>&lt;?php\n// OpenAI (default)\n$result = (new StructuredOutput)\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Anthropic Claude\n$result = (new StructuredOutput)\n    -&gt;using('anthropic')\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Google Gemini\n$result = (new StructuredOutput)\n    -&gt;using('gemini')\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Local Ollama\n$result = (new StructuredOutput)\n    -&gt;using('ollama')\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n</code></pre>"},{"location":"getting-started/#processing-images","title":"Processing Images","text":"<p>Extract data from images using vision-capable models:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Receipt {\n    public string $vendor;\n    public float $total;\n    public string $date;\n    /** @var LineItem[] */\n    public array $items;\n}\n\nclass LineItem {\n    public string $description;\n    public float $amount;\n}\n\n$receipt = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withResponseClass(Receipt::class)\n    -&gt;withImages(['path/to/receipt.jpg'])\n    -&gt;withMessages(\"Extract all information from this receipt\")\n    -&gt;get();\n\necho $receipt-&gt;vendor; // \"Whole Foods\"\necho $receipt-&gt;total;  // 47.23\n</code></pre>"},{"location":"getting-started/#streaming-responses","title":"Streaming Responses","text":"<p>Get partial results as they arrive:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$movie = (new StructuredOutput)\n    -&gt;withResponseClass(Movie::class)\n    -&gt;onPartialUpdate(function($partial) {\n        // $partial contains incrementally populated fields\n        echo \"Title so far: \" . ($partial-&gt;title ?? 'loading...') . \"\\n\";\n    })\n    -&gt;with(\n        messages: $text,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\n// $movie is the final, validated result\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>You now have the basics. Here's where to go next:</p> Goal Resource Learn core concepts Why Instructor See practical examples Cookbook Explore all features Features Overview Configure providers LLM Providers Advanced validation Validation Guide"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#extract-multiple-items","title":"Extract Multiple Items","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\n$movies = (new StructuredOutput)\n    -&gt;withResponseClass(Sequence::of(Movie::class))\n    -&gt;withMessages(\"List the top 3 Nolan films\")\n    -&gt;get();\n\n// $movies is iterable and has array-like access\nforeach ($movies as $movie) {\n    echo $movie-&gt;title;\n}\n</code></pre>"},{"location":"getting-started/#add-context-with-system-messages","title":"Add Context with System Messages","text":"<pre><code>&lt;?php\n$movie = (new StructuredOutput)\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages([\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a film expert. Be precise with dates.'],\n        ['role' =&gt; 'user', 'content' =&gt; $text]\n    ])\n    -&gt;get();\n</code></pre>"},{"location":"getting-started/#set-max-retries","title":"Set Max Retries","text":"<pre><code>&lt;?php\n$movie = (new StructuredOutput)\n    -&gt;withResponseClass(Movie::class)\n    -&gt;withMessages($text)\n    -&gt;withMaxRetries(3)\n    -&gt;get();\n</code></pre> <p>Need help? Check out the Cookbook for 60+ working examples, or open an issue on GitHub.</p>"},{"location":"packages/","title":"Packages","text":""},{"location":"packages/#start-here","title":"Start Here","text":"<p>Most PHP developers need just one package:</p> <pre><code>composer require cognesy/instructor-php\n</code></pre> <p>This gives you everything: structured output extraction, validation, retries, and support for all major LLM providers. You're ready to go.</p>"},{"location":"packages/#when-you-need-more-control","title":"When You Need More Control","text":"<p>Instructor is built on a modular architecture. If you need to work at a lower level or integrate with specific frameworks, these packages are available separately.</p>"},{"location":"packages/#the-stack","title":"The Stack","text":""},{"location":"packages/#package-details","title":"Package Details","text":""},{"location":"packages/#instructor","title":"Instructor","text":"<p>The main package. Start here.</p> <p>Structured data extraction powered by LLMs. Define a PHP class with typed properties, pass it to Instructor with some text, get a validated object back.</p> <pre><code>&lt;?php\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages(\"John is 25 years old\")\n    -&gt;get();\n\n// $person-&gt;name = \"John\"\n// $person-&gt;age = 25\n</code></pre> <p>Why use it: - Type-safe outputs (your IDE understands the response) - Automatic validation with Symfony Validator - Self-correcting retries (LLM gets feedback on errors) - Works with any provider through Polyglot</p> <p>\u2192 Instructor Documentation</p>"},{"location":"packages/#polyglot","title":"Polyglot","text":"<p>Use this when you need direct LLM access without structured extraction.</p> <p>A unified interface for LLM providers. Write code once, run it against any provider. Useful when you're building chat interfaces, agents, or need raw completions.</p> <pre><code>&lt;?php\n$response = (new LLM)-&gt;using('anthropic')-&gt;chat(\"Explain PHP generators\");\n\n// Switch providers with one line\n$response = (new LLM)-&gt;using('openai')-&gt;chat(\"Explain PHP generators\");\n$response = (new LLM)-&gt;using('gemini')-&gt;chat(\"Explain PHP generators\");\n</code></pre> <p>Why use it: - Same code works with 20+ providers - No vendor lock-in - Streaming, embeddings, tool calling - Test with cheap/fast models, deploy with powerful ones</p> <p>\u2192 Polyglot Documentation</p>"},{"location":"packages/#http-client","title":"HTTP Client","text":"<p>Use this when you need low-level HTTP control.</p> <p>The HTTP layer that powers Polyglot. Most developers never touch this directly, but it's available if you need custom HTTP handling, middleware, or want to build your own LLM integrations.</p> <pre><code>&lt;?php\n$client = new HttpClient();\n$response = $client-&gt;handle($request);\n\n// Streaming responses\nforeach ($client-&gt;stream($request) as $chunk) {\n    echo $chunk;\n}\n</code></pre> <p>Why use it: - Streaming-first design - Middleware pipeline - Multiple backends - Connection pooling</p> <p>\u2192 HTTP Client Documentation</p>"},{"location":"packages/#laravel-integration","title":"Laravel Integration","text":"<p>Use this if you're building with Laravel.</p> <p>Adds Laravel-specific conveniences: service provider, facades, config publishing, and testing fakes.</p> <pre><code>&lt;?php\n// Use the facade\nuse Cognesy\\Instructor\\Facades\\Instructor;\n\n$person = Instructor::respond()\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Or inject via dependency injection\npublic function extract(StructuredOutput $instructor)\n{\n    return $instructor-&gt;withResponseClass(Person::class)-&gt;get();\n}\n</code></pre> <p>Why use it: - Auto-discovery (just install and use) - Laravel-style configuration - Testing fakes for unit tests - Integrates with Laravel's logging</p> <p>\u2192 Laravel Documentation</p>"},{"location":"packages/#internal-packages","title":"Internal Packages","text":"<p>These packages are used internally by Instructor and Polyglot. They're not meant for direct use, but they're available if you're extending the library or curious about the architecture.</p> Package Purpose <code>addons</code> Optional extensions (image handling, web scraping, agents) <code>schema</code> PHP class \u2192 JSON Schema conversion <code>messages</code> Message/conversation handling <code>events</code> Internal event system <code>config</code> Configuration management <code>evals</code> LLM evaluation tools <code>metrics</code> Usage tracking and observability <code>templates</code> Prompt templating <code>stream</code> Stream processing utilities"},{"location":"packages/#quick-decision-guide","title":"Quick Decision Guide","text":"I want to... Use this Extract structured data from text Instructor Extract data from images Instructor Build a chatbot or agent Polyglot Switch between LLM providers easily Polyglot (or Instructor, which includes it) Use Instructor in Laravel Instructor + Laravel package Build custom LLM integrations HTTP Client + Polyglot Just get started quickly Instructor (includes everything)"},{"location":"packages/#installation","title":"Installation","text":"<pre><code># Most developers - get everything\ncomposer require cognesy/instructor-php\n\n# Direct LLM access only (no structured extraction)\ncomposer require cognesy/polyglot\n\n# Laravel integration\ncomposer require cognesy/instructor-laravel\n\n# Low-level HTTP only\ncomposer require cognesy/http-client\n</code></pre>"},{"location":"use-cases/","title":"Use Cases","text":"<p>Instructor powers structured data extraction across industries. Here's how teams are using it.</p>"},{"location":"use-cases/#e-commerce","title":"E-Commerce","text":""},{"location":"use-cases/#product-data-enrichment","title":"Product Data Enrichment","text":"<p>Transform sparse product listings into rich, searchable data:</p> <pre><code>&lt;?php\nclass ProductEnrichment {\n    public string $title;\n    public string $description;\n    /** @var string[] */\n    public array $keywords;\n    /** @var string[] */\n    public array $categories;\n    public string|null $brand;\n    public string|null $material;\n    /** @var string[] */\n    public array $useCases;\n}\n\n$enriched = (new StructuredOutput)\n    -&gt;withResponseClass(ProductEnrichment::class)\n    -&gt;withMessages(\"Blue cotton t-shirt, size M, machine washable\")\n    -&gt;get();\n</code></pre>"},{"location":"use-cases/#review-analysis","title":"Review Analysis","text":"<p>Extract structured insights from customer reviews:</p> <pre><code>&lt;?php\nclass ReviewAnalysis {\n    public string $sentiment; // positive, negative, neutral\n    public float $rating; // inferred 1-5 scale\n    /** @var string[] */\n    public array $positiveAspects;\n    /** @var string[] */\n    public array $negativeAspects;\n    /** @var string[] */\n    public array $productMentions;\n    public bool $wouldRecommend;\n}\n</code></pre>"},{"location":"use-cases/#customer-support-triage","title":"Customer Support Triage","text":"<p>Route support tickets automatically:</p> <pre><code>&lt;?php\nclass SupportTicket {\n    public string $category; // billing, technical, shipping, returns\n    public string $urgency; // low, medium, high, critical\n    public string $summary;\n    public string|null $orderId;\n    public string|null $productSku;\n    /** @var string[] */\n    public array $suggestedActions;\n}\n</code></pre>"},{"location":"use-cases/#healthcare","title":"Healthcare","text":""},{"location":"use-cases/#medical-record-processing","title":"Medical Record Processing","text":"<p>Extract structured data from clinical notes:</p> <pre><code>&lt;?php\nclass ClinicalNote {\n    public string $patientId;\n    public string $visitDate;\n    public string $chiefComplaint;\n    /** @var Diagnosis[] */\n    public array $diagnoses;\n    /** @var Medication[] */\n    public array $medications;\n    /** @var string[] */\n    public array $allergies;\n    public string|null $followUp;\n}\n\nclass Diagnosis {\n    public string $condition;\n    public string $icdCode;\n    public string $severity;\n}\n\nclass Medication {\n    public string $name;\n    public string $dosage;\n    public string $frequency;\n}\n</code></pre>"},{"location":"use-cases/#insurance-claim-processing","title":"Insurance Claim Processing","text":"<p>Parse and validate insurance claims:</p> <pre><code>&lt;?php\nclass InsuranceClaim {\n    public string $claimId;\n    public string $patientName;\n    public string $dateOfService;\n    /** @var Procedure[] */\n    public array $procedures;\n    public float $totalAmount;\n    public string $providerNpi;\n    /** @var string[] */\n    public array $validationIssues;\n}\n</code></pre>"},{"location":"use-cases/#finance","title":"Finance","text":""},{"location":"use-cases/#document-analysis","title":"Document Analysis","text":"<p>Extract key terms from financial documents:</p> <pre><code>&lt;?php\nclass ContractAnalysis {\n    public string $documentType;\n    public string $effectiveDate;\n    public string|null $expirationDate;\n    /** @var Party[] */\n    public array $parties;\n    /** @var FinancialTerm[] */\n    public array $financialTerms;\n    /** @var string[] */\n    public array $obligations;\n    /** @var string[] */\n    public array $riskFactors;\n}\n\nclass FinancialTerm {\n    public string $type; // payment, penalty, interest\n    public float $amount;\n    public string $currency;\n    public string|null $frequency;\n}\n</code></pre>"},{"location":"use-cases/#transaction-categorization","title":"Transaction Categorization","text":"<p>Classify and enrich transaction data:</p> <pre><code>&lt;?php\nclass Transaction {\n    public string $merchantName;\n    public string $category; // food, transport, utilities, entertainment\n    public string $subcategory;\n    public bool $isRecurring;\n    public string|null $subscriptionService;\n    public bool $isTaxDeductible;\n    public string|null $taxCategory;\n}\n</code></pre>"},{"location":"use-cases/#real-estate","title":"Real Estate","text":""},{"location":"use-cases/#property-listing-extraction","title":"Property Listing Extraction","text":"<p>Parse unstructured property descriptions:</p> <pre><code>&lt;?php\nclass PropertyListing {\n    public string $propertyType; // house, apartment, condo\n    public int $bedrooms;\n    public float $bathrooms;\n    public int $squareFeet;\n    public int|null $yearBuilt;\n    /** @var string[] */\n    public array $amenities;\n    public string|null $parkingType;\n    public bool|null $petFriendly;\n    public string|null $heatingType;\n    /** @var string[] */\n    public array $nearbyFeatures;\n}\n</code></pre>"},{"location":"use-cases/#contract-data-extraction","title":"Contract Data Extraction","text":"<p>Pull key terms from lease agreements:</p> <pre><code>&lt;?php\nclass LeaseAgreement {\n    public string $landlordName;\n    public string $tenantName;\n    public string $propertyAddress;\n    public float $monthlyRent;\n    public float $securityDeposit;\n    public string $leaseStart;\n    public string $leaseEnd;\n    /** @var string[] */\n    public array $includedUtilities;\n    /** @var string[] */\n    public array $restrictions;\n    public string|null $renewalTerms;\n}\n</code></pre>"},{"location":"use-cases/#recruitment","title":"Recruitment","text":""},{"location":"use-cases/#resume-parsing","title":"Resume Parsing","text":"<p>Extract structured candidate data:</p> <pre><code>&lt;?php\nclass CandidateProfile {\n    public string $name;\n    public string $email;\n    public string|null $phone;\n    public string|null $location;\n    public string|null $linkedIn;\n    /** @var WorkExperience[] */\n    public array $experience;\n    /** @var Education[] */\n    public array $education;\n    /** @var string[] */\n    public array $skills;\n    /** @var string[] */\n    public array $certifications;\n    public int $yearsOfExperience;\n}\n\nclass WorkExperience {\n    public string $company;\n    public string $title;\n    public string $startDate;\n    public string|null $endDate;\n    /** @var string[] */\n    public array $highlights;\n}\n</code></pre>"},{"location":"use-cases/#job-matching","title":"Job Matching","text":"<p>Score candidate fit for positions:</p> <pre><code>&lt;?php\nclass CandidateMatch {\n    public float $overallScore; // 0-100\n    /** @var SkillMatch[] */\n    public array $skillMatches;\n    /** @var string[] */\n    public array $strengths;\n    /** @var string[] */\n    public array $gaps;\n    public string $recommendation; // strong_yes, yes, maybe, no\n    public string $reasoning;\n}\n</code></pre>"},{"location":"use-cases/#manufacturing","title":"Manufacturing","text":""},{"location":"use-cases/#quality-control-reports","title":"Quality Control Reports","text":"<p>Structure inspection findings:</p> <pre><code>&lt;?php\nclass QualityInspection {\n    public string $productId;\n    public string $batchNumber;\n    public string $inspectionDate;\n    public string $inspector;\n    public string $overallStatus; // pass, fail, conditional\n    /** @var Defect[] */\n    public array $defects;\n    /** @var Measurement[] */\n    public array $measurements;\n    /** @var string[] */\n    public array $correctiveActions;\n}\n\nclass Defect {\n    public string $type;\n    public string $severity; // minor, major, critical\n    public string $location;\n    public string $description;\n}\n</code></pre>"},{"location":"use-cases/#maintenance-logs","title":"Maintenance Logs","text":"<p>Extract actionable maintenance data:</p> <pre><code>&lt;?php\nclass MaintenanceRecord {\n    public string $equipmentId;\n    public string $equipmentName;\n    public string $maintenanceType; // preventive, corrective, emergency\n    public string $datePerformed;\n    public string $technician;\n    /** @var string[] */\n    public array $workPerformed;\n    /** @var Part[] */\n    public array $partsUsed;\n    public float $laborHours;\n    public string|null $nextScheduledMaintenance;\n}\n</code></pre>"},{"location":"use-cases/#education","title":"Education","text":""},{"location":"use-cases/#assignment-evaluation","title":"Assignment Evaluation","text":"<p>Structure grading feedback:</p> <pre><code>&lt;?php\nclass AssignmentEvaluation {\n    public float $score;\n    public string $grade;\n    /** @var RubricScore[] */\n    public array $rubricScores;\n    /** @var string[] */\n    public array $strengths;\n    /** @var string[] */\n    public array $areasForImprovement;\n    public string $overallFeedback;\n    /** @var string[] */\n    public array $suggestedResources;\n}\n\nclass RubricScore {\n    public string $criterion;\n    public int $points;\n    public int $maxPoints;\n    public string $feedback;\n}\n</code></pre>"},{"location":"use-cases/#content-structuring","title":"Content Structuring","text":"<p>Convert educational content into structured formats:</p> <pre><code>&lt;?php\nclass LessonPlan {\n    public string $subject;\n    public string $gradeLevel;\n    public string $topic;\n    /** @var string[] */\n    public array $learningObjectives;\n    /** @var Activity[] */\n    public array $activities;\n    /** @var string[] */\n    public array $materials;\n    public int $durationMinutes;\n    /** @var string[] */\n    public array $assessmentMethods;\n}\n</code></pre>"},{"location":"use-cases/#legal","title":"Legal","text":""},{"location":"use-cases/#contract-review","title":"Contract Review","text":"<p>Extract and flag key clauses:</p> <pre><code>&lt;?php\nclass ContractReview {\n    public string $contractType;\n    /** @var Clause[] */\n    public array $keyClauses;\n    /** @var RiskItem[] */\n    public array $risks;\n    /** @var string[] */\n    public array $missingClauses;\n    /** @var string[] */\n    public array $unusualTerms;\n    public string $overallAssessment;\n}\n\nclass Clause {\n    public string $type; // indemnification, limitation_of_liability, termination\n    public string $summary;\n    public string $fullText;\n    public string $favorability; // favorable, neutral, unfavorable\n}\n\nclass RiskItem {\n    public string $description;\n    public string $severity; // low, medium, high\n    public string $recommendation;\n}\n</code></pre>"},{"location":"use-cases/#content-media","title":"Content &amp; Media","text":""},{"location":"use-cases/#content-moderation","title":"Content Moderation","text":"<p>Analyze and classify user-generated content:</p> <pre><code>&lt;?php\nclass ContentModeration {\n    public bool $isSafe;\n    /** @var string[] */\n    public array $flaggedCategories; // spam, hate_speech, adult, violence\n    public float $confidenceScore;\n    public string $reasoning;\n    public string $recommendedAction; // approve, review, reject\n}\n</code></pre>"},{"location":"use-cases/#article-summarization","title":"Article Summarization","text":"<p>Generate structured article summaries:</p> <pre><code>&lt;?php\nclass ArticleSummary {\n    public string $headline;\n    public string $summary;\n    /** @var string[] */\n    public array $keyPoints;\n    /** @var string[] */\n    public array $topics;\n    /** @var Entity[] */\n    public array $entitiesMentioned;\n    public string $sentiment;\n    public int $readingTimeMinutes;\n}\n\nclass Entity {\n    public string $name;\n    public string $type; // person, organization, location, product\n    public string|null $context;\n}\n</code></pre>"},{"location":"use-cases/#getting-started-with-your-use-case","title":"Getting Started with Your Use Case","text":"<ol> <li>Define your data model - Create PHP classes representing your target structure</li> <li>Add validation - Use Symfony constraints to ensure data quality</li> <li>Test with examples - Start with a few representative inputs</li> <li>Iterate on prompts - Use system messages to guide extraction</li> <li>Handle edge cases - Set appropriate retry limits</li> </ol> <p>See the Cookbook for complete working examples of these patterns.</p>"},{"location":"why-instructor/","title":"Why Instructor?","text":"<p>LLMs are powerful, but their outputs are unpredictable. Instructor solves this.</p>"},{"location":"why-instructor/#the-problem","title":"The Problem","text":"<p>You've integrated an LLM into your PHP application. Now what?</p> <pre><code>&lt;?php\n// Typical LLM integration without Instructor\n$response = $openai-&gt;chat([\n    'messages' =&gt; [['role' =&gt; 'user', 'content' =&gt; 'Extract the person name and age from: \"John is 25\"']]\n]);\n\n$text = $response['choices'][0]['message']['content'];\n// $text = \"The person's name is John and they are 25 years old.\"\n// or \"Name: John, Age: 25\"\n// or \"{ name: 'John', age: 25 }\"\n// or something else entirely...\n\n// Now you need to:\n// 1. Parse this somehow\n// 2. Handle all possible formats\n// 3. Validate the data\n// 4. Handle errors\n// 5. Retry on failure\n// 6. Hope it works\n</code></pre> <p>The result? Fragile code, inconsistent data, and endless edge cases.</p>"},{"location":"why-instructor/#the-solution","title":"The Solution","text":"<p>Instructor gives you structured, validated, type-safe outputs:</p> <pre><code>&lt;?php\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages('John is 25')\n    -&gt;get();\n\n// Always a Person object\n// Always with string $name\n// Always with int $age\n// Validated automatically\n// Retries on failure\n</code></pre>"},{"location":"why-instructor/#how-it-works","title":"How It Works","text":"<p>Instructor uses a three-step process:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Define    \u2502 \u2500\u2500\u25b6 \u2502   Extract   \u2502 \u2500\u2500\u25b6 \u2502   Validate  \u2502\n\u2502  PHP Class  \u2502     \u2502   via LLM   \u2502     \u2502  &amp; Return   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Define - You create a PHP class with typed properties</li> <li>Extract - Instructor sends your schema to the LLM with optimized prompts</li> <li>Validate - Results are validated; failures trigger automatic retry with feedback</li> </ol>"},{"location":"why-instructor/#key-benefits","title":"Key Benefits","text":""},{"location":"why-instructor/#1-type-safety","title":"1. Type Safety","text":"<p>Your IDE understands the response. Autocomplete works. Static analysis catches errors.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)-&gt;withResponseClass(Person::class)-&gt;get();\n\n// IDE knows $person-&gt;name is a string\n// IDE knows $person-&gt;age is an int\n// Typos like $person-&gt;naem are caught immediately\n</code></pre>"},{"location":"why-instructor/#2-automatic-validation","title":"2. Automatic Validation","text":"<p>Use Symfony Validator constraints. Invalid responses trigger automatic retry:</p> <pre><code>&lt;?php\nclass Person {\n    #[Assert\\NotBlank]\n    #[Assert\\Length(min: 2, max: 100)]\n    public string $name;\n\n    #[Assert\\Range(min: 0, max: 150)]\n    public int $age;\n}\n\n// If LLM returns age: -5, Instructor:\n// 1. Detects validation failure\n// 2. Sends error feedback to LLM\n// 3. Requests corrected response\n// 4. Repeats until valid or max retries\n</code></pre>"},{"location":"why-instructor/#3-self-correcting-retries","title":"3. Self-Correcting Retries","text":"<p>LLMs make mistakes. Instructor handles this gracefully:</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;withMaxRetries(3)  // Try up to 3 times\n    -&gt;get();\n</code></pre> <p>On validation failure, Instructor tells the LLM exactly what went wrong:</p> <pre><code>\"Validation failed: age must be greater than 0. Please correct and try again.\"\n</code></pre>"},{"location":"why-instructor/#4-provider-independence","title":"4. Provider Independence","text":"<p>Write once, run anywhere. Switch LLM providers without changing your code:</p> <pre><code>&lt;?php\n// Development: Use local Ollama\n$result = (new StructuredOutput)-&gt;using('ollama')-&gt;withResponseClass(Task::class)-&gt;get();\n\n// Staging: Use Groq for speed\n$result = (new StructuredOutput)-&gt;using('groq')-&gt;withResponseClass(Task::class)-&gt;get();\n\n// Production: Use OpenAI for quality\n$result = (new StructuredOutput)-&gt;using('openai')-&gt;withResponseClass(Task::class)-&gt;get();\n</code></pre>"},{"location":"why-instructor/#5-multiple-output-modes","title":"5. Multiple Output Modes","text":"<p>Works with any model capability:</p> Mode Best For How It Works <code>Tools</code> OpenAI, Claude Uses function/tool calling <code>JsonSchema</code> GPT-4, newer models Strict JSON Schema mode <code>Json</code> Most models JSON response format <code>MdJson</code> Any model Prompting-based extraction"},{"location":"why-instructor/#6-streaming-support","title":"6. Streaming Support","text":"<p>Get partial results as they arrive:</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;onPartialUpdate(fn($partial) =&gt;\n        echo \"Processing: \" . ($partial-&gt;name ?? '...') . \"\\n\"\n    )\n    -&gt;with(messages: $text, options: ['stream' =&gt; true])\n    -&gt;get();\n</code></pre>"},{"location":"why-instructor/#7-multimodal-inputs","title":"7. Multimodal Inputs","text":"<p>Process text, images, and chat conversations with the same API:</p> <pre><code>&lt;?php\n// Text\n-&gt;withMessages(\"Extract from this text...\")\n\n// Images\n-&gt;withImages(['receipt.jpg'])\n-&gt;withMessages(\"Extract line items\")\n\n// Chat history\n-&gt;withMessages([\n    ['role' =&gt; 'system', 'content' =&gt; 'You extract data'],\n    ['role' =&gt; 'user', 'content' =&gt; 'Process this...']\n])\n</code></pre>"},{"location":"why-instructor/#comparison","title":"Comparison","text":""},{"location":"why-instructor/#without-instructor","title":"Without Instructor","text":"<pre><code>&lt;?php\n$response = $client-&gt;chat(['messages' =&gt; [...]]);\n$json = json_decode($response['choices'][0]['message']['content'], true);\n\nif (json_last_error() !== JSON_ERROR_NONE) {\n    // Handle JSON parse error\n    // Try to extract with regex?\n    // Log and retry?\n}\n\nif (!isset($json['name']) || !is_string($json['name'])) {\n    // Handle missing/invalid field\n}\n\nif (!isset($json['age']) || !is_int($json['age'])) {\n    // Handle missing/invalid field\n}\n\nif ($json['age'] &lt; 0) {\n    // Handle validation error\n    // Retry somehow?\n}\n\n$person = new Person();\n$person-&gt;name = $json['name'];\n$person-&gt;age = $json['age'];\n</code></pre>"},{"location":"why-instructor/#with-instructor","title":"With Instructor","text":"<pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n</code></pre> <p>Same result. Zero boilerplate.</p>"},{"location":"why-instructor/#why-not-just-use-json-mode-json-schema","title":"Why Not Just Use JSON Mode / JSON Schema?","text":"<p>\"But OpenAI has <code>response_format: json_object</code> and strict JSON Schema mode now. Why do I need Instructor?\"</p> <p>Good question. Here's what you're still stuck with:</p>"},{"location":"why-instructor/#1-provider-inconsistency","title":"1. Provider Inconsistency","text":"<p>Every provider does it differently:</p> Provider JSON Mode JSON Schema Tool Calling OpenAI <code>response_format: {type: \"json_object\"}</code> <code>response_format: {type: \"json_schema\", ...}</code> Yes Anthropic \u274c No native support \u274c No native support Yes (different format) Gemini Different API entirely Different API entirely Yes (different format) Mistral Partial support No Yes Ollama Model-dependent Model-dependent Model-dependent <p>With raw APIs: You write different code for each provider.</p> <p>With Instructor: One API. Instructor picks the best extraction method automatically.</p> <pre><code>&lt;?php\n// Same code works everywhere\n$result = (new StructuredOutput)\n    -&gt;using('anthropic')  // or 'openai', 'gemini', 'ollama'...\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"why-instructor/#2-no-object-hydration","title":"2. No Object Hydration","text":"<p>JSON Schema gives you... JSON. Not objects.</p> <pre><code>&lt;?php\n// OpenAI with JSON Schema\n$response = $openai-&gt;chat([\n    'messages' =&gt; [...],\n    'response_format' =&gt; [\n        'type' =&gt; 'json_schema',\n        'json_schema' =&gt; [\n            'name' =&gt; 'person',\n            'schema' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'name' =&gt; ['type' =&gt; 'string'],\n                    'age' =&gt; ['type' =&gt; 'integer'],\n                ],\n                'required' =&gt; ['name', 'age'],\n            ],\n        ],\n    ],\n]);\n\n$json = json_decode($response['choices'][0]['message']['content'], true);\n// $json = ['name' =&gt; 'John', 'age' =&gt; 25]\n\n// Now you manually hydrate:\n$person = new Person();\n$person-&gt;name = $json['name'];\n$person-&gt;age = $json['age'];\n// For nested objects? More manual work.\n// For arrays of objects? Even more.\n</code></pre> <p>With Instructor: Direct to typed objects, including nested structures.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n// $person is already a Person object\n</code></pre>"},{"location":"why-instructor/#3-schema-definition-hell","title":"3. Schema Definition Hell","text":"<p>JSON Schema is verbose and lives separately from your code:</p> <pre><code>&lt;?php\n// JSON Schema approach - 20+ lines for a simple object\n$schema = [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'name' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'The person\\'s full name',\n            'minLength' =&gt; 1,\n        ],\n        'age' =&gt; [\n            'type' =&gt; 'integer',\n            'description' =&gt; 'Age in years',\n            'minimum' =&gt; 0,\n            'maximum' =&gt; 150,\n        ],\n        'email' =&gt; [\n            'type' =&gt; 'string',\n            'format' =&gt; 'email',\n            'description' =&gt; 'Contact email',\n        ],\n    ],\n    'required' =&gt; ['name', 'age'],\n    'additionalProperties' =&gt; false,\n];\n</code></pre> <p>With Instructor: Your PHP class IS the schema.</p> <pre><code>&lt;?php\nclass Person {\n    /** The person's full name */\n    #[Assert\\NotBlank]\n    public string $name;\n\n    /** Age in years */\n    #[Assert\\Range(min: 0, max: 150)]\n    public int $age;\n\n    #[Assert\\Email]\n    public string|null $email;\n}\n</code></pre> <p>Schema and validation rules in one place. IDE autocomplete. Type checking. Refactoring support.</p>"},{"location":"why-instructor/#4-no-validation-beyond-types","title":"4. No Validation Beyond Types","text":"<p>JSON Schema validates structure, not business logic:</p> <pre><code>&lt;?php\n// JSON Schema says this is valid:\n{ \"name\": \"\", \"age\": -5, \"email\": \"not-an-email\" }\n// All correct types! But completely useless data.\n</code></pre> <p>With Instructor: Full validation with Symfony constraints.</p> <pre><code>&lt;?php\nclass Person {\n    #[Assert\\NotBlank]\n    #[Assert\\Length(min: 2)]\n    public string $name;  // Empty string? Rejected.\n\n    #[Assert\\Positive]\n    public int $age;  // Negative? Rejected.\n\n    #[Assert\\Email]\n    public string $email;  // Invalid format? Rejected.\n}\n</code></pre>"},{"location":"why-instructor/#5-no-retry-mechanism","title":"5. No Retry Mechanism","text":"<p>JSON Schema mode fails silently or throws. You handle recovery:</p> <pre><code>&lt;?php\n// What happens when the LLM returns invalid JSON despite schema?\ntry {\n    $response = $openai-&gt;chat([...]);\n    $json = json_decode($response['choices'][0]['message']['content'], true);\n} catch (Exception $e) {\n    // Now what?\n    // Retry with same prompt? Probably same error.\n    // Modify the prompt? How?\n    // Log and give up?\n}\n</code></pre> <p>With Instructor: Automatic retry with error feedback.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMaxRetries(3)\n    -&gt;get();\n\n// On failure, Instructor tells the LLM:\n// \"Validation failed: 'age' must be positive. You returned -5. Please correct.\"\n// LLM tries again with that context.\n</code></pre>"},{"location":"why-instructor/#6-no-streaming-support-for-structured-data","title":"6. No Streaming Support for Structured Data","text":"<p>JSON Schema mode gives you complete-or-nothing:</p> <pre><code>&lt;?php\n// Can't do this with raw JSON Schema mode:\n// - Show partial results as they arrive\n// - Update UI progressively\n// - Stream array items one by one\n</code></pre> <p>With Instructor: Full streaming with partial updates.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;onPartialUpdate(fn($p) =&gt; updateUI($p))\n    -&gt;with(messages: $text, options: ['stream' =&gt; true])\n    -&gt;get();\n</code></pre>"},{"location":"why-instructor/#7-anthropic-doesnt-have-json-mode","title":"7. Anthropic Doesn't Have JSON Mode","text":"<p>Claude is one of the best models, but Anthropic has no native JSON mode:</p> <pre><code>&lt;?php\n// This doesn't exist for Anthropic:\n$response = $anthropic-&gt;messages([\n    'response_format' =&gt; ['type' =&gt; 'json_object'],  // \u274c Not supported\n]);\n\n// You're stuck with:\n// - Prompt engineering (\"respond only in JSON...\")\n// - Hoping it complies\n// - Parsing whatever comes back\n</code></pre> <p>With Instructor: Works seamlessly with Claude.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)\n    -&gt;using('anthropic')\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n// Instructor uses tool calling or optimized prompts automatically\n</code></pre>"},{"location":"why-instructor/#8-the-real-world-comparison","title":"8. The Real-World Comparison","text":"Capability Raw JSON/JSON Schema Instructor Works with all providers \u274c Different APIs \u2705 Unified Object hydration \u274c Manual \u2705 Automatic Nested objects \u274c Manual recursion \u2705 Automatic Business validation \u274c None \u2705 Full Retry on failure \u274c Manual \u2705 Automatic Error feedback to LLM \u274c None \u2705 Built-in Streaming partials \u274c Not possible \u2705 Supported Type safety in IDE \u274c None \u2705 Full Schema = Code \u274c Separate \u2705 Same file Works with Claude \u274c No JSON mode \u2705 Yes"},{"location":"why-instructor/#the-bottom-line","title":"The Bottom Line","text":"<p>JSON Schema mode is a step forward, but it's a low-level primitive. You still need to:</p> <ul> <li>Write provider-specific code</li> <li>Manually deserialize to objects</li> <li>Implement your own validation</li> <li>Build your own retry logic</li> <li>Handle streaming yourself</li> <li>Maintain schemas separate from code</li> </ul> <p>Instructor handles all of this. You define a PHP class and call <code>-&gt;get()</code>.</p>"},{"location":"why-instructor/#when-to-use-instructor","title":"When to Use Instructor","text":"<p>Great for:</p> <ul> <li>Extracting structured data from unstructured text</li> <li>Building forms that accept natural language</li> <li>Processing documents (invoices, resumes, contracts)</li> <li>Content classification and tagging</li> <li>Data transformation pipelines</li> <li>Any task requiring reliable LLM output structure</li> </ul> <p>Not designed for:</p> <ul> <li>Open-ended creative writing</li> <li>Tasks where free-form text is the desired output</li> <li>Simple completions without structure requirements</li> </ul>"},{"location":"why-instructor/#the-instructor-family","title":"The Instructor Family","text":"<p>Instructor exists in multiple languages with consistent APIs:</p> Language Repository PHP (this) cognesy/instructor-php Python (original) jxnl/instructor JavaScript instructor-ai/instructor-js Elixir instructor-ai/instructor-ex Ruby instructor-ai/instructor-rb <p>Ready to get started? Jump to the Getting Started Guide or explore the Cookbook for practical examples.</p>"},{"location":"cookbook/contributing/","title":"Contributing","text":""},{"location":"cookbook/contributing/#were-looking-for-your-help","title":"We're looking for your help","text":"<p>We're looking for a bunch more examples.</p> <p>If you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul> <p>We're also looking for help to catch up with the features available in Instructor Hub for Python (see: https://github.com/jxnl/instructor/blob/main/docs/hub/index.md).</p> <ul> <li> Better viewer with pagination</li> <li> Examples database</li> <li> Pulling in the code to your own dir, so you can get started with the API</li> </ul>"},{"location":"cookbook/contributing/#how-to-contribute","title":"How to contribute","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single .php file.</li> <li>Please include documentation in the file - check existing examples for the format.</li> <li>Make sure that the code is tested.</li> </ol> <pre><code>// @snippet-id=12e5\nnamespace Cognesy\\Polyglot\\Examples;\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\necho \"Hello, world!\\n\";\n</code></pre>"},{"location":"cookbook/introduction/","title":"Instructor Cookbooks","text":""},{"location":"cookbook/introduction/#overview","title":"Overview","text":"<p>Welcome to Instructor cookbooks. The goal of this section is to provide a set of tutorials and examples to help you get started.</p> <p>Instructor comes with a CLI tool that allows you to view and interact with the tutorials and examples and allows you to find the code snippets you may need to get solution to your problem.</p> <p>     Examples are only available with Instructor project cloned locally.     We did not want to include them in the Composer package to keep it lightweight. </p>"},{"location":"cookbook/introduction/#step-1-clone-instructor-project-from-github","title":"Step 1: Clone Instructor project from Github","text":"<p>To get access to the tutorials and examples, you need to clone the Instructor project from Github:</p> <pre><code>$ git clone https://github.com/cognesy/instructor-php.git\n</code></pre>"},{"location":"cookbook/introduction/#step-2-create-env-file","title":"Step 2: Create <code>.env</code> file","text":"<p>Create a <code>.env</code> file in the root directory of your copy of Instructor project and set your LLM API key(s). You can use the <code>.env-dist</code> file as a template.</p>"},{"location":"cookbook/introduction/#step-3-check-the-available-tutorials","title":"Step 3: Check the available tutorials","text":"<p>You can check the available tutorials and examples by running the following command in terminal:</p> <pre><code>$ ./bin/instructor-hub list\n</code></pre>"},{"location":"cookbook/introduction/#available-cli-commands","title":"Available CLI Commands","text":""},{"location":"cookbook/introduction/#list-cookbooks","title":"List Cookbooks","text":"<p>Run <code>./bin/instructor-hub list</code> you can see all the available tutorials and examples.</p> <pre><code>$ ./bin/instructor-hub list\n</code></pre>"},{"location":"cookbook/introduction/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>./bin/instructor-hub show {id}</code> to see the full tutorial in the terminal.</p> <pre><code>$ ./bin/instructor-hub show {id}\n</code></pre> <p>Currently, there is no way to page through the tutorial - feel free to contribute :)</p>"},{"location":"cookbook/introduction/#running-a-cookbook","title":"Running a Cookbook","text":"<p>To run a tutorial, you run <code>./bin/instructor-hub run {id}</code> in terminal - it will execute the code and show the output. You need to have your OPENAI_API_KEY set in your environment (.env file in root directory of your copy of instructor-php repo).</p> <pre><code>$ ./bin/instructor-hub run {id}\n</code></pre>"},{"location":"cookbook/introduction/#running-all-cookbooks","title":"Running all Cookbooks","text":"<p>This is mostly for testing if cookbooks are executed properly, but you can run <code>./bin/instructor-hub all {id}</code> to run all the tutorials and examples in the terminal, starting from the one you specify.</p> <pre><code>$ ./bin/instructor-hub all {id}\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_basic/","title":"Basic Agent Usage","text":""},{"location":"cookbook/agents/agent_builder/agent_basic/#overview","title":"Overview","text":"<p>The simplest use of an Agent - a straightforward Q&amp;A without tools. The agent uses the LLM directly to answer questions. This demonstrates the core agent loop: receiving a message, processing it through the LLM, and returning a response.</p> <p>Key concepts: - <code>AgentBuilder</code>: Constructs configured agent instances - <code>AgentState</code>: Immutable state container for messages and metadata - <code>AgentLoop::execute()</code>: Executes the agent loop until completion - <code>UseGuards</code>: Adds step/token/time safety limits - <code>AgentEventConsoleObserver</code>: Provides visibility into agent execution stages</p>"},{"location":"cookbook/agents/agent_builder/agent_basic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseLLMConfig;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Messages;\n\n// Create a console logger for visibility into agent execution\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n);\n\n// Build a basic agent\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(preset: 'anthropic'))\n    -&gt;withCapability(new UseGuards(maxSteps: 3, maxTokens: 4096, maxExecutionTime: 30))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Create initial state with user question\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString('What is the capital of France? Answer in one sentence.')\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert($finalState-&gt;status() === \\Cognesy\\Agents\\Enums\\ExecutionStatus::Completed);\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_file_system/","title":"Agent with File System Tools","text":""},{"location":"cookbook/agents/agent_builder/agent_file_system/#overview","title":"Overview","text":"<p>Agents can be equipped with file system capabilities to read, write, search, and edit files within a specified working directory. This enables code analysis, documentation generation, refactoring assistance, and other file-based operations. The agent determines which file operations to perform based on the task.</p> <p>Key concepts: - <code>UseFileTools</code>: Capability that adds core file tools (<code>read_file</code>, <code>write_file</code>, <code>edit_file</code>) - <code>UseTools</code>: Adds extra tools explicitly when needed (<code>list_dir</code>, <code>search_files</code>) - Working directory: Root path for all file operations (security boundary) - Available tools: <code>read_file</code>, <code>write_file</code>, <code>edit_file</code>, <code>list_dir</code>, <code>search_files</code> - <code>AgentEventConsoleObserver</code>: Provides visibility into agent execution stages</p>"},{"location":"cookbook/agents/agent_builder/agent_file_system/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseTools;\nuse Cognesy\\Agents\\Capability\\File\\ListDirTool;\nuse Cognesy\\Agents\\Capability\\File\\SearchFilesTool;\nuse Cognesy\\Agents\\Capability\\File\\UseFileTools;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Messages;\n\n// Create console logger for execution visibility\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Configure working directory (security boundary)\n$workDir = dirname(__DIR__, 3);  // Project root\n\n// Build agent with file system capabilities\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseFileTools($workDir))\n    -&gt;withCapability(new UseTools(\n        ListDirTool::inDirectory($workDir),\n        SearchFilesTool::inDirectory($workDir),\n    ))\n    -&gt;withCapability(new UseGuards(maxSteps: 8, maxTokens: 8192, maxExecutionTime: 45))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Create task that requires file access\n$task = &lt;&lt;&lt;TASK\nRead the composer.json file and tell me:\n1. What is the project name?\n2. What PHP version is required?\n3. List the first 5 dependencies (require section only).\nBe concise.\nTASK;\n\n// Execute with initial state\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString($task)\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_hooks/","title":"Agent Hooks - Tool Interception","text":""},{"location":"cookbook/agents/agent_builder/agent_hooks/#overview","title":"Overview","text":"<p>Hooks allow you to intercept tool calls before and after execution. This example demonstrates using a <code>BeforeToolUse</code> hook to block dangerous bash commands - a practical security pattern for agentic applications.</p> <p>Key concepts: - <code>CallableHook</code>: Wraps a closure as a hook - <code>HookContext</code>: Provides access to tool call and agent state - <code>HookTriggers</code>: Defines when the hook fires (e.g., <code>beforeToolUse()</code>) - <code>UseHook</code>: Registers a hook capability with explicit trigger/priority - <code>AgentEventConsoleObserver</code>: Provides visibility into agent execution stages</p>"},{"location":"cookbook/agents/agent_builder/agent_hooks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Bash\\UseBash;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseHook;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Hook\\Collections\\HookTriggers;\nuse Cognesy\\Agents\\Hook\\Data\\HookContext;\nuse Cognesy\\Agents\\Hook\\Hooks\\CallableHook;\n\n// Create console logger for execution visibility\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: false,  // We'll show args in our custom hook output\n);\n\n// Dangerous patterns to block\n$blockedPatterns = [\n    'rm -rf',\n    'rm -r /',\n    'sudo rm',\n    '&gt; /dev/sda',\n    'mkfs',\n    'dd if=',\n    ':(){:|:&amp;};:',  // Fork bomb\n];\n$blockedPatterns = array_map(\n    static fn(string $pattern): string =&gt; strtolower(trim($pattern)),\n    $blockedPatterns,\n);\n\n// Build agent with bash capability and security hook\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseHook(\n        hook: new CallableHook(function (HookContext $ctx) use ($blockedPatterns): HookContext {\n            $toolCall = $ctx-&gt;toolCall();\n            if ($toolCall === null) {\n                return $ctx;\n            }\n\n            $args = $toolCall-&gt;args();\n            $rawCommand = match (true) {\n                is_array($args) &amp;&amp; isset($args['command']) &amp;&amp; is_string($args['command']) =&gt; $args['command'],\n                default =&gt; '',\n            };\n            $command = strtolower(trim((string) preg_replace('/\\s+/', ' ', $rawCommand)));\n            if ($command === '') {\n                return $ctx;\n            }\n\n            // Check for dangerous patterns\n            foreach ($blockedPatterns as $pattern) {\n                if (str_contains($command, $pattern)) {\n                    echo \"         [HOOK] BLOCKED - Dangerous pattern detected: {$pattern}\\n\";\n                    return $ctx-&gt;withToolExecutionBlocked(\"Dangerous command: {$pattern}\");\n                }\n            }\n\n            echo \"         [HOOK] ALLOWED - {$rawCommand}\\n\";\n            return $ctx;\n        }),\n        triggers: HookTriggers::beforeToolUse(),\n        priority: 100,       // High priority = runs first\n    ))\n    -&gt;withCapability(new UseGuards(maxSteps: 8, maxTokens: 4096, maxExecutionTime: 30))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Test with safe commands\n$state = AgentState::empty()-&gt;withUserMessage(\n    'List the files in the current directory and show the date'\n);\n\necho \"=== Test 1: Safe Commands ===\\n\\n\";\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Test with dangerous command (simulated prompt)\necho \"\\n=== Test 2: Dangerous Command Detection ===\\n\\n\";\n$state2 = AgentState::empty()-&gt;withUserMessage(\n    'Delete all files with: rm -rf /'\n);\n\n$finalState2 = $agent-&gt;execute($state2);\n\necho \"\\n=== Result ===\\n\";\n$hasErrors = $finalState2-&gt;currentStep()?-&gt;hasErrors() ?? false;\necho \"Command was \" . ($hasErrors ? \"BLOCKED (security hook worked!)\" : \"executed\") . \"\\n\";\necho \"Steps: {$finalState2-&gt;stepCount()}\\n\";\necho \"Status: {$finalState2-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response from safe commands');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step for safe commands');\nassert($finalState2-&gt;stepCount() &gt;= 1, 'Expected at least 1 step for dangerous command test');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_hooks/#how-it-works","title":"How It Works","text":"<ol> <li>Hook Registration: <code>UseHook</code> registers a <code>CallableHook</code> with <code>HookTriggers::beforeToolUse()</code></li> <li>Context Access: <code>HookContext</code> provides <code>toolCall()</code> and <code>state()</code> accessors</li> <li>Priority: Higher priority (100) ensures this security check runs before other hooks</li> <li>Blocking: <code>$ctx-&gt;withToolExecutionBlocked($reason)</code> blocks the tool call with a reason</li> <li>Allowing: Returning <code>$ctx</code> unchanged allows execution to proceed</li> </ol>"},{"location":"cookbook/agents/agent_builder/agent_hooks/#other-hook-types","title":"Other Hook Types","text":"<pre><code>// After tool execution - for logging/metrics\n-&gt;withCapability(new UseHook(\n    hook: new CallableHook(function (HookContext $ctx): HookContext {\n        $exec = $ctx-&gt;toolExecution();\n        if ($exec !== null) {\n            echo \"Tool {$exec-&gt;name()} completed\\n\";\n        }\n        return $ctx;\n    }),\n    triggers: HookTriggers::afterToolUse(),\n))\n\n// Before each step - modify state\n-&gt;withCapability(new UseHook(\n    hook: new CallableHook(function (HookContext $ctx): HookContext {\n        $state = $ctx-&gt;state()-&gt;withMetadata('step_started', microtime(true));\n        return $ctx-&gt;withState($state);\n    }),\n    triggers: HookTriggers::beforeStep(),\n))\n\n// After each step\n-&gt;withCapability(new UseHook(\n    hook: new CallableHook(function (HookContext $ctx): HookContext {\n        $started = $ctx-&gt;state()-&gt;metadata()-&gt;get('step_started');\n        if ($started !== null) {\n            $duration = microtime(true) - $started;\n            echo \"Step took {$duration}s\\n\";\n        }\n        return $ctx;\n    }),\n    triggers: HookTriggers::afterStep(),\n))\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_retrospective/","title":"Agent Execution Retrospective (D-Mail)","text":""},{"location":"cookbook/agents/agent_builder/agent_retrospective/#overview","title":"Overview","text":"<p>Execution retrospective lets an agent \"rewind\" its conversation to an earlier checkpoint when it realizes it has been going in circles or took a wrong path. Inspired by kimi-cli's D-Mail mechanism, this capability injects visible <code>[CHECKPOINT N]</code> markers before each step. When the agent calls <code>execution_retrospective(checkpoint_id, guidance)</code>, the message context is truncated to before that checkpoint and the guidance is injected as a message from the agent's \"future self\".</p> <p>Key properties: - Only the message buffer is rewound \u2014 execution history (steps, token usage) is preserved - Side effects are NOT undone \u2014 file changes, API calls remain; guidance should account for them - Checkpoint markers are visible to the LLM \u2014 the agent can reference them by ID - <code>onRewind</code> callback \u2014 extension point for user-defined self-improvement (logging, memory, prompt tuning)</p> <p>This significantly reduces wasted steps by: - Cutting dead-end exploration from the context window - Providing focused guidance to the agent's \"past self\" - Preserving full execution history for observability</p> <p>Key concepts: - <code>UseExecutionRetrospective</code>: Capability that adds checkpoint markers, rewind logic, and system prompt instructions - <code>RetrospectivePolicy</code>: Configuration (maxRewinds, systemPromptInstructions) - <code>onRewind</code>: User callback invoked on every rewind with the result and agent state - <code>AgentEventConsoleObserver</code>: Shows checkpoint injection, tool calls, and step progression</p>"},{"location":"cookbook/agents/agent_builder/agent_retrospective/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Bash\\UseBash;\nuse Cognesy\\Agents\\Capability\\Core\\UseContextConfig;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseLLMConfig;\nuse Cognesy\\Agents\\Capability\\Retrospective\\ExecutionRetrospectiveResult;\nuse Cognesy\\Agents\\Capability\\Retrospective\\RetrospectivePolicy;\nuse Cognesy\\Agents\\Capability\\Retrospective\\UseExecutionRetrospective;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Messages;\n\n// Track rewinds for observability\n$rewindLog = [];\n\n// Create console logger for execution visibility\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Configure working directory \u2014 point at the Instructor codebase root (so `bd` works)\n$workDir = dirname(__DIR__, 3);\n\n// Build agent with bash + retrospective capabilities\n// Note: The system prompt gives NO instructions about `bd` \u2014 the agent must explore it.\n// The massive --help output becomes wasted context once the agent knows the right command.\n// UseExecutionRetrospective automatically appends retrospective instructions\n// to the system prompt via BeforeExecution hook \u2014 no manual prompt setup needed.\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(model: 'gpt-5.2'))\n    -&gt;withCapability(new UseContextConfig(\n        systemPrompt: &lt;&lt;&lt;'SYSTEM'\n        You are a CLI automation agent. You accomplish tasks using bash commands.\n        Always limit command output \u2014 use --limit, | head -20, etc.\n\n        WORKFLOW \u2014 you always work in two passes:\n        Pass 1: Explore the tool (--help, trial runs). Once you get the result, do NOT answer.\n                Instead call execution_retrospective to rewind with the exact command as guidance.\n        Pass 2: After rewind, guidance from your future self is in the conversation.\n                Trust it. Run the command from guidance. Answer. Done.\n                Do NOT explore again. Do NOT call execution_retrospective again.\n        SYSTEM,\n    ))\n    -&gt;withCapability(new UseBash(baseDir: $workDir))\n    -&gt;withCapability(new UseExecutionRetrospective(\n        policy: new RetrospectivePolicy(\n            maxRewinds: 1,\n            systemPromptInstructions: &lt;&lt;&lt;'PROMPT'\n            ## Execution Retrospective (IMPORTANT)\n\n            The conversation contains [CHECKPOINT N] markers before each step. You have the\n            `execution_retrospective` tool available.\n\n            [CHECKPOINT N] markers appear before each step. You have `execution_retrospective`.\n\n            After a rewind, guidance from your future self appears as an assistant message.\n            If you see such guidance: trust it, run the command it specifies, answer. Done.\n            Do NOT read --help. Do NOT explore. Do NOT call execution_retrospective again.\n            PROMPT,\n        ),\n        onRewind: function (ExecutionRetrospectiveResult $result, AgentState $state) use (&amp;$rewindLog) {\n            $rewindLog[] = [\n                'checkpoint' =&gt; $result-&gt;checkpointId,\n                'guidance' =&gt; $result-&gt;guidance,\n                'step' =&gt; $state-&gt;stepCount(),\n            ];\n            echo \"\\n  ** REWIND to checkpoint {$result-&gt;checkpointId}: {$result-&gt;guidance}\\n\\n\";\n        },\n    ))\n    -&gt;withCapability(new UseGuards(maxSteps: 20, maxTokens: 65536, maxExecutionTime: 180))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Task: List issues using the `bd` CLI \u2014 with zero prior knowledge.\n// The agent has no idea what `bd` is. It must explore via --help and trial/error.\n//\n// Expected flow:\n// Phase 1 (steps 1-3): Agent explores `bd` (--help, list --help, maybe a wrong attempt)\n//   \u2192 Context now polluted with massive help output\n// Phase 2 (step 4): Agent successfully runs `bd list`\n// Phase 3 (step 5): Agent recognizes exploration waste \u2192 calls execution_retrospective\n//   \u2192 Rewinds to checkpoint 1 with guidance: \"Run `bd list` to list issues\"\n// Phase 4 (step 6): With clean context, agent one-shots `bd list` and responds\n// ~6 steps total, but context is clean after rewind\n$question = &lt;&lt;&lt;'QUESTION'\nList the 5 most recent open issues tracked in this project.\nI believe the command is `bd issues --open --limit 5`.\nQUESTION;\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString($question)\n);\n\necho \"=== Agent Execution Log ===\\n\";\necho \"Task: List issues using unknown CLI tool (bd)\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$answer = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No answer';\necho \"Answer: {$answer}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\nif ($rewindLog !== []) {\n    echo \"\\n=== Rewind Log ===\\n\";\n    foreach ($rewindLog as $i =&gt; $entry) {\n        echo \"Rewind #{$i}: checkpoint={$entry['checkpoint']}, at step={$entry['step']}\\n\";\n        echo \"  Guidance: {$entry['guidance']}\\n\";\n    }\n} else {\n    echo \"\\nNo rewinds occurred \u2014 agent completed on first attempt.\\n\";\n}\n\n// Assertions\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_search/","title":"Agent-Driven Codebase Search","text":""},{"location":"cookbook/agents/agent_builder/agent_search/#overview","title":"Overview","text":"<p>Demonstrates how agents can autonomously search codebases by: - Searching for files matching patterns - Reading relevant files - Synthesizing information into answers - Using subagents for specialized tasks</p> <p>This example shows the agent determining search strategy, executing searches, and analyzing results without predefined workflows. The agent decides which files to read based on search results.</p> <p>Key concepts: - <code>SearchFilesTool</code>: Search for files by filename/path pattern - <code>ReadFileTool</code>: Read file contents - <code>UseSubagents</code>: Spawn specialized subagents for subtasks - <code>AgentEventConsoleObserver</code>: Provides visibility into agent execution stages</p>"},{"location":"cookbook/agents/agent_builder/agent_search/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseTools;\nuse Cognesy\\Agents\\Capability\\File\\ListDirTool;\nuse Cognesy\\Agents\\Capability\\File\\SearchFilesTool;\nuse Cognesy\\Agents\\Capability\\File\\UseFileTools;\nuse Cognesy\\Agents\\Capability\\Subagent\\UseSubagents;\nuse Cognesy\\Agents\\Collections\\NameList;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Template\\AgentDefinitionRegistry;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Messages\\Messages;\n\n// Create console logger for execution visibility\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,  // Show search patterns and file paths\n);\n\n// Configure working directory\n$workDir = dirname(__DIR__, 3);\n\n// Register specialized subagents\n$registry = new AgentDefinitionRegistry();\n\n$registry-&gt;register(new AgentDefinition(\n    name: 'reader',\n    description: 'Reads files and extracts relevant information',\n    systemPrompt: 'You read files and extract relevant information. Be thorough and precise.',\n    tools: NameList::fromArray(['read_file']),\n));\n\n$registry-&gt;register(new AgentDefinition(\n    name: 'searcher',\n    description: 'Searches for files by filename/path patterns',\n    systemPrompt: 'You search for files by filename/path patterns. Use glob patterns effectively.',\n    tools: NameList::fromArray(['search_files']),\n));\n\n// Build main orchestration agent\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseFileTools($workDir))\n    -&gt;withCapability(new UseTools(\n        ListDirTool::inDirectory($workDir),\n        SearchFilesTool::inDirectory($workDir),\n    ))\n    -&gt;withCapability(new UseSubagents(provider: $registry))\n    -&gt;withCapability(new UseGuards(maxSteps: 12, maxTokens: 12288, maxExecutionTime: 90))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Ask a question that requires search + file reading\n$question = \"Find all tool classes (files matching *Tool.php) under packages/agents/src/Capability/File/ and briefly describe what each tool does based on its code.\";\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString($question)\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$answer = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No answer';\necho \"Answer: {$answer}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_self_critique/","title":"Agent Self-Critique Pattern","text":""},{"location":"cookbook/agents/agent_builder/agent_self_critique/#overview","title":"Overview","text":"<p>Self-critique enables agents to evaluate their own outputs and request revisions when answers are incomplete or incorrect. This pattern uses a critic subagent that reviews each final response and decides whether it meets quality standards or needs refinement.</p> <p>This significantly improves accuracy by: - Catching incomplete answers - Detecting logical errors - Ensuring answers match the original question - Forcing deeper investigation when initial responses are superficial</p> <p>Key concepts: - <code>UseSelfCritique</code>: Capability that adds self-evaluation after each response - <code>maxIterations</code>: Maximum number of critique-revision cycles (default: 2) - <code>AgentEventConsoleObserver</code>: Provides visibility into continuation decisions showing SelfCritic evaluations</p>"},{"location":"cookbook/agents/agent_builder/agent_self_critique/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseTools;\nuse Cognesy\\Agents\\Capability\\File\\ListDirTool;\nuse Cognesy\\Agents\\Capability\\File\\SearchFilesTool;\nuse Cognesy\\Agents\\Capability\\File\\UseFileTools;\nuse Cognesy\\Agents\\Capability\\SelfCritique\\UseSelfCritique;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Messages;\n\n// Create console logger - showContinuation reveals self-critique decisions\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,  // Shows SelfCritic criterion in evaluation\n    showToolArgs: true,\n);\n\n// Configure working directory\n$workDir = dirname(__DIR__, 3);\n\n// Build agent with self-critique capability\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseFileTools($workDir))\n    -&gt;withCapability(new UseTools(\n        ListDirTool::inDirectory($workDir),\n        SearchFilesTool::inDirectory($workDir),\n    ))\n    -&gt;withCapability(new UseSelfCritique(\n        maxIterations: 2,  // Allow up to 2 critique iterations\n    ))\n    -&gt;withCapability(new UseGuards(maxSteps: 12, maxTokens: 12288, maxExecutionTime: 90))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Ask a question where the agent might give a superficial answer\n$question = \"What testing framework does this project use? Be specific. Provide fragments of files as evidence.\";\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString($question)\n);\n\necho \"=== Agent Execution Log ===\\n\";\necho \"Question: {$question}\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$answer = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No answer';\necho \"Answer: {$answer}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_structured_output/","title":"Agent with Structured Output Extraction","text":""},{"location":"cookbook/agents/agent_builder/agent_structured_output/#overview","title":"Overview","text":"<p>Demonstrates how agents can extract structured data from unstructured text using the <code>UseStructuredOutputs</code> capability powered by Instructor. This pattern enables:</p> <ul> <li>Form autofill: Extract lead/contact data from pasted text or web content</li> <li>Data transformation: Convert unstructured text into validated PHP objects</li> <li>Multi-step workflows: Chain extraction with API calls using metadata storage</li> <li>Validation with retry: Automatic retry on validation failures</li> </ul> <p>Key concepts: - <code>UseStructuredOutputs</code>: Capability for LLM-powered data extraction - <code>SchemaRegistry</code>: Pre-registered extraction schemas - <code>structured_output</code>: Tool to extract data into schema - <code>AgentEventConsoleObserver</code>: Provides visibility into agent execution stages</p>"},{"location":"cookbook/agents/agent_builder/agent_structured_output/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseTools;\nuse Cognesy\\Agents\\Capability\\Metadata\\UseMetadataTools;\nuse Cognesy\\Agents\\Capability\\StructuredOutput\\SchemaDefinition;\nuse Cognesy\\Agents\\Capability\\StructuredOutput\\StructuredOutputPolicy;\nuse Cognesy\\Agents\\Capability\\StructuredOutput\\UseStructuredOutputs;\nuse Cognesy\\Agents\\Capability\\StructuredOutput\\SchemaRegistry;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Tool\\Tools\\BaseTool;\nuse Cognesy\\Messages\\Messages;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\n// =============================================================================\n// 1. Define the Lead schema (what we want to extract)\n// =============================================================================\n\nclass Lead\n{\n    public function __construct(\n        public string $name = '',\n        #[Assert\\Email(message: 'Invalid email format')]\n        public string $email = '',\n        public ?string $company = null,\n        public ?string $phone = null,\n        public ?string $role = null,\n        public ?string $address = null,\n        public ?string $city = null,\n        public ?string $country = null,\n    ) {}\n}\n\n// =============================================================================\n// 2. Create a Lead API tool (simulates CRM API call with metadata access)\n// =============================================================================\n\nclass CreateLeadTool extends BaseTool\n{\n    public function __construct() {\n        parent::__construct(\n            name: 'create_lead',\n            description: 'Creates a new lead in the CRM system using data from agent metadata.',\n        );\n    }\n\n    #[\\Override]\n    public function __invoke(mixed ...$args): string {\n        $metadataKey = $args['metadata_key'] ?? $args[0] ?? 'current_lead';\n\n        // Read lead data from agent metadata\n        if ($this-&gt;agentState === null) {\n            return 'Error: Agent state not available';\n        }\n\n        $leadData = $this-&gt;agentState-&gt;metadata()-&gt;get($metadataKey);\n\n        if ($leadData === null) {\n            return \"Error: No lead data found at metadata key '{$metadataKey}'\";\n        }\n\n        // Extract lead info for the response\n        $name = match (true) {\n            is_object($leadData) &amp;&amp; property_exists($leadData, 'name') =&gt; $leadData-&gt;name,\n            is_array($leadData) &amp;&amp; isset($leadData['name']) =&gt; $leadData['name'],\n            default =&gt; 'Unknown',\n        };\n\n        $email = match (true) {\n            is_object($leadData) &amp;&amp; property_exists($leadData, 'email') =&gt; $leadData-&gt;email,\n            is_array($leadData) &amp;&amp; isset($leadData['email']) =&gt; $leadData['email'],\n            default =&gt; 'Unknown',\n        };\n\n        // Simulate API call - in real implementation, call actual CRM API\n        $leadId = 'LEAD-' . strtoupper(substr(md5((string) time()), 0, 8));\n\n        return \"Lead created successfully!\\n\" .\n               \"  ID: {$leadId}\\n\" .\n               \"  Name: {$name}\\n\" .\n               \"  Email: {$email}\\n\" .\n               \"  Source: metadata key '{$metadataKey}'\";\n    }\n\n    #[\\Override]\n    public function toToolSchema(): array {\n        return [\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; $this-&gt;name(),\n                'description' =&gt; $this-&gt;description(),\n                'parameters' =&gt; [\n                    'type' =&gt; 'object',\n                    'properties' =&gt; [\n                        'metadata_key' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'The metadata key where lead data is stored (e.g., \"current_lead\")',\n                        ],\n                    ],\n                    'required' =&gt; ['metadata_key'],\n                ],\n            ],\n        ];\n    }\n}\n\n// =============================================================================\n// 3. Build the agent with structured output and API capabilities\n// =============================================================================\n\n// Create console logger for execution visibility\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Register extraction schemas\n$schemas = new SchemaRegistry([\n    'lead' =&gt; new SchemaDefinition(\n        class: Lead::class,\n        description: 'Business lead with contact information',\n        prompt: 'Extract lead information from the text. Look for names, emails, ' .\n                'phone numbers, company names, job titles, and addresses.',\n    ),\n]);\n\n// Build agent\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseStructuredOutputs(\n        schemas: $schemas,\n        policy: new StructuredOutputPolicy(\n            llmPreset: 'openai',\n            defaultMaxRetries: 3,\n        ),\n    ))\n    -&gt;withCapability(new UseMetadataTools())\n    -&gt;withCapability(new UseTools(new CreateLeadTool()))\n    -&gt;withCapability(new UseGuards(maxSteps: 10, maxTokens: 12288, maxExecutionTime: 90))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// =============================================================================\n// 4. Prepare input data (unstructured text with lead information)\n// =============================================================================\n\n$inputText = &lt;&lt;&lt;TEXT\nHey, I just got off the phone with a potential client. Here are the details:\n\nHis name is John Smith and he works as VP of Engineering at TechCorp Industries.\nThey're based in San Francisco, California, USA. You can reach him at\njohn.smith@techcorp.io or call him at +1-555-0123.\n\nHe's interested in our enterprise plan and wants a demo next week.\nTEXT;\n\n// =============================================================================\n// 5. Create task for the agent\n// =============================================================================\n\n$task = &lt;&lt;&lt;TASK\nProcess this lead information and save it to our CRM:\n\n{$inputText}\n\nSteps:\n1. Extract the lead information into structured format using 'lead' schema\n2. Store the extracted data as 'current_lead' in metadata\n3. Call create_lead API with the metadata key 'current_lead'\n\nReport back when complete.\nTASK;\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString($task)\n);\n\n// =============================================================================\n// 6. Execute agent\n// =============================================================================\n\necho \"=== Agent Execution Log ===\\n\";\necho \"Input text:\\n{$inputText}\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\n// =============================================================================\n// 7. Show final results\n// =============================================================================\n\necho \"\\n=== Result ===\\n\";\n\n// Get the extracted lead from metadata\n$extractedLead = $finalState-&gt;metadata()-&gt;get('current_lead');\n$fields = match(true) {\n    is_object($extractedLead) =&gt; get_object_vars($extractedLead),\n    is_array($extractedLead) =&gt; $extractedLead,\n    default =&gt; [],\n};\n\nif ($extractedLead !== null) {\n    echo \"Extracted Lead (from metadata):\\n\";\n    foreach ($fields as $key =&gt; $value) {\n        if ($value !== null &amp;&amp; $value !== '') {\n            echo \"  {$key}: {$value}\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert($extractedLead !== null, 'Expected extracted lead in metadata');\nassert(!empty($fields), 'Expected non-empty lead fields');\nassert(!empty($fields['name'] ?? null), 'Expected lead to have a name');\nassert(!empty($fields['email'] ?? null), 'Expected lead to have an email');\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_builder/agent_subagents/","title":"Agent Subagent Orchestration","text":""},{"location":"cookbook/agents/agent_builder/agent_subagents/#overview","title":"Overview","text":"<p>Subagents enable decomposition of complex tasks into isolated subtasks. The main agent orchestrates multiple subagents, each with specialized roles and tools. This pattern provides:</p> <ul> <li>Context isolation: Each subagent has clean context without cross-contamination</li> <li>Isolated execution: Each subagent runs independently with its own state</li> <li>Specialized capabilities: Each subagent has specific tools for its role</li> <li>Scalability: Handle many independent subtasks without context overflow</li> <li>Result aggregation: Main agent synthesizes subagent outputs</li> </ul> <p>Key concepts: - <code>UseSubagents</code>: Capability that enables subagent spawning - <code>AgentDefinitionRegistry</code>: Registry of available subagent definitions - <code>AgentDefinition</code>: Defines subagent role, tools, and behavior - <code>AgentEventConsoleObserver</code>: Shows parent/child agent IDs for tracking orchestration</p>"},{"location":"cookbook/agents/agent_builder/agent_subagents/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\File\\UseFileTools;\nuse Cognesy\\Agents\\Capability\\Subagent\\UseSubagents;\nuse Cognesy\\Agents\\Collections\\NameList;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Template\\AgentDefinitionRegistry;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Messages\\Messages;\n\n// Create console logger - shows agent IDs for parent/child tracking\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Configure working directory\n$workDir = dirname(__DIR__, 3);\n\n// Create subagent registry\n$registry = new AgentDefinitionRegistry();\n\n// Register code reviewer subagent\n$registry-&gt;register(new AgentDefinition(\n    name: 'reviewer',\n    description: 'Reviews code files and identifies issues',\n    systemPrompt: 'You review code files and identify issues. Read the file and provide a concise assessment focusing on code quality, potential bugs, and improvements.',\n    tools: NameList::fromArray(['read_file']),\n));\n\n// Register documentation generator subagent\n$registry-&gt;register(new AgentDefinition(\n    name: 'documenter',\n    description: 'Generates documentation for code',\n    systemPrompt: 'You generate documentation for code. Read the file and create brief, clear documentation explaining what the code does and how to use it.',\n    tools: NameList::fromArray(['read_file']),\n));\n\n// Build main orchestration agent\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseFileTools($workDir))\n    -&gt;withCapability(new UseSubagents(provider: $registry))\n    -&gt;withCapability(new UseGuards(maxSteps: 10, maxTokens: 12288, maxExecutionTime: 90))\n    -&gt;build()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Task requiring multiple isolated reviews (small files to keep token usage low)\n$task = &lt;&lt;&lt;TASK\nReview these three capability files and provide a summary:\n1. packages/agents/src/Capability/Core/UseTools.php\n2. packages/agents/src/Capability/File/UseFileTools.php\n3. packages/agents/src/Capability/SelfCritique/UseSelfCritique.php\n\nFor each file, spawn a reviewer subagent. Then summarize the findings.\nTASK;\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString($task)\n);\n\necho \"=== Agent Execution Log ===\\n\";\necho \"Task: Review multiple files using subagents\\n\\n\";\n\n// Execute agent until completion\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$summary = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No summary';\necho \"Answer: {$summary}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_basic/","title":"Basic Agent Control Usage","text":""},{"location":"cookbook/agents/agent_controllers/agent_ctrl_basic/#overview","title":"Overview","text":"<p>AgentCtrl provides a unified interface for executing prompts against CLI-based code agents (like Claude Code, OpenCode, Codex, etc.). This example demonstrates the simplest possible usage: sending a prompt and receiving a structured response with metadata.</p> <p>Key concepts: - <code>AgentCtrl::make()</code>: Factory for creating agent instances - <code>AgentType</code>: Enum specifying which CLI agent to use - <code>AgentResponse</code>: Structured response with text, session info, usage stats, and cost - <code>AgentCtrlConsoleLogger</code>: Provides visibility into agent execution stages</p>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_basic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\nuse Cognesy\\AgentCtrl\\Enum\\AgentType;\n\n// Create a console logger for visibility into agent execution\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,\n);\n\n// Execute a prompt against OpenCode agent\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::make(AgentType::OpenCode)\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;execute('Explain the SOLID principles in software design. List each principle with a one-line explanation.');\n\necho \"\\n=== Result ===\\n\";\nif ($response-&gt;isSuccess()) {\n    echo \"Answer: \" . $response-&gt;text() . \"\\n\";\n    echo \"Agent: {$response-&gt;agentType-&gt;value}\\n\";\n\n    if ($response-&gt;sessionId) {\n        echo \"Session: {$response-&gt;sessionId}\\n\";\n    }\n    if ($response-&gt;usage) {\n        echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n    }\n    if ($response-&gt;cost) {\n        echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n    }\n} else {\n    echo \"ERROR: Request failed with exit code {$response-&gt;exitCode}\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_basic/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [opencode] [EXEC] Execution started [prompt=Explain the SOLID principles...]\n14:32:16.456 [opencode] [DONE] Execution completed [exit=0, tools=0, tokens=198]\n\n=== Result ===\nAnswer: The SOLID principles are:\n1. Single Responsibility Principle (SRP): A class should have only one reason to change\n2. Open/Closed Principle (OCP): Open for extension but closed for modification\n3. Liskov Substitution Principle (LSP): Derived classes must be substitutable for base classes\n4. Interface Segregation Principle (ISP): Don't force clients to depend on unused interfaces\n5. Dependency Inversion Principle (DIP): Depend on abstractions, not concretions\nAgent: opencode\nSession: session-abc123\nTokens: 42 in / 156 out\nCost: $0.0023\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_basic/#key-points","title":"Key Points","text":"<ul> <li>Unified interface: Same API works across different CLI agents</li> <li>Agent selection: Use <code>AgentType</code> enum to specify which agent to use</li> <li>Console logger: <code>AgentCtrlConsoleLogger</code> shows execution stages with color-coded labels</li> <li>Response metadata: Access session IDs, token usage, and cost information</li> <li>Error handling: Check <code>isSuccess()</code> before accessing response data</li> <li>Simple execution: One method call (<code>execute()</code>) handles the entire interaction</li> </ul>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_events/","title":"Agent Control Events & Monitoring","text":""},{"location":"cookbook/agents/agent_controllers/agent_ctrl_events/#overview","title":"Overview","text":"<p>AgentCtrl provides a comprehensive event system for monitoring agent execution. Use the built-in <code>AgentCtrlConsoleLogger</code> for formatted output, or attach custom listeners for targeted monitoring. Events fire in real-time during execution.</p> <p>Key concepts: - <code>AgentCtrlConsoleLogger</code>: Built-in wiretap that formats events for console output - <code>wiretap()</code>: Observe ALL events with a single callback - <code>onEvent()</code>: Listen to specific event types (started, completed, text received, etc.) - Event types: <code>AgentExecutionStarted</code>, <code>AgentTextReceived</code>, <code>AgentToolUsed</code>, <code>AgentExecutionCompleted</code>, <code>AgentErrorOccurred</code></p>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_events/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\nuse Cognesy\\AgentCtrl\\Enum\\AgentType;\nuse Cognesy\\AgentCtrl\\Event\\AgentExecutionCompleted;\nuse Cognesy\\AgentCtrl\\Event\\AgentToolUsed;\n\n// 1. Built-in console logger: formatted, color-coded output\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,   // Show tool input args\n    showSandbox: false,   // Hide sandbox setup events\n    showPipeline: false,  // Hide request/response pipeline\n    showStreaming: false,  // Hide stream events\n);\n\n$agent = AgentCtrl::make(AgentType::OpenCode)\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// 2. Targeted listeners: subscribe to specific event types\n$agent-&gt;onEvent(AgentToolUsed::class, function (AgentToolUsed $event) {\n    echo \"\\n  &gt;&gt;&gt; Tool used: {$event-&gt;tool}\\n\\n\";\n});\n\n$agent-&gt;onEvent(AgentExecutionCompleted::class, function (AgentExecutionCompleted $event) {\n    echo \"\\n=== Execution Complete ===\\n\";\n    echo \"  Tools: {$event-&gt;toolCallCount}\\n\";\n    if ($event-&gt;cost !== null) {\n        echo \"  Cost: $\" . number_format($event-&gt;cost, 4) . \"\\n\";\n    }\n    $tokens = ($event-&gt;inputTokens ?? 0) + ($event-&gt;outputTokens ?? 0);\n    if ($tokens &gt; 0) {\n        echo \"  Tokens: {$tokens}\\n\";\n    }\n});\n\n// Run the agent\necho \"=== Agent Execution Log ===\\n\\n\";\n$response = $agent-&gt;executeStreaming('List files in current directory and explain what you see.');\n\necho \"\\n=== Result ===\\n\";\nif ($response-&gt;isSuccess()) {\n    echo \"Answer: \" . $response-&gt;text() . \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_events/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [opencode] [EXEC] Execution started [prompt=List files in current directory...]\n14:32:15.234 [opencode] [PROC] Process started [commands=1]\n14:32:15.456 [opencode] [TEXT] Text received [length=48]\n14:32:16.234 [opencode] [TOOL] bash {command=ls -la}\n\n  &gt;&gt;&gt; Tool used: bash\n\n14:32:16.567 [opencode] [TEXT] Text received [length=156]\n14:32:17.890 [opencode] [DONE] Execution completed [exit=0, tools=1, cost=$0.0034, tokens=198]\n\n=== Execution Complete ===\n  Tools: 1\n  Cost: $0.0034\n  Tokens: 198\n\n=== Result ===\nAnswer: The directory contains several PHP project files including composer.json for\ndependencies, a src/ directory with source code, and configuration files.\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_events/#key-points","title":"Key Points","text":"<ul> <li>Console logger: <code>AgentCtrlConsoleLogger</code> provides clean, color-coded event output with configurable toggles</li> <li>Wiretap pattern: Observe all events with <code>wiretap()</code> for comprehensive logging</li> <li>Targeted listening: Use <code>onEvent()</code> for specific event types when you only care about certain events</li> <li>Composable: Combine the console logger with targeted listeners</li> <li>Real-time monitoring: Events fire as execution progresses, not after completion</li> <li>Rich metadata: Events include timestamps, model info, token usage, costs, and tool details</li> <li>Use cases: Logging, telemetry, progress bars, debugging, analytics</li> </ul>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_streaming/","title":"Agent Control Streaming","text":""},{"location":"cookbook/agents/agent_controllers/agent_ctrl_streaming/#overview","title":"Overview","text":"<p>Streaming execution provides real-time visibility into agent operations. Instead of waiting for completion, you see text output and tool calls as they happen. Combine streaming callbacks with the <code>AgentCtrlConsoleLogger</code> for full execution introspection.</p> <p>Key concepts: - <code>executeStreaming()</code>: Execute with real-time output instead of waiting for completion - <code>onText()</code>: Callback for each text chunk as it arrives - <code>onToolUse()</code>: Callback for each tool call with inputs and outputs - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle alongside streaming output - <code>withMaxTurns()</code>: Limit the number of agent loop iterations</p>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_streaming/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\n\n// Console logger for execution lifecycle visibility\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,\n);\n\n$toolCalls = [];\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::claudeCode()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;withMaxTurns(10)\n    -&gt;onText(function (string $text) {\n        // Stream text as it arrives\n        echo $text;\n    })\n    -&gt;onToolUse(function (string $tool, array $input, ?string $output) use (&amp;$toolCalls) {\n        // Show each tool call as it happens\n        $target = $input['pattern'] ?? $input['file_path'] ?? $input['command'] ?? '';\n        if (strlen($target) &gt; 40) {\n            $target = '...' . substr($target, -37);\n        }\n        $toolCalls[] = $tool;\n        echo \"\\n  &gt;&gt; [{$tool}] {$target}\\n\";\n    })\n    -&gt;executeStreaming('Find the AgentCtrl class and explain the make() factory method. Be concise.');\n\necho \"\\n=== Result ===\\n\";\necho \"Tools used: \" . implode(' &gt; ', $toolCalls) . \"\\n\";\necho \"Total tool calls: \" . count($toolCalls) . \"\\n\";\n\nif ($response-&gt;usage) {\n    echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n}\nif ($response-&gt;cost) {\n    echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_streaming/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [claude-code] [EXEC] Execution started [model=claude-sonnet-4-5-20250514, prompt=Find the AgentCtrl class...]\n14:32:15.234 [claude-code] [PROC] Process started [commands=1]\nI'll search for the AgentCtrl class to understand the factory pattern.\n14:32:15.456 [claude-code] [TOOL] Glob {pattern=src/**/*AgentCtrl*.php}\n\n  &gt;&gt; [Glob] src/**/*AgentCtrl*.php\n\nI found the AgentCtrl class. Let me examine the make() method.\n14:32:16.234 [claude-code] [TOOL] Read {file_path=src/AgentCtrl/AgentCtrl.php}\n\n  &gt;&gt; [Read] src/AgentCtrl/AgentCtrl.php\n\nThe make() factory method provides a clean way to instantiate agents by:\n1. Accepting an AgentType enum to specify which CLI agent to use\n2. Returning a configured AgentCtrl instance\n3. Allowing method chaining for further configuration\n14:32:17.890 [claude-code] [DONE] Execution completed [exit=0, tools=2, cost=$0.0021, tokens=214]\n\n=== Result ===\nTools used: Glob &gt; Read\nTotal tool calls: 2\nTokens: 125 in / 89 out\nCost: $0.0021\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_streaming/#key-points","title":"Key Points","text":"<ul> <li>Real-time output: Text appears as the agent generates it, not after completion</li> <li>Console logger: <code>AgentCtrlConsoleLogger</code> shows execution lifecycle alongside streaming output</li> <li>Tool visibility: See each tool call with its arguments as it executes</li> <li>Progress tracking: Know what the agent is doing at any moment</li> <li>Same response: Final response object has same structure as non-streaming execution</li> <li>Fluent interface: Combine <code>wiretap()</code>, <code>onText()</code>, <code>onToolUse()</code>, and configuration methods</li> <li>Use cases: Progress bars, interactive UIs, debugging, long-running tasks</li> </ul>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_switching/","title":"Agent Control Runtime Switching","text":""},{"location":"cookbook/agents/agent_controllers/agent_ctrl_switching/#overview","title":"Overview","text":"<p>AgentCtrl provides a unified API that works across multiple CLI-based code agents. This enables runtime switching between different backends (Claude Code, OpenCode, Codex, Gemini) without changing your application code. Useful for comparing agent performance, failover scenarios, or A/B testing.</p> <p>Key concepts: - <code>AgentType</code> enum: Specify which agent backend to use - Unified API: Same methods work across all agent types - Runtime selection: Choose agent dynamically based on configuration or logic - <code>AgentCtrlConsoleLogger</code>: Shared logger works across all agent types</p>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_switching/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\nuse Cognesy\\AgentCtrl\\Enum\\AgentType;\n\n// Shared console logger - works across all agent types\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n);\n\n$prompt = 'What design pattern does a class with a static make() method implement? Answer in one sentence.';\n\n// Test the same prompt with multiple agents\n$agents = [\n    'opencode' =&gt; 'OpenCode',\n    'claude-code' =&gt; 'Claude Code',\n    'codex' =&gt; 'Codex',\n];\n\nforeach ($agents as $agentId =&gt; $agentName) {\n    echo \"=== Testing: {$agentName} ===\\n\\n\";\n\n    $startTime = microtime(true);\n\n    try {\n        $builder = AgentCtrl::make(AgentType::from($agentId))\n            -&gt;wiretap($logger-&gt;wiretap());\n\n        // Apply agent-specific configuration\n        if ($agentId === 'claude-code') {\n            $builder-&gt;withMaxTurns(1);\n        }\n\n        $response = $builder-&gt;execute($prompt);\n        $elapsed = round((microtime(true) - $startTime) * 1000);\n\n        echo \"\\n=== Result ({$elapsed}ms) ===\\n\";\n        if ($response-&gt;isSuccess()) {\n            echo \"Answer: \" . $response-&gt;text() . \"\\n\";\n\n            if ($response-&gt;usage) {\n                echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n            }\n            if ($response-&gt;cost) {\n                echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n            }\n        } else {\n            echo \"Failed (exit code: {$response-&gt;exitCode})\\n\";\n        }\n    } catch (Throwable $e) {\n        echo \"Error: {$e-&gt;getMessage()}\\n\";\n    }\n\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_switching/#expected-output","title":"Expected Output","text":"<pre><code>=== Testing: OpenCode ===\n\n14:32:15.123 [opencode] [EXEC] Execution started [prompt=What design pattern does a class...]\n14:32:16.357 [opencode] [DONE] Execution completed [exit=0, tools=0, tokens=62]\n\n=== Result (1234ms) ===\nAnswer: The Factory Method pattern, which uses a static method to create\nand return instances of different subclasses based on parameters.\nTokens: 38 in / 24 out\nCost: $0.0008\n\n=== Testing: Claude Code ===\n\n14:32:17.123 [claude-code] [EXEC] Execution started [prompt=What design pattern does a class...]\n14:32:19.279 [claude-code] [DONE] Execution completed [exit=0, tools=0, cost=$0.0015, tokens=64]\n\n=== Result (2156ms) ===\nAnswer: This is the Factory Method pattern, where a static factory method\ndetermines which concrete class to instantiate based on input.\nTokens: 38 in / 26 out\nCost: $0.0015\n\n=== Testing: Codex ===\n\n14:32:20.123 [codex] [EXEC] Execution started [prompt=What design pattern does a class...]\n14:32:21.110 [codex] [DONE] Execution completed [exit=0, tools=0, tokens=60]\n\n=== Result (987ms) ===\nAnswer: A class using static make() for conditional instantiation typically\nimplements the Factory Method or Static Factory pattern.\nTokens: 38 in / 22 out\nCost: $0.0012\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/agent_ctrl_switching/#key-points","title":"Key Points","text":"<ul> <li>Unified API: Same interface across all agent backends</li> <li>Shared logger: One <code>AgentCtrlConsoleLogger</code> works across all agent types, prefixing output with <code>[opencode]</code>, <code>[claude-code]</code>, etc.</li> <li>Runtime selection: Choose agent dynamically based on requirements</li> <li>Agent comparison: Run the same prompt across multiple agents to compare</li> <li>Failover capability: Try alternative agents if primary fails</li> <li>Agent-specific tuning: Apply configuration based on agent characteristics</li> <li>Performance comparison: Measure response time and cost across agents</li> <li>Use cases: A/B testing, load balancing, fallback strategies, feature parity testing</li> </ul>"},{"location":"cookbook/agents/agent_controllers/claude_code_basic/","title":"Claude Code CLI - Basic","text":""},{"location":"cookbook/agents/agent_controllers/claude_code_basic/#overview","title":"Overview","text":"<p>This example demonstrates how to use the Claude Code CLI integration to execute simple prompts. The <code>AgentCtrl</code> facade provides a clean API for invoking the <code>claude</code> CLI in headless mode with full event observability via <code>AgentCtrlConsoleLogger</code>.</p> <p>Key concepts: - <code>AgentCtrl::claudeCode()</code>: Factory for Claude Code agent builder - <code>withMaxTurns()</code>: Limit agentic loop iterations - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle with color-coded labels</p>"},{"location":"cookbook/agents/agent_controllers/claude_code_basic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\n\n// Create a console logger for visibility into agent execution\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showPipeline: true,  // Show request/response pipeline details\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::claudeCode()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;withMaxTurns(1)\n    -&gt;execute('What is the capital of France? Answer briefly.');\n\necho \"\\n=== Result ===\\n\";\nif ($response-&gt;isSuccess()) {\n    echo \"Answer: \" . $response-&gt;text() . \"\\n\";\n\n    if ($response-&gt;sessionId) {\n        echo \"Session: {$response-&gt;sessionId}\\n\";\n    }\n    if ($response-&gt;usage) {\n        echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n    }\n    if ($response-&gt;cost) {\n        echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n    }\n} else {\n    echo \"Error: Command failed with exit code {$response-&gt;exitCode}\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/claude_code_basic/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [claude-code] [EXEC] Execution started [prompt=What is the capital of France? Answer briefly.]\n14:32:15.124 [claude-code] [REQT] Request built [type=ClaudeRequest, duration=0ms]\n14:32:15.125 [claude-code] [CMD ] Command spec created [args=8, duration=0ms]\n14:32:15.126 [claude-code] [SBOX] Policy configured [driver=host, timeout=120s, network=on]\n14:32:15.127 [claude-code] [SBOX] Ready [driver=host, setup=1ms]\n14:32:15.128 [claude-code] [PROC] Process started [commands=8]\n14:32:16.234 [claude-code] [RESP] Parsing started [format=stream-json, size=456]\n14:32:16.235 [claude-code] [RESP] Data extracted [events=3, tools=0, text=42 chars, duration=1ms]\n14:32:16.236 [claude-code] [RESP] Parsing completed [duration=2ms]\n14:32:16.237 [claude-code] [DONE] Execution completed [exit=0, tools=0]\n\n=== Result ===\nAnswer: The capital of France is Paris.\nSession: session-abc123\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/claude_code_basic/#key-points","title":"Key Points","text":"<ul> <li>Simple execution: One method call handles the entire interaction</li> <li>Full observability: Console logger shows request building, sandbox setup, and response parsing</li> <li>Pipeline visibility: Enable <code>showPipeline</code> to see request/response internals</li> </ul>"},{"location":"cookbook/agents/agent_controllers/claude_code_search/","title":"Claude Code CLI - Agentic Search","text":""},{"location":"cookbook/agents/agent_controllers/claude_code_search/#overview","title":"Overview","text":"<p>This example demonstrates the agentic capabilities of Claude Code CLI by having it search through the codebase to find and explain validation examples. The <code>AgentCtrlConsoleLogger</code> provides full visibility into tool calls, streaming, and the agent's decision-making process.</p> <p>Key concepts: - <code>AgentCtrl::claudeCode()</code>: Factory for Claude Code agent builder - <code>withMaxTurns()</code>: Allow multiple turns for exploration - <code>onText()</code> / <code>onToolUse()</code>: Real-time streaming callbacks - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle alongside streaming output</p>"},{"location":"cookbook/agents/agent_controllers/claude_code_search/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\n\n// Console logger for execution lifecycle visibility\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,   // Show tool input args\n    showSandbox: true,    // Show sandbox setup\n);\n\n$toolCalls = [];\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::claudeCode()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;withMaxTurns(10)\n    -&gt;onText(function (string $text) {\n        echo $text;\n    })\n    -&gt;onToolUse(function (string $tool, array $input, ?string $output) use (&amp;$toolCalls) {\n        $target = $input['pattern'] ?? $input['file_path'] ?? $input['command'] ?? '';\n        if (strlen($target) &gt; 50) {\n            $target = '...' . substr($target, -47);\n        }\n        $toolCalls[] = $tool;\n        echo \"\\n  &gt;&gt; [{$tool}] {$target}\\n\";\n    })\n    -&gt;executeStreaming(&lt;&lt;&lt;'PROMPT'\nComplete this task in steps:\n1. Use Glob or find command to locate PHP files with \"validation\" in the filename under ./examples\n2. Read the contents of relevant PHP file\n3. Analyze the code and provide a concise explanation (under 200 words) covering:\n   - What validation is being performed\n   - What validation constraints/attributes are used\n   - How validation is triggered\n   - What happens when validation fails\n\nProvide your final explanation as a clear, structured response.\nPROMPT);\n\necho \"\\n=== Result ===\\n\";\necho \"Tools used: \" . implode(' &gt; ', $toolCalls) . \"\\n\";\necho \"Total tool calls: \" . count($toolCalls) . \"\\n\";\necho \"Exit code: {$response-&gt;exitCode}\\n\";\n\nif ($response-&gt;usage) {\n    echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n}\nif ($response-&gt;cost) {\n    echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n}\n\nif (!$response-&gt;isSuccess()) {\n    echo \"Error: Agent search failed with exit code {$response-&gt;exitCode}\\n\";\n    exit(1);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/claude_code_search/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [claude-code] [EXEC] Execution started [prompt=Complete this task in steps:...]\n14:32:15.234 [claude-code] [SBOX] Policy configured [driver=host, timeout=120s, network=on]\n14:32:15.235 [claude-code] [SBOX] Ready [driver=host, setup=1ms]\n14:32:15.236 [claude-code] [PROC] Process started [commands=12]\nI'll search for PHP files with \"validation\" in the filename under ./examples.\n14:32:16.456 [claude-code] [TOOL] Glob {pattern=./examples/**/*validation*.php}\n\n  &gt;&gt; [Glob] ./examples/**/*validation*.php\n\nFound a validation example. Let me read it.\n14:32:17.234 [claude-code] [TOOL] Read {file_path=./examples/A01_Basics/Validation/run.php}\n\n  &gt;&gt; [Read] ./examples/A01_Basics/Validation/run.php\n\nThe validation example demonstrates...\n14:32:19.890 [claude-code] [DONE] Execution completed [exit=0, tools=2]\n\n=== Result ===\nTools used: Glob &gt; Read\nTotal tool calls: 2\nExit code: 0\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/claude_code_search/#key-points","title":"Key Points","text":"<ul> <li>Agentic search: Agent autonomously explores files and synthesizes answers</li> <li>Full visibility: Console logger shows every tool call alongside streaming output</li> <li>Sandbox awareness: Enable <code>showSandbox</code> to see sandbox initialization details</li> <li>Multi-turn: <code>withMaxTurns(10)</code> allows the agent to explore iteratively</li> </ul>"},{"location":"cookbook/agents/agent_controllers/codex_basic/","title":"OpenAI Codex CLI - Basic","text":""},{"location":"cookbook/agents/agent_controllers/codex_basic/#overview","title":"Overview","text":"<p>This example demonstrates how to use the OpenAI Codex CLI integration to execute simple prompts. The <code>AgentCtrl</code> facade provides a clean API for invoking the <code>codex exec</code> command with full event observability.</p> <p>Key concepts: - <code>AgentCtrl::codex()</code>: Factory for Codex agent builder - <code>withSandbox()</code>: Configure sandbox mode for file/network access - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle with color-coded labels</p>"},{"location":"cookbook/agents/agent_controllers/codex_basic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\nuse Cognesy\\AgentCtrl\\OpenAICodex\\Domain\\Enum\\SandboxMode;\n\n// Create a console logger for visibility into agent execution\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showPipeline: true,  // Show request/response pipeline details\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::codex()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;withSandbox(SandboxMode::ReadOnly)\n    -&gt;execute('What is the capital of France? Answer briefly.');\n\necho \"\\n=== Result ===\\n\";\nif ($response-&gt;isSuccess()) {\n    echo \"Answer: \" . $response-&gt;text() . \"\\n\";\n\n    if ($response-&gt;sessionId) {\n        echo \"Thread ID: {$response-&gt;sessionId}\\n\";\n    }\n    if ($response-&gt;usage) {\n        echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n    }\n} else {\n    echo \"Error: Command failed with exit code {$response-&gt;exitCode}\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/codex_basic/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [codex] [EXEC] Execution started [prompt=What is the capital of France? Answer briefly.]\n14:32:15.124 [codex] [REQT] Request built [type=CodexRequest, duration=0ms]\n14:32:15.125 [codex] [CMD ] Command spec created [args=6, duration=0ms]\n14:32:15.126 [codex] [SBOX] Policy configured [driver=host, timeout=120s, network=on]\n14:32:15.127 [codex] [SBOX] Ready [driver=host, setup=1ms]\n14:32:15.128 [codex] [PROC] Process started [commands=6]\n14:32:16.234 [codex] [RESP] Parsing started [format=json, size=312]\n14:32:16.235 [codex] [RESP] Data extracted [events=2, tools=0, text=32 chars, duration=1ms]\n14:32:16.236 [codex] [RESP] Parsing completed [duration=2ms, session=thread-abc123]\n14:32:16.237 [codex] [DONE] Execution completed [exit=0, tools=0, tokens=62]\n\n=== Result ===\nAnswer: The capital of France is Paris.\nThread ID: thread-abc123\nTokens: 38 in / 24 out\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/codex_basic/#key-points","title":"Key Points","text":"<ul> <li>Simple execution: One method call handles the entire interaction</li> <li>Full observability: Console logger shows request building, sandbox, and response parsing</li> <li>Sandbox control: Use <code>withSandbox()</code> for file/network access control</li> </ul>"},{"location":"cookbook/agents/agent_controllers/codex_streaming/","title":"OpenAI Codex CLI - Streaming","text":""},{"location":"cookbook/agents/agent_controllers/codex_streaming/#overview","title":"Overview","text":"<p>This example demonstrates real-time streaming output from the Codex CLI. Text and tool calls are displayed as they arrive, with <code>AgentCtrlConsoleLogger</code> providing execution lifecycle visibility alongside the streaming output.</p> <p>Key concepts: - <code>executeStreaming()</code>: Execute with real-time output - <code>onText()</code> / <code>onToolUse()</code>: Callbacks for streaming events - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle alongside streaming - <code>inDirectory()</code>: Set working directory for sandbox access</p>"},{"location":"cookbook/agents/agent_controllers/codex_streaming/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\nuse Cognesy\\AgentCtrl\\OpenAICodex\\Domain\\Enum\\SandboxMode;\n\n// Console logger for execution lifecycle visibility\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,\n);\n\n$toolCalls = [];\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::codex()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;withSandbox(SandboxMode::ReadOnly)\n    -&gt;inDirectory(getcwd())\n    -&gt;onText(function (string $text) {\n        echo $text;\n    })\n    -&gt;onToolUse(function (string $tool, array $input, ?string $output) use (&amp;$toolCalls) {\n        $target = $input['command'] ?? $input['pattern'] ?? '';\n        if (strlen($target) &gt; 40) {\n            $target = '...' . substr($target, -37);\n        }\n        $toolCalls[] = $tool;\n        echo \"\\n  &gt;&gt; [{$tool}] {$target}\\n\";\n    })\n    -&gt;executeStreaming('List the files in the current directory and explain what you see.');\n\necho \"\\n=== Result ===\\n\";\necho \"Tools used: \" . implode(' &gt; ', $toolCalls) . \"\\n\";\necho \"Total tool calls: \" . count($toolCalls) . \"\\n\";\n\nif ($response-&gt;usage) {\n    echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n}\nif ($response-&gt;cost) {\n    echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n}\n\nif (!$response-&gt;isSuccess()) {\n    echo \"Error: Command failed with exit code {$response-&gt;exitCode}\\n\";\n    exit(1);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/codex_streaming/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [codex] [EXEC] Execution started [prompt=List the files in the current directory...]\n14:32:15.234 [codex] [PROC] Process started [commands=8]\nI'll list the files in the current directory.\n14:32:16.234 [codex] [TOOL] bash {command=ls -la}\n\n  &gt;&gt; [bash] ls -la\n\nThe directory contains several PHP project files including:\n- composer.json for dependency management\n- src/ directory with source code\n14:32:17.890 [codex] [DONE] Execution completed [exit=0, tools=1, tokens=98]\n\n=== Result ===\nTools used: bash\nTotal tool calls: 1\nTokens: 42 in / 56 out\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/codex_streaming/#key-points","title":"Key Points","text":"<ul> <li>Real-time output: Text appears as the agent generates it</li> <li>Tool visibility: See each tool call with arguments as it executes</li> <li>Console logger: Execution lifecycle events interleaved with streaming output</li> <li>Working directory: Use <code>inDirectory()</code> to set the sandbox working directory</li> </ul>"},{"location":"cookbook/agents/agent_controllers/opencode_basic/","title":"OpenCode CLI - Basic","text":""},{"location":"cookbook/agents/agent_controllers/opencode_basic/#overview","title":"Overview","text":"<p>This example demonstrates how to use the OpenCode CLI integration to execute simple prompts. The <code>AgentCtrl</code> facade provides a clean API for invoking the <code>opencode run</code> command with full event observability.</p> <p>Key concepts: - <code>AgentCtrl::openCode()</code>: Factory for OpenCode agent builder - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle with color-coded labels - <code>AgentResponse</code>: Structured response with text, session info, usage stats, and cost</p>"},{"location":"cookbook/agents/agent_controllers/opencode_basic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\n\n// Create a console logger for visibility into agent execution\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showPipeline: true,  // Show request/response pipeline details\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::openCode()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;execute('What is the capital of France? Answer briefly.');\n\necho \"\\n=== Result ===\\n\";\nif ($response-&gt;isSuccess()) {\n    echo \"Answer: \" . $response-&gt;text() . \"\\n\";\n\n    if ($response-&gt;sessionId) {\n        echo \"Session ID: {$response-&gt;sessionId}\\n\";\n    }\n    if ($response-&gt;usage) {\n        echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n    }\n    if ($response-&gt;cost) {\n        echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n    }\n} else {\n    echo \"Error: Command failed with exit code {$response-&gt;exitCode}\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/opencode_basic/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [opencode] [EXEC] Execution started [prompt=What is the capital of France? Answer briefly.]\n14:32:15.124 [opencode] [REQT] Request built [type=OpenCodeRequest, duration=0ms]\n14:32:15.125 [opencode] [CMD ] Command spec created [args=5, duration=0ms]\n14:32:15.126 [opencode] [SBOX] Policy configured [driver=host, timeout=120s, network=on]\n14:32:15.127 [opencode] [SBOX] Ready [driver=host, setup=1ms]\n14:32:15.128 [opencode] [PROC] Process started [commands=5]\n14:32:16.234 [opencode] [RESP] Parsing started [format=json, size=289]\n14:32:16.235 [opencode] [RESP] Data extracted [events=2, tools=0, text=32 chars, duration=1ms]\n14:32:16.236 [opencode] [RESP] Parsing completed [duration=2ms, session=session-abc123]\n14:32:16.237 [opencode] [DONE] Execution completed [exit=0, tools=0, cost=$0.0008, tokens=62]\n\n=== Result ===\nAnswer: The capital of France is Paris.\nSession ID: session-abc123\nTokens: 38 in / 24 out\nCost: $0.0008\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/opencode_basic/#key-points","title":"Key Points","text":"<ul> <li>Simple execution: One method call handles the entire interaction</li> <li>Full observability: Console logger shows request building, sandbox, and response parsing</li> <li>Cost tracking: OpenCode exposes cost information in the response</li> </ul>"},{"location":"cookbook/agents/agent_controllers/opencode_streaming/","title":"OpenCode CLI - Streaming","text":""},{"location":"cookbook/agents/agent_controllers/opencode_streaming/#overview","title":"Overview","text":"<p>This example demonstrates real-time streaming output from the OpenCode CLI. Text and tool calls are displayed as they arrive, with <code>AgentCtrlConsoleLogger</code> providing execution lifecycle visibility alongside the streaming output.</p> <p>Key concepts: - <code>executeStreaming()</code>: Execute with real-time output - <code>onText()</code> / <code>onToolUse()</code>: Callbacks for streaming events - <code>AgentCtrlConsoleLogger</code>: Shows execution lifecycle alongside streaming - OpenCode exposes cost and session information</p>"},{"location":"cookbook/agents/agent_controllers/opencode_streaming/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\AgentCtrl\\AgentCtrl;\nuse Cognesy\\AgentCtrl\\Broadcasting\\AgentCtrlConsoleLogger;\n\n// Console logger for execution lifecycle visibility\n$logger = new AgentCtrlConsoleLogger(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,\n);\n\n$toolCalls = [];\n\necho \"=== Agent Execution Log ===\\n\\n\";\n\n$response = AgentCtrl::openCode()\n    -&gt;wiretap($logger-&gt;wiretap())\n    -&gt;onText(function (string $text) {\n        echo $text;\n    })\n    -&gt;onToolUse(function (string $tool, array $input, ?string $output) use (&amp;$toolCalls) {\n        $target = $input['command'] ?? $input['file_path'] ?? $input['pattern'] ?? '';\n        if (strlen($target) &gt; 40) {\n            $target = '...' . substr($target, -37);\n        }\n        $toolCalls[] = $tool;\n        echo \"\\n  &gt;&gt; [{$tool}] {$target}\\n\";\n    })\n    -&gt;executeStreaming('Read the first 5 lines of composer.json in this directory and describe what you see.');\n\necho \"\\n=== Result ===\\n\";\necho \"Tools used: \" . implode(' &gt; ', $toolCalls) . \"\\n\";\necho \"Total tool calls: \" . count($toolCalls) . \"\\n\";\n\nif ($response-&gt;usage) {\n    echo \"Tokens: {$response-&gt;usage-&gt;input} in / {$response-&gt;usage-&gt;output} out\\n\";\n}\nif ($response-&gt;cost) {\n    echo \"Cost: $\" . number_format($response-&gt;cost, 4) . \"\\n\";\n}\n\nif (!$response-&gt;isSuccess()) {\n    echo \"Error: Command failed with exit code {$response-&gt;exitCode}\\n\";\n    exit(1);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/opencode_streaming/#expected-output","title":"Expected Output","text":"<pre><code>=== Agent Execution Log ===\n\n14:32:15.123 [opencode] [EXEC] Execution started [prompt=Read the first 5 lines of composer.json...]\n14:32:15.234 [opencode] [PROC] Process started [commands=5]\nI'll read the first 5 lines of composer.json.\n14:32:16.234 [opencode] [TOOL] Read {file_path=composer.json}\n\n  &gt;&gt; [Read] composer.json\n\nThe first 5 lines of composer.json show:\n- The project name and description\n- The license type\n- Autoload configuration\n14:32:17.890 [opencode] [DONE] Execution completed [exit=0, tools=1, cost=$0.0012, tokens=98]\n\n=== Result ===\nTools used: Read\nTotal tool calls: 1\nTokens: 42 in / 56 out\nCost: $0.0012\n</code></pre>"},{"location":"cookbook/agents/agent_controllers/opencode_streaming/#key-points","title":"Key Points","text":"<ul> <li>Real-time output: Text appears as the agent generates it</li> <li>Tool visibility: See each tool call with arguments as it executes</li> <li>Console logger: Execution lifecycle events interleaved with streaming output</li> <li>Cost visibility: OpenCode provides cost tracking in the response</li> </ul>"},{"location":"cookbook/agents/agent_sessions/session_conflict_handling/","title":"Session Conflict Handling","text":""},{"location":"cookbook/agents/agent_sessions/session_conflict_handling/#overview","title":"Overview","text":"<p>Conflicts are explicit exceptions. This example simulates stale write conflict.</p>"},{"location":"cookbook/agents/agent_sessions/session_conflict_handling/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Session\\Exceptions\\SessionConflictException;\nuse Cognesy\\Agents\\Session\\SessionFactory;\nuse Cognesy\\Agents\\Session\\SessionId;\nuse Cognesy\\Agents\\Session\\SessionRepository;\nuse Cognesy\\Agents\\Session\\Store\\InMemorySessionStore;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\n\n$factory = new SessionFactory(new DefinitionStateFactory());\n$repo = new SessionRepository(new InMemorySessionStore());\n\n$created = $repo-&gt;create($factory-&gt;create(new AgentDefinition(\n    name: 'conflict-agent',\n    description: 'Conflict demo',\n    systemPrompt: 'You are helpful.',\n)));\n\n$sessionId = SessionId::from($created-&gt;sessionId());\n$copyA = $repo-&gt;load($sessionId);\n$copyB = $repo-&gt;load($sessionId);\n\nassert($copyA !== null &amp;&amp; $copyB !== null);\n\n$repo-&gt;save($copyA-&gt;withState($copyA-&gt;state()-&gt;withMetadata('writer', 'A')));\n\ntry {\n    $repo-&gt;save($copyB-&gt;withState($copyB-&gt;state()-&gt;withMetadata('writer', 'B')));\n    throw new RuntimeException('Expected conflict was not thrown');\n} catch (SessionConflictException $e) {\n    echo \"=== Result ===\\n\";\n    echo 'Conflict detected as expected: ' . $e-&gt;getMessage() . \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_sessions/session_create_and_persist/","title":"Session Create and Persist","text":""},{"location":"cookbook/agents/agent_sessions/session_create_and_persist/#overview","title":"Overview","text":"<p>Create a session from <code>AgentDefinition</code>, persist it, then load and save updated state.</p>"},{"location":"cookbook/agents/agent_sessions/session_create_and_persist/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Session\\SessionFactory;\nuse Cognesy\\Agents\\Session\\SessionRepository;\nuse Cognesy\\Agents\\Session\\Store\\InMemorySessionStore;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\n\n$factory = new SessionFactory(new DefinitionStateFactory());\n$repo = new SessionRepository(new InMemorySessionStore());\n\n$definition = new AgentDefinition(\n    name: 'session-agent',\n    description: 'Session persistence demo',\n    systemPrompt: 'You are persistent.',\n);\n\n$session = $factory-&gt;create($definition, AgentState::empty()-&gt;withUserMessage('hello'));\n$created = $repo-&gt;create($session);\n$loaded = $repo-&gt;load(\\Cognesy\\Agents\\Session\\SessionId::from($created-&gt;sessionId()));\n\n$updated = $repo-&gt;save($created-&gt;withState($created-&gt;state()-&gt;withMetadata('phase', 'saved')));\n\necho \"=== Result ===\\n\";\necho \"Session ID: {$created-&gt;sessionId()}\\n\";\necho \"Version after create: {$created-&gt;version()}\\n\";\necho \"Version after save: {$updated-&gt;version()}\\n\";\necho 'Metadata phase: ' . ($updated-&gt;state()-&gt;metadata()-&gt;get('phase') ?? 'missing') . \"\\n\";\n\nassert($loaded !== null);\nassert($created-&gt;version() === 1);\nassert($updated-&gt;version() === 2);\nassert($updated-&gt;state()-&gt;metadata()-&gt;get('phase') === 'saved');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_sessions/session_fork_action/","title":"Session Fork Action","text":""},{"location":"cookbook/agents/agent_sessions/session_fork_action/#overview","title":"Overview","text":"<p>Fork an existing session into a new one, then continue each branch independently.</p>"},{"location":"cookbook/agents/agent_sessions/session_fork_action/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Session\\Actions\\ForkSession;\nuse Cognesy\\Agents\\Session\\Actions\\SendMessage;\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Session\\SessionFactory;\nuse Cognesy\\Agents\\Session\\SessionId;\nuse Cognesy\\Agents\\Session\\SessionRepository;\nuse Cognesy\\Agents\\Session\\SessionRuntime;\nuse Cognesy\\Agents\\Session\\Store\\InMemorySessionStore;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Contracts\\CanInstantiateAgentLoop;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\n\n$capabilities = new AgentCapabilityRegistry();\n$loopFactory = new DefinitionLoopFactory($capabilities);\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true, showContinuation: true);\n$loopFactoryWithLogger = new class($loopFactory, $logger) implements CanInstantiateAgentLoop {\n    public function __construct(\n        private readonly DefinitionLoopFactory $factory,\n        private readonly AgentEventConsoleObserver $logger,\n    ) {}\n\n    public function instantiateAgentLoop(AgentDefinition $definition): \\Cognesy\\Agents\\CanControlAgentLoop {\n        echo \"[runtime] Rebuilding agent loop from saved definition: {$definition-&gt;name}\\n\";\n        return $this-&gt;factory-&gt;instantiateAgentLoop($definition)-&gt;wiretap($this-&gt;logger-&gt;wiretap());\n    }\n};\n\n$factory = new SessionFactory(new DefinitionStateFactory());\n$repo = new SessionRepository(new InMemorySessionStore());\n$runtime = new SessionRuntime($repo, new EventDispatcher('session-runtime-example'));\n\n$parent = $repo-&gt;create($factory-&gt;create(new AgentDefinition(\n    name: 'parent-agent',\n    description: 'Travel planner session',\n    systemPrompt: 'You are a travel planner. Answer in one short sentence.',\n    llmConfig: 'openai',\n)));\n$parentId = SessionId::from($parent-&gt;sessionId());\necho \"=== Agent Execution Log ===\\n\\n\";\necho \"[runtime] Seed parent session {$parentId-&gt;toString()}\\n\";\n$parentWithContext = $runtime-&gt;execute(\n    $parentId,\n    new SendMessage('Suggest 3 attractions in Paris.', $loopFactoryWithLogger),\n);\n\n$forkedId = SessionId::from('forked-session-demo');\n$forked = (new ForkSession($forkedId))-&gt;executeOn($parentWithContext);\n$storedFork = $repo-&gt;create($forked);\necho \"[runtime] Fork created {$storedFork-&gt;sessionId()} from parent {$parentId-&gt;toString()}\\n\";\n\n$parentBranch = $runtime-&gt;execute(\n    $parentId,\n    new SendMessage('Now add one low-budget food recommendation.', $loopFactoryWithLogger),\n);\n$forkBranch = $runtime-&gt;execute(\n    $forkedId,\n    new SendMessage('Now add one luxury dining recommendation.', $loopFactoryWithLogger),\n);\n\necho \"\\n=== Result ===\\n\";\necho 'Parent session: ' . $parentId-&gt;toString() . \"\\n\";\necho 'Forked session: ' . $storedFork-&gt;sessionId() . \"\\n\";\necho 'Fork parent ID: ' . ($storedFork-&gt;info()-&gt;parentId() ?? 'none') . \"\\n\";\necho \"\\nParent branch last response:\\n\";\necho ($parentBranch-&gt;state()-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\necho \"\\nFork branch last response:\\n\";\necho ($forkBranch-&gt;state()-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\necho \"\\nParent transcript:\\n\";\necho $parentBranch-&gt;state()-&gt;messages()-&gt;toString() . \"\\n\";\necho \"\\nFork transcript:\\n\";\necho $forkBranch-&gt;state()-&gt;messages()-&gt;toString() . \"\\n\";\n\nassert($storedFork-&gt;sessionId() === 'forked-session-demo');\nassert($storedFork-&gt;info()-&gt;parentId() === $parentId-&gt;toString());\nassert($parentBranch-&gt;state()-&gt;messages()-&gt;count() &gt;= 4);\nassert($forkBranch-&gt;state()-&gt;messages()-&gt;count() &gt;= 4);\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_sessions/session_runtime_execute_action/","title":"SessionRuntime Execute Action","text":""},{"location":"cookbook/agents/agent_sessions/session_runtime_execute_action/#overview","title":"Overview","text":"<p>Run simple lifecycle actions through <code>SessionRuntime::execute()</code>.</p>"},{"location":"cookbook/agents/agent_sessions/session_runtime_execute_action/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Session\\Actions\\ResumeSession;\nuse Cognesy\\Agents\\Session\\Actions\\SuspendSession;\nuse Cognesy\\Agents\\Session\\SessionFactory;\nuse Cognesy\\Agents\\Session\\SessionId;\nuse Cognesy\\Agents\\Session\\SessionRepository;\nuse Cognesy\\Agents\\Session\\SessionRuntime;\nuse Cognesy\\Agents\\Session\\Store\\InMemorySessionStore;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\n\n$factory = new SessionFactory(new DefinitionStateFactory());\n$repo = new SessionRepository(new InMemorySessionStore());\n$runtime = new SessionRuntime($repo, new EventDispatcher('session-runtime-example'));\n\n$created = $repo-&gt;create($factory-&gt;create(new AgentDefinition(\n    name: 'runtime-agent',\n    description: 'Runtime action demo',\n    systemPrompt: 'You are helpful.',\n)));\n\n$sessionId = SessionId::from($created-&gt;sessionId());\n$suspended = $runtime-&gt;execute($sessionId, new SuspendSession());\n$resumed = $runtime-&gt;execute($sessionId, new ResumeSession());\n\necho \"=== Result ===\\n\";\necho 'Initial status: ' . $created-&gt;status()-&gt;value . \"\\n\";\necho 'After suspend: ' . $suspended-&gt;status()-&gt;value . \"\\n\";\necho 'After resume: ' . $resumed-&gt;status()-&gt;value . \"\\n\";\necho 'Current version: ' . $resumed-&gt;version() . \"\\n\";\n\nassert($suspended-&gt;status()-&gt;value === 'suspended');\nassert($resumed-&gt;status()-&gt;value === 'active');\nassert($resumed-&gt;version() === 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_sessions/session_runtime_read_apis/","title":"SessionRuntime Read APIs","text":""},{"location":"cookbook/agents/agent_sessions/session_runtime_read_apis/#overview","title":"Overview","text":"<p>Use runtime read APIs: <code>getSession</code>, <code>getSessionInfo</code>, and <code>listSessions</code>.</p>"},{"location":"cookbook/agents/agent_sessions/session_runtime_read_apis/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Session\\SessionFactory;\nuse Cognesy\\Agents\\Session\\SessionId;\nuse Cognesy\\Agents\\Session\\SessionRepository;\nuse Cognesy\\Agents\\Session\\SessionRuntime;\nuse Cognesy\\Agents\\Session\\Store\\InMemorySessionStore;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\n\n$factory = new SessionFactory(new DefinitionStateFactory());\n$repo = new SessionRepository(new InMemorySessionStore());\n$runtime = new SessionRuntime($repo, new EventDispatcher('session-runtime-example'));\n\n$one = $repo-&gt;create($factory-&gt;create(new AgentDefinition('agent-one', 'first', 'You are one.')));\n$two = $repo-&gt;create($factory-&gt;create(new AgentDefinition('agent-two', 'second', 'You are two.')));\n\n$sessionId = SessionId::from($one-&gt;sessionId());\n$session = $runtime-&gt;getSession($sessionId);\n$info = $runtime-&gt;getSessionInfo($sessionId);\n$list = $runtime-&gt;listSessions();\n\necho \"=== Result ===\\n\";\necho 'Loaded session: ' . $session-&gt;sessionId() . \"\\n\";\necho 'Session info status: ' . $info-&gt;status()-&gt;value . \"\\n\";\necho 'List count: ' . $list-&gt;count() . \"\\n\";\n\nassert($session-&gt;sessionId() === $one-&gt;sessionId());\nassert($info-&gt;sessionId() === $one-&gt;sessionId());\nassert($list-&gt;count() === 2);\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_sessions/session_send_message_action/","title":"Session SendMessage Action","text":""},{"location":"cookbook/agents/agent_sessions/session_send_message_action/#overview","title":"Overview","text":"<p>Use <code>SendMessage</code> action to wake up the agent loop from persisted session state, execute turns, and persist updated state between wake-ups.</p>"},{"location":"cookbook/agents/agent_sessions/session_send_message_action/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Session\\Actions\\SendMessage;\nuse Cognesy\\Agents\\Session\\SessionFactory;\nuse Cognesy\\Agents\\Session\\SessionId;\nuse Cognesy\\Agents\\Session\\SessionRepository;\nuse Cognesy\\Agents\\Session\\SessionRuntime;\nuse Cognesy\\Agents\\Session\\Store\\InMemorySessionStore;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\n\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true, showContinuation: true);\n\n$capabilities = new AgentCapabilityRegistry();\n$loopFactory = new DefinitionLoopFactory($capabilities);\n\n$factory = new SessionFactory(new DefinitionStateFactory());\n$repo = new SessionRepository(new InMemorySessionStore());\n$runtime = new SessionRuntime($repo, new EventDispatcher('session-runtime-example'));\n\n$created = $repo-&gt;create($factory-&gt;create(new AgentDefinition(\n    name: 'message-agent',\n    description: 'Executes SendMessage action.',\n    systemPrompt: 'You are a geography assistant. Answer in one short sentence.',\n    llmConfig: 'openai',\n)));\n\n$sessionId = SessionId::from($created-&gt;sessionId());\n$loopFactoryWithLogger = new class($loopFactory, $logger) implements \\Cognesy\\Agents\\Template\\Contracts\\CanInstantiateAgentLoop {\n    public function __construct(\n        private readonly DefinitionLoopFactory $factory,\n        private readonly AgentEventConsoleObserver $logger,\n    ) {}\n\n    public function instantiateAgentLoop(AgentDefinition $definition): \\Cognesy\\Agents\\CanControlAgentLoop {\n        echo \"[runtime] Rebuilding agent loop from saved definition: {$definition-&gt;name}\\n\";\n        return $this-&gt;factory-&gt;instantiateAgentLoop($definition)-&gt;wiretap($this-&gt;logger-&gt;wiretap());\n    }\n};\n\necho \"=== Agent Execution Log ===\\n\\n\";\necho \"[runtime] Wake-up #1: loading persisted session {$sessionId-&gt;toString()}\\n\";\n$beforeFirstWakeUp = $runtime-&gt;getSession($sessionId);\necho \"[runtime] Wake-up #1: loaded version {$beforeFirstWakeUp-&gt;version()}, messages={$beforeFirstWakeUp-&gt;state()-&gt;messages()-&gt;count()}\\n\";\n$afterFirstWakeUp = $runtime-&gt;execute(\n    $sessionId,\n    new SendMessage('What is the capital of France?', $loopFactoryWithLogger),\n);\n\necho \"[runtime] Wake-up #2: loading persisted session {$sessionId-&gt;toString()}\\n\";\n$beforeSecondWakeUp = $runtime-&gt;getSession($sessionId);\necho \"[runtime] Wake-up #2: loaded version {$beforeSecondWakeUp-&gt;version()}, messages={$beforeSecondWakeUp-&gt;state()-&gt;messages()-&gt;count()}\\n\";\n$afterSecondWakeUp = $runtime-&gt;execute(\n    $sessionId,\n    new SendMessage('What is the closest major river to that city?', $loopFactoryWithLogger),\n);\n\necho \"\\n=== Result ===\\n\";\necho 'Version after first wake-up: ' . $afterFirstWakeUp-&gt;version() . \"\\n\";\necho 'Version after second wake-up: ' . $afterSecondWakeUp-&gt;version() . \"\\n\";\necho 'Conversation messages count: ' . $afterSecondWakeUp-&gt;state()-&gt;messages()-&gt;count() . \"\\n\";\necho 'Last response: ' . ($afterSecondWakeUp-&gt;state()-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\necho \"\\nConversation transcript:\\n\";\necho $afterSecondWakeUp-&gt;state()-&gt;messages()-&gt;toString() . \"\\n\";\n\nassert($afterFirstWakeUp-&gt;version() === 2);\nassert($afterSecondWakeUp-&gt;version() === 3);\nassert($afterSecondWakeUp-&gt;state()-&gt;messages()-&gt;count() &gt;= 2);\nassert($afterSecondWakeUp-&gt;state()-&gt;messages()-&gt;toString() !== '');\nassert(str_contains($afterSecondWakeUp-&gt;state()-&gt;messages()-&gt;toString(), 'What is the capital of France?'));\nassert(str_contains($afterSecondWakeUp-&gt;state()-&gt;messages()-&gt;toString(), 'What is the closest major river to that city?'));\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_templates/template_from_definition/","title":"Template from Definition","text":""},{"location":"cookbook/agents/agent_templates/template_from_definition/#overview","title":"Overview","text":"<p>Instantiate an agent directly from an in-memory <code>AgentDefinition</code>.</p> <p>Key concepts: - <code>AgentDefinition</code>: template data object - <code>DefinitionLoopFactory</code>: builds executable loop from template - <code>DefinitionStateFactory</code>: builds initial state from template - <code>llmConfig</code>: selects real LLM provider preset for execution - <code>AgentEventConsoleObserver</code>: execution visibility</p>"},{"location":"cookbook/agents/agent_templates/template_from_definition/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\nuse Cognesy\\Agents\\Data\\AgentBudget;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\n\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true, showContinuation: true);\n\n$capabilities = new AgentCapabilityRegistry();\n\n$definition = new AgentDefinition(\n    name: 'geo-agent',\n    description: 'Answers short geography questions.',\n    systemPrompt: 'You are concise and precise.',\n    llmConfig: 'openai',\n    budget: new AgentBudget(maxSteps: 3),\n);\n\n$loop = (new DefinitionLoopFactory($capabilities))\n    -&gt;instantiateAgentLoop($definition)\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$seed = AgentState::empty()-&gt;withUserMessage('What is the capital of France?');\n$state = (new DefinitionStateFactory())-&gt;instantiateAgentState($definition, $seed);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n$final = $loop-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\necho 'Answer: ' . ($final-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\necho 'Steps: ' . $final-&gt;stepCount() . \"\\n\";\necho 'Status: ' . $final-&gt;status()-&gt;value . \"\\n\";\n\nassert(in_array($final-&gt;status()-&gt;value, ['completed', 'failed'], true));\nassert($final-&gt;stepCount() &gt;= 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_templates/template_from_markdown/","title":"Template from Markdown","text":""},{"location":"cookbook/agents/agent_templates/template_from_markdown/#overview","title":"Overview","text":"<p>Load <code>AgentDefinition</code> from markdown frontmatter and execute it.</p>"},{"location":"cookbook/agents/agent_templates/template_from_markdown/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Template\\AgentDefinitionLoader;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\n\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true, showContinuation: true);\n\n$definition = (new AgentDefinitionLoader())\n    -&gt;loadFile('examples/D03_AgentTemplates/TemplateFromMarkdown/agent.md');\n\n$capabilities = new AgentCapabilityRegistry();\n\n$loop = (new DefinitionLoopFactory($capabilities))\n    -&gt;instantiateAgentLoop($definition)\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$seed = AgentState::empty()-&gt;withUserMessage('Name one famous landmark in Paris.');\n$state = (new DefinitionStateFactory())-&gt;instantiateAgentState($definition, $seed);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n$final = $loop-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\necho \"Template: {$definition-&gt;name}\\n\";\necho 'Answer: ' . ($final-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\necho 'Status: ' . $final-&gt;status()-&gt;value . \"\\n\";\n\nassert($definition-&gt;name === 'md-agent');\nassert(in_array($final-&gt;status()-&gt;value, ['completed', 'failed'], true));\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_templates/template_from_yaml/","title":"Template from YAML","text":""},{"location":"cookbook/agents/agent_templates/template_from_yaml/#overview","title":"Overview","text":"<p>Load <code>AgentDefinition</code> from YAML and run it with the same factories.</p>"},{"location":"cookbook/agents/agent_templates/template_from_yaml/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Template\\AgentDefinitionLoader;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\n\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true, showContinuation: true);\n\n$definition = (new AgentDefinitionLoader())\n    -&gt;loadFile('examples/D03_AgentTemplates/TemplateFromYaml/agent.yaml');\n\n$capabilities = new AgentCapabilityRegistry();\n\n$loop = (new DefinitionLoopFactory($capabilities))\n    -&gt;instantiateAgentLoop($definition)\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$state = (new DefinitionStateFactory())-&gt;instantiateAgentState(\n    $definition,\n    AgentState::empty()-&gt;withUserMessage('What is 7 multiplied by 8?'),\n);\n\necho \"=== Agent Execution Log ===\\n\\n\";\n$final = $loop-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\necho \"Template: {$definition-&gt;name}\\n\";\necho 'Answer: ' . ($final-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\necho 'Status: ' . $final-&gt;status()-&gt;value . \"\\n\";\n\nassert($definition-&gt;name === 'yaml-agent');\nassert(in_array($final-&gt;status()-&gt;value, ['completed', 'failed'], true));\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_templates/template_override_seed_state/","title":"Template Overrides Seed State","text":""},{"location":"cookbook/agents/agent_templates/template_override_seed_state/#overview","title":"Overview","text":"<p>Show how <code>DefinitionStateFactory</code> merges template data with a provided seed state.</p>"},{"location":"cookbook/agents/agent_templates/template_override_seed_state/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Data\\AgentBudget;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\nuse Cognesy\\Utils\\Metadata;\n\n$definition = new AgentDefinition(\n    name: 'seed-override-agent',\n    description: 'Shows seed + template merge behavior.',\n    systemPrompt: 'Template prompt overrides seed prompt.',\n    budget: new AgentBudget(maxSteps: 3),\n    metadata: Metadata::fromArray(['tier' =&gt; 'gold', 'region' =&gt; 'eu']),\n);\n\n$seed = AgentState::empty()\n    -&gt;withUserMessage('Keep this message from seed state.')\n    -&gt;withSystemPrompt('Seed prompt should be replaced.')\n    -&gt;withMetadata('session', 'abc-123')\n    -&gt;withBudget(new AgentBudget(maxSteps: 10));\n\n$state = (new DefinitionStateFactory())-&gt;instantiateAgentState($definition, $seed);\n\necho \"=== Result ===\\n\";\necho 'System prompt: ' . $state-&gt;context()-&gt;systemPrompt() . \"\\n\";\necho 'Messages count: ' . $state-&gt;messages()-&gt;count() . \"\\n\";\necho 'Budget maxSteps: ' . ($state-&gt;budget()-&gt;maxSteps ?? 0) . \"\\n\";\necho 'Metadata tier: ' . $state-&gt;metadata()-&gt;get('tier') . \"\\n\";\necho 'Metadata session: ' . $state-&gt;metadata()-&gt;get('session') . \"\\n\";\n\nassert($state-&gt;context()-&gt;systemPrompt() === 'Template prompt overrides seed prompt.');\nassert($state-&gt;messages()-&gt;count() === 1);\nassert($state-&gt;budget()-&gt;maxSteps === 3);\nassert($state-&gt;metadata()-&gt;get('tier') === 'gold');\nassert($state-&gt;metadata()-&gt;get('session') === 'abc-123');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agent_templates/template_with_tools_and_capabilities/","title":"Template with Tools and Capabilities","text":""},{"location":"cookbook/agents/agent_templates/template_with_tools_and_capabilities/#overview","title":"Overview","text":"<p>Template declares both a capability (<code>guards.basic</code>) and a tool allow-list. The loop factory resolves both from registries while using a real LLM driver.</p>"},{"location":"cookbook/agents/agent_templates/template_with_tools_and_capabilities/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Collections\\NameList;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Tool\\ToolRegistry;\nuse Cognesy\\Agents\\Tool\\Tools\\MockTool;\n\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true, showContinuation: true);\n\n$capabilities = new AgentCapabilityRegistry();\n$capabilities-&gt;register('guards.basic', new UseGuards(maxSteps: 4, maxTokens: 2000, maxExecutionTime: 30));\n\n$tools = new ToolRegistry();\n$tools-&gt;register(MockTool::returning(\n    'city_fact',\n    'Returns one city fact',\n    'Paris has a population of about 2.1 million residents.',\n));\n\n$definition = new AgentDefinition(\n    name: 'tool-agent',\n    description: 'Template-declared tools and capabilities.',\n    systemPrompt: 'Use tools when needed.',\n    llmConfig: 'openai',\n    capabilities: NameList::fromArray(['guards.basic']),\n    tools: NameList::fromArray(['city_fact']),\n);\n\n$loop = (new DefinitionLoopFactory($capabilities, $tools))\n    -&gt;instantiateAgentLoop($definition)\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$final = $loop-&gt;execute(AgentState::empty()-&gt;withUserMessage(\n    'Use city_fact and answer with one short fact about Paris.',\n));\n\necho \"=== Result ===\\n\";\necho 'Steps: ' . $final-&gt;stepCount() . \"\\n\";\necho 'Final answer: ' . ($final-&gt;finalResponse()-&gt;toString() ?: 'No response') . \"\\n\";\n\nassert($final-&gt;status()-&gt;value === 'completed');\nassert($final-&gt;finalResponse()-&gt;toString() !== '');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_bash_tool/","title":"Agent with Bash Tool","text":""},{"location":"cookbook/agents/agents/agent_loop_bash_tool/#overview","title":"Overview","text":"<p>Attaching <code>BashTool</code> directly to <code>AgentLoop</code> gives the agent the ability to execute shell commands. This is useful for system administration tasks, running scripts, or gathering system information. The agent decides which commands to run based on the task.</p> <p>Key concepts: - <code>AgentLoop::withTool()</code>: Adds a tool directly to the loop - <code>BashTool</code>: Executes shell commands with configurable sandboxing - The agent autonomously decides which commands to run - <code>AgentEventConsoleObserver</code>: Shows tool arguments including executed commands</p>"},{"location":"cookbook/agents/agents/agent_loop_bash_tool/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Capability\\Bash\\BashTool;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\n\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showToolArgs: true,\n);\n\n// Build loop with BashTool directly\n$loop = AgentLoop::default()\n    -&gt;withTool(BashTool::inDirectory(getcwd()))\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$state = AgentState::empty()-&gt;withUserMessage(\n    'What is the current date and time? Also show the hostname and working directory. Be concise.'\n);\n\necho \"=== Agent Execution ===\\n\\n\";\n$finalState = $loop-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?? 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_context_compiler/","title":"Custom Context Compiler","text":""},{"location":"cookbook/agents/agents/agent_loop_context_compiler/#overview","title":"Overview","text":"<p>Context compilers control which messages the LLM sees at each step. By default, <code>ConversationWithCurrentToolTrace</code> includes all conversation messages plus only the current execution's tool traces. You can swap in a different compiler to change what context the agent reasons over.</p> <p>This example builds a custom compiler that wraps the default one and applies two transformations: - Filtering: Truncates long tool results so they don't overwhelm the context - Enrichment: Injects a dynamic system message with execution progress info</p> <p>The compiler logs what it does at each step, so you can see exactly what the LLM receives.</p> <p>Key concepts: - <code>CanCompileMessages</code>: Interface for message compilers - <code>ConversationWithCurrentToolTrace</code>: Default \u2014 includes conversation + current tool traces - Custom compilers can filter, truncate, enrich, or transform the message list</p>"},{"location":"cookbook/agents/agents/agent_loop_context_compiler/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Capability\\Bash\\BashTool;\nuse Cognesy\\Agents\\Context\\CanAcceptMessageCompiler;\nuse Cognesy\\Agents\\Context\\CanCompileMessages;\nuse Cognesy\\Agents\\Context\\Compilers\\ConversationWithCurrentToolTrace;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Message;\nuse Cognesy\\Messages\\Messages;\n\n$logger = new AgentEventConsoleObserver(\n        useColors: true,\n        showTimestamps: true,\n        showContinuation: true,\n        showToolArgs: true,\n);\n\n// Custom compiler that filters and enriches the context\nclass InstrumentedCompiler implements CanCompileMessages\n{\n    public function __construct(\n        private readonly CanCompileMessages $inner,\n        private readonly int $maxToolResultLength = 200,\n    ) {}\n\n    #[\\Override]\n    public function compile(AgentState $state): Messages\n    {\n        // 1. Get messages from the inner compiler\n        $messages = $this-&gt;inner-&gt;compile($state);\n        $originalCount = $messages-&gt;count();\n\n        // 2. FILTER: truncate long tool results to keep context lean\n        $messages = $messages-&gt;filter(function (Message $msg) {\n            if ($msg-&gt;role()-&gt;value !== 'tool') {\n                return true; // keep non-tool messages as-is\n            }\n            $content = $msg-&gt;content()-&gt;toString();\n            if (strlen($content) &lt;= $this-&gt;maxToolResultLength) {\n                return true; // short enough, keep it\n            }\n            return true; // keep but we'll truncate below\n        });\n\n        // Apply truncation\n        $truncated = [];\n        $truncatedCount = 0;\n        foreach ($messages-&gt;all() as $msg) {\n            if ($msg-&gt;role()-&gt;value === 'tool') {\n                $content = $msg-&gt;content()-&gt;toString();\n                if (strlen($content) &gt; $this-&gt;maxToolResultLength) {\n                    $msg = new Message(\n                        role: 'tool',\n                        content: substr($content, 0, $this-&gt;maxToolResultLength) . '... [truncated]',\n                        metadata: $msg-&gt;metadata(),\n                    );\n                    $truncatedCount++;\n                }\n            }\n            $truncated[] = $msg;\n        }\n        $messages = Messages::fromMessages($truncated);\n\n        // 3. ENRICH: inject execution context as a user instruction\n        $step = $state-&gt;stepCount() + 1;\n        $tokens = $state-&gt;usage()-&gt;total();\n        $context = \"[System note] You are on step {$step}. Tokens used so far: {$tokens}. Be concise.\";\n        $messages = $messages-&gt;appendMessage(\n            new Message(role: 'user', content: $context)\n        );\n\n        // 4. LOG: show what the LLM will see\n        echo \"  [compiler] Compiled {$messages-&gt;count()} messages (from {$originalCount} original\";\n        if ($truncatedCount &gt; 0) {\n            echo \", {$truncatedCount} tool results truncated\";\n        }\n        echo \")\\n\";\n        foreach ($messages-&gt;all() as $msg) {\n            $role = $msg-&gt;role()-&gt;value;\n            $content = $msg-&gt;content()-&gt;toString();\n            $len = strlen($content);\n            if ($len === 0) {\n                echo \"    [{$role}] (tool calls only)\\n\";\n            } else {\n                $preview = substr(str_replace(\"\\n\", ' ', $content), 0, 72);\n                echo \"    [{$role}] ({$len}ch) {$preview}\" . ($len &gt; 72 ? '...' : '') . \"\\n\";\n            }\n        }\n\n        return $messages;\n    }\n}\n\n// Wrap the default compiler with our instrumented one\n$compiler = new InstrumentedCompiler(\n    inner: new ConversationWithCurrentToolTrace(),\n    maxToolResultLength: 200,\n);\n\n$agent = AgentLoop::default();\n$agent = $agent\n    -&gt;withTool(BashTool::inDirectory(getcwd() ?: __DIR__))\n    -&gt;withDriver($agent-&gt;driver()-&gt;withMessageCompiler($compiler))\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$state = AgentState::empty()-&gt;withUserMessage(\n    'List all files in the current directory, then tell me how many there are.'\n);\n\necho \"=== Agent with Custom Context Compiler ===\\n\\n\";\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_custom_tool/","title":"Agent with Custom Tool","text":""},{"location":"cookbook/agents/agents/agent_loop_custom_tool/#overview","title":"Overview","text":"<p>Build a custom tool by extending <code>BaseTool</code>. Override <code>__invoke(mixed ...$args)</code> to implement the tool logic, use <code>$this-&gt;arg()</code> to extract named parameters, and override <code>toToolSchema()</code> to define the parameter schema for the LLM.</p> <p>This example creates a <code>SystemInfoTool</code> that reports memory usage, PHP version, and other runtime information. The agent calls it when asked about the system.</p> <p>Key concepts: - <code>BaseTool</code>: Abstract base class for custom tools - <code>__invoke(mixed ...$args)</code>: The method the agent calls - <code>$this-&gt;arg()</code>: Extract named or positional parameters from args - <code>toToolSchema()</code>: Define the JSON Schema the LLM sees for this tool - <code>$this-&gt;agentState</code>: Access current agent state from within a tool</p>"},{"location":"cookbook/agents/agents/agent_loop_custom_tool/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Tool\\Tools\\BaseTool;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\nuse Cognesy\\Utils\\JsonSchema\\ToolSchema;\n\n// Custom tool that reports system information\nclass SystemInfoTool extends BaseTool\n{\n    public function __construct() {\n        parent::__construct(\n            name: 'system_info',\n            description: 'Returns current system resource usage and PHP runtime information.',\n        );\n    }\n\n    #[\\Override]\n    public function __invoke(mixed ...$args): string {\n        $category = (string) $this-&gt;arg($args, 'category', 0, 'all');\n        $info = [];\n\n        if ($category === 'memory' || $category === 'all') {\n            $memUsage = memory_get_usage(true);\n            $memPeak = memory_get_peak_usage(true);\n            $info[] = sprintf(\"Memory: %.2f MB (peak: %.2f MB)\", $memUsage / 1048576, $memPeak / 1048576);\n        }\n\n        if ($category === 'php' || $category === 'all') {\n            $info[] = \"PHP Version: \" . PHP_VERSION;\n            $info[] = \"OS: \" . PHP_OS;\n            $info[] = \"SAPI: \" . PHP_SAPI;\n        }\n\n        if ($category === 'all') {\n            $info[] = \"PID: \" . getmypid();\n            $info[] = \"Uptime: \" . (int)(microtime(true) - $_SERVER['REQUEST_TIME_FLOAT']) . \"s\";\n\n            // Show agent context if available\n            if ($this-&gt;agentState !== null) {\n                $info[] = \"Agent step: \" . $this-&gt;agentState-&gt;stepCount();\n                $info[] = \"Agent tokens: \" . $this-&gt;agentState-&gt;usage()-&gt;total();\n            }\n        }\n\n        return implode(\"\\n\", $info);\n    }\n\n    #[\\Override]\n    public function toToolSchema(): array {\n        return ToolSchema::make(\n            name: $this-&gt;name(),\n            description: $this-&gt;description(),\n            parameters: JsonSchema::object('parameters')\n                -&gt;withProperties([\n                    JsonSchema::string('category', 'What to check: \"memory\", \"php\", or \"all\"'),\n                ])\n                -&gt;withRequiredProperties([])\n        )-&gt;toArray();\n    }\n}\n\n// AgentEventConsoleObserver shows execution lifecycle events on the console\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Create loop with the custom tool\n$loop = AgentLoop::default()\n    -&gt;withTool(new SystemInfoTool())\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString('Check the current memory usage and PHP version. Report back concisely.')\n);\n\necho \"=== Agent Execution ===\\n\\n\";\n$finalState = $loop-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens: {$finalState-&gt;usage()-&gt;total()}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_events/","title":"Agent Events and Wiretap","text":""},{"location":"cookbook/agents/agents/agent_loop_events/#overview","title":"Overview","text":"<p>The agent emits events throughout its lifecycle. Events are read-only observations \u2014 they cannot modify agent behavior (use hooks for that).</p> <p>Two ways to observe events: - <code>wiretap(callable)</code>: Receives every event \u2014 <code>AgentEventConsoleObserver</code> uses this internally - <code>onEvent(EventClass, callable)</code>: Subscribes to a specific event type for custom logic</p> <p>Both can be used together. The logger provides general visibility while <code>onEvent()</code> lets you collect metrics, trigger side effects, or react to specific events.</p> <p>Key concepts: - <code>AgentEventConsoleObserver</code>: Built-in wiretap that formats all events for console output - <code>onEvent()</code>: Targeted listener for a single event class - Events include: <code>AgentStepCompleted</code>, <code>ToolCallStarted</code>, <code>ToolCallCompleted</code>,   <code>InferenceResponseReceived</code>, <code>AgentExecutionCompleted</code>, <code>ContinuationEvaluated</code>, and more</p>"},{"location":"cookbook/agents/agents/agent_loop_events/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Capability\\Bash\\BashTool;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\AgentExecutionCompleted;\nuse Cognesy\\Agents\\Events\\InferenceResponseReceived;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\n\n// AgentEventConsoleObserver uses wiretap() internally to show all lifecycle events\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n$agent = AgentLoop::default()\n    -&gt;withTool(BashTool::inDirectory(getcwd()))\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// onEvent(): subscribe to specific event types for custom logic\n// This runs alongside the logger \u2014 use it to collect metrics, trigger\n// side effects, or react to specific events the logger doesn't cover.\n\n$totalInferenceMs = 0;\n\n$agent-&gt;onEvent(InferenceResponseReceived::class, function (InferenceResponseReceived $event) use (&amp;$totalInferenceMs) {\n    $ms = $event-&gt;receivedAt-&gt;getTimestamp() * 1000 + (int)($event-&gt;receivedAt-&gt;format('u') / 1000)\n        - $event-&gt;requestStartedAt-&gt;getTimestamp() * 1000 - (int)($event-&gt;requestStartedAt-&gt;format('u') / 1000);\n    $totalInferenceMs += $ms;\n});\n\n$agent-&gt;onEvent(AgentExecutionCompleted::class, function (AgentExecutionCompleted $event) use (&amp;$totalInferenceMs) {\n    echo \"\\n  [custom] Execution summary:\\n\";\n    echo \"    Steps: {$event-&gt;totalSteps}\\n\";\n    echo \"    Total tokens: {$event-&gt;totalUsage-&gt;total()}\\n\";\n    echo \"    LLM time: {$totalInferenceMs}ms\\n\";\n});\n\n// Run the agent\n$state = AgentState::empty()-&gt;withUserMessage(\n    'What is today\\'s date? Use bash to find out. Be concise.'\n);\n\necho \"=== Agent Events Demo ===\\n\\n\";\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($totalInferenceMs &gt; 0, 'Expected inference time to be tracked');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_execute/","title":"AgentLoop::execute()","text":""},{"location":"cookbook/agents/agents/agent_loop_execute/#overview","title":"Overview","text":"<p>The simplest way to run an agent: <code>AgentLoop::execute()</code> runs the loop to completion and returns the final <code>AgentState</code>. The loop sends messages to the LLM, processes any tool calls, and repeats until the LLM produces a final response with no pending tool calls.</p> <p>Key concepts: - <code>AgentLoop::default()</code>: Creates a minimal agent loop with sensible defaults - <code>AgentState::empty()</code>: Creates an empty immutable state container - <code>execute()</code>: Runs the full loop and returns the final state - <code>AgentEventConsoleObserver</code>: Attach via <code>wiretap()</code> to see execution lifecycle events - <code>finalResponse()</code>: Access the agent's final text output</p>"},{"location":"cookbook/agents/agents/agent_loop_execute/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Messages;\n\n// AgentEventConsoleObserver shows execution lifecycle events on the console\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n);\n\n// Create a default agent loop (no tools, just LLM conversation)\n$loop = AgentLoop::default()\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Prepare initial state with a user message\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString('What are the three primary colors? Answer in one sentence.')\n);\n\n// Execute the loop to completion\necho \"=== Agent Execution ===\\n\\n\";\n$finalState = $loop-&gt;execute($state);\n\n// Read the result\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Agent ID: {$finalState-&gt;agentId()-&gt;toString()}\\n\";\necho \"Execution ID: {$finalState-&gt;execution()?-&gt;executionId()-&gt;toString()}\\n\";\necho \"Last Step ID: {$finalState-&gt;lastStep()?-&gt;stepId()-&gt;toString()}\\n\";\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Tokens used: {$finalState-&gt;usage()-&gt;total()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert($finalState-&gt;status() === \\Cognesy\\Agents\\Enums\\ExecutionStatus::Completed);\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert($finalState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_hooks/","title":"Agent Lifecycle Hooks","text":""},{"location":"cookbook/agents/agents/agent_loop_hooks/#overview","title":"Overview","text":"<p>Hooks intercept agent lifecycle events to observe or modify state. Each hook receives a <code>HookContext</code> and returns a (potentially modified) <code>HookContext</code>. Hooks can:</p> <ul> <li>Observe: Log events, collect metrics without changing state</li> <li>Modify: Inject metadata, adjust system prompts, transform messages</li> <li>Block: Prevent tool execution (for <code>BeforeToolUse</code> hooks)</li> </ul> <p>Key concepts: - <code>CallableHook</code>: Wraps a closure as a <code>HookInterface</code> - <code>HookTriggers</code>: Specifies when the hook fires (e.g., <code>beforeStep()</code>, <code>afterStep()</code>) - <code>HookContext</code>: Carries <code>AgentState</code>, tool call info, and trigger type - <code>HookStack</code>: Registers hooks directly on <code>AgentLoop</code> via <code>withInterceptor()</code></p>"},{"location":"cookbook/agents/agents/agent_loop_hooks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Capability\\Bash\\BashTool;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Hook\\Collections\\HookTriggers;\nuse Cognesy\\Agents\\Hook\\Collections\\RegisteredHooks;\nuse Cognesy\\Agents\\Hook\\Data\\HookContext;\nuse Cognesy\\Agents\\Hook\\HookStack;\nuse Cognesy\\Agents\\Hook\\Hooks\\CallableHook;\n\n// AgentEventConsoleObserver shows execution lifecycle alongside hook output\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n    showHooks: true,\n);\n\n// Track timing data\n$timings = [];\n\n$hooks = (new HookStack(new RegisteredHooks()))\n    // Hook 1: Before each step \u2014 inject timing metadata\n    -&gt;with(\n        hook: new CallableHook(function (HookContext $ctx) use (&amp;$timings): HookContext {\n            $step = $ctx-&gt;state()-&gt;stepCount() + 1;\n            $timings[$step] = microtime(true);\n            return $ctx-&gt;withState(\n                $ctx-&gt;state()-&gt;withMetadata('step_started_at', microtime(true))\n            );\n        }),\n        triggerTypes: HookTriggers::beforeStep(),\n        name: 'timing:start',\n    )\n    // Hook 2: After each step \u2014 calculate duration\n    -&gt;with(\n        hook: new CallableHook(function (HookContext $ctx) use (&amp;$timings): HookContext {\n            $step = $ctx-&gt;state()-&gt;stepCount();\n            $started = $timings[$step] ?? null;\n            $duration = $started ? round((microtime(true) - $started) * 1000) : 0;\n            $tokens = $ctx-&gt;state()-&gt;usage()-&gt;total();\n            echo \"  [timing] Step {$step}: {$duration}ms (total tokens: {$tokens})\\n\";\n            return $ctx;\n        }),\n        triggerTypes: HookTriggers::afterStep(),\n        name: 'timing:end',\n    )\n    // Hook 3: Before tool use \u2014 log which tool is about to run\n    -&gt;with(\n        hook: new CallableHook(function (HookContext $ctx): HookContext {\n            $toolName = $ctx-&gt;toolCall()?-&gt;name() ?? 'unknown';\n            echo \"  [audit] About to execute: {$toolName}\\n\";\n            return $ctx;\n        }),\n        triggerTypes: HookTriggers::beforeToolUse(),\n        name: 'audit:tool',\n    )\n    // Hook 4: After tool use \u2014 log tool result status\n    -&gt;with(\n        hook: new CallableHook(function (HookContext $ctx): HookContext {\n            $exec = $ctx-&gt;toolExecution();\n            if ($exec !== null) {\n                $status = $exec-&gt;wasBlocked() ? 'BLOCKED' : 'OK';\n                echo \"  [audit] Tool {$exec-&gt;name()} -&gt; {$status}\\n\";\n            }\n            return $ctx;\n        }),\n        triggerTypes: HookTriggers::afterToolUse(),\n        name: 'audit:result',\n    )\n    // Hook 5: On stop \u2014 final summary\n    -&gt;with(\n        hook: new CallableHook(function (HookContext $ctx): HookContext {\n            $state = $ctx-&gt;state();\n            echo \"  [summary] Agent stopping after {$state-&gt;stepCount()} steps\\n\";\n            return $ctx;\n        }),\n        triggerTypes: HookTriggers::onStop(),\n        name: 'summary',\n    );\n\n$agent = AgentLoop::default()\n    -&gt;withTool(BashTool::inDirectory(getcwd()))\n    -&gt;withInterceptor($hooks)\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// Run the agent\n$state = AgentState::empty()-&gt;withUserMessage(\n    'What is the current date? Use bash to find out. Be concise.'\n);\n\necho \"=== Agent Execution with Hooks ===\\n\\n\";\n$finalState = $agent-&gt;execute($state);\n\necho \"\\n=== Result ===\\n\";\n$response = $finalState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\n\n// Assertions\nassert(!empty($finalState-&gt;finalResponse()-&gt;toString()), 'Expected non-empty response');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\nassert(!empty($timings), 'Expected timing data from hooks');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_iterate/","title":"AgentLoop::iterate()","text":""},{"location":"cookbook/agents/agents/agent_loop_iterate/#overview","title":"Overview","text":"<p><code>AgentLoop::iterate()</code> yields the agent state after each step, giving you fine-grained control over the execution. This is useful for streaming progress, implementing custom stop logic, or inspecting intermediate states between LLM calls.</p> <p>You can combine <code>iterate()</code> with <code>AgentEventConsoleObserver</code> to get detailed event output (tool calls, inference, continuation decisions) alongside your own step-by-step logic. The logger hooks into the event system and prints as events fire during iteration.</p> <p>Key concepts: - <code>iterate()</code>: Returns an iterable that yields <code>AgentState</code> after each step - <code>AgentEventConsoleObserver</code>: Attach via <code>wiretap()</code> for detailed execution logging - Step-by-step inspection of tool calls, token usage, and agent decisions - Early termination by breaking out of the loop</p>"},{"location":"cookbook/agents/agents/agent_loop_iterate/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Capability\\File\\ReadFileTool;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Messages\\Messages;\n\n$workDir = dirname(__DIR__, 3);\n\n// AgentEventConsoleObserver provides detailed event output during iteration\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n);\n\n$loop = AgentLoop::default()\n    -&gt;withTool(ReadFileTool::inDirectory($workDir))\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString('Read the composer.json file and tell me the project name.')\n);\n\necho \"=== Stepping through agent loop ===\\n\\n\";\n\n// iterate() yields state after each step; the logger prints events as they fire\n$stepNum = 0;\nforeach ($loop-&gt;iterate($state) as $stepState) {\n    $stepNum++;\n    // At yield time the step has been completed, so use lastStep()\n    $step = $stepState-&gt;lastStep();\n\n    $hasToolCalls = $step?-&gt;hasToolCalls() ?? false;\n    $tokens = $stepState-&gt;usage()-&gt;total();\n    $status = $stepState-&gt;status()-&gt;value;\n\n    // Custom per-step output alongside the logger's event output\n    $toolsLabel = $hasToolCalls ? 'yes' : 'no';\n    echo \"  &gt;&gt; Step {$stepNum}: status={$status}, has_tools={$toolsLabel}, tokens={$tokens}\\n\\n\";\n\n    // Early termination example: stop after 5 steps regardless\n    if ($stepNum &gt;= 5) {\n        echo \"\\n[Breaking early after {$stepNum} steps]\\n\";\n        break;\n    }\n}\n\necho \"\\n=== Final Result ===\\n\";\n$response = $stepState-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"Answer: {$response}\\n\";\necho \"Total steps: {$stepState-&gt;stepCount()}\\n\";\necho \"Total tokens: {$stepState-&gt;usage()-&gt;total()}\\n\";\n\n// Assertions\nassert($stepNum &gt;= 1, 'Expected at least 1 step from iterate()');\nassert(!empty($stepState-&gt;currentResponse()-&gt;toString()), 'Expected non-empty response');\nassert($stepState-&gt;usage()-&gt;total() &gt; 0, 'Expected token usage &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_multi_execution/","title":"Multi-Execution Conversations","text":""},{"location":"cookbook/agents/agents/agent_loop_multi_execution/#overview","title":"Overview","text":"<p>An agent can handle multiple rounds of execution, where each round builds on the previous conversation history. Just add a new user message to the returned state and call <code>execute()</code> again \u2014 <code>AgentLoop</code> automatically resets completed executions before starting a new one.</p> <p>This enables multi-turn interactions where the agent reasons over past tool results to answer follow-up questions without re-executing tools.</p> <p>Key concepts: - <code>withUserMessage()</code>: Appends a follow-up user message to the existing conversation - The agent sees all prior messages including tool calls and results from previous executions - Follow-up questions can reference data gathered in earlier rounds - <code>AgentLoop</code> auto-resets terminal execution state (completed/failed) on entry</p>"},{"location":"cookbook/agents/agents/agent_loop_multi_execution/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Capability\\File\\ReadFileTool;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\n\n$workDir = dirname(__DIR__, 3);\n\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Build an agent with direct file reading tool\n$loop = AgentLoop::default()\n    -&gt;withTool(ReadFileTool::inDirectory($workDir))\n    -&gt;wiretap($logger-&gt;wiretap());\n\n// === Execution 1: Ask the agent to read composer.json ===\n$query1 = 'Read the composer.json file and tell me the project name and its PHP version requirement.';\necho \"=== Execution 1 ===\\n\";\necho \"Query: {$query1}\\n\\n\";\n\n$state = AgentState::empty()-&gt;withUserMessage($query1);\n$state = $loop-&gt;execute($state);\n\n$response1 = $state-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"\\nResponse: {$response1}\\n\\n\";\n\n// === Execution 2: Follow-up question using context from Execution 1 ===\n$query2 = 'Based on what you read, does the project use PSR-4 autoloading? What are the namespace prefixes?';\necho \"=== Execution 2 ===\\n\";\necho \"Query: {$query2}\\n\\n\";\n\n// Just add a new message \u2014 AgentLoop auto-resets the completed execution\n$state = $state-&gt;withUserMessage($query2);\n$state = $loop-&gt;execute($state);\n\n$response2 = $state-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"\\nResponse: {$response2}\\n\\n\";\n\n// === Execution 3: Another follow-up \u2014 agent reasons without tools ===\n$query3 = 'Given what you know about this project, what type of project is it \u2014 a library, framework, or application? Explain briefly.';\necho \"=== Execution 3 ===\\n\";\necho \"Query: {$query3}\\n\\n\";\n\n$state = $state-&gt;withUserMessage($query3);\n$state = $loop-&gt;execute($state);\n\n$response3 = $state-&gt;finalResponse()-&gt;toString() ?: 'No response';\necho \"\\nResponse: {$response3}\\n\";\n\n// Assertions\nassert(!empty($response1) &amp;&amp; $response1 !== 'No response', 'Expected non-empty response from execution 1');\nassert(!empty($response2) &amp;&amp; $response2 !== 'No response', 'Expected non-empty response from execution 2');\nassert(!empty($response3) &amp;&amp; $response3 !== 'No response', 'Expected non-empty response from execution 3');\nassert($state-&gt;stepCount() &gt;= 1, 'Expected at least 1 step in final execution');\n?&gt;\n</code></pre>"},{"location":"cookbook/agents/agents/agent_loop_stop_conditions/","title":"Agent Stop Conditions","text":""},{"location":"cookbook/agents/agents/agent_loop_stop_conditions/#overview","title":"Overview","text":"<p>The agent loop stops when <code>AgentState::shouldStop()</code> returns true. You can trigger stops from within tools using <code>AgentStopException</code>, or apply guard hooks directly (<code>StepsLimitHook</code>, <code>TokenUsageLimitHook</code>, <code>ExecutionTimeLimitHook</code>).</p> <p>When a tool stops the agent, the last step is a <code>ToolExecution</code> \u2014 not a <code>FinalResponse</code>. This means <code>finalResponse()</code> returns empty. Use <code>currentResponse()</code> to get the best available output regardless of how the agent stopped:</p> <ul> <li><code>finalResponse()</code> \u2014 strict: only returns text when the LLM completed naturally (no pending tool calls)</li> <li><code>currentResponse()</code> \u2014 pragmatic: returns <code>finalResponse()</code> if available, otherwise the last step's output</li> </ul> <p>Key concepts: - <code>AgentStopException</code>: Throw from a tool to stop the loop immediately - <code>StopSignal</code> / <code>StopReason</code>: Describes why the loop stopped - <code>finalResponse()</code> vs <code>currentResponse()</code>: Strict vs pragmatic response access - Guard hooks: step/token/time limits implemented via lifecycle interception</p>"},{"location":"cookbook/agents/agents/agent_loop_stop_conditions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Continuation\\AgentStopException;\nuse Cognesy\\Agents\\Continuation\\StopReason;\nuse Cognesy\\Agents\\Continuation\\StopSignal;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\nuse Cognesy\\Agents\\Tool\\Tools\\BaseTool;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\nuse Cognesy\\Utils\\JsonSchema\\ToolSchema;\n\n// A tool that counts up and stops when it reaches a target\nclass CounterTool extends BaseTool\n{\n    private static int $count = 0;\n\n    public function __construct(private int $stopAt = 3) {\n        parent::__construct(\n            name: 'counter',\n            description: 'Increments a counter and returns the current value. Call this tool repeatedly.',\n        );\n    }\n\n    #[\\Override]\n    public function __invoke(mixed ...$args): string {\n        self::$count++;\n        echo \"  [Counter] Value: \" . self::$count . \"\\n\";\n\n        if (self::$count &gt;= $this-&gt;stopAt) {\n            // Throw AgentStopException to halt the loop\n            throw new AgentStopException(\n                signal: new StopSignal(\n                    reason: StopReason::StopRequested,\n                    message: \"Counter reached target: {$this-&gt;stopAt}\",\n                ),\n                context: ['final_count' =&gt; self::$count],\n                source: self::class,\n            );\n        }\n\n        return \"Counter is at \" . self::$count . \". Keep going \u2014 call counter again.\";\n    }\n\n    #[\\Override]\n    public function toToolSchema(): array {\n        return ToolSchema::make(\n            name: $this-&gt;name(),\n            description: $this-&gt;description(),\n            parameters: JsonSchema::object('parameters'),\n        )-&gt;toArray();\n    }\n}\n\n$logger = new AgentEventConsoleObserver(\n    useColors: true,\n    showTimestamps: true,\n    showContinuation: true,\n    showToolArgs: true,\n);\n\n// Create loop with the counter tool\n$loop = AgentLoop::default()\n    -&gt;withTool(new CounterTool(stopAt: 3))\n    -&gt;wiretap($logger-&gt;wiretap());\n\n$state = AgentState::empty()-&gt;withMessages(\n    Messages::fromString('Call the counter tool repeatedly until it stops you.')\n);\n\necho \"=== Agent with Stop Condition ===\\n\\n\";\n$finalState = $loop-&gt;execute($state);\n\n// =========================================================================\n// Reading the response after a forced stop\n// =========================================================================\n//\n// When a tool throws AgentStopException, the last step is a ToolExecution\n// (the LLM was requesting tool calls when the stop happened). This means:\n//\n//   finalResponse()   -&gt; empty (no FinalResponse step exists)\n//   currentResponse() -&gt; last step's LLM output (best available text)\n//\n// For stop-exception scenarios the real \"answer\" is typically in the stop\n// signal context or agent metadata \u2014 not in the LLM's text output.\n\necho \"\\n=== Result ===\\n\";\n\n// finalResponse() is empty because the agent was stopped mid-tool-execution\n$final = $finalState-&gt;finalResponse()-&gt;toString();\necho \"finalResponse():  \" . ($final !== '' ? $final : '(empty \u2014 agent was stopped, not completed)') . \"\\n\";\n\n// currentResponse() falls back to the last step's output\n$current = $finalState-&gt;currentResponse()-&gt;toString();\necho \"currentResponse(): \" . ($current !== '' ? $current : '(empty)') . \"\\n\";\n\n// hasFinalResponse() lets you branch on how the agent ended\necho \"hasFinalResponse(): \" . ($finalState-&gt;hasFinalResponse() ? 'true' : 'false') . \"\\n\";\n\n// The stop signal carries the reason and context set by the tool\n$stopSignal = $finalState-&gt;lastStopSignal();\necho \"Stop reason: \" . ($stopSignal?-&gt;toString() ?? 'unknown') . \"\\n\";\necho \"Stop context: \" . json_encode($stopSignal?-&gt;context ?? []) . \"\\n\";\n\necho \"Steps: {$finalState-&gt;stepCount()}\\n\";\necho \"Status: {$finalState-&gt;status()-&gt;value}\\n\";\n\n// Assertions\nassert($finalState-&gt;hasFinalResponse() === false, 'Expected no final response (agent was stopped)');\nassert($finalState-&gt;lastStopSignal() !== null, 'Expected a stop signal');\nassert($finalState-&gt;lastStopSignal()-&gt;context['final_count'] === 3, 'Expected counter to reach 3');\nassert($finalState-&gt;stepCount() &gt;= 1, 'Expected at least 1 step');\n?&gt;\n</code></pre>"},{"location":"cookbook/http_client/http_client/http_client_basics/","title":"HTTP Client \u2013 Basics","text":""},{"location":"cookbook/http_client/http_client/http_client_basics/#overview","title":"Overview","text":"<p>Demonstrates basic HTTP client usage: building a client, sending a request, and reading status/headers/body from the response. Uses a Mock driver so this example runs without network.</p>"},{"location":"cookbook/http_client/http_client/http_client_basics/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Creation\\HttpClientBuilder;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Data\\HttpResponse;\n\n// Basics: build a client, send a request, read status/headers/body.\n// We use a Mock driver so this example runs without network.\n\n$client = (new HttpClientBuilder())\n    -&gt;withMock(function ($mock) {\n        // Respond to a specific request shape\n        $mock-&gt;addResponse(\n            HttpResponse::sync(\n                statusCode: 200,\n                headers: ['Content-Type' =&gt; 'application/json'],\n                body: json_encode(['ok' =&gt; true, 'message' =&gt; 'Welcome!']),\n            ),\n            url: 'https://api.example.local/welcome',\n            method: 'GET'\n        );\n    })\n    -&gt;create();\n\n$request = new HttpRequest(\n    url: 'https://api.example.local/welcome',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: '',\n    options: [],\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\necho \"Status:  \" . $response-&gt;statusCode() . \"\\n\";\necho \"Headers: \" . json_encode($response-&gt;headers()) . \"\\n\";\necho \"Body:    \" . $response-&gt;body() . \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/http_client/http_client/http_client_pool_basics/","title":"HTTP Client \u2013 Pool Basics","text":""},{"location":"cookbook/http_client/http_client/http_client_pool_basics/#overview","title":"Overview","text":"<p>Demonstrates executing multiple HTTP requests in parallel using the pool feature. Shows how to handle both successful and failed responses from the pool results.</p>"},{"location":"cookbook/http_client/http_client/http_client_pool_basics/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Collections\\HttpRequestList;\n\n// Pool basics: execute multiple requests in parallel using the configured driver.\n// Note: This example uses real HTTP clients under the hood (default preset),\n// so it requires network access to actually run.\n\n$client = HttpClient::default();\n\n$requests = HttpRequestList::of(\n    new HttpRequest(\n        url: 'https://example.com',\n        method: 'GET',\n        headers: ['User-Agent' =&gt; 'InstructorPHP/1.0'],\n        body: '',\n        options: [],\n    ),\n    new HttpRequest(\n        url: 'https://www.iana.org/domains/reserved',\n        method: 'GET',\n        headers: ['User-Agent' =&gt; 'InstructorPHP/1.0'],\n        body: '',\n        options: [],\n    ),\n    new HttpRequest(\n        url: 'https://example.com',\n        method: 'GET',\n        headers: ['User-Agent' =&gt; 'InstructorPHP/1.0'],\n        body: '',\n        options: [],\n    ),\n);\n\n// Run pool with a concurrency limit (e.g., 2)\n$results = $client-&gt;pool($requests, maxConcurrent: 2);\n\necho \"Total: \" . count($results) . \"\\n\";\necho \"Successes: \" . $results-&gt;successCount() . \", Failures: \" . $results-&gt;failureCount() . \"\\n\";\n\nforeach ($results as $i =&gt; $result) {\n    if ($result-&gt;isSuccess()) {\n        $resp = $result-&gt;unwrap();\n        echo sprintf(\"[%d] %d bytes, status %d\\n\", $i, strlen($resp-&gt;body()), $resp-&gt;statusCode());\n    } else {\n        echo sprintf(\"[%d] ERROR: %s\\n\", $i, (string)$result-&gt;error());\n    }\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/http_client/http_client/http_client_streaming_basics/","title":"HTTP Client \u2013 Streaming Basics","text":""},{"location":"cookbook/http_client/http_client/http_client_streaming_basics/#overview","title":"Overview","text":"<p>Demonstrates requesting a streamed response and iterating over chunks. Uses a Mock driver to simulate SSE-like stream.</p>"},{"location":"cookbook/http_client/http_client/http_client_streaming_basics/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Creation\\HttpClientBuilder;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Data\\HttpResponse;\nuse Cognesy\\Http\\Stream\\ArrayStream;\n\n// Basics: request a streamed response and iterate chunks.\n// Use Mock driver to simulate SSE-like stream.\n\n$client = (new HttpClientBuilder())\n    -&gt;withMock(function ($mock) {\n        $mock-&gt;addResponse(\n            HttpResponse::streaming(\n                statusCode: 200,\n                headers: ['Content-Type' =&gt; 'text/event-stream'],\n                stream: ArrayStream::from([\"hello\\n\", \"from\\n\", \"stream\\n\"]),\n            ),\n            url: 'https://api.example.local/stream',\n            method: 'GET'\n        );\n    })\n    -&gt;create();\n\n$request = new HttpRequest(\n    url: 'https://api.example.local/stream',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'text/event-stream'],\n    body: '',\n    options: ['stream' =&gt; true],\n);\n\nforeach ($client-&gt;withRequest($request)-&gt;stream() as $chunk) {\n    echo $chunk; // handle streamed data incrementally\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/http_client/http_client/http_middleware_hooks/","title":"HTTP Middleware (Hooks + Conditional Decoration)","text":""},{"location":"cookbook/http_client/http_client/http_middleware_hooks/#overview","title":"Overview","text":"<p>Demonstrates BaseMiddleware hooks and conditional response decoration. Shows how to attach request headers, log responses, and conditionally decorate streamed responses.</p>"},{"location":"cookbook/http_client/http_client/http_middleware_hooks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Data\\HttpResponse;\nuse Cognesy\\Http\\Drivers\\Mock\\MockHttpDriver;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\nuse Cognesy\\Http\\Middleware\\Base\\BaseResponseDecorator;\nuse Cognesy\\Http\\Stream\\ArrayStream;\n\n// Scenario: Demonstrate BaseMiddleware hooks and conditional decoration\n\nclass HooksExampleMiddleware extends BaseMiddleware\n{\n    protected function beforeRequest(HttpRequest $request): HttpRequest {\n        // attach a request-id header\n        return $request-&gt;withHeader('X-Request-ID', 'hooks-demo-123');\n    }\n\n    protected function afterRequest(HttpRequest $request, HttpResponse $response): HttpResponse {\n        // log status to stdout (example side-effect)\n        fwrite(STDOUT, \"afterRequest: status=\" . $response-&gt;statusCode() . \"\\n\");\n        return $response;\n    }\n\n    protected function shouldDecorateResponse(HttpRequest $request, HttpResponse $response): bool {\n        return $response-&gt;isStreamed();\n    }\n\n    protected function toResponse(HttpRequest $request, HttpResponse $response): HttpResponse {\n        // decorate: number each emitted chunk (composition)\n        $i = 0;\n        return BaseResponseDecorator::decorate(\n            $response,\n            function (string $chunk) use (&amp;$i): string {\n                $i += 1;\n                return sprintf(\"[%02d] %s\", $i, $chunk);\n            }\n        );\n    }\n}\n\n// Mock driver returns a streamed response (full lines as chunks for simplicity)\n$driver = new MockHttpDriver();\n$driver-&gt;addResponse(\n    HttpResponse::streaming(\n        statusCode: 200,\n        headers: ['Content-Type' =&gt; 'text/plain'],\n        stream: new ArrayStream([\"alpha\\n\", \"beta\\n\", \"gamma\\n\"]),\n    ),\n    url: 'https://api.example.local/lines',\n    method: 'GET'\n);\n\n$client = new HttpClient(driver: $driver);\n$client = $client-&gt;withMiddleware(new HooksExampleMiddleware());\n\n$request = new HttpRequest(\n    url: 'https://api.example.local/lines',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'text/plain'],\n    body: '',\n    options: ['stream' =&gt; true],\n);\n\nforeach ($client-&gt;withRequest($request)-&gt;stream() as $chunk) {\n    echo $chunk; // chunks are numbered by middleware\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/http_client/http_client/http_middleware_stream/","title":"HTTP Middleware (Stream)","text":""},{"location":"cookbook/http_client/http_client/http_middleware_stream/#overview","title":"Overview","text":"<p>Demonstrates streaming HTTP middleware that prefixes each chunk with a label. Uses composition-based stream transformation.</p>"},{"location":"cookbook/http_client/http_client/http_middleware_stream/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpMiddleware;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Data\\HttpResponse;\nuse Cognesy\\Http\\Drivers\\Mock\\MockHttpDriver;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Middleware\\Base\\BaseResponseDecorator;\nuse Cognesy\\Http\\Stream\\ArrayStream;\n\n// Scenario: Streamed response where middleware prefixes each chunk\n\nclass PrefixChunksMiddleware implements HttpMiddleware\n{\n    public function handle(HttpRequest $request, CanHandleHttpRequest $next): HttpResponse {\n        // ensure we request streaming\n        $request = $request-&gt;withStreaming(true);\n        $response = $next-&gt;handle($request);\n        // decorate using composition-based stream transform\n        return BaseResponseDecorator::decorate(\n            $response,\n            fn(string $chunk): string =&gt; \"[CHUNK] \" . $chunk,\n        );\n    }\n}\n\n// Mock driver returns SSE-like chunks\n$driver = new MockHttpDriver();\n$driver-&gt;addResponse(\n    HttpResponse::streaming(\n        statusCode: 200,\n        headers: ['Content-Type' =&gt; 'text/event-stream'],\n        stream: new ArrayStream([\"hello\\n\", \"world\\n\", \"from\\n\", \"middleware\\n\"]),\n    ),\n    url: 'https://api.example.local/stream',\n    method: 'GET',\n);\n\n$client = new HttpClient(driver: $driver);\n$client = $client-&gt;withMiddleware(new PrefixChunksMiddleware());\n\n$request = new HttpRequest(\n    url: 'https://api.example.local/stream',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'text/event-stream'],\n    body: '',\n    options: ['stream' =&gt; true],\n);\n\nforeach ($client-&gt;withRequest($request)-&gt;stream() as $chunk) {\n    echo $chunk; // chunks will be prefixed by middleware\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/http_client/http_client/http_middleware_sync/","title":"HTTP Middleware (Sync)","text":""},{"location":"cookbook/http_client/http_client/http_middleware_sync/#overview","title":"Overview","text":"<p>Demonstrates synchronous HTTP middleware that adds a request header and transforms the response body by uppercasing a JSON field.</p>"},{"location":"cookbook/http_client/http_client/http_middleware_sync/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Data\\HttpResponse;\nuse Cognesy\\Http\\Drivers\\Mock\\MockHttpDriver;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\n// Scenario: Add a request header and uppercase a JSON field in a sync response\n\nclass UppercaseBodyMiddleware extends BaseMiddleware\n{\n    protected function beforeRequest(HttpRequest $request): HttpRequest {\n        // add a trace header\n        return $request-&gt;withHeader('X-Trace', 'sync-demo');\n    }\n\n    protected function afterRequest(HttpRequest $request, HttpResponse $response): HttpResponse {\n        // transform JSON body by uppercasing the \"message\" field\n        $body = $response-&gt;body();\n        $data = json_decode($body, true) ?? [];\n        if (isset($data['message']) &amp;&amp; is_string($data['message'])) {\n            $data['message'] = strtoupper($data['message']);\n            return HttpResponse::sync(\n                statusCode: $response-&gt;statusCode(),\n                headers: $response-&gt;headers(),\n                body: json_encode($data),\n            );\n        }\n        return $response;\n    }\n}\n\n// Mock driver returns a simple JSON payload\n$driver = new MockHttpDriver();\n$driver-&gt;addResponse(\n    HttpResponse::sync(\n        statusCode: 200,\n        headers: ['Content-Type' =&gt; 'application/json'],\n        body: json_encode(['message' =&gt; 'hello']),\n    ),\n    url: 'https://api.example.local/echo',\n    method: 'POST'\n);\n\n$client = new HttpClient(driver: $driver);\n$client = $client-&gt;withMiddleware(new UppercaseBodyMiddleware());\n\n$request = new HttpRequest(\n    url: 'https://api.example.local/echo',\n    method: 'POST',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: ['message' =&gt; 'hello'],\n    options: [],\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\necho \"Status: {$response-&gt;statusCode()}\\n\";\necho \"Body:   {$response-&gt;body()}\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/context_cache_llm/","title":"Context caching (text inference)","text":""},{"location":"cookbook/llm_inference/llm_advanced/context_cache_llm/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching (currently only Anthropic API), so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note 1: Instructor supports context caching for Anthropic API and OpenAI API.</p> <p>Note 2: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/llm_inference/llm_advanced/context_cache_llm/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its summary for 2 target audiences.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$cacheNonce = bin2hex(random_bytes(8));\n\n// Note: Prompt caching has minimum token requirements that vary by model:\n// - Claude Opus/Sonnet: 1,024 tokens minimum\n// - Claude Haiku 3.5: 2,048 tokens minimum\n// - Claude Haiku 4.5: 4,096 tokens minimum\n// If cached content is below threshold, caching silently doesn't occur.\n$model = 'claude-sonnet-4-20250514'; // Using Sonnet for lower cache threshold (1,024 tokens)\n\n$data = file_get_contents(__DIR__ . '/../../../README.md');\n\n$inference = (new Inference)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // wiretap to print all events\n    //-&gt;withDebugPreset('on') // debug HTTP traffic\n    -&gt;using('anthropic')\n    -&gt;withCachedContext(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Here is content of README.md file'],\n            ['role' =&gt; 'user', 'content' =&gt; $data],\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a short, very domain specific pitch of the project described in README.md. List relevant, domain specific problems that this project could solve. Use domain specific concepts and terminology to make the description resonate with the target audience.'],\n            ['role' =&gt; 'assistant', 'content' =&gt; \"For whom do you want to generate the pitch?\\nCache nonce: {$cacheNonce}\"],\n        ],\n    );\n\n$response = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'founder of lead gen SaaS startup']],\n        model: $model,\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CTO of lead gen vendor\\n\");\nprint(\"  ({$response-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response-&gt;content() . \"\\n\");\n\nassert(!empty($response-&gt;content()));\nassert(Str::contains($response-&gt;content(), 'Instructor'));\nassert(Str::contains($response-&gt;content(), 'lead', false));\nassert($response-&gt;usage()-&gt;cacheWriteTokens &gt; 0);\n\n$response2 = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'CIO of insurance company']],\n        model: $model,\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CIO of insurance company\\n\");\nprint(\"  ({$response2-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response2-&gt;content() . \"\\n\");\n\nassert(!empty($response2-&gt;content()));\nassert(Str::contains($response2-&gt;content(), 'Instructor'));\nassert(Str::contains($response2-&gt;content(), 'insurance', false));\nassert($response2-&gt;usage()-&gt;cacheReadTokens &gt; 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/context_cache_llm_oai/","title":"Context caching (text inference, OpenAI)","text":""},{"location":"cookbook/llm_inference/llm_advanced/context_cache_llm_oai/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching, so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/llm_inference/llm_advanced/context_cache_llm_oai/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its summary for 2 target audiences.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$data = file_get_contents(__DIR__ . '/../../../README.md');\n\n$inference = (new Inference)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // wiretap to print all events\n    //-&gt;withDebugPreset('on') // debug HTTP traffic\n    -&gt;using('openai')\n    -&gt;withCachedContext(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Here is content of README.md file'],\n            ['role' =&gt; 'user', 'content' =&gt; $data],\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a short, very domain specific pitch of the project described in README.md. List relevant, domain specific problems that this project could solve. Use domain specific concepts and terminology to make the description resonate with the target audience.'],\n            ['role' =&gt; 'assistant', 'content' =&gt; 'For whom do you want to generate the pitch?'],\n        ],\n    );\n\n$response = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'founder of lead gen SaaS startup']],\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CTO of lead gen vendor\\n\");\nprint(\"  ({$response-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response-&gt;content() . \"\\n\");\n\nassert(!empty($response-&gt;content()));\nassert(Str::contains($response-&gt;content(), 'Instructor'));\nassert(Str::contains($response-&gt;content(), 'lead', false));\nif ($response-&gt;usage()-&gt;cacheReadTokens === 0 &amp;&amp; $response-&gt;usage()-&gt;cacheWriteTokens === 0) {\n    print(\"Note: cacheReadTokens/cacheWriteTokens are 0. Prompt caching applies only to eligible models and prompt sizes.\\n\");\n}\n\n$response2 = $inference\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'CIO of insurance company']],\n        options: ['max_tokens' =&gt; 512],\n    )\n    -&gt;response();\n\nprint(\"----------------------------------------\\n\");\nprint(\"\\n# Summary for CIO of insurance company\\n\");\nprint(\"  ({$response2-&gt;usage()-&gt;cacheReadTokens} tokens read from cache)\\n\\n\");\nprint(\"----------------------------------------\\n\");\nprint($response2-&gt;content() . \"\\n\");\n\nassert(!empty($response2-&gt;content()));\nassert(Str::contains($response2-&gt;content(), 'Instructor'));\nassert(Str::contains($response2-&gt;content(), 'insurance', false));\nif ($response2-&gt;usage()-&gt;cacheReadTokens === 0) {\n    print(\"Note: cacheReadTokens is 0. Prompt caching applies only to eligible models and prompt sizes.\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/custom_embeddings_config/","title":"Custom Embeddings Config","text":""},{"location":"cookbook/llm_inference/llm_advanced/custom_embeddings_config/#overview","title":"Overview","text":""},{"location":"cookbook/llm_inference/llm_advanced/custom_embeddings_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n\n$query = \"technology news\";\n\n$config = new EmbeddingsConfig(\n    apiUrl    : 'https://api.cohere.ai/v1',\n    apiKey    : Env::get('COHERE_API_KEY', ''),\n    endpoint  : '/embed',\n    model     : 'embed-multilingual-v3.0',\n    dimensions: 1024,\n    maxInputs : 96,\n    driver: 'cohere',\n);\n\n$provider = EmbeddingsProvider::new()\n    -&gt;withConfig($config);\n\n$bestMatches = EmbedUtils::findSimilar(\n    provider: $provider,\n    query: $query,\n    documents: $documents,\n    topK: 3\n);\n\ndump($bestMatches);\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/custom_llm_driver/","title":"Using custom LLM driver","text":""},{"location":"cookbook/llm_inference/llm_advanced/custom_llm_driver/#overview","title":"Overview","text":"<p>You can register and use your own LLM driver, either using a new driver name or overriding an existing driver bundled with Polyglot.</p>"},{"location":"cookbook/llm_inference/llm_advanced/custom_llm_driver/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Data\\HttpResponse;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI\\OpenAIDriver;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// we will use existing, bundled driver as an example, but you can provide any class that implements\n// a required interface (CanHandleInference)\n\nInference::registerDriver(\n    name: 'custom-driver',\n    driver: fn($config, $httpClient, $events) =&gt; new class($config, $httpClient, $events) extends OpenAIDriver {\n        #[\\Override]\n        protected function makeHttpResponse(HttpRequest $request): HttpResponse {\n            // some extra functionality to demonstrate our driver is being used\n            echo \"&gt;&gt;&gt; Handling request...\\n\";\n            return parent::makeHttpResponse($request);\n        }\n    }\n);\n\n// Create instance of LLM client initialized with custom parameters\n$config = new LLMConfig(\n    apiUrl          : 'https://api.openai.com/v1',\n    apiKey          : Env::get('OPENAI_API_KEY'),\n    endpoint        : '/chat/completions', model: 'gpt-4o-mini', maxTokens: 128,\n    driver          : 'custom-driver',\n);\n\n$answer = (new Inference)\n    -&gt;withConfig($config)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/custom_llm_via_dsn/","title":"Customize LLM Configuration with DSN string","text":""},{"location":"cookbook/llm_inference/llm_advanced/custom_llm_via_dsn/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration data to <code>Inference</code> object with DSN string. This is useful for inline configuration or for building configuration from admin UI, CLI arguments or environment variables.</p>"},{"location":"cookbook/llm_inference/llm_advanced/custom_llm_via_dsn/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n$answer = (new Inference)\n    -&gt;withDsn('preset=xai,model=grok-3')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/embed_utils/","title":"Embeddings utils","text":""},{"location":"cookbook/llm_inference/llm_advanced/embed_utils/#overview","title":"Overview","text":"<p><code>EmbedUtils</code> class offers convenient methods to find top K vectors or documents most similar to provided query.</p> <p>Check out the <code>EmbedUtils</code> class for more details.  - <code>EmbedUtils::findTopK()</code>  - <code>EmbedUtils::findSimilar()</code></p> <p>Embeddings providers access details can be found and modified via <code>/config/embed.php</code>.</p>"},{"location":"cookbook/llm_inference/llm_advanced/embed_utils/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n\n$query = \"technology news\";\n\n$presets = [\n    'azure',\n    'cohere',\n    'gemini',\n    'jina',\n    'mistral',\n    //'ollama',\n    'openai'\n];\n\nforeach($presets as $preset) {\n    $bestMatches = EmbedUtils::findSimilar(\n        provider: EmbeddingsProvider::using($preset),\n        query: $query,\n        documents: $documents,\n        topK: 3\n    );\n\n    echo \"\\n[$preset]\\n\";\n    dump($bestMatches);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/embeddings/","title":"Embeddings","text":""},{"location":"cookbook/llm_inference/llm_advanced/embeddings/#overview","title":"Overview","text":"<p><code>Embeddings</code> class offers access to embeddings APIs which allows to generate vector representations of inputs. These embeddings can be used to compare semantic similarity between inputs, e.g. to find relevant documents based on a query.</p> <p><code>Embeddings</code> class supports following embeddings providers:  - Azure  - Cohere  - Gemini  - Jina  - Mistral  - OpenAI</p> <p>Embeddings providers access details can be found and modified via <code>/config/embed.php</code>.</p> <p>To store and search across large sets of vector embeddings you may want to use one of the popular vector databases: PGVector, Chroma, Pinecone, Weaviate, Milvus, etc.</p>"},{"location":"cookbook/llm_inference/llm_advanced/embeddings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\nuse Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$query = \"technology news\";\n$documents = [\n    'Computer vision models are used to analyze images and videos.',\n    'The bakers at the Nashville Bakery baked 200 loaves of bread on Monday morning.',\n    'The new movie starring Tom Hanks is now playing in theaters.',\n    'Famous soccer player Lionel Messi has arrived in town.',\n    'News about the latest iPhone model has been leaked.',\n    'New car model by Tesla is now available for pre-order.',\n    'Philip K. Dick is an author of many sci-fi novels.',\n];\n$inputs = array_merge([$query], $documents);\n\n$topK = 3;\n\n// generate embeddings for query and documents (in a single request)\n$response = (new Embeddings)\n    -&gt;using('openai')\n    -&gt;withInputs($inputs)\n    -&gt;get();\n\n// get query and doc vectors from the response\n[$queryVectors, $docVectors] = $response-&gt;split(1);\n\n$queryVector = $queryVectors[0]\n    ?? throw new \\InvalidArgumentException('Query vector not found');\n\n// calculate cosine similarities\n$similarities = EmbedUtils::findTopK($queryVector, $docVectors, $topK);\n\n// print documents most similar to the query\necho \"Query: \" . $query . PHP_EOL;\n$count = 1;\nforeach($similarities as $index =&gt; $similarity) {\n    echo $count++;\n    echo ': ' . $documents[$index];\n    echo ' - cosine similarity to query = ' . $similarities[$index];\n    echo PHP_EOL;\n}\n\nassert(!empty($similarities));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/http_client/","title":"Work directly with HTTP client facade","text":""},{"location":"cookbook/llm_inference/llm_advanced/http_client/#overview","title":"Overview","text":""},{"location":"cookbook/llm_inference/llm_advanced/http_client/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// check with default HTTP client facade\n$httpClient = new HttpClient();\n\n$answer = (new Inference)\n    -&gt;withDsn('preset=openai,model=gpt-3.5-turbo')\n    -&gt;withHttpClient($httpClient)\n    -&gt;withMessages('What is the capital of France')\n    -&gt;withMaxTokens(64)\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/llm_config_providers/","title":"Customize configuration providers of LLM driver","text":""},{"location":"cookbook/llm_inference/llm_advanced/llm_config_providers/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to <code>Inference</code> object. This is useful when you want to initialize LLM client with custom values.</p>"},{"location":"cookbook/llm_inference/llm_advanced/llm_config_providers/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Adbar\\Dot;\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Creation\\HttpClientBuilder;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\n$configData = [\n    'http' =&gt; [\n        'defaultPreset' =&gt; 'symfony',\n        'presets' =&gt; [\n            'symfony' =&gt; [\n                'driver' =&gt; 'symfony',\n                'connectTimeout' =&gt; 10,\n                'requestTimeout' =&gt; 30,\n                'idleTimeout' =&gt; -1,\n                'maxConcurrent' =&gt; 5,\n                'poolTimeout' =&gt; 60,\n                'failOnError' =&gt; true,\n            ],\n            // Add more HTTP presets as needed\n        ],\n    ],\n    'debug' =&gt; [\n        'defaultPreset' =&gt; 'off',\n        'presets' =&gt; [\n            'off' =&gt; [\n                'httpEnabled' =&gt; false,\n            ],\n            'on' =&gt; [\n                'httpEnabled' =&gt; true,\n                'httpTrace' =&gt; true,\n                'httpRequestUrl' =&gt; true,\n                'httpRequestHeaders' =&gt; true,\n                'httpRequestBody' =&gt; true,\n                'httpResponseHeaders' =&gt; true,\n                'httpResponseBody' =&gt; true,\n                'httpResponseStream' =&gt; true,\n                'httpResponseStreamByLine' =&gt; true,\n            ],\n        ],\n    ],\n    'llm' =&gt; [\n        'defaultPreset' =&gt; 'deepseek',\n        'presets' =&gt; [\n            'deepseek' =&gt; [\n                'apiUrl' =&gt; 'https://api.deepseek.com',\n                'apiKey' =&gt; Env::get('DEEPSEEK_API_KEY'),\n                'endpoint' =&gt; '/chat/completions',\n                'model' =&gt; 'deepseek-chat',\n                'maxTokens' =&gt; 128,\n                'driver' =&gt; 'deepseek',\n            ],\n            'openai' =&gt; [\n                'apiUrl' =&gt; 'https://api.openai.com',\n                'apiKey' =&gt; Env::get('OPENAI_API_KEY'),\n                'endpoint' =&gt; '/v1/chat/completions',\n                'model' =&gt; 'gpt-4',\n                'maxTokens' =&gt; 256,\n                'driver' =&gt; 'openai',\n            ],\n        ],\n    ],\n];\n\nclass CustomConfigProvider implements CanProvideConfig\n{\n    private Dot $dot;\n\n    public function __construct(array $data = []) {\n        $this-&gt;dot = new Dot($data);\n    }\n\n    public function get(string $path, mixed $default = null): mixed {\n        return $this-&gt;dot-&gt;get($path, $default);\n    }\n\n    public function has(string $path): bool {\n        return $this-&gt;dot-&gt;has($path);\n    }\n}\n\n$configProvider = new CustomConfigProvider($configData);\n\n$events = new EventDispatcher();\n$customClient = (new HttpClientBuilder(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withClientInstance(\n        driverName: 'symfony',\n        clientInstance: SymfonyHttpClient::create(['http_version' =&gt; '2.0']),\n    )\n    -&gt;create();\n\n$inference = (new Inference(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withHttpClient($customClient);\n\n$answer = $inference\n    -&gt;using('deepseek') // Use 'deepseek' preset from CustomLLMConfigProvider\n    //-&gt;withDebugPreset('on')\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']])\n    -&gt;withMaxTokens(256)\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/llm_custom_config/","title":"Customize configuration of LLM driver","text":""},{"location":"cookbook/llm_inference/llm_advanced/llm_custom_config/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to <code>Inference</code> object. This is useful when you want to initialize LLM client with custom values.</p>"},{"location":"cookbook/llm_inference/llm_advanced/llm_custom_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Creation\\HttpClientBuilder;\nuse Cognesy\\Http\\Drivers\\Symfony\\SymfonyDriver;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\n$events = new EventDispatcher();\n\n// Build fully customized HTTP client\n\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 30,\n    requestTimeout: 60,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true,\n);\n\n$yourClientInstance = SymfonyHttpClient::create(['http_version' =&gt; '2.0']);\n\n$customClient = (new HttpClientBuilder)\n    -&gt;withEventBus($events)\n    -&gt;withDriver(new SymfonyDriver(\n        config: $httpConfig,\n        clientInstance: $yourClientInstance,\n        events: $events,\n    ))\n    -&gt;create();\n\n// Create instance of LLM client initialized with custom parameters\n\n$config = new LLMConfig(\n    apiUrl  : 'https://api.deepseek.com',\n    apiKey  : Env::get('DEEPSEEK_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'deepseek-chat',\n    maxTokens: 128,\n    driver: 'deepseek',\n);\n\n// Call inference API with custom client and configuration\n\n$answer = (new Inference)\n    -&gt;withEventHandler($events)\n    -&gt;withConfig($config)\n    -&gt;withHttpClient($customClient)\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/parallel_calls/","title":"Parallel Calls","text":""},{"location":"cookbook/llm_inference/llm_advanced/parallel_calls/#overview","title":"Overview","text":"<p>Work in progress.</p>"},{"location":"cookbook/llm_inference/llm_advanced/parallel_calls/#example","title":"Example","text":"<pre><code>&lt;?php\necho \"This example is not yet implemented.\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_advanced/reasoning_content/","title":"Reasoning Content Access","text":""},{"location":"cookbook/llm_inference/llm_advanced/reasoning_content/#overview","title":"Overview","text":"<p>Deepseek API allows to access reasoning content, which is a detailed explanation of how the response was generated. This feature is useful for debugging and understanding the reasoning behind the response.</p>"},{"location":"cookbook/llm_inference/llm_advanced/reasoning_content/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: regular API, allows to customize inference options\n$response = (new Inference)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;using('deepseek-r')\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France. Answer with just a name.']])\n    -&gt;withMaxTokens(256)\n    -&gt;response();\n\necho \"\\nCASE #1: Sync response\\n\";\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: {$response-&gt;content()}\\n\";\necho \"REASONING: {$response-&gt;reasoningContent()}\\n\";\nassert($response-&gt;content() !== '');\nassert(Str::contains($response-&gt;content(), 'Paris'));\nassert($response-&gt;reasoningContent() !== '');\n\n\n// EXAMPLE 2: streaming response\n$stream = (new Inference)\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;using('deepseek-r') // optional, default is set in /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of Brasil. Answer with just a name.']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;withStreaming()\n    -&gt;stream();\n\necho \"\\nCASE #2: Streamed response\\n\";\necho \"USER: What is capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream-&gt;responses() as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\necho \"REASONING: {$stream-&gt;final()-&gt;reasoningContent()}\\n\";\nassert($stream-&gt;final()-&gt;reasoningContent() !== '');\nassert($stream-&gt;final()-&gt;content() !== '');\nassert(Str::contains($stream-&gt;final()-&gt;content(), 'Bras\u00edlia'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_a21/","title":"A21","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_a21/#overview","title":"Overview","text":"<p>Support for A21 Jamba - MAMBA architecture models, very strong at handling long context.</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_a21/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('a21') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_anthropic/","title":"Anthropic","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_anthropic/#overview","title":"Overview","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - OutputMode::MdJson, OutputMode::Json - supported - OutputMode::Tools - not supported yet</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('anthropic') // see /config/llm.php\n    //-&gt;withHttpClientPreset('guzzle')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 128]\n    )\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_azure_openai/","title":"Azure OpenAI","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_azure_openai/#overview","title":"Overview","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_azure_openai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_cerebras/","title":"Cerebras","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_cerebras/#overview","title":"Overview","text":"<p>Support for Cerebras API which uses custom hardware for super fast inference. Cerebras provides Llama models.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_cerebras/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('cerebras') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_cohere/","title":"Cohere","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_cohere/#overview","title":"Overview","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - OutputMode::MdJson - supported, recommended as a fallback from JSON mode  - OutputMode::Json - supported, recommended  - OutputMode::Tools - partially supported, not recommended</p> <p>Reasons OutputMode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple, flat data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul>"},{"location":"cookbook/llm_inference/llm_api_support/llm_cohere/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('cohere') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_deepseek/","title":"DeepSeek","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_deepseek/#overview","title":"Overview","text":"<p>Support for DeepSeek API which provides strong models at affordable price.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_deepseek/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('deepseek') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_fireworks/","title":"Fireworks.ai","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_fireworks/#overview","title":"Overview","text":"<p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - selected models - OutputMode::Json - selected models - OutputMode::MdJson</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_fireworks/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('fireworks') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_google_gemini/","title":"Google Gemini","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_google_gemini/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('gemini')\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // optional, for debugging\n    -&gt;withDebugPreset('detailed')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_google_gemini_oai/","title":"Google Gemini (OpenAI-compatible)","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_google_gemini_oai/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API in OpenAI-compatible mode.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('gemini-oai') // use OpenAI-compatible Gemini preset (v1beta/openai)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print()) // optional, for debugging\n    -&gt;withDebugPreset('detailed')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_groq/","title":"Groq","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_groq/#overview","title":"Overview","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma.</p> <p>Supported modes depend on the specific model, but generally include:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Groq API.</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_groq/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('groq') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_inception/","title":"Inception","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_inception/#overview","title":"Overview","text":"<p>Inception API provides OpenAI-compatible endpoints for chat completions.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (supported)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_inception/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('inception') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_meta/","title":"Meta","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_meta/#overview","title":"Overview","text":"<p>Instructor supports Meta LLM inference API. You can find the details on how to configure</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_meta/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('meta') // see /config/llm.php\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_minimaxi/","title":"Minimaxi","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_minimaxi/#overview","title":"Overview","text":"<p>Support for Minimaxi's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_minimaxi/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('minimaxi') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_mistralai/","title":"Mistral AI","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_mistralai/#overview","title":"Overview","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility:  - OutputMode::Tools - supported (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::Json - recommended (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::MdJson - fallback mode (Mistral 7B / Mixtral 8x7B)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_mistralai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('mistral') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_moonshotai/","title":"MoonshotAI","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_moonshotai/#overview","title":"Overview","text":"<p>Support for MoonshotAI's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_moonshotai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('moonshot-kimi') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_ollama/","title":"Local / Ollama","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_ollama/#overview","title":"Overview","text":"<p>You can use Instructor with local Ollama instance.</p> <p>Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model for complex data schemas.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode, works with any capable model  - OutputMode::Json - recommended  - OutputMode::Tools - supported (for selected models - check Ollama docs)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_ollama/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('ollama') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_openai-responses/","title":"OpenAI Responses API","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_openai-responses/#overview","title":"Overview","text":"<p>OpenAI's Responses API is their new recommended API for inference, offering improved performance and features compared to Chat Completions.</p> <p>Key features: - 3% better performance on reasoning tasks - 40-80% improved cache utilization - Built-in tools: web search, file search, code interpreter - Server-side conversation state via <code>previous_response_id</code> - Semantic streaming events</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_openai-responses/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai-responses') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_openai/","title":"OpenAI","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_openai/#overview","title":"Overview","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended for new models)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_openai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_openrouter/","title":"OpenRouter","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_openrouter/#overview","title":"Overview","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_openrouter/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('openrouter') // see /config/llm.php\n    -&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_perplexity/","title":"Perplexity","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_perplexity/#overview","title":"Overview","text":"<p>Perplexity is a search engine that provides an API for generating text. It is designed to be used in a variety of applications, including chatbots, content generation, and more.</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_perplexity/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('perplexity') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 256]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_sambanova/","title":"SambaNova","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_sambanova/#overview","title":"Overview","text":"<p>Support for SambaNova's API, which provide fast inference endpoints for Llama and Qwen LLMs.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_sambanova/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('sambanova') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_togetherai/","title":"Together.ai","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_togetherai/#overview","title":"Overview","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support OutputMode::Tools or OutputMode::Json, which are much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - supported for selected models - OutputMode::Json - supported for selected models - OutputMode::MdJson - fallback mode</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_togetherai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('together') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_api_support/llm_xai/","title":"xAI / Grok","text":""},{"location":"cookbook/llm_inference/llm_api_support/llm_xai/#overview","title":"Overview","text":"<p>Support for xAI's API, which offers access to X.com's Grok model.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/llm_inference/llm_api_support/llm_xai/#example","title":"Example","text":"<pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\nrequire 'examples/boot.php';\n\n$answer = (new Inference)\n    -&gt;using('xai') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\nassert(Str::contains($answer, 'Paris'));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/inference/","title":"Working directly with LLMs","text":""},{"location":"cookbook/llm_inference/llm_basics/inference/#overview","title":"Overview","text":"<p><code>Inference</code> class offers access to LLM APIs and convenient methods to execute model inference, incl. chat completions, tool calling or JSON output generation.</p> <p>LLM providers access details can be found and modified via <code>/config/llm.php</code>.</p>"},{"location":"cookbook/llm_inference/llm_basics/inference/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: use default LLM connection preset for convenient ad-hoc calls\n$answer = (new Inference)\n    -&gt;with(messages: 'What is capital of Germany')\n    -&gt;get();\n\necho \"USER: What is capital of Germany\\n\";\necho \"ASSISTANT: $answer\\n\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n\n\n\n// EXAMPLE 2: customize inference options using fluent API\n$response = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;create();\n\n$answer = $response-&gt;get();\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\\n\";\nassert(Str::contains($answer, 'Paris'));\n\n\n\n\n// EXAMPLE 3: streaming response\n$stream = (new Inference)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'Describe capital of Brasil']])\n    -&gt;withOptions(['max_tokens' =&gt; 128])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: Describe capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/llm_json/","title":"Working directly with LLMs and JSON - JSON mode","text":""},{"location":"cookbook/llm_inference/llm_basics/llm_json/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p> <p><code>Inference</code> class supports multiple inference modes, like <code>Tools</code>, <code>Json</code> <code>JsonSchema</code> or <code>MdJson</code>, which gives you flexibility to choose the best approach for your use case.</p>"},{"location":"cookbook/llm_inference/llm_basics/llm_json/#example","title":"Example","text":"<p>In this example we will use OpenAI JSON mode, which guarantees that the response will be in a JSON format.</p> <p>It does not guarantee compliance with a specific schema (for some providers including OpenAI). We can try to work around it by providing an example of the expected JSON output in the prompt.</p> <p>NOTE: Some model providers allow to specify a JSON schema for model to follow via <code>schema</code> parameter of <code>response_format</code>. OpenAI does not support this feature in JSON mode (only in JSON Schema mode).</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with JSON data containing name\", population and year of founding. \\\n           Example: {\"name\": \"Berlin\", \"population\": 3700000, \"founded\": 1237}']],\n        responseFormat: [\n            'type' =&gt; 'json_object',\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Json,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/llm_json_schema/","title":"Working directly with LLMs and JSON - JSON Schema mode","text":""},{"location":"cookbook/llm_inference/llm_basics/llm_json_schema/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/llm_inference/llm_basics/llm_json_schema/#example","title":"Example","text":"<p>In this example we will use OpenAI JSON Schema mode, which guarantees that the response will be in a JSON format that matches the provided schema.</p> <p>NOTE: Json Schema mode with guaranteed structured outputs is not supported by all language model providers.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n        Respond with JSON data.']],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; [\n                    'type' =&gt; 'object',\n                    'description' =&gt; 'City information',\n                    'properties' =&gt; [\n                        'name' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'City name',\n                        ],\n                        'founded' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Founding year',\n                        ],\n                        'population' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Current population',\n                        ],\n                    ],\n                    'additionalProperties' =&gt; false,\n                    'required' =&gt; ['name', 'founded', 'population'],\n                ],\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/llm_md_json/","title":"Working directly with LLMs and JSON - MdJSON mode","text":""},{"location":"cookbook/llm_inference/llm_basics/llm_md_json/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/llm_inference/llm_basics/llm_md_json/#example","title":"Example","text":"<p>In this example we will use emulation mode - MdJson, which tries to force the model to generate a JSON output by asking it to respond with a JSON object within a Markdown code block.</p> <p>This is useful for the models which do not support JSON output directly.</p> <p>We will also provide an example of the expected JSON output in the prompt to guide the model in generating the correct response.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with a JSON object in a ```json``` code block containing \"name\", \"population\", and \"founded\". \\\n           Use integer values for population and founded year (negative for BC). Do not include extra text. \\\n           Example: {\"name\":\"Paris\",\"population\":2139000,\"founded\":-250}']],\n        options: ['max_tokens' =&gt; 64, 'temperature' =&gt; 0],\n        mode: OutputMode::MdJson,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/llm_tools/","title":"Working directly with LLMs and JSON - Tools mode","text":""},{"location":"cookbook/llm_inference/llm_basics/llm_tools/#overview","title":"Overview","text":"<p>While working with <code>Inference</code> class, you can also generate JSON output from the model inference. This is useful for example when you need to process the response in a structured way or when you want to store the elements of the response in a database.</p>"},{"location":"cookbook/llm_inference/llm_basics/llm_tools/#example","title":"Example","text":"<p>In this example we will use OpenAI tools mode, in which model will generate a JSON containing arguments for a function call. This way we can make the model generate a JSON object with specific structure of parameters.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$response = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n           Respond with function call.']],\n        tools: [[\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'extract_data',\n                'description' =&gt; 'Extract city data',\n                'parameters' =&gt; [\n                    'type' =&gt; 'object',\n                    'description' =&gt; 'City information',\n                    'properties' =&gt; [\n                        'name' =&gt; [\n                            'type' =&gt; 'string',\n                            'description' =&gt; 'City name',\n                        ],\n                        'founded' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Founding year',\n                        ],\n                        'population' =&gt; [\n                            'type' =&gt; 'integer',\n                            'description' =&gt; 'Current population',\n                        ],\n                    ],\n                    'required' =&gt; ['name', 'founded', 'population'],\n                    'additionalProperties' =&gt; false,\n                ],\n            ],\n        ]],\n        toolChoice: [\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'extract_data'\n            ]\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Tools,\n    )\n    -&gt;response();\n\n$data = $response-&gt;findJsonData(OutputMode::Tools)-&gt;toArray();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(is_string($data['name']) &amp;&amp; $data['name'] !== '', 'City name should be a non-empty string');\nassert(array_key_exists('population', $data), 'Response should have \"population\" field');\nassert(array_key_exists('founded', $data), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/llm_with_schema_helper/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/llm_inference/llm_basics/llm_with_schema_helper/#overview","title":"Overview","text":"<p>Polyglot has a built-in support for dynamically constructing JSON Schema using <code>JsonSchema</code> class. It is useful when you want to shape the structures during runtime.</p>"},{"location":"cookbook/llm_inference/llm_basics/llm_with_schema_helper/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'City name'),\n        JsonSchema::integer('population', 'City population'),\n        JsonSchema::integer('founded', 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with JSON data.']\n        ],\n        responseFormat: $schema-&gt;toResponseFormat(\n            schemaName: 'city_data',\n            schemaDescription: 'City data',\n            strict: true,\n        ),\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data));\nassert(is_string($data['name']));\nassert(is_int($data['population']));\nassert(is_int($data['founded']));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_basics/llm_with_tools_helper/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/llm_inference/llm_basics/llm_with_tools_helper/#overview","title":"Overview","text":"<p>Polyglot has a built-in support for dynamically constructing tool calling schema using <code>JsonSchema</code> class.</p>"},{"location":"cookbook/llm_inference/llm_basics/llm_with_tools_helper/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'City name'),\n        JsonSchema::integer('population', 'City population'),\n        JsonSchema::integer('founded', 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n$data = (new Inference)\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with function call.']\n        ],\n        tools: [\n            $schema-&gt;toFunctionCall(\n               functionName: 'provide_data',\n               functionDescription: 'Provide city data'\n            )\n        ],\n        toolChoice: [\n            'type' =&gt; 'function',\n            'function' =&gt; [\n                'name' =&gt; 'provide_data'\n            ]\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::Tools,\n    )\n    -&gt;asJsonData();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data));\nassert(is_string($data['name']));\nassert(is_int($data['population']));\nassert(is_int($data['founded']));\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/chat_with_many_participants/","title":"Multi-Participant AI Chat Panel Discussion","text":""},{"location":"cookbook/llm_inference/llm_extras/chat_with_many_participants/#overview","title":"Overview","text":"<p>This example demonstrates a sophisticated multi-participant chat system featuring: - System prompt isolation - each AI participant has their own persona - Role normalization - proper LLM role mapping for multi-participant conversations - AI-powered moderation - LLM coordinator decides who should speak next based on context - Clean state management - everything configured in immutable ChatState - Type-safe participant selection - StructuredOutput for decision making</p>"},{"location":"cookbook/llm_inference/llm_extras/chat_with_many_participants/#example","title":"Example","text":"<pre><code>&lt;?php\n\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Chat\\ChatFactory;\nuse Cognesy\\Addons\\Chat\\Collections\\Participants;\nuse Cognesy\\Addons\\Chat\\Data\\ChatState;\nuse Cognesy\\Addons\\Chat\\Participants\\LLMParticipant;\nuse Cognesy\\Addons\\Chat\\Participants\\ScriptedParticipant;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ResponseContentCheck;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\necho \"\ud83c\udf99\ufe0f AI PANEL DISCUSSION: The Future of AI Development\\n\";\necho \"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\";\n\n// Create participants with distinct personas using system prompts\n\n$moderator = new ScriptedParticipant(\n    name: 'moderator',\n    messages: [\n        \"Welcome to our panel! Could you each introduce yourselves and share your main focus area in AI?\",\n        \"What do you see as the biggest challenge in AI adoption today?\",\n        \"How do you balance rapid innovation with responsible deployment?\",\n        \"What role should public funding play in AI research and development?\",\n        \"Any final thoughts for our audience about the future of AI?\",\n        \"\", // Empty string to signal end of discussion\n    ],\n);\n\n$researcher = new LLMParticipant(\n    name: 'dr_chen',\n    llmProvider: LLMProvider::using('openai'),\n    systemPrompt: 'You are Dr. Sarah Chen, a distinguished AI researcher at MIT focusing on machine reasoning and safety. You participate in a panel with other experts and a moderator. You speak from deep academic knowledge, cite research when relevant, and always consider long-term implications. Keep responses concise but insightful - 2-3 sentences max. Always end with your signature: \"- Dr. Chen\"'\n);\n\n$engineer = new LLMParticipant(\n    name: 'marcus',\n    llmProvider: LLMProvider::using('openai'),\n    systemPrompt: 'You are Marcus Rodriguez, a Senior AI Engineer at a major tech company with 10+ years building production AI systems. You participate in a panel with other experts and a moderator. You focus on practical implementation, scalability, and real-world challenges. Keep responses brief and pragmatic - 2-3 sentences max. Always end with: \"- Marcus\"'\n);\n\n// Run the panel discussion\n$chat = ChatFactory::default(\n    participants: new Participants($moderator, $engineer, $researcher),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(15, fn(ChatState $state): int =&gt; $state-&gt;stepCount()),\n        new ResponseContentCheck(\n            fn(ChatState $state): ?Messages =&gt; $state-&gt;currentStep()?-&gt;outputMessages(),\n            static fn(Messages $lastResponse): bool =&gt; $lastResponse-&gt;last()-&gt;content()-&gt;toString() !== '',\n        ),\n    ),\n); //-&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print());\n\n$participantNames = [\n    'moderator' =&gt; '\ud83c\udf99\ufe0f Moderator',\n    'dr_chen' =&gt; '\ud83d\udd2c Dr. Chen',\n    'marcus' =&gt; '\u2699\ufe0f Marcus',\n];\n\n$state = new ChatState();\n\nwhile ($chat-&gt;hasNextStep($state)) {\n    $state = $chat-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n\n    if ($step) {\n        $participantName = $step-&gt;participantName();\n        $content = trim($step-&gt;outputMessages()-&gt;toString());\n\n        // Only display if there's actual content\n        if (!empty($content)) {\n            $displayName = $participantNames[$participantName] ?? \"\ud83e\udd16 $participantName\";\n            echo \"\\n$displayName:\\n\";\n            echo str_repeat('-', strlen($displayName)) . \"\\n\";\n            echo \"$content\\n\\n\";\n        }\n    }\n}\necho \"\ud83c\udfac Panel discussion concluded!\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/chat_with_summary/","title":"Chat with summary","text":""},{"location":"cookbook/llm_inference/llm_extras/chat_with_summary/#overview","title":"Overview","text":""},{"location":"cookbook/llm_inference/llm_extras/chat_with_summary/#example","title":"Example","text":"<pre><code>&lt;?php\n\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Chat\\ChatFactory;\nuse Cognesy\\Addons\\Chat\\Collections\\Participants;\nuse Cognesy\\Addons\\Chat\\Data\\ChatState;\nuse Cognesy\\Addons\\Chat\\Participants\\LLMParticipant;\nuse Cognesy\\Addons\\Chat\\Participants\\ScriptedParticipant;\nuse Cognesy\\Addons\\Chat\\Utils\\SummarizeMessages;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ResponseContentCheck;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\AccumulateTokenUsage;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\AppendStepMessages;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\MoveMessagesToBuffer;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\Processors\\SummarizeBuffer;\nuse Cognesy\\Addons\\StepByStep\\StateProcessing\\StateProcessors;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n$events = new EventDispatcher();\n\n$student = new ScriptedParticipant(\n    name: 'student',\n    messages: [\n        'Help me get better sales results.',\n        'What should I do next?',\n        'Give me one more actionable tip.',\n        'How could I apply this in practice?',\n        \"What are some common pitfalls to avoid?\",\n        \"Is there a specific mindset I should adopt?\",\n        \"Can you provide an example of a successful sales interaction using Challenger Sale?\",\n        \"How can I tailor my approach to different types of clients?\",\n        \"What questions should I be asking my prospects?\",\n        \"How do I handle objections effectively?\",\n        \"What should I focus on to improve my sales approach?\",\n        \"How can I measure the success of these strategies?\",\n        \"What resources can I use to learn more about Challenger Sale?\",\n        \"Any final advice for implementing these techniques effectively?\",\n        '' // Empty string to signal end of conversation\n    ],\n);\n\n$expert = new LLMParticipant(\n    name: 'expert',\n    llmProvider: LLMProvider::using('openai'),\n    systemPrompt: 'You are a helpful assistant explaining Challenger Sale. Be very brief (one sentence), pragmatic and focused on practical bizdev problems.'\n);\n\n// Build a Chat with summary + buffer processors and an assistant participant\n$chat = ChatFactory::default(\n    participants: new Participants($student, $expert),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(30, fn(ChatState $state): int =&gt; $state-&gt;stepCount()),\n        new ResponseContentCheck(\n            fn(ChatState $state): ?Messages =&gt; $state-&gt;currentStep()?-&gt;outputMessages(),\n            static fn(Messages $lastResponse): bool =&gt; $lastResponse-&gt;toString() !== '',\n        ),\n    ),\n    processors: new StateProcessors(\n        new AccumulateTokenUsage(),\n        new AppendStepMessages(),\n        new MoveMessagesToBuffer(\n            maxTokens: 128,\n            bufferSection: 'buffer',\n            events: $events\n        ),\n        new SummarizeBuffer(\n            maxBufferTokens: 128,\n            maxSummaryTokens: 512,\n            bufferSection: 'buffer',\n            summarySection: 'summary',\n            summarizer: new SummarizeMessages(llm: LLMProvider::using('openai')),\n            events: $events,\n        ),\n    ),\n    events: $events,\n);//-&gt;wiretap(fn(Event $e) =&gt; $e-&gt;printDebug());\n\n$context = \"# CONTEXT\\n\\n\" . file_get_contents(__DIR__ . '/summary.md');\n\n$state = (new ChatState)-&gt;withMessages(\n    Messages::fromString(content: $context, role: 'system')\n);\n\nwhile ($chat-&gt;hasNextStep($state)) {\n    $state = $chat-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n\n    $name = $step?-&gt;participantName() ?? 'unknown';\n    $content = trim($step?-&gt;outputMessages()-&gt;toString() ?? '');\n    echo \"\\n--- Step \" . ($state-&gt;stepCount()) . \" ($name) ---\\n\";\n    echo ($content ?: '[eot]'). \"\\n\";\n//    echo \"---------------------\\n\";\n//    echo \"SUMMARY:\\n\" . $state-&gt;store()-&gt;section('summary')-&gt;get()?-&gt;toString();\n//    echo \"---------------------\\n\";\n//    echo \"BUFFER:\\n\" . $state-&gt;store()-&gt;section('buffer')-&gt;get()?-&gt;toString();\n//    echo \"---------------------\\n\";\n//    echo \"MESSAGES:\\n\" . $state-&gt;store()-&gt;section('messages')-&gt;get()?-&gt;toString();\n//    echo \"=====================\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/image_data/","title":"Using images in prompts","text":""},{"location":"cookbook/llm_inference/llm_extras/image_data/#overview","title":"Overview","text":"<p><code>Image</code> class in Instructor PHP provides an easy way to include images in your prompts. It supports loading images from files, URLs, or base64 encoded strings. The image can be sent as part of the message content to the LLM.</p>"},{"location":"cookbook/llm_inference/llm_extras/image_data/#example","title":"Example","text":"<p>```php</p> asSystem('You are an expert in car damage assessment.')     -&gt;asUser(Content::empty()         -&gt;addContentPart(ContentPart::text('Describe the car damage in the image.'))         -&gt;addContentPart(Image::fromFile(__DIR__ . '/car-damage.jpg')-&gt;toContentPart())     );  $response = (new Inference)     -&gt;using('openai')     -&gt;withModel('gpt-4o-mini')     -&gt;withMessages($messages)     -&gt;get();  echo \"Response: \" . $response . \"\\n\";   ?&gt;"},{"location":"cookbook/llm_inference/llm_extras/metrics_streaming/","title":"Streaming metrics (Polyglot)","text":""},{"location":"cookbook/llm_inference/llm_extras/metrics_streaming/#overview","title":"Overview","text":"<p>Collect simple streaming metrics from Polyglot inference events: time to first chunk, stream duration, chunk count (streamed deltas), and average output tokens per second.</p>"},{"location":"cookbook/llm_inference/llm_extras/metrics_streaming/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Metrics\\Collectors\\MetricsCollector;\nuse Cognesy\\Metrics\\Data\\Metric;\nuse Cognesy\\Metrics\\Exporters\\CallbackExporter;\nuse Cognesy\\Metrics\\Metrics;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceCompleted;\nuse Cognesy\\Polyglot\\Inference\\Events\\PartialInferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Events\\StreamFirstChunkReceived;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nfinal class StreamMetricsCollector extends MetricsCollector\n{\n    private int $chunkCount = 0;\n\n    protected function listeners(): array {\n        return [\n            StreamFirstChunkReceived::class =&gt; 'onFirstChunk',\n            PartialInferenceResponseCreated::class =&gt; 'onChunk',\n            InferenceCompleted::class =&gt; 'onCompleted',\n        ];\n    }\n\n    public function onFirstChunk(StreamFirstChunkReceived $event): void {\n        $this-&gt;timer('llm.stream.ttfc_ms', $event-&gt;timeToFirstChunkMs, [\n            'model' =&gt; $this-&gt;modelTag($event-&gt;model),\n        ]);\n    }\n\n    public function onChunk(PartialInferenceResponseCreated $event): void {\n        $this-&gt;chunkCount += 1;\n    }\n\n    public function onCompleted(InferenceCompleted $event): void {\n        $durationSeconds = max(0.001, $event-&gt;durationMs / 1000);\n        $outputTokens = $event-&gt;usage-&gt;output();\n        $tokensPerSecond = $outputTokens / $durationSeconds;\n\n        $this-&gt;timer('llm.stream.duration_ms', $event-&gt;durationMs);\n        $this-&gt;gauge('llm.stream.chunk_count', (float) $this-&gt;chunkCount);\n        $this-&gt;gauge('llm.stream.output_tokens', (float) $outputTokens);\n        $this-&gt;gauge('llm.stream.output_tokens_per_second', $tokensPerSecond);\n\n        $this-&gt;chunkCount = 0;\n    }\n\n    private function modelTag(?string $model): string {\n        if ($model !== null &amp;&amp; $model !== '') {\n            return $model;\n        }\n        return 'default';\n    }\n}\n\n$events = new EventDispatcher();\n$metrics = new Metrics($events);\n$metrics-&gt;collect(new StreamMetricsCollector());\n\n$metrics-&gt;exportTo(new CallbackExporter(function (iterable $metrics): void {\n    $aggregates = aggregateMetrics($metrics);\n    foreach ($aggregates as $aggregate) {\n        $tagsOutput = formatTags($aggregate['tags']);\n        $value = aggregatedValue($aggregate);\n        printf(\"[%s] %s%s = %.2f\\n\", $aggregate['type'], $aggregate['name'], $tagsOutput, $value);\n    }\n}));\n\n$prompt = 'In one sentence, explain why streaming responses help UX.';\n$stream = (new Inference($events))\n    -&gt;withMessages($prompt)\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: {$prompt}\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\\n\";\n\n$metrics-&gt;export();\n\nfunction formatTags(array $tags): string {\n    if ($tags === []) {\n        return '';\n    }\n\n    $keys = array_keys($tags);\n    $values = array_values($tags);\n    $tagList = array_map(\n        static fn (string $key, mixed $value): string =&gt; \"{$key}=\\\"{$value}\\\"\",\n        $keys,\n        $values,\n    );\n\n    return ' {' . implode(', ', $tagList) . '}';\n}\n\n/**\n * @param iterable&lt;Metric&gt; $metrics\n * @return array&lt;int, array{type: string, name: string, tags: array, count: int, sum: float, last: float}&gt;\n */\nfunction aggregateMetrics(iterable $metrics): array {\n    $aggregates = [];\n    foreach ($metrics as $metric) {\n        $key = $metric-&gt;type() . '|' . $metric-&gt;name() . '|' . $metric-&gt;tags()-&gt;toKey();\n        if (!array_key_exists($key, $aggregates)) {\n            $aggregates[$key] = [\n                'type' =&gt; $metric-&gt;type(),\n                'name' =&gt; $metric-&gt;name(),\n                'tags' =&gt; $metric-&gt;tags()-&gt;toArray(),\n                'count' =&gt; 0,\n                'sum' =&gt; 0.0,\n                'last' =&gt; 0.0,\n            ];\n        }\n\n        $aggregates[$key]['count'] += 1;\n        $aggregates[$key]['sum'] += $metric-&gt;value();\n        $aggregates[$key]['last'] = $metric-&gt;value();\n    }\n\n    return array_values($aggregates);\n}\n\n/**\n * @param array{type: string, count: int, sum: float, last: float} $aggregate\n */\nfunction aggregatedValue(array $aggregate): float {\n    return match ($aggregate['type']) {\n        'counter' =&gt; $aggregate['sum'],\n        'timer', 'histogram' =&gt; $aggregate['sum'] / max(1, $aggregate['count']),\n        default =&gt; $aggregate['last'],\n    };\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/prompt_templates/","title":"Prompt Templates","text":""},{"location":"cookbook/llm_inference/llm_extras/prompt_templates/#overview","title":"Overview","text":"<p><code>Template</code> class in Instructor PHP provides a way to define and use prompt templates using Twig, Blade or custom 'arrowpipe' template syntax.</p>"},{"location":"cookbook/llm_inference/llm_extras/prompt_templates/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Template\\Template;\nuse Cognesy\\Utils\\Str;\n\n// EXAMPLE 1: Define prompt template inline (don't use files) and use short syntax\n\n$prompt = Template::twig()\n    -&gt;from('What is capital of {{country}}')\n    -&gt;with(['country' =&gt; 'Germany'])\n    -&gt;toText();\n\n$answer = (new Inference)-&gt;withMessages($prompt)-&gt;get();\n\necho \"EXAMPLE 1: prompt = $prompt\\n\";\necho \"ASSISTANT: $answer\\n\";\necho \"\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n// EXAMPLE 2: Load prompt from file\n\n// use default template language, prompt files are in /prompts/twig/&lt;prompt&gt;.twig\n$prompt = Template::text(\n    pathOrDsn: 'demo-twig:capital',\n    variables: ['country' =&gt; 'Germany'],\n);\n\n$answer = (new Inference)-&gt;withMessages($prompt)-&gt;get();\n\necho \"EXAMPLE 2: prompt = $prompt\\n\";\necho \"ASSISTANT: $answer\\n\";\necho \"\\n\";\nassert(Str::contains($answer, 'Berlin'));\n\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/streaming_inference_openai_responses/","title":"Streaming (Inference, OpenAI Responses)","text":""},{"location":"cookbook/llm_inference/llm_extras/streaming_inference_openai_responses/#overview","title":"Overview","text":"<p>A minimal streaming example using the <code>openai-responses</code> preset from <code>config/llm.php</code>. The example verifies that streaming produces deltas and that the final response contains the expected marker.</p>"},{"location":"cookbook/llm_inference/llm_extras/streaming_inference_openai_responses/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Events\\PartialInferenceResponseCreated;\nuse Cognesy\\Utils\\Str;\n\n$expectedPhrase = 'paris';\n$prompt = 'Describe the history of Paris in exactly 3 sentences.';\n\n$stream = (new Inference)\n    -&gt;using('openai-responses')\n    -&gt;onEvent(PartialInferenceResponseCreated::class, fn(PartialInferenceResponseCreated $e) =&gt; $e-&gt;print())\n    -&gt;withMessages($prompt)\n    -&gt;withOptions(['max_output_tokens' =&gt; 256])\n    -&gt;withStreaming()\n    -&gt;stream();\n\n$assembled = '';\n$deltaCount = 0;\n\nforeach ($stream-&gt;responses() as $partial) {\n    $delta = $partial-&gt;contentDelta;\n    if ($delta === '') {\n        continue;\n    }\n    $deltaCount += 1;\n    $assembled .= $delta;\n}\n\n$final = $stream-&gt;final();\nassert($final !== null, 'Expected a final response');\n$finalContent = $final-&gt;content();\n\necho \"\\nFinal response:\\n{$finalContent}\\n\";\n\nassert($deltaCount &gt; 0, 'Expected at least one streamed delta');\nassert($assembled !== '', 'Expected non-empty assembled content');\nassert(Str::contains($assembled, $expectedPhrase, false), 'Expected phrase in streamed content');\nassert(Str::contains($final-&gt;content(), $expectedPhrase, false), 'Expected phrase in final content');\nassert(trim($assembled) === trim($finalContent), 'Expected assembled content to match final content');\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/summary_with_llm/","title":"Simple content summary","text":""},{"location":"cookbook/llm_inference/llm_extras/summary_with_llm/#overview","title":"Overview","text":"<p>This is an example of a simple summarization.</p>"},{"location":"cookbook/llm_inference/llm_extras/summary_with_llm/#example","title":"Example","text":"<pre><code>&lt;?php\n\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n$summary = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(messages: [\n        ['role' =&gt; 'user', 'content' =&gt; 'Content to summarize:'],\n        ['role' =&gt; 'user', 'content' =&gt; $report],\n        ['role' =&gt; 'user', 'content' =&gt; 'Concise summary of project report in 2-3 sentences:'],\n    ])\n    -&gt;get();\n\ndump($summary);\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/tool_use/","title":"Inference and tool use","text":""},{"location":"cookbook/llm_inference/llm_extras/tool_use/#overview","title":"Overview","text":"<p><code>ToolUse</code> class automates the process of using tools by LLM, i.e.:  - calling LLM with provided context (message sequence),  - extracting tool calls requested by LLM from the response,  - calling the requested tool and storing its results,  - constructing message sequence with the result of call,  - sending updated message sequence back to LLM.</p> <p>This cycle is repeated until one of the exit criteria is met: - LLM no longer requests any tool calls, - specified maximum number of iterations is reached, - specified token usage limit is reached - there are any errors during the process (e.g. LLM requested a tool that is not available).</p> <p><code>ToolUse</code> class provides 3 ways to iterate through the process: - manual control - code is responsible for checking <code>hasNextStep()</code> and calling <code>nextStep()</code> in a loop, - using iterator - code uses foreach loop to iterate through the steps (internally it checks <code>hasNextStep()</code> and calls <code>nextStep()</code>), - just get final step - you only get the final step, iteration process is done internally.</p>"},{"location":"cookbook/llm_inference/llm_extras/tool_use/#example","title":"Example","text":"<p>This example demonstrates 3 ways to use <code>ToolUse</code> class to allow LLM call functions if needed to answer simple math question. We provide 2 functions (<code>add_numbers</code> and <code>subtract_numbers</code>) as tools available to LLM and specify the task in plain language. The LLM is expected to call the functions in the correct order to get the final result.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ExecutionTimeLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\RetryLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\TokenUsageLimit;\nuse Cognesy\\Addons\\ToolUse\\Collections\\Tools;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseState;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseStep;\nuse Cognesy\\Addons\\ToolUse\\Drivers\\ReAct\\ContinuationCriteria\\StopOnFinalDecision;\nuse Cognesy\\Addons\\ToolUse\\Tools\\FunctionTool;\nuse Cognesy\\Addons\\ToolUse\\ToolUseFactory;\nuse Cognesy\\Messages\\Messages;\n\nfunction add_numbers(int $a, int $b) : int { return $a + $b; }\nfunction subtract_numbers(int $a, int $b) : int { return $a - $b; }\n\n$toolUse = ToolUseFactory::default(\n    tools: new Tools(\n        FunctionTool::fromCallable(add_numbers(...)),\n        FunctionTool::fromCallable(subtract_numbers(...))\n    ),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(6, fn(ToolUseState $state) =&gt; $state-&gt;stepCount()),\n        new TokenUsageLimit(8192, fn(ToolUseState $state) =&gt; $state-&gt;usage()-&gt;total()),\n        new ExecutionTimeLimit(60, fn(ToolUseState $state) =&gt; $state-&gt;startedAt()),\n        new RetryLimit(2, fn(ToolUseState $state) =&gt; $state-&gt;steps(), fn(ToolUseStep $step) =&gt; $step-&gt;hasErrors()),\n        new StopOnFinalDecision(),\n    ),\n);\n\n//\n// PATTERN #1 - manual control\n//\necho \"\\nPATTERN #1 - manual control\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n// iterate until no more steps\nwhile ($toolUse-&gt;hasNextStep($state)) {\n    $state = $toolUse-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n}\n\n// print final response\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n//\n// PATTERN #2 - using iterator\n//\necho \"\\nPATTERN #2 - using iterator\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n// iterate until no more steps\nforeach ($toolUse-&gt;iterator($state) as $currentState) {\n    $step = $currentState-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n    $state = $currentState; // keep the latest state\n}\n\n// print final response\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n\n//\n// PATTERN #3 - just get final step (fast forward to it)\n//\necho \"\\nPATTERN #3 - get only final result\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n// print final response\n$finalState = $toolUse-&gt;finalStep($state);\n$result = $finalState-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_extras/tool_use_react/","title":"Inference and tool use (ReAct driver)","text":""},{"location":"cookbook/llm_inference/llm_extras/tool_use_react/#overview","title":"Overview","text":""},{"location":"cookbook/llm_inference/llm_extras/tool_use_react/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\StepByStep\\Continuation\\ContinuationCriteria;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\ExecutionTimeLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\RetryLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\StepsLimit;\nuse Cognesy\\Addons\\StepByStep\\Continuation\\Criteria\\TokenUsageLimit;\nuse Cognesy\\Addons\\ToolUse\\Collections\\Tools;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseState;\nuse Cognesy\\Addons\\ToolUse\\Data\\ToolUseStep;\nuse Cognesy\\Addons\\ToolUse\\Drivers\\ReAct\\ContinuationCriteria\\StopOnFinalDecision;\nuse Cognesy\\Addons\\ToolUse\\Drivers\\ReAct\\ReActDriver;\nuse Cognesy\\Addons\\ToolUse\\Tools\\FunctionTool;\nuse Cognesy\\Addons\\ToolUse\\ToolUseFactory;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\nfunction add_numbers(int $a, int $b) : int { return $a + $b; }\nfunction subtract_numbers(int $a, int $b) : int { return $a - $b; }\n\n$driver = new ReActDriver(\n    llm: LLMProvider::using('openai'),\n    finalViaInference: true,\n);\n\n$toolUse = ToolUseFactory::default(\n    tools: new Tools(\n        FunctionTool::fromCallable(add_numbers(...)),\n        FunctionTool::fromCallable(subtract_numbers(...))\n    ),\n    continuationCriteria: new ContinuationCriteria(\n        new StepsLimit(6, fn(ToolUseState $state) =&gt; $state-&gt;stepCount()),\n        new TokenUsageLimit(8192, fn(ToolUseState $state) =&gt; $state-&gt;usage()-&gt;total()),\n        new ExecutionTimeLimit(60, fn(ToolUseState $state) =&gt; $state-&gt;startedAt()),\n        new RetryLimit(2, fn(ToolUseState $state) =&gt; $state-&gt;steps(), fn(ToolUseStep $step) =&gt; $step-&gt;hasErrors()),\n        new StopOnFinalDecision(),\n    ),\n    driver: $driver\n);\n\n\n//\n// PATTERN #1 - manual control\n//\necho \"\\nReAct PATTERN #1 - manual control\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\nwhile ($toolUse-&gt;hasNextStep($state)) {\n    $state = $toolUse-&gt;nextStep($state);\n    $step = $state-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n}\n\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n//\n// PATTERN #2 - using iterator\n//\necho \"\\nReAct PATTERN #2 - using iterator\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\nforeach ($toolUse-&gt;iterator($state) as $currentState) {\n    $step = $currentState-&gt;currentStep();\n    print(\"STEP - tokens used: \" . ($step-&gt;usage()?-&gt;total() ?? 0)  . ' [' . $step-&gt;toString() . ']' . \"\\n\");\n    $state = $currentState; // keep the latest state\n}\n\n$result = $state-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n\n//\n// PATTERN #3 - just get final step (fast forward to it)\n//\necho \"\\nReAct PATTERN #3 - final via Inference (optional)\\n\";\n$state = (new ToolUseState)\n    -&gt;withMessages(Messages::fromString('Add 2455 and 3558 then subtract 4344 from the result.'));\n\n$finalState = $toolUse-&gt;finalStep($state);\n$result = $finalState-&gt;currentStep()-&gt;outputMessages()-&gt;toString();\nprint(\"RESULT: \" . $result . \"\\n\");\n\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_troubleshooting/http_debug/","title":"Debugging HTTP Calls","text":""},{"location":"cookbook/llm_inference/llm_troubleshooting/http_debug/#overview","title":"Overview","text":"<p>Instructor PHP provides a way to debug HTTP calls made to LLM APIs via <code>withHttpDebugPreset()</code> (or the legacy <code>withDebugPreset()</code>) on the <code>Inference</code> object.</p> <p>When HTTP debug mode is enabled, the HTTP middleware stack prints request and response details (including streaming data) to the console and dispatches HTTP debug events.</p>"},{"location":"cookbook/llm_inference/llm_troubleshooting/http_debug/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$response = (new Inference)\n    -&gt;withHttpDebugPreset('on') // Enable HTTP debug middleware\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of Brasil']],\n        options: ['max_tokens' =&gt; 128]\n    )\n    -&gt;get();\n\necho \"USER: What is capital of Brasil\\n\";\necho \"ASSISTANT: $response\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_laravel_embeddings/","title":"Embeddings Logging with Laravel","text":""},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_laravel_embeddings/#overview","title":"Overview","text":"<p>Simple Embeddings operation logging with Laravel-style context.</p>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_laravel_embeddings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Logging\\Enrichers\\LazyEnricher;\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\PsrLoggerWriter;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\nuse Illuminate\\Http\\Request;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\n\n// Mock Laravel request\n$request = Request::create('/api/embeddings');\n$request-&gt;headers-&gt;set('X-Request-ID', 'req_' . uniqid());\n\n// Create logger\n$logger = new Logger('embeddings');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create pipeline with Laravel context\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;enrich(LazyEnricher::framework(fn() =&gt; [\n        'request_id' =&gt; $request-&gt;headers-&gt;get('X-Request-ID'),\n    ]))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Polyglot\\Embeddings\\Events\\EmbeddingsRequested::class =&gt;\n            '\ud83d\udd24 Embeddings requested: {provider}/{model} (Request: {framework.request_id})',\n        \\Cognesy\\Polyglot\\Embeddings\\Events\\EmbeddingsResponseReceived::class =&gt;\n            '\u2705 Embeddings generated: {dimensions}D vectors',\n    ], channel: 'embeddings'))\n    -&gt;write(new PsrLoggerWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udccb About to demonstrate Embeddings logging with Laravel context...\\n\\n\";\n\n// Create embeddings with logging\n$embeddings = (new Embeddings)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline);\n\necho \"\ud83d\ude80 Starting Embeddings generation...\\n\";\n$vectors = $embeddings\n    -&gt;withInputs([\n        \"The quick brown fox\",\n        \"Jumps over the lazy dog\"\n    ])\n    -&gt;get();\n\necho \"\\n\u2705 Embeddings completed!\\n\";\necho \"\ud83d\udcca Generated \" . count($vectors-&gt;vectors()) . \" embedding vectors\\n\";\necho \"\ud83d\udcca Vector dimensions: \" . count($vectors-&gt;first()?-&gt;values() ?? []) . \"\\n\";\n\n// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// \ud83d\udccb About to demonstrate Embeddings logging with Laravel...\n// \ud83d\ude80 Starting Embeddings request...\n// [2025-12-07 01:18:13] embeddings.DEBUG: \ud83d\udd04 [Laravel] Embeddings requested: openai/text-embedding-3-small\n// [2025-12-07 01:18:14] embeddings.DEBUG: \u2705 [Laravel] Embeddings completed: openai/text-embedding-3-small\n// \u2705 Embeddings completed!\n// \ud83d\udcca Generated 2 embedding vectors\n// \ud83d\udcca Vector dimensions: 1536\n?&gt;\n</code></pre>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_laravel_inference/","title":"Inference Logging with Laravel","text":""},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_laravel_inference/#overview","title":"Overview","text":"<p>Simple Inference operation logging using Monolog.</p>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_laravel_inference/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\MonologChannelWriter;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\n\n// Create Monolog logger\n$logger = new Logger('inference');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create logging pipeline\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested::class =&gt;\n            '\ud83e\udd16 Inference requested: {provider}/{model}',\n        \\Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated::class =&gt;\n            '\u2705 Inference completed: {provider}/{model}',\n    ], channel: 'inference'))\n    -&gt;write(new MonologChannelWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udccb About to demonstrate Inference logging with Monolog...\\n\\n\";\n\n// Create inference with logging\n$inference = (new Inference)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline);\n\necho \"\ud83d\ude80 Starting Inference request...\\n\";\n$response = $inference\n    -&gt;withMessages([\n        ['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']\n    ])\n    -&gt;withMaxTokens(50)\n    -&gt;get();\n\necho \"\ud83d\udcca Response: \" . ($response ?: \"Empty response\") . \"\\n\";\n?&gt;\n</code></pre> <pre><code>// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// \ud83d\udccb About to demonstrate Inference logging with Monolog...\n// \ud83d\ude80 Starting Inference request...\n// [2025-12-07T01:18:13.475202+00:00] inference.DEBUG: \ud83e\udd16 Inference requested: openai/gpt-4o-mini\n// [2025-12-07T01:18:14.659417+00:00] inference.DEBUG: \u2705 Inference completed: openai/gpt-4o-mini\n// \u2705 Inference completed!\n// \ud83d\udcca Response: The capital of France is Paris.\n</code></pre>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_monolog/","title":"Inference Logging with Monolog","text":""},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_monolog/#overview","title":"Overview","text":"<p>Simple Inference operation logging using Monolog.</p>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_monolog/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\MonologChannelWriter;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\n\n// Create Monolog logger\n$logger = new Logger('inference');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create logging pipeline\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested::class =&gt;\n            '\ud83e\udd16 Inference requested: {provider}/{model}',\n        \\Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated::class =&gt;\n            '\u2705 Inference completed: {provider}/{model}',\n    ], channel: 'inference'))\n    -&gt;write(new MonologChannelWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udccb About to demonstrate Inference logging with Monolog...\\n\\n\";\n\n// Create inference with logging\n$inference = (new Inference)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline);\n\necho \"\ud83d\ude80 Starting Inference request...\\n\";\n$response = $inference\n    -&gt;withMessages([\n        ['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']\n    ])\n    -&gt;withMaxTokens(50)\n    -&gt;get();\n\necho \"\ud83d\udcca Response: \" . ($response ?: \"Empty response\") . \"\\n\";\n?&gt;\n</code></pre> <pre><code>// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// \ud83d\udccb About to demonstrate Inference logging with Monolog...\n// \ud83d\ude80 Starting Inference request...\n// [2025-12-07T01:18:13.475202+00:00] inference.DEBUG: \ud83e\udd16 Inference requested: openai/gpt-4o-mini\n// [2025-12-07T01:18:14.659417+00:00] inference.DEBUG: \u2705 Inference completed: openai/gpt-4o-mini\n// \u2705 Inference completed!\n// \ud83d\udcca Response: The capital of France is Paris.\n</code></pre>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_symfony/","title":"Inference Logging with Symfony","text":""},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_symfony/#overview","title":"Overview","text":"<p>Inference operation logging with Symfony-style context.</p>"},{"location":"cookbook/llm_inference/llm_troubleshooting/llm_logging_symfony/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Logging\\Enrichers\\LazyEnricher;\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\PsrLoggerWriter;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\nuse Symfony\\Component\\HttpFoundation\\Request;\n\n// Mock Symfony request\n$request = Request::create('/api/stream');\n$request-&gt;attributes-&gt;set('_route', 'api.stream');\n\n// Create logger\n$logger = new Logger('inference');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create pipeline with Symfony context\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;enrich(LazyEnricher::framework(fn() =&gt; [\n        'route' =&gt; $request-&gt;attributes-&gt;get('_route'),\n    ]))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested::class =&gt;\n            '\ud83e\udd16 [SYMFONY] Inference requested: {provider}/{model} (Route: {framework.route})',\n        \\Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated::class =&gt;\n            '\u2705 [SYMFONY] Inference completed: {provider}/{model}',\n    ], channel: 'inference'))\n    -&gt;write(new PsrLoggerWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udccb About to demonstrate Inference logging with Symfony...\\n\\n\";\n\n// Create inference with logging\n$inference = (new Inference)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline);\n\necho \"\ud83d\ude80 Starting simple Inference to demonstrate logging...\\n\";\n\n$response = $inference\n    -&gt;withMessages([\n        ['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']\n    ])\n    -&gt;withMaxTokens(50)\n    -&gt;get();\n\necho \"\\n\u2705 Inference completed!\\n\";\n\n// Handle response properly - it might be a string or object\nif (is_string($response)) {\n    echo \"\ud83d\udcca Response: \" . ($response ?: \"Empty response\") . \"\\n\";\n} else {\n    echo \"\ud83d\udcca Response: \" . ($response-&gt;content ?? \"Response object has no content property\") . \"\\n\";\n}\n?&gt;\n</code></pre> <pre><code>// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// \ud83d\udccb About to demonstrate Inference logging with Symfony...\n// \ud83d\ude80 Starting Inference request...\n// [2025-12-07 01:18:13] inference.DEBUG: \ud83d\udd04 [Symfony] Inference requested: openai/gpt-4o-mini\n// [2025-12-07 01:18:14] inference.DEBUG: \u2705 [Symfony] Inference completed: openai/gpt-4o-mini\n// \u2705 Inference completed!\n// \ud83d\udcca Response: The capital of France is Paris.\n</code></pre>"},{"location":"cookbook/prompting/decomposition/break_down_complexity/","title":"Break Down Complex Tasks","text":""},{"location":"cookbook/prompting/decomposition/break_down_complexity/#overview","title":"Overview","text":"<p>How can we help LLMs handle complex tasks more effectively?</p> <p>Decomposed Prompting leverages a Language Model (LLM) to deconstruct a complex task into a series of manageable sub-tasks. Each sub-task is then processed by specific functions, enabling the LLM to handle intricate problems more effectively and systematically.</p> <p>This approach breaks down complexity by: - Generating an action plan using the LLM - Executing each step systematically - Using specific operations like Split, StrPos, and Merge</p>"},{"location":"cookbook/prompting/decomposition/break_down_complexity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum ActionType: string {\n    case Split = 'split';\n    case StrPos = 'strpos';\n    case Merge = 'merge';\n}\n\nclass Split {\n    public function __construct(\n        public string $split_char\n    ) {}\n\n    public function execute(string $input): array {\n        return explode($this-&gt;split_char, $input);\n    }\n}\n\nclass StrPos {\n    public function __construct(\n        public int $index\n    ) {}\n\n    public function execute(array $input): array {\n        return array_map(fn($str) =&gt; $str[$this-&gt;index] ?? '', $input);\n    }\n}\n\nclass Merge {\n    public function __construct(\n        public string $merge_char\n    ) {}\n\n    public function execute(array $input): string {\n        return implode($this-&gt;merge_char, $input);\n    }\n}\n\nclass Action {\n    public function __construct(\n        public int $id,\n        public ActionType $type,\n        public string|int $parameter\n    ) {}\n}\n\nclass ActionPlan {\n    public function __construct(\n        public string $initial_data,\n        /** @var Action[] */\n        public array $plan\n    ) {}\n}\n\nclass DecomposedTaskSolver {\n    public function __invoke(string $taskDescription): string {\n        $plan = $this-&gt;deriveActionPlan($taskDescription);\n        return $this-&gt;executePlan($plan);\n    }\n\n    private function deriveActionPlan(string $taskDescription): ActionPlan {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'Generate an action plan to help complete the task. Available actions: Split (split string by character), StrPos (get character at index from each string), Merge (join strings with character)'],\n                ['role' =&gt; 'user', 'content' =&gt; $taskDescription],\n            ],\n            responseModel: ActionPlan::class,\n        )-&gt;get();\n    }\n\n    private function executePlan(ActionPlan $plan): string {\n        $current = $plan-&gt;initial_data;\n\n        foreach ($plan-&gt;plan as $action) {\n            match ($action-&gt;type) {\n                ActionType::Split =&gt; $current = (new Split($action-&gt;parameter))-&gt;execute($current),\n                ActionType::StrPos =&gt; $current = (new StrPos($action-&gt;parameter))-&gt;execute($current),\n                ActionType::Merge =&gt; $current = (new Merge($action-&gt;parameter))-&gt;execute($current),\n            };\n        }\n\n        return $current;\n    }\n}\n\n$result = (new DecomposedTaskSolver)('Concatenate the second letter of every word in \"Jack Ryan\" together');\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/break_down_complexity/#references","title":"References","text":"<ol> <li>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</li> </ol>"},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/","title":"Ditch Vanilla Chain Of Thought","text":""},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/#overview","title":"Overview","text":"<p>How can we improve the effectiveness of Zero-Shot Chain of Thought (CoT) prompts?</p> <p>Plan and Solve improves the use of Zero-Shot Chain of Thought by adding more detailed instructions to the prompt given to large language models.</p> <p>Plan and Solve Process:</p> <ol> <li>Generate Reasoning: Prompt the model to explicitly devise a plan for solving a problem before generating intermediate reasoning</li> <li>Extract Answer: Extract the final answer from the model's chain of thought</li> </ol> <p>The key improvement is guiding the LLM to pay more attention to calculation and intermediate results to ensure they are correctly performed.</p>"},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Reasoning {\n    public function __construct(\n        public string $chain_of_thought\n    ) {}\n}\n\nclass Response {\n    public function __construct(\n        public string $correct_answer\n    ) {}\n}\n\nclass PlanAndSolveSolver {\n    public function __invoke(string $query): string {\n        $reasoning = $this-&gt;generateReasoning($query);\n        $response = $this-&gt;extractAnswer($query, $reasoning);\n        return $response-&gt;correct_answer;\n    }\n\n    private function generateReasoning(string $query): Reasoning {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"&lt;user_query&gt;\n{$query}\n&lt;/user_query&gt;\n\nLet's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.\"\n                ],\n            ],\n            responseModel: Reasoning::class,\n        )-&gt;get();\n    }\n\n    private function extractAnswer(string $query, Reasoning $reasoning): Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"&lt;user_query&gt;\n{$query}\n&lt;/user_query&gt;\n\nLet's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.\n\n&lt;reasoning&gt;\n{$reasoning-&gt;chain_of_thought}\n&lt;/reasoning&gt;\n\nTherefore the answer (arabic numerals) is\"\n                ],\n            ],\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$result = (new PlanAndSolveSolver)(\n    \"In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?\"\n);\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/ditch_vanilla_cot/#references","title":"References","text":"<ol> <li>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</li> </ol>"},{"location":"cookbook/prompting/decomposition/generate_code/","title":"Generate Code for Intermediate Steps","text":""},{"location":"cookbook/prompting/decomposition/generate_code/#overview","title":"Overview","text":"<p>How can we leverage external code execution to generate intermediate reasoning steps?</p> <p>Program of Thought aims to leverage an external code interpreter to generate intermediate reasoning steps. This helps achieve greater performance in mathematical and programming-related tasks by grounding our final response in deterministic code.</p> <p>The approach involves: 1. Generate Code: Create a solver function that implements step-by-step logic 2. Execute Code: Run the generated code to get deterministic results 3. Extract Answer: Use the computed result to make final predictions</p>"},{"location":"cookbook/prompting/decomposition/generate_code/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Choice: string {\n    case A = 'A';\n    case B = 'B';\n    case C = 'C';\n    case D = 'D';\n    case E = 'E';\n}\n\nclass Prediction {\n    public function __construct(\n        public Choice $choice\n    ) {}\n}\n\nclass ProgramExecution {\n    public function __construct(\n        public string $program_code\n    ) {}\n}\n\nclass ProgramOfThoughtSolver {\n    private const PREFIX = '\\&lt;?php\n// Answer this question by implementing a solver()\n// function, use for loop if necessary.\nfunction solver() {\n    // Let\\'s write a PHP program step by step,\n    // and then return the answer\n    // Firstly, we need to define the following\n    // variables:';\n\n    public function __invoke(string $query, array $options): string {\n        $reasoning = $this-&gt;generateIntermediateReasoning($query);\n        $answer = $this-&gt;executeProgram($reasoning-&gt;program_code);\n        $prediction = $this-&gt;generatePrediction($answer, $options, $query);\n        return $prediction-&gt;choice-&gt;value;\n    }\n\n    private function generateIntermediateReasoning(string $query): ProgramExecution {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'system',\n                    'content' =&gt; 'You are a world class AI system that excels at answering user queries in a systematic and detailed manner. Generate a valid PHP program that can be executed to answer the user query.\n\nMake sure to begin your generated program with the following structure:\n```php\n&lt;?php\nfunction solver() {\n    // Your step-by-step logic here\n    // Define variables, perform calculations\n    // Return the final answer\n}\nreturn solver();\n```'\n                ],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n            responseModel: ProgramExecution::class,\n        )-&gt;get();\n    }\n\n    private function executeProgram(string $code): mixed {\n        try {\n            $sanitized = $this-&gt;sanitizeCode($code);\n            // Ensure we get a value even if the model forgot to return it explicitly\n            if (strpos($sanitized, 'return') === false) {\n                $sanitized .= \"\\nreturn (function(){ return function_exists('solver') ? solver() : null; })();\";\n            }\n            return eval($sanitized);\n        } catch (Throwable $e) {\n            throw new Exception(\"Program execution failed: \" . $e-&gt;getMessage());\n        }\n    }\n\n    private function sanitizeCode(string $code): string {\n        // Strip Markdown fences\n        $code = preg_replace('/^\\s*```[a-zA-Z]*\\s*/', '', $code);\n        $code = preg_replace('/```\\s*$/', '', (string) $code);\n        // Remove PHP open/close tags \u2014 eval() expects pure PHP code body\n        $code = str_replace(['&lt;?php', '&lt;?', '?&gt;'], '', (string) $code);\n        // Normalize line endings and trim\n        $code = trim((string) $code);\n        return $code;\n    }\n\n    private function generatePrediction(mixed $predictedAnswer, array $options, string $query): Prediction {\n        $formattedOptions = implode(', ', $options);\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'system',\n                    'content' =&gt; \"Find the closest option based on the question and prediction.\n\nQuestion: {$query}\nPrediction: {$predictedAnswer}\nOptions: [{$formattedOptions}]\"\n                ],\n            ],\n            responseModel: Prediction::class,\n        )-&gt;get();\n    }\n}\n\n$result = (new ProgramOfThoughtSolver)(\n    \"A trader sold an article at a profit of 20% for Rs.360. What is the cost price of the article?\",\n    [\"A)270\", \"B)300\", \"C)280\", \"D)320\", \"E)315\"]\n);\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/generate_code/#references","title":"References","text":"<ol> <li>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</li> </ol>"},{"location":"cookbook/prompting/decomposition/generate_in_parallel/","title":"Generate in Parallel","text":""},{"location":"cookbook/prompting/decomposition/generate_in_parallel/#overview","title":"Overview","text":"<p>How can we decrease the latency of an LLM pipeline?</p> <p>Skeleton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel. The parallelism can be achieved by parallel API calls or batched processing.</p> <p>The approach involves: 1. Generate Skeleton: Create a brief outline of the response structure 2. Parallel Expansion: Complete each skeleton point concurrently 3. Assembly: Combine the expanded points into the final response</p>"},{"location":"cookbook/prompting/decomposition/generate_in_parallel/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Point {\n    public function __construct(\n        public int $index,\n        public string $description\n    ) {}\n}\n\nclass Skeleton {\n    public function __construct(\n        /** @var Point[] */\n        public array $points\n    ) {}\n}\n\nclass Response {\n    public function __construct(\n        public string $response\n    ) {}\n}\n\nclass SkeletonOfThoughtGenerator {\n    public function __invoke(string $question): array {\n        $skeleton = $this-&gt;getSkeleton($question);\n        return $this-&gt;expandPointsSequentially($question, $skeleton);\n    }\n\n    private function getSkeleton(string $question): Skeleton {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.\nProvide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question.\nInstead of writing a full sentence, each skeleton point should be very short with only 3-5 words.\nGenerally, the skeleton should have 3-10 points.\n\nNow, please provide the skeleton for the following question.\n\n&lt;question&gt;\n{$question}\n&lt;/question&gt;\n\nSkeleton:\"\n                ],\n            ],\n            responseModel: Skeleton::class,\n        )-&gt;get();\n    }\n\n    private function expandPoint(string $question, Skeleton $skeleton, int $pointIndex): Response {\n        $skeletonText = '';\n        foreach ($skeleton-&gt;points as $point) {\n            $skeletonText .= \"{$point-&gt;index}. {$point-&gt;description}\\n\";\n        }\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; \"You're responsible for continuing the writing of one and only one point in the overall answer to the following question.\n\n&lt;question&gt;\n{$question}\n&lt;/question&gt;\n\nThe skeleton of the answer is:\n\n&lt;skeleton&gt;\n{$skeletonText}\n&lt;/skeleton&gt;\n\nContinue and only continue the writing of point {$pointIndex}.\nWrite it **very shortly** in 1-2 sentences and do not continue with other points!\"\n                ],\n            ],\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n\n    private function expandPointsSequentially(string $question, Skeleton $skeleton): array {\n        $responses = [];\n        foreach ($skeleton-&gt;points as $point) {\n            $response = $this-&gt;expandPoint($question, $skeleton, $point-&gt;index);\n            $responses[] = [\n                'point' =&gt; $point,\n                'content' =&gt; $response-&gt;response\n            ];\n        }\n        return $responses;\n    }\n}\n\n$results = (new SkeletonOfThoughtGenerator)(\n    \"Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\"\n);\n\necho \"Generated Content:\\n\";\necho str_repeat(\"=\", 50) . \"\\n\";\n\nforeach ($results as $result) {\n    echo \"Point {$result['point']-&gt;index}: {$result['point']-&gt;description}\\n\";\n    echo \"{$result['content']}\\n\\n\";\n}\n\ndump($results);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/generate_in_parallel/#references","title":"References","text":"<ol> <li>Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/","title":"Solve Simpler Subproblems","text":""},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/#overview","title":"Overview","text":"<p>How can we encourage an LLM to solve complex problems by breaking them down?</p> <p>Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.</p> <p>Subproblems Example: - Original problem: Adam is twice as old as Mary. Adam will be 11 in 1 year. How old is Mary? - Subproblems: (1) How old is Adam now? (2) What is half of Adam's current age?</p> <p>These subproblems are solved sequentially, allowing the answers from earlier (simpler) subproblems to inform the LLM while solving later (more complex) subproblems.</p>"},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Subquestion {\n    public function __construct(\n        public string $question\n    ) {}\n}\n\nclass Answer {\n    public function __construct(\n        public int $answer\n    ) {}\n}\n\nclass SubquestionWithAnswer {\n    public function __construct(\n        public string $question,\n        public int $answer\n    ) {}\n}\n\nclass LeastToMostSolver {\n    public function __invoke(string $question): array {\n        $subquestions = $this-&gt;decompose($question);\n        return $this-&gt;solveSequentially($subquestions, $question);\n    }\n\n    private function decompose(string $question): array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user', \n                    'content' =&gt; \"Break this question down into subquestions to solve sequentially: {$question}\"\n                ],\n            ],\n            responseModel: Sequence::of(Subquestion::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    private function solve(string $question, array $solvedQuestions, string $originalQuestion): int {\n        $solvedContext = '';\n        foreach ($solvedQuestions as $solved) {\n            $solvedContext .= \"{$solved-&gt;question} {$solved-&gt;answer}\\n\";\n        }\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                [\n                    'role' =&gt; 'user',\n                    'content' =&gt; &lt;&lt;&lt;PROMPT\n                        &lt;original_question&gt;\n                        {$originalQuestion}\n                        &lt;/original_question&gt;\n\n                        &lt;solved_subquestions&gt;\n                        {$solvedContext}\n                        &lt;/solved_subquestions&gt;\n\n                        Solve this next subquestion: {$question}\n                    PROMPT,\n                ],\n            ],\n            responseModel: Answer::class,\n        )-&gt;get()-&gt;answer;\n    }\n\n    private function solveSequentially(array $subquestions, string $originalQuestion): array {\n        $solvedQuestions = [];\n\n        foreach ($subquestions as $subquestion) {\n            $answer = $this-&gt;solve($subquestion-&gt;question, $solvedQuestions, $originalQuestion);\n            $solvedQuestions[] = new SubquestionWithAnswer($subquestion-&gt;question, $answer);\n        }\n\n        return $solvedQuestions;\n    }\n}\n\n$results = (new LeastToMostSolver)(\n    \"Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody?\"\n);\n\nforeach ($results as $result) {\n    echo \"{$result-&gt;question} {$result-&gt;answer}\\n\";\n}\n\ndump($results);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/solve_simpler_subtasks/#references","title":"References","text":"<ol> <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/decomposition/task_specific_systems/","title":"Leverage Task Specific Systems","text":""},{"location":"cookbook/prompting/decomposition/task_specific_systems/#overview","title":"Overview","text":"<p>How can we improve the faithfulness of reasoning chains generated by Language Models?</p> <p>Faithful Chain of Thought improves the faithfulness of reasoning chains by breaking it up into two stages:</p> <ol> <li>Translation: Translate a user query into a series of reasoning steps - task-specific steps that can be executed deterministically</li> <li>Problem Solving: Execute steps and arrive at a final answer that is consistent with the reasoning steps</li> </ol> <p>Examples of task-specific systems: - Math Word Problems: PHP code that can be evaluated to derive a final answer - Multi-Hop QA: Multi-step reasoning process using programming logic - Planning: Generate symbolic goals and use planning systems to solve queries</p>"},{"location":"cookbook/prompting/decomposition/task_specific_systems/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningStep {\n    public function __construct(\n        public int $id,\n        /** @var string[] */\n        public array $rationale,\n        /** @var int[] */\n        public array $dependencies,\n        public string $eval_string = ''\n    ) {}\n}\n\nclass TaskSpecificReasoner {\n    public function __invoke(string $query): mixed {\n        $steps = $this-&gt;generateReasoningSteps($query);\n        return $this-&gt;executeSteps($steps);\n    }\n\n    private function generateReasoningSteps(string $query): array {\n        return (new StructuredOutput)-&gt;with(\n                messages: [\n                    [\n                        'role' =&gt; 'system',\n                        'content' =&gt; 'You are a world class AI who excels at generating reasoning steps to answer a question. Generate a list of reasoning steps needed to answer the question.\n\nFor each reasoning step, provide:\n- id: step number starting from 1\n- rationale: array of strings explaining the reasoning\n- dependencies: array of step IDs this step depends on\n- eval_string: valid PHP code (without &lt;?php tags) that can be evaluated\n\nAt each step you should either:\n- declare a variable to be referenced later on (e.g., \"$cars = 3;\")\n- combine multiple variables together to generate a new result (e.g., \"$total = $cars + $more;\")\n\nThe final step must store the answer in a variable called $answer.\n\nExample eval_string values:\n- \"$cars = 3;\"\n- \"$more_cars = 2;\"\n- \"$answer = $cars + $more_cars;\"\n\nUse only valid PHP syntax for variable assignments in eval_string.'\n                    ],\n                    ['role' =&gt; 'user', 'content' =&gt; $query],\n                ],\n                responseModel: Sequence::of(ReasoningStep::class),\n            )-&gt;get()-&gt;toArray();\n    }\n\n    private function executeSteps(array $steps): mixed {\n        $code = [];\n        foreach ($steps as $step) {\n            $code[] = $step-&gt;eval_string;\n        }\n\n        $fullCode = \"&lt;?php\\n\" . implode(\"\\n\", $code) . \"\\nreturn \\$answer;\";\n\n        $result = eval(substr($fullCode, 5));\n        return $result;\n    }\n}\n\n$result = (new TaskSpecificReasoner)(\n    'If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot after another 2 more arrive?'\n);\n\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/decomposition/task_specific_systems/#references","title":"References","text":"<ol> <li>Faithful Chain-of-Thought Reasoning</li> </ol>"},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/","title":"Combine Multiple Reasoning Chains","text":""},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/#overview","title":"Overview","text":"<p>Meta Chain-of-Thought (Meta-CoT) decomposes a query into sub-queries, solves each with its own reasoning chain, then composes a final answer from those chains.</p>"},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningAndResponse {\n    public string $intermediate_reasoning;\n    public string $correct_answer;\n}\n\nclass MaybeResponse {\n    public ?ReasoningAndResponse $result = null;\n    public ?bool $error = null;\n    public ?string $error_message = null;\n}\n\nclass QueryDecomposition { /** @var string[] */ public array $queries; }\n\nclass MetaCOT {\n    public function __invoke(string $query) : ReasoningAndResponse {\n        $subs = $this-&gt;decompose($query)-&gt;queries;\n        $chains = [];\n        foreach ($subs as $q) { $chains[] = $this-&gt;chain($q); }\n        return $this-&gt;final($query, $chains);\n    }\n\n    public function decompose(string $query) : QueryDecomposition {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'Decompose the user query into minimal sub-queries needed to derive the answer.'],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n            responseModel: QueryDecomposition::class,\n        )-&gt;get();\n    }\n\n    public function chain(string $query) : MaybeResponse {\n        $system = &lt;&lt;&lt;TXT\n        Given a question, answer step-by-step.\n        Provide intermediate reasoning and the final answer.\n        If impossible, set error=true and include an error_message.\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $system], ['role' =&gt; 'user', 'content' =&gt; $query] ],\n            responseModel: MaybeResponse::class,\n        )-&gt;get();\n    }\n\n    public function final(string $query, array $context) : ReasoningAndResponse {\n        $parts = [];\n        foreach ($context as $c) {\n            if ($c instanceof MaybeResponse &amp;&amp; !$c-&gt;error &amp;&amp; $c-&gt;result) {\n                $parts[] = $c-&gt;result-&gt;intermediate_reasoning . \"\\n\" . $c-&gt;result-&gt;correct_answer;\n            }\n        }\n        $formatted = implode(\"\\n\", $parts);\n        $system = &lt;&lt;&lt;TXT\n        Given a question and context, answer step-by-step.\n        If unsure, answer \"Unknown\".\n        TXT;\n        $prompt = &lt;&lt;&lt;PR\n        &lt;question&gt;\n        {$query}\n        &lt;/question&gt;\n        &lt;context&gt;\n        {$formatted}\n        &lt;/context&gt;\n        PR;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $system], ['role' =&gt; 'user', 'content' =&gt; $prompt] ],\n            responseModel: ReasoningAndResponse::class,\n        )-&gt;get();\n    }\n}\n\n$query = \"Would Arnold Schwarzenegger have been able to deadlift an adult Black rhinoceros at his peak strength?\";\n$result = (new MetaCOT)($query);\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/combine_reasoning_chains/#references","title":"References","text":"<p>1) Answering Questions by Meta-Reasoning over Multiple Chains of Thought (https://arxiv.org/pdf/2304.13007)</p>"},{"location":"cookbook/prompting/ensembling/combine_responses/","title":"Use LLMs to Combine Different Responses","text":""},{"location":"cookbook/prompting/ensembling/combine_responses/#overview","title":"Overview","text":"<p>Universal Self-Consistency uses a second LLM to judge the quality of multiple responses to a query and select the most consistent one.</p>"},{"location":"cookbook/prompting/ensembling/combine_responses/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ResponseItem { public string $chain_of_thought; public string $answer; }\nclass SelectedResponse { public int $most_consistent_response_id; }\n\nclass CombineResponses {\n    public function __invoke(string $query, int $k = 3) : ResponseItem {\n        $responses = [];\n        for ($i = 0; $i &lt; $k; $i++) { $responses[] = $this-&gt;generate($query); }\n        $sel = $this-&gt;select($responses, $query);\n        $idx = max(0, min($sel-&gt;most_consistent_response_id, count($responses)-1));\n        return $responses[$idx];\n    }\n\n    private function generate(string $query) : ResponseItem {\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user', 'content'=&gt;$query] ],\n            responseModel: ResponseItem::class,\n        )-&gt;get();\n    }\n\n    private function select(array $responses, string $query) : SelectedResponse {\n        $formatted = [];\n        foreach ($responses as $i =&gt; $r) { $formatted[] = \"Response {$i}: {$r-&gt;chain_of_thought}. {$r-&gt;answer}\"; }\n        $content = \"&lt;user query&gt;\\n{$query}\\n&lt;/user query&gt;\\n\\n\" . implode(\"\\n\", $formatted) . \"\\n\\nEvaluate these responses. Select the most consistent response based on majority consensus.\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$content] ],\n            responseModel: SelectedResponse::class,\n        )-&gt;get();\n    }\n}\n\n$query = \"The three-digit number 'ab5' is divisible by 3. How many different three-digit numbers can 'ab5' represent?\";\n$result = (new CombineResponses)($query, k: 3);\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/combine_responses/#references","title":"References","text":"<p>1) Universal Self-Consistency For Large Language Model Generation (https://arxiv.org/pdf/2311.17311)</p>"},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/","title":"Combine Different Specialized LLMs","text":""},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/#overview","title":"Overview","text":"<p>Mixture of Reasoning Experts (MoRE) combines specialized experts (e.g., factual with evidence, and multi-hop reasoning) and selects the best answer using a scorer.</p>"},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass MultihopExpert { public string $chain_of_thought; public string $answer; }\nclass FactualExpert { public string $answer; }\nclass ModelScore { public float $score; }\n\nclass MoRE {\n    public function factual(string $query, array $evidences) : FactualExpert {\n        $formatted = '- ' . implode(\"\\n-\", $evidences);\n        $system = \"&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\\n\\n&lt;evidences&gt;\\n{$formatted}\\n&lt;/evidences&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system] ],\n            responseModel: FactualExpert::class,\n        )-&gt;get();\n    }\n\n    public function multihop(string $query) : MultihopExpert {\n        $system = \"&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system] ],\n            responseModel: MultihopExpert::class,\n        )-&gt;get();\n    }\n\n    public function score(string $query, string $answer) : ModelScore {\n        $messages = [\n            ['role'=&gt;'system','content'=&gt;'You score answers by how well they answer the user query (0..1).'],\n            ['role'=&gt;'user','content'=&gt;\"&lt;user query&gt;\\n{$query}\\n&lt;/user query&gt;\\n\\n&lt;response&gt;\\n{$answer}\\n&lt;/response&gt;\"],\n        ];\n        return (new StructuredOutput)-&gt;with(\n            messages: $messages,\n            responseModel: ModelScore::class,\n        )-&gt;get();\n    }\n}\n\n$query = \"Who's the original singer of Help Me Make It Through The Night?\";\n$evidences = [\"Help Me Make It Through The Night is a country music ballad written and composed by Kris Kristofferson and released on his 1970 album 'Kristofferson'\"];\n$threshold = 0.8;\n\n$more = new MoRE();\n$factual = $more-&gt;factual($query, $evidences);\n$multihop = $more-&gt;multihop($query);\n\n$fScore = $more-&gt;score($query, $factual-&gt;answer)-&gt;score ?? 0.0;\n$mScore = $more-&gt;score($query, $multihop-&gt;answer)-&gt;score ?? 0.0;\n\n$answer = '';\nif (max($fScore, $mScore) &lt; $threshold) {\n    $answer = 'Abstaining from responding';\n} else {\n    $answer = ($fScore &gt; $mScore) ? $factual-&gt;answer : $multihop-&gt;answer;\n}\n\ndump($answer);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/combine_specialized_llms/#references","title":"References","text":"<p>1) Getting MoRE out of Mixture of Language Model Reasoning Experts (https://arxiv.org/pdf/2305.14628)</p>"},{"location":"cookbook/prompting/ensembling/consistent_examples/","title":"Prioritize Consistent Examples","text":"<p>Consistency Based Self Adaptive Prompting (COSP)1 aims to improve LLM output quality by generating high quality few shot examples to be included in the final prompt. These are examples without labelled ground truth so they use self-consistency and a metric known as normalized entropy to select the best examples.</p> <p>Once they've selected the examples, they then append them to the prompt and generate multiple reasoning chains before selecting the final result using Self-Consistency.</p> <p>COSP process\u00b6</p> <p>How does this look in practice? Let's dive into greater detail.</p> <p>Step 1 - Selecting Examples\u00b6 In the first step, we try to generate high quality examples from questions that don't have ground truth labels. This is challenging because we want to find a way to automatically determine answer quality when sampling our model multiple times.</p> <p>In this case, we have n questions which we want to generate m possible reasoning chains for each question. This gives a total of nm examples. We then want to filter out k final few shot examples from these nm examples to be included inside our final prompt.</p> <p>Using chain of thought, we first generate m responses for each question. These responses contain a final answer and a rationale behind that answer. We compute a score for each response using a weighted sum of two values - normalized entropy and repetitiveness ( How many times this rationale appears for this amswer ) We rank all of our nm responses using this score and choose the k examples with the lowest scores as our final few shot examples.</p> <p>Normalized Entropy</p> <p>In the paper, the authors write that normalized entropy is a good proxy over a number of different tasks where low entropy is positively correlated with correctness. Entropy is also supposed to range from 0 to 1.</p> <p>Therefore in order to do so, we introduce a - term in our implementation so that the calculated values range from 0 to 1.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass CoTResponse { /** @var string[] */ public array $chain_of_thought; public int $answer; }\nclass ScoredExample { public string $query; public CoTResponse $response; public float $score; }\n\nclass COSPSelector {\n    public function __construct(private int $m = 3) {}\n\n    public function generate(string $query) : array {\n        $out = [];\n        for ($i = 0; $i &lt; $this-&gt;m; $i++) { $out[] = $this-&gt;cot($query); }\n        return $out;\n    }\n\n    public function scoreExamples(string $query, array $responses) : array {\n        $entropy = $this-&gt;normalizedEntropy(array_map(fn($r)=&gt;$r-&gt;answer, $responses));\n        $repetitiveness = $this-&gt;repetitiveness($responses);\n        $scored = [];\n        foreach ($responses as $r) {\n            $s = $entropy - $repetitiveness; // lower is better\n            $se = new ScoredExample(); $se-&gt;query = $query; $se-&gt;response = $r; $se-&gt;score = $s; $scored[] = $se;\n        }\n        return $scored;\n    }\n\n    public function select(array $candidates, int $k) : array {\n        $all = [];\n        foreach ($candidates as $q) { $all = array_merge($all, $this-&gt;scoreExamples($q, $this-&gt;generate($q))); }\n        usort($all, fn($a,$b)=&gt; $a-&gt;score &lt;=&gt; $b-&gt;score);\n        return array_slice($all, 0, $k);\n    }\n\n    private function cot(string $query) : CoTResponse {\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$query] ],\n            responseModel: CoTResponse::class,\n        )-&gt;get();\n    }\n\n    private function normalizedEntropy(array $answers) : float {\n        $n = count($answers); if ($n === 0) return 0.0; $freq=[]; foreach($answers as $a){$freq[$a]=($freq[$a]??0)+1;}\n        $h = 0.0; foreach ($freq as $c) { $p = $c / $n; $h += ($p &gt; 0) ? -$p * log($p) : 0.0; }\n        $maxH = log(max(1, count($freq)));\n        return $maxH &gt; 0 ? $h / $maxH : 0.0;\n    }\n\n    private function repetitiveness(array $responses) : float {\n        // approximate: 1 - (unique rationales / total)\n        $rationales = array_map(fn($r)=&gt; implode(' ', $r-&gt;chain_of_thought), $responses);\n        $unique = count(array_unique($rationales));\n        $n = max(1, count($responses));\n        return 1.0 - ($unique / $n);\n    }\n}\n\n$questions = [\n    'How many loaves of bread did they have left?',\n    'How many pages does James write in a year?',\n];\n$selector = new COSPSelector(m: 3);\n$best = $selector-&gt;select($questions, k: 3);\ndump($best);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/consistent_examples/#references","title":"References","text":"<p>1: Better Zero-Shot Reasoning with Self-Adaptive Prompting (https://arxiv.org/pdf/2305.14106)</p>"},{"location":"cookbook/prompting/ensembling/distinct_examples/","title":"Use Distinct Example Subsets","text":""},{"location":"cookbook/prompting/ensembling/distinct_examples/#overview","title":"Overview","text":"<p>Demonstration Ensembling (DENSE) runs multiple prompts, each with a different subset of examples, then aggregates the outputs.</p>"},{"location":"cookbook/prompting/ensembling/distinct_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Sentiment: string { case Positive='Positive'; case Negative='Negative'; case Neutral='Neutral'; }\nclass DemonstrationResponse { public Sentiment $correct_answer; }\n\nclass DenseEnsembling {\n    public function __invoke(string $prompt, array $examples, int $numResponses) : array {\n        if ($numResponses &lt;= 0 || count($examples) % $numResponses !== 0) return [];\n        $batch = intdiv(count($examples), $numResponses);\n        $outputs = [];\n        for ($i = 0; $i &lt; count($examples); $i += $batch) {\n            $subset = array_slice($examples, $i, $batch);\n            $outputs[] = $this-&gt;one($prompt, $subset);\n        }\n        return $outputs;\n    }\n\n    private function one(string $prompt, array $examples) : DemonstrationResponse {\n        $joined = implode(\"\\n\", $examples);\n        $system = &lt;&lt;&lt;TXT\n        You classify queries as Positive, Negative, or Neutral.\n        Refer to the provided examples. Examine each before deciding.\n        Here are the examples:\n        {$joined}\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;$prompt] ],\n            responseModel: DemonstrationResponse::class,\n            options: ['temperature' =&gt; 0.0],\n        )-&gt;get();\n    }\n}\n\n$userQuery = 'What is the weather like today?';\n$examples = [\n    'I love this product! [Positive]',\n    'This is the worst service ever. [Negative]',\n    'The movie was okay, not great but not terrible. [Neutral]',\n    \"I'm so happy with my new phone! [Positive]\",\n    'The food was terrible and the service was slow. [Negative]',\n    \"It's an average day, nothing special. [Neutral]\",\n    'Fantastic experience, will come again! [Positive]',\n    \"I wouldn't recommend this to anyone. [Negative]\",\n    'The book was neither good nor bad. [Neutral]',\n    'Absolutely thrilled with the results! [Positive]',\n];\n\n$responses = (new DenseEnsembling)($userQuery, $examples, 5);\n$counts = [];\nforeach ($responses as $r) { $k = $r-&gt;correct_answer-&gt;value; $counts[$k] = ($counts[$k] ?? 0) + 1; }\narsort($counts);\n$mostCommon = array_key_first($counts);\ndump($mostCommon);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/distinct_examples/#references","title":"References","text":"<p>1) Exploring Demonstration Ensembling for In-Context Learning (https://arxiv.org/pdf/2308.08780)</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/","title":"Use Ensembles To Test Prompts","text":""},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#overview","title":"Overview","text":""},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#whats-max-mutual-information","title":"What's Max Mutual Information?","text":"<p>Max Mutual Information Method aims to find the best prompt to elicit the desired response from an LLM by maximizing a mutual information proxy \u2014 i.e., reducing model uncertainty with the prompt.</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#entropy","title":"Entropy","text":"<p>When a language model receives a prompt, it produces a distribution over outputs. Lower entropy suggests higher confidence.</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#mutual-information","title":"Mutual Information","text":"<p>We approximate mutual information as the difference between marginal and conditional entropies of outputs across multiple samples for a given prompt. Below, we use a lightweight proxy based on answer diversity and rationale repetitiveness.</p>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass CoT { /** @var string[] */ public array $chain_of_thought; public int $answer; }\nclass PromptScore { public string $prompt; public float $score; }\n\nclass PromptEnsembler {\n    public function __construct(private int $k = 3) {}\n    public function evaluate(array $prompts, string $question) : array {\n        $scored = [];\n        foreach ($prompts as $p) { $scored[] = $this-&gt;scorePrompt($p, $question); }\n        usort($scored, fn($a,$b)=&gt; $a-&gt;score &lt;=&gt; $b-&gt;score);\n        return $scored; // lower is better (proxy for MI)\n    }\n    private function scorePrompt(string $prompt, string $question) : PromptScore {\n        $answers = [];\n        for ($i = 0; $i &lt; $this-&gt;k; $i++) { $answers[] = $this-&gt;run(\"{$prompt}\\n\\n{$question}\"); }\n        $entropy = $this-&gt;normalizedEntropy(array_map(fn($r)=&gt;$r-&gt;answer, $answers));\n        $rep = $this-&gt;repetitiveness($answers);\n        $ps = new PromptScore(); $ps-&gt;prompt = $prompt; $ps-&gt;score = $entropy - $rep; return $ps;\n    }\n    private function run(string $input) : CoT {\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$input] ],\n            responseModel: CoT::class,\n        )-&gt;get();\n    }\n    private function normalizedEntropy(array $answers) : float {\n        $n = count($answers); if ($n===0) return 0.0; $freq=[]; foreach($answers as $a){$freq[$a]=($freq[$a]??0)+1;}\n        $h=0.0; foreach($freq as $c){$p=$c/$n; $h += ($p&gt;0)? -$p*log($p):0.0;} $max=log(max(1,count($freq))); return $max&gt;0? $h/$max:0.0;\n    }\n    private function repetitiveness(array $responses) : float {\n        $r = array_map(fn($x)=&gt; implode(' ', $x-&gt;chain_of_thought), $responses);\n        $uniq = count(array_unique($r)); $n = max(1,count($responses)); return 1.0 - ($uniq/$n);\n    }\n}\n\n$prompts = [\n    'Explain step-by-step then answer:',\n    'Think carefully and provide reasoning before the answer:',\n    'Reason in numbered steps and conclude with the final number:',\n];\n$question = 'If a store sold 93 in the morning and 39 in the afternoon from 200 baked, and 6 were returned, how many remain?';\n$scores = (new PromptEnsembler)-&gt;evaluate($prompts, $question);\ndump($scores);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/ensemble_test_prompts/#references","title":"References","text":"<p>1) https://learnprompting.org/docs/advanced/ensembling/max_mutual_information_method</p>"},{"location":"cookbook/prompting/ensembling/multiple_candidates/","title":"Generate Multiple Candidate Responses","text":""},{"location":"cookbook/prompting/ensembling/multiple_candidates/#overview","title":"Overview","text":"<p>Generate multiple candidate responses and pick the most common answer (Self-Consistency).</p>"},{"location":"cookbook/prompting/ensembling/multiple_candidates/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass SelfConsistencyResponse {\n    public string $chain_of_thought;\n    public int $correct_answer;\n}\n\nclass SelfConsistency {\n    public function __invoke(string $prompt, int $k = 5) : int {\n        $answers = [];\n        for ($i = 0; $i &lt; $k; $i++) { $answers[] = $this-&gt;one($prompt)-&gt;correct_answer; }\n        $counts = [];\n        foreach ($answers as $a) { $key = (string)$a; $counts[$key] = ($counts[$key] ?? 0) + 1; }\n        arsort($counts);\n        return (int) array_key_first($counts);\n    }\n\n    private function one(string $prompt) : SelfConsistencyResponse {\n        $system = 'You are an intelligent QA system. First think step-by-step, then provide the final answer.';\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;$prompt] ],\n            responseModel: SelfConsistencyResponse::class,\n            options: ['temperature' =&gt; 0.5],\n        )-&gt;get();\n    }\n}\n\n$prompt = &lt;&lt;&lt;TXT\nJanet's ducks lay 16 eggs per day.\nShe eats 3 for breakfast and bakes muffins with 4 each day.\nShe sells the remainder for $2 per egg. How much does she make per day?\nTXT;\n\n$answer = (new SelfConsistency)($prompt, k: 5);\ndump($answer);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/multiple_candidates/#references","title":"References","text":"<p>1) Self-Consistency Improves Chain Of Thought Reasoning In Language Models (https://arxiv.org/pdf/2210.03350)</p>"},{"location":"cookbook/prompting/ensembling/task_specific_evals/","title":"Use Task Specific Evaluation Metrics","text":""},{"location":"cookbook/prompting/ensembling/task_specific_evals/#overview","title":"Overview","text":"<p>Universal Self Prompting is a two stage process similar to Consistency Based Self Adaptive Prompting (COSP). Here is a breakdown of the two stages.</p> <p>Generate Examples : LLMs are prompted to generate a collection of candidate responses using a test dataset Answer Query : We then select a few of these model-generated responses as examples to prompt the LLM to obtain a final prediction. Note here that the final answer is obtained using a single forward pass with greedy decoding.</p> <p>USP Process\u00b6</p> <p>Let's see how this works in greater detail.</p> <p>Generate Few Shot Examples\u00b6 We first prompt our model to generate responses for a given set of prompts. Instead of measuring the entropy and repetitiveness as in COSP, we use one of three possible methods to measure the quality of the generated responses. These methods are decided based on the three categories supported.</p> <p>This category has to be specified by a user ahead of time.</p> <p>Note that for Short Form and Long Form generation, we generate m m different samples. This is not the case for classification tasks.</p> <p>Classification : Classification Tasks are evaluated using the normalized probability of each label using the raw logits from the LLM.</p> <p>In short, we take the raw logit for each token corresponding to the label, use a softmax to normalize each of them and then sum across the individual probabilities and their log probs. We also try to sample enough queries such that we have a balanced number of predictions across each class ( so that our model doesn't have a bias towards specific classes )</p> <p>Short Form Generation: This is done by using a similar formula to COSP but without the normalizing term</p> <p>Long Form Generation: This is done by using the average pairwise ROUGE score between all pairs of the m responses.</p> <p>What is key here is that depending on the task specified by the user, we have a task-specific form of evaluation. This eventually allows us to better evaluate our individual generated examples. Samples of tasks for each category include</p> <p>Classification: Natural Language Inference, Topic Classification and Sentiment Analysis Short Form Generation : Question Answering and Sentence Completion Long Form Generation : Text Summarization and Machine Translation This helps to ultimately improve the performance of these large language models across different types of tasks.</p> <p>Generate Single Response\u00b6 Once we've selected our examples, the second step is relatively simple. We just need to append a few of our chosen examples that score best on our chosen metric to append to our solution.</p> <p>Implementation\u00b6 We've implemented a classification example below that tries to sample across different classes in a balanced manner before generating a response using a single inference call.</p> <p>We bias this sampling towards samples that the model is more confident towards by using a confidence label.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Emotion: string { case Happy='Happy'; case Angry='Angry'; case Sadness='Sadness'; }\nenum Confidence: string { case Uncertain='Uncertain'; case Somewhat='Somewhat Confident'; case Confident='Confident'; case Highly='Highly Confident'; }\n\nclass Classification { public string $chain_of_thought; public Emotion $label; public Confidence $confidence; }\n\nclass USPClassification {\n    public function classify(string $query) : Classification {\n        $content = 'Classify the following query into one of: Happy, Angry, Sadness';\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$content], ['role'=&gt;'user','content'=&gt;$query] ],\n            responseModel: Classification::class,\n        )-&gt;get();\n    }\n\n    public function balancedSample(array $queries, int $k) : array {\n        $preds = [];\n        foreach ($queries as $q) { $preds[] = [$this-&gt;classify($q), $q]; }\n        $by = [];\n        foreach ($preds as $p) { $by[$p[0]-&gt;label-&gt;value][] = $p; }\n        $per = max(1, intdiv($k, max(1, count($by))));\n        $out = [];\n        foreach ($by as $label =&gt; $items) {\n            usort($items, fn($a,$b) =&gt; $this-&gt;score($b[0]-&gt;confidence) &lt;=&gt; $this-&gt;score($a[0]-&gt;confidence));\n            $slice = array_slice($items, 0, $per);\n            foreach ($slice as $it) { $out[] = $it[1] . \" ({$label})\"; }\n        }\n        return $out;\n    }\n\n    public function finalWithExamples(string $query, array $examples) : Classification {\n        $formatted = implode(\"\\n\", $examples);\n        $system = \"You classify queries into Happy, Angry, or Sadness.\\n&lt;examples&gt;\\n{$formatted}\\n&lt;/examples&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;$query] ],\n            responseModel: Classification::class,\n        )-&gt;get();\n    }\n\n    private function score(Confidence $c) : int {\n        return match($c) { Confidence::Highly =&gt; 4, Confidence::Confident =&gt; 3, Confidence::Somewhat =&gt; 2, Confidence::Uncertain =&gt; 1 };\n    }\n}\n?&gt;\n</code></pre> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n$examples = [\n        \"i do feel that running is a divine experience and\n        that i can expect to have some type of spiritual\n        encounter\",\n        \"i get giddy over feeling elegant in a perfectly\n        fitted pencil skirt\",\n        \"\n        i plan to share my everyday life stories traveling\n        adventures inspirations and handmade creations with\n        you and hope you will also feel inspired\n        \",\n        \"\n        i need to feel the dough to make sure its just\n        perfect\n        \",\n        \"\n        i found myself feeling a little discouraged that\n        morning\n        \",\n        \"i didnt really feel that embarrassed\",\n        \"i feel like a miserable piece of garbage\",\n        \"\n        i feel like throwing away the shitty piece of shit\n        paper\n        \",\n        \"\n        i feel irritated and rejected without anyone doing\n        anything or saying anything\n        \",\n        \"i feel angered and firey\",\n        \"\n        im feeling bitter today my mood has been strange the\n        entire day so i guess its that\n        \",\n        \"i just feel really violent right now\",\n        \"i know there are days in which you feel distracted\",\n    ];\n\n// Run the selection and final classification\n$usp = new USPClassification();\n$balanced = $usp-&gt;balancedSample($examples, 3);\n$final = $usp-&gt;finalWithExamples('i feel furious that right to life advocates can and do tell me how to live and die', $balanced);\ndump($balanced, $final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/","title":"Use Translation for Paraphrasing","text":""},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/#overview","title":"Overview","text":"<p>Back-translation can produce diverse paraphrases: translate to another language and back to English, encouraging varied phrasing.</p>"},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TranslatedPrompt { public string $translation; }\n\nclass Paraphraser {\n    public function translate(string $prompt, string $from, string $to) : TranslatedPrompt {\n        $system = \"You are an expert translation assistant. Translate from {$from} to {$to}. Paraphrase and use synonyms where possible.\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system], ['role'=&gt;'user','content'=&gt;\"Prompt: {$prompt}\"] ],\n            responseModel: TranslatedPrompt::class,\n        )-&gt;get();\n    }\n\n    public function backTranslate(string $prompt, string $lang) : string {\n        $step1 = $this-&gt;translate($prompt, 'english', $lang)-&gt;translation;\n        $step2 = $this-&gt;translate($step1, $lang, 'english')-&gt;translation;\n        return $step2;\n    }\n\n    public function generate(string $prompt, array $languages, int $permutations = 5) : array {\n        $out = [];\n        for ($i = 0; $i &lt; $permutations; $i++) {\n            $lang = $languages[$i % max(1, count($languages))] ?? 'spanish';\n            $out[] = $this-&gt;backTranslate($prompt, $lang);\n        }\n        return $out;\n    }\n}\n\n$prompt = 'Explain how photosynthesis works for a 10-year-old.';\n$languages = ['spanish','french','german'];\n$variants = (new Paraphraser)-&gt;generate($prompt, $languages, permutations: 3);\ndump($variants);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/translation_paraphrasing/#references","title":"References","text":"<p>1) Prompt paraphrasing approaches</p>"},{"location":"cookbook/prompting/ensembling/verify_majority_voting/","title":"Verify Responses over Majority Voting","text":""},{"location":"cookbook/prompting/ensembling/verify_majority_voting/#overview","title":"Overview","text":"<p>Diverse verifier scoring aggregates quality over unique answers, improving over majority vote.</p>"},{"location":"cookbook/prompting/ensembling/verify_majority_voting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ResponseItem { public string $chain_of_thought; public int $answer; }\nenum Grade: string { case Poor='Poor'; case Average='Average'; case Good='Good'; case Excellent='Excellent'; }\nclass Grading { public Grade $grade; }\n\nclass DiverseVerifier {\n    public function __invoke(string $query, array $examples, int $k = 6) : int {\n        $responses = [];\n        for ($i = 0; $i &lt; $k; $i++) { $responses[] = $this-&gt;generate($query, $examples); }\n        $scores = [];\n        foreach ($responses as $r) {\n            $g = $this-&gt;score($query, $r);\n            $scores[$r-&gt;answer] = ($scores[$r-&gt;answer] ?? 0.0) + $this-&gt;map($g);\n        }\n        arsort($scores);\n        return (int) array_key_first($scores);\n    }\n\n    private function generate(string $query, array $examples) : ResponseItem {\n        $formatted = implode(\"\\n\", $examples);\n        $content = \"You answer succinctly.\\n&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\\n\\n&lt;examples&gt;\\n{$formatted}\\n&lt;/examples&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$content] ],\n            responseModel: ResponseItem::class,\n        )-&gt;get();\n    }\n\n    private function score(string $query, ResponseItem $response) : Grading {\n        $content = \"Score the response to the query. Output only the grade.\\n&lt;query&gt;\\n{$query}\\n&lt;/query&gt;\\n&lt;response&gt;\\nChain: {$response-&gt;chain_of_thought}\\nAnswer: {$response-&gt;answer}\\n&lt;/response&gt;\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'user','content'=&gt;$content] ],\n            responseModel: Grading::class,\n        )-&gt;get();\n    }\n\n    private function map(Grading $g) : float {\n        return match($g-&gt;grade) { Grade::Excellent =&gt; 1.0, Grade::Good =&gt; 0.75, Grade::Average =&gt; 0.5, Grade::Poor =&gt; 0.25 };\n    }\n}\n\n$examples = [\n    \"Q: James runs 3 sprints, 3 times a week, 60m each. How many meters per week? A: ... The answer is 540.\",\n    \"Q: Brandon's iPhone age puzzle... A: ... The answer is 8.\",\n    \"Q: Jean has 30 lollipops ... bags? A: ... The answer is 14.\",\n    \"Q: Weng earns $12/hour, worked 50 minutes. How much? A: ... The answer is 10.\",\n];\n$query = 'Betty needs $100; has half; parents give $15; grandparents twice parents. How much more needed?';\n\n$best = (new DiverseVerifier)($query, $examples, k: 6);\ndump($best);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/ensembling/verify_majority_voting/#references","title":"References","text":"<p>1) Making Language Models Better Reasoners with Step-Aware Verifier (https://aclanthology.org/2023.acl-long.291/)</p>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/","title":"Consistency Based Self Adaptive Prompting (COSP)","text":""},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#overview","title":"Overview","text":"<p>COSP is a technique that improves few-shot learning by selecting high-quality examples based on consistency and confidence of model responses. It identifies examples the model can process reliably.</p> <p>The process involves: 1. Example Generation: Generate multiple responses per example, collect confidence scores 2. Example Selection: Select examples with low entropy and high repetitiveness</p>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ResponseWithConfidence {\n    public string $content;\n    /** Confidence score between 0 and 1 */\n    public float $confidence;\n}\n\nclass COSPSelector {\n    private int $nSamples;\n\n    public function __construct(int $nSamples = 3) {\n        $this-&gt;nSamples = $nSamples;\n    }\n\n    public function generateResponses(string $prompt): array {\n        $responses = [];\n        for ($i = 0; $i &lt; $this-&gt;nSamples; $i++) {\n            $responses[] = (new StructuredOutput)-&gt;with(\n                messages: [['role' =&gt; 'user', 'content' =&gt; $prompt]],\n                responseModel: ResponseWithConfidence::class,\n            )-&gt;get();\n        }\n        return $responses;\n    }\n\n    public function calculateMetrics(array $responses): array {\n        $confidences = array_map(fn($r) =&gt; $r-&gt;confidence, $responses);\n        $entropyScore = $this-&gt;entropy($confidences);\n\n        $uniqueResponses = count(array_unique(array_map(fn($r) =&gt; $r-&gt;content, $responses)));\n        $repetitiveness = 1 - ($uniqueResponses / count($responses));\n\n        return ['entropy' =&gt; $entropyScore, 'repetitiveness' =&gt; $repetitiveness];\n    }\n\n    private function entropy(array $values): float {\n        $sum = array_sum($values);\n        if ($sum == 0) return 0.0;\n        $normalized = array_map(fn($v) =&gt; $v / $sum, $values);\n        $entropy = 0.0;\n        foreach ($normalized as $p) {\n            if ($p &gt; 0) $entropy -= $p * log($p);\n        }\n        return $entropy;\n    }\n\n    public function selectBestExamples(array $candidates, int $k): array {\n        $scored = [];\n        foreach ($candidates as $text) {\n            $responses = $this-&gt;generateResponses(\"Classify this text: {$text}\");\n            $metrics = $this-&gt;calculateMetrics($responses);\n            $score = $metrics['entropy'] - $metrics['repetitiveness'];\n            $scored[] = ['text' =&gt; $text, 'score' =&gt; $score, 'metrics' =&gt; $metrics];\n        }\n        usort($scored, fn($a, $b) =&gt; $a['score'] &lt;=&gt; $b['score']);\n        return array_slice($scored, 0, $k);\n    }\n}\n\n$selector = new COSPSelector(nSamples: 3);\n\n$candidates = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Python is a high-level programming language\",\n];\n\n$bestExamples = $selector-&gt;selectBestExamples($candidates, k: 2);\n\ndump($bestExamples);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#benefits","title":"Benefits","text":"<ul> <li>Improved Consistency: Select examples with low entropy and high repetitiveness</li> <li>Automated Selection: No manual example curation needed</li> <li>Quality Metrics: Quantifiable measure of example quality</li> </ul>"},{"location":"cookbook/prompting/few_shot/consistency_based_examples/#references","title":"References","text":"<p>1) Original COSP Paper (https://arxiv.org/abs/2305.14121) 2) Self-Consistency Improves Chain of Thought Reasoning (https://arxiv.org/abs/2203.11171)</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/","title":"Example Ordering","text":""},{"location":"cookbook/prompting/few_shot/example_ordering/#overview","title":"Overview","text":"<p>The order of few-shot examples in the prompt can affect LLM outputs. Consider permutating the order of these examples in your prompt to achieve better results.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#choosing-your-examples","title":"Choosing Your Examples","text":"<p>Depending on your use-case, here are a few different methods that you can consider using to improve the quality of your examples.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#combinatorics","title":"Combinatorics","text":"<p>One of the easiest methods is for us to manually iterate over each of the examples that we have and try all possible combinations we could create. This will in turn allow us to find the best combination that we can find.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#kate","title":"KATE","text":"<p>KATE (k-Nearest Example Tuning) is a method designed to enhance GPT-3's performance by selecting the most relevant in-context examples. The method involves:</p> <p>For each example in the test set, K nearest neighbors (examples) are retrieved based on semantic similarity. Among these K examples, those that appear most frequently across different queries are selected as the best in-context examples.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#using-an-unsupervised-retriever","title":"Using an Unsupervised Retriever","text":"<p>We can use a large LLM to compute a single score for each example with respect to a given prompt. This allows us to create a training set that scores an example's relevance when compared against a prompt. Using this training set, we can train a model that mimics this functionality. This allows us to determine the top k most relevant and most irrelevant examples when a user makes a query so that we can include this in our final prompt.</p>"},{"location":"cookbook/prompting/few_shot/example_ordering/#references","title":"References","text":"<ol> <li>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity (https://arxiv.org/abs/2104.08786)</li> <li>Reordering Examples Helps during Priming-based Few-Shot Learning (https://arxiv.org/abs/2106.01751)</li> <li>What Makes Good In-Context Examples for GPT-3? (https://arxiv.org/abs/2101.06804)</li> <li>Learning To Retrieve Prompts for In-Context Learning (https://aclanthology.org/2022.naacl-main.191/)</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</li> </ol>"},{"location":"cookbook/prompting/few_shot/example_ordering/#example","title":"Example","text":"<pre><code>&lt;?php\n// This example demonstrates concepts - no executable code.\necho \"See the overview for techniques on ordering few-shot examples.\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/in_context_examples/","title":"Generate In-Context Examples","text":""},{"location":"cookbook/prompting/few_shot/in_context_examples/#overview","title":"Overview","text":"<p>How can we generate examples for our prompt?</p> <p>Self-Generated In-Context Learning (SG-ICL) is a technique which uses an LLM to generate examples to be used during the task. This allows for in-context learning, where examples of the task are provided in the prompt.</p> <p>We can implement SG-ICL using Instructor as seen below.</p>"},{"location":"cookbook/prompting/few_shot/in_context_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum ReviewSentiment : string {\n    case Positive = 'positive';\n    case Negative = 'negative';\n}\n\nclass GeneratedReview {\n    public string $review;\n    public ReviewSentiment $sentiment;\n}\n\n\nclass PredictSentiment {\n    private int $n = 4;\n\n    public function __invoke(string $review) : ReviewSentiment {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Review: {$review}\"],\n            ],\n            responseModel: Scalar::enum(ReviewSentiment::class),\n            examples: $this-&gt;generateExamples($review),\n        )-&gt;get();\n    }\n\n    private function generate(string $inputReview, ReviewSentiment $sentiment) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Generate {$this-&gt;n} various {$sentiment-&gt;value} reviews based on the input review:\\n{$inputReview}\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"Generated review:\"],\n            ],\n            responseModel: Sequence::of(GeneratedReview::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    private function generateExamples(string $inputReview) : array {\n        $examples = [];\n        foreach ([ReviewSentiment::Positive, ReviewSentiment::Negative] as $sentiment) {\n            $samples = $this-&gt;generate($inputReview, $sentiment);\n            foreach ($samples as $sample) {\n                $examples[] = Example::fromData($sample-&gt;review, $sample-&gt;sentiment-&gt;value);\n            }\n        }\n        return $examples;\n    }\n}\n\n$predictSentiment = (new PredictSentiment)('This movie has been very impressive, even considering I lost half of the plot.');\n\ndump($predictSentiment);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/in_context_examples/#references","title":"References","text":"<ol> <li>Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques</li> </ol>"},{"location":"cookbook/prompting/few_shot/select_effective_samples/","title":"Select Effective Examples","text":""},{"location":"cookbook/prompting/few_shot/select_effective_samples/#overview","title":"Overview","text":"<p>Select effective in-context examples by choosing those semantically closest to the query using KNN (k-Nearest Neighbors) with embeddings.</p> <p>Steps: 1. Embed the candidate examples 2. Embed the query to answer 3. Find the k examples closest to the query 4. Use chosen examples as context for the LLM</p>"},{"location":"cookbook/prompting/few_shot/select_effective_samples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass Answer {\n    public string $answer;\n}\n\nclass KNNExampleSelector {\n    private Embeddings $embeddings;\n\n    public function __construct() {\n        $this-&gt;embeddings = new Embeddings();\n    }\n\n    public function cosineSimilarity(array $a, array $b): float {\n        $dotProduct = 0.0;\n        $normA = 0.0;\n        $normB = 0.0;\n        for ($i = 0; $i &lt; count($a); $i++) {\n            $dotProduct += $a[$i] * $b[$i];\n            $normA += $a[$i] * $a[$i];\n            $normB += $b[$i] * $b[$i];\n        }\n        return $dotProduct / (sqrt($normA) * sqrt($normB));\n    }\n\n    public function embed(array $texts): array {\n        $result = $this-&gt;embeddings-&gt;create($texts)-&gt;first()-&gt;embedding;\n        return $result;\n    }\n\n    public function embedAll(array $texts): array {\n        $results = [];\n        foreach ($texts as $text) {\n            $results[] = $this-&gt;embed([$text]);\n        }\n        return $results;\n    }\n\n    public function selectKNearest(array $examples, string $query, int $k): array {\n        $questions = array_column($examples, 'question');\n        $exampleEmbeddings = $this-&gt;embedAll($questions);\n        $queryEmbedding = $this-&gt;embed([$query]);\n\n        $scored = [];\n        foreach ($examples as $i =&gt; $example) {\n            $similarity = $this-&gt;cosineSimilarity($exampleEmbeddings[$i], $queryEmbedding);\n            $scored[] = ['example' =&gt; $example, 'similarity' =&gt; $similarity];\n        }\n\n        usort($scored, fn($a, $b) =&gt; $b['similarity'] &lt;=&gt; $a['similarity']);\n        return array_slice($scored, 0, $k);\n    }\n\n    public function generateWithExamples(array $selectedExamples, string $query): Answer {\n        $context = \"\";\n        foreach ($selectedExamples as $item) {\n            $ex = $item['example'];\n            $context .= \"&lt;example&gt;\\n&lt;question&gt;{$ex['question']}&lt;/question&gt;\\n&lt;answer&gt;{$ex['answer']}&lt;/answer&gt;\\n&lt;/example&gt;\\n\";\n        }\n\n        $prompt = \"Respond to the query using the examples as guidance.\\n\\n{$context}\\n&lt;query&gt;{$query}&lt;/query&gt;\";\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role' =&gt; 'user', 'content' =&gt; $prompt]],\n            responseModel: Answer::class,\n        )-&gt;get();\n    }\n}\n\n$selector = new KNNExampleSelector();\n\n$examples = [\n    ['question' =&gt; 'What is the capital of France?', 'answer' =&gt; 'Paris'],\n    ['question' =&gt; 'Who wrote Romeo and Juliet?', 'answer' =&gt; 'Shakespeare'],\n    ['question' =&gt; 'What is the capital of Germany?', 'answer' =&gt; 'Berlin'],\n];\n\n$query = 'What is the capital of Italy?';\n\n$kClosest = $selector-&gt;selectKNearest($examples, $query, k: 2);\ndump($kClosest);\n\n$response = $selector-&gt;generateWithExamples($kClosest, $query);\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/few_shot/select_effective_samples/#references","title":"References","text":"<p>1) What Makes Good In-Context Examples for GPT-3? (https://arxiv.org/abs/2101.06804) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties/","title":"Arbitrary properties","text":""},{"location":"cookbook/prompting/misc/arbitrary_properties/#overview","title":"Overview","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n?&gt;\n</code></pre> <p>Now we can use this data model to extract arbitrary properties from a text message in a form that is easier for future processing.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives\n    in a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    mode: OutputMode::Json,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(!empty($user-&gt;properties));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/","title":"Consistent values of arbitrary properties","text":""},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/#overview","title":"Overview","text":"<p>For multiple records containing arbitrary properties, instruct LLM to get more consistent key names when extracting properties.</p>"},{"location":"cookbook/prompting/misc/arbitrary_properties_consistent/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $id;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetails\n{\n    /**\n     * @var UserDetail[] Extract information for multiple users.\n     * Use consistent key names for properties across users.\n     */\n    public array $users = [];\n}\n\n$text = \"Jason is 25 years old. He is a Python programmer.\\\n Amanda is UX designer.\\\n John is 40yo and he's CEO.\";\n\n$list = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetails::class,\n)-&gt;get();\n\ndump($list);\n\nassert(!empty($list-&gt;users));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/chain_of_summaries/","title":"Chain of Summaries","text":""},{"location":"cookbook/prompting/misc/chain_of_summaries/#overview","title":"Overview","text":"<p>This is an example of summarization with increasing amount of details. Instructor is provided with data structure containing instructions on how to create increasingly detailed summaries of the project report.</p> <p>It starts with generating an overview of the project, followed by X iterations of increasingly detailed summaries. Each iteration should contain all the information from the previous summary, plus a few additional facts from the content which are most relevant and missing from the previous iteration.</p>"},{"location":"cookbook/prompting/misc/chain_of_summaries/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n/** Executive level summary of the project */\nclass Summary {\n    /** current summary iteration, not bigger than 3 */\n    public int $iteration = 0;\n    /** @var string[] 1-3 facts most relevant from executive perspective and missing from the summary (avoid technical details) */\n    public array $missingFacts = [];\n    /** denser summary in 1-3 sentences, which covers every fact from the previous summary plus the missing ones */\n    public string $expandedSummary = '';\n}\n\n/** Increasingly denser, expanded summaries */\nclass ChainOfSummaries {\n    /** simplified, executive view with no details, just a single statement of overall situation */\n    public string $overview;\n    /** @var Summary[] contains at least 3 gradually more expanded summaries of the content */\n    public array $summaries;\n}\n\n$summaries = (new StructuredOutput)\n    -&gt;with(\n        messages: $report,\n        responseModel: ChainOfSummaries::class,\n        prompt: 'Generate a denser summary based on the provided content.',\n        options: [\n            'max_tokens' =&gt; 4096,\n        ],\n        toolName: 'summarizer',\n        toolDescription: 'Generates a summary based on the provided content.',\n    )\n    -&gt;get();\n\nprint(\"\\n# Summaries with increasing density:\\n\\n\");\nprint(\"Overview:\\n\");\nprint(\"{$summaries-&gt;overview}\\n\\n\");\nforeach ($summaries-&gt;summaries as $summary) {\n    print(\"Expanded summary - iteration #{$summary-&gt;iteration}:\\n\");\n    print(\"{$summary-&gt;expandedSummary}\\n\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/chain_of_thought/","title":"Chain of Thought","text":""},{"location":"cookbook/prompting/misc/chain_of_thought/#overview","title":"Overview","text":"<p>This approach to \"chain of thought\" improves data quality, by eliciting LLM reasoning to self-explain approach to generating the response.</p> <p>With Instructor you can achieve a 'modular' CoT, where multiple explanations can be generated by LLM for different parts of the response, driving a more granular control and improvement of the response.</p>"},{"location":"cookbook/prompting/misc/chain_of_thought/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass Employee {\n    #[Instructions('Think step by step to determine the correct year of employment.')]\n    public string $reasoning;\n    public int $yearOfEmployment;\n    // ... other data fields of your employee class\n}\n\n$text = 'He was working here for 5 years. Now, in 2019, he is a manager.';\n\n$employee = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Employee::class\n)-&gt;get();\n\n\ndump($employee);\n\nassert($employee-&gt;yearOfEmployment === 2014);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/","title":"Single label classification","text":""},{"location":"cookbook/prompting/misc/classification/#overview","title":"Overview","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p>"},{"location":"cookbook/prompting/misc/classification/#example","title":"Example","text":"<p>Let's start by defining the data structures.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Enumeration for single-label text classification.\nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction.\nclass SinglePrediction {\n    public Label $classLabel;\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/#classifying-text","title":"Classifying Text","text":"<p>The function classify will perform the single-label classification.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction classify(string $data) : SinglePrediction {\n    return (new StructuredOutput)-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n    )-&gt;get();\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\n\ndump($prediction);\n\nassert($prediction-&gt;classLabel == Label::SPAM);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/","title":"Multiclass classification","text":""},{"location":"cookbook/prompting/misc/classification_multiclass/#overview","title":"Overview","text":"<p>We start by defining the structures.</p> <p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass TicketLabels {\n    /** @var Label[] */\n    public array $labels = [];\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/#classifying-text0","title":"Classifying Text0","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : TicketLabels {\n    return (new StructuredOutput)\n        -&gt;withMessages(\"Label following support ticket: {$data}\")\n        -&gt;withResponseModel(TicketLabels::class)\n        -&gt;get();\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/classification_multiclass/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\ndump($prediction);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;labels));\nassert(in_array(Label::BILLING, $prediction-&gt;labels));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/component_reuse/","title":"Reusing components","text":""},{"location":"cookbook/prompting/misc/component_reuse/#overview","title":"Overview","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p>"},{"location":"cookbook/prompting/misc/component_reuse/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public string $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Yesterday Jason worked from 9 for 5 hours. After that I watched 2 hour movie which I finished at 19.\"]],\n    responseModel: UserDetail::class,\n    model: 'gpt-4o-mini',\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name == \"Jason\");\nassert($user-&gt;workTime-&gt;startTime === 9);\nassert($user-&gt;workTime-&gt;endTime === 14);\nassert($user-&gt;leisureTime-&gt;startTime === 17);\nassert($user-&gt;leisureTime-&gt;endTime === 19);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/component_reuse_cot/","title":"Using CoT to improve interpretation of component data","text":""},{"location":"cookbook/prompting/misc/component_reuse_cot/#overview","title":"Overview","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <p>We're additionally starting the data structure with a Chain of Thought field to elicit LLM reasoning for the time range calculation, which can improve the accuracy of the response.</p>"},{"location":"cookbook/prompting/misc/component_reuse_cot/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours (0-23 format) */\n    public int $startTime;\n    /** The end time in hours (0-23 format) */\n    public int $endTime;\n}\n\n$timeRange = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Workshop with Apex Industries started 9 and it took us 6 hours to complete.\"]],\n    responseModel: TimeRange::class,\n    maxRetries: 2\n)-&gt;get();\n\ndump($timeRange);\n\nassert($timeRange-&gt;startTime === 9);\nassert($timeRange-&gt;endTime === 15);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/entity_relationships/","title":"Entity relationship extraction","text":""},{"location":"cookbook/prompting/misc/entity_relationships/#overview","title":"Overview","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> fields.</p>"},{"location":"cookbook/prompting/misc/entity_relationships/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public ?int $age;\n    public string $name;\n    public string $role;\n    /**\n     * @var int[] Correct and complete list of coworker IDs, representing\n     * collaboration between users.\n     */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /**\n     * @var UserDetail[] Collection of users, correctly capturing the\n     * relationships among them.\n     */\n    public array $users;\n}\n\n$text = \"Jason is 25 years old. He is a Python programmer of Apex website.\\\n Amanda is a contractor working with Jason on Apex website. John is 40yo\\\n and he's CEO - Jason reports to him.\";\n\n$relationships = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserRelationships::class,\n)-&gt;get();\n\ndump($relationships);\n\nassert(!empty($relationships-&gt;users));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/handling_errors/","title":"Handling errors","text":""},{"location":"cookbook/prompting/misc/handling_errors/#overview","title":"Overview","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <p>NOTE: Instructor offers a built-in Maybe wrapper class that you can use to handle errors. See the example in Basics section for more details.</p>"},{"location":"cookbook/prompting/misc/handling_errors/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $user = null;\n    public bool $noUserData = false;\n    /** If no user data, provide reason */\n    public ?string $errorMessage = '';\n\n    public function get(): ?UserDetail {\n        return $this-&gt;noUserData ? null : $this-&gt;user;\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'We don\\'t know anything about this guy.']])\n    -&gt;withResponseModel(MaybeUser::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;noUserData);\nassert(!empty($user-&gt;errorMessage));\nassert($user-&gt;get() === null);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/limiting_lists/","title":"Limiting the length of lists","text":""},{"location":"cookbook/prompting/misc/limiting_lists/#overview","title":"Overview","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length of list. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <p>To be 100% certain the list does not exceed the limit, add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"cookbook/prompting/misc/limiting_lists/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass Property\n{\n    /**  Monotonically increasing ID, not larger than 2 */\n    public string $index;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    use ValidationMixin;\n\n    public int $age;\n    public string $name;\n    /** @var Property[] List other extracted properties - not more than 2. */\n    public array $properties;\n\n    public function validate() : ValidationResult\n    {\n        if (count($this-&gt;properties) &lt; 3) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'properties',\n            value: $this-&gt;name,\n            message: \"Number of properties must be not more than 2.\",\n        );\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives in\n    a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    maxRetries: 1 // change to 0 to see validation error\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(count($user-&gt;properties) &lt; 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/reflection_prompting/","title":"Reflection Prompting","text":""},{"location":"cookbook/prompting/misc/reflection_prompting/#overview","title":"Overview","text":"<p>This implementation of Reflection Prompting with Instructor provides a structured way to encourage LLM to engage in more thorough and self-critical thinking processes, potentially leading to higher quality and more reliable outputs.</p>"},{"location":"cookbook/prompting/misc/reflection_prompting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Contracts\\CanValidateSelf;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass ReflectiveResponse implements CanValidateSelf {\n    #[Instructions('Is problem solvable and what domain expertise it requires')]\n    public string $assessment;\n    #[Instructions('Describe an expert persona who would be able to solve this problem, their skills and experience')]\n    public string $persona;\n    #[Instructions(\"Initial analysis and expert persona's approach to the problem\")]\n    public string $initialThinking;\n    #[Instructions('Steps of reasoning leading to the final answer - expert persona thinking through the problem')]\n    /** @var string[] */\n    public array $chainOfThought;\n    #[Instructions('Critical examination of the reasoning process - what could go wrong, what are the assumptions')]\n    public string $reflection;\n    #[Instructions('Final answer after reflection')]\n    public string $finalOutput;\n\n    // Validation method to ensure thorough reflection\n    #[\\Override]\n    public function validate(): ValidationResult {\n        $errors = [];\n        if (empty($this-&gt;reflection)) {\n            $errors[] = \"Reflection is required for a thorough response.\";\n        }\n        if (count($this-&gt;chainOfThought) &lt; 2) {\n            $errors[] = \"Please provide at least two steps in the chain of thought.\";\n        }\n        return ValidationResult::make($errors);\n    }\n}\n\n$problem = 'Solve the equation x+y=x-y';\n$solution = (new StructuredOutput)-&gt;using('anthropic')-&gt;with(\n    messages: $problem,\n    responseModel: ReflectiveResponse::class,\n    mode: OutputMode::MdJson,\n    options: ['max_tokens' =&gt; 2048]\n)-&gt;get();\n\nprint(\"Problem:\\n$problem\\n\\n\");\ndump($solution);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/restate_instructions/","title":"Restating instructions","text":""},{"location":"cookbook/prompting/misc/restate_instructions/#overview","title":"Overview","text":"<p>Make Instructor restate long or complex instructions and rules to improve inference accuracy.</p>"},{"location":"cookbook/prompting/misc/restate_instructions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /** Restate instructions and rules, so you can correctly determine the title. */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /* Guess job title */\n    public string $title;\n}\n\n/**\n * Details of analyzed user. The key information we're looking for\n * is appropriate role data.\n */\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am the head of Apex Software, responsible for\n    driving growth of our company.\n    TEXT;\n\n$structuredOutput = new StructuredOutput;\n$user = ($structuredOutput)-&gt;with(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\n//assert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/rewrite_instructions/","title":"Ask LLM to rewrite instructions","text":""},{"location":"cookbook/prompting/misc/rewrite_instructions/#overview","title":"Overview","text":"<p>Asking LLM to rewrite the instructions and rules is another way to improve inference results.</p> <p>You can provide arbitrary instructions on the data handling in the class and property PHPDocs. Instructor will use these instructions to guide LLM in the inference process.</p>"},{"location":"cookbook/prompting/misc/rewrite_instructions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /**\n     * Rewrite the instructions and rules in a concise form to correctly\n     * determine the user's title - just the essence.\n     */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /** Most likely job title */\n    public string $title;\n}\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am responsible for driving growth of our\n    company.\n    TEXT;\n\n$structuredOutput = new StructuredOutput;\n$user = $structuredOutput-&gt;with(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n)-&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\nassert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/search_query_expansion/","title":"Expanding search queries","text":""},{"location":"cookbook/prompting/misc/search_query_expansion/#overview","title":"Overview","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#why-it-matters","title":"Why it matters","text":"<p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#structure-of-the-data","title":"Structure of the data","text":"<p>The <code>SearchQuery</code> is a PHP class that defines the structure of an individual search query.</p> <p>It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p>"},{"location":"cookbook/prompting/misc/search_query_expansion/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/search_query_expansion/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries.</p> <p>It uses the <code>StructuredOutput::create()</code> method to extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nfunction segment(string $data) : Search {\n    return (new StructuredOutput)\n        //-&gt;withDebugPreset('on')\n        -&gt;withMessages(\"Consider the data below: '\\n$data' and segment it into multiple search queries\")\n        -&gt;withResponseClass(Search::class)\n        -&gt;get();\n}\n\n$search = segment(\"Find a picture of a cat and a video of a dog\");\nforeach ($search-&gt;queries as $query) {\n    $query-&gt;execute();\n}\n// Results:\n// Searching with query `picture of a cat` using `image`\n// Searching with query `video of a dog` using `video`\n\nassert(count($search-&gt;queries) === 2);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/misc/summary_with_keywords/","title":"Summary with Keywords","text":""},{"location":"cookbook/prompting/misc/summary_with_keywords/#overview","title":"Overview","text":"<p>This is an example of a simple summarization with keyword extraction.</p>"},{"location":"cookbook/prompting/misc/summary_with_keywords/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\nclass Summary {\n    #[Description('Project summary, not longer than 3 sentences')]\n    public string $summary = '';\n    #[Description('5 most relevant keywords extracted from the summary')]\n    public array $keywords = [];\n}\n\n$summary = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: $report,\n        responseModel: Summary::class,\n    )\n    -&gt;get();\n\ndump($summary);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/","title":"Break Down Reasoning Into Multiple Steps","text":""},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/#overview","title":"Overview","text":"<p>Cumulative Reasoning separates reasoning into propose \u2192 verify \u2192 report for better logical inference.</p>"},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nenum Prediction : string { case False = 'False'; case True = 'True'; case Unknown = 'Unknown'; }\n\nclass Proposition {\n    public string $premise1;\n    public string $premise2;\n    public string $reasoning;\n    public string $proposition;\n}\n\nclass ProposerOutput {\n    public string $reasoning;\n    #[Description('Deduced propositions relevant to the hypothesis')]\n    public array $valid_propositions; // of Proposition\n    public Prediction $prediction;\n}\n\nclass VerifiedProposition {\n    public string $proposition;\n    public string $reasoning;\n    public bool $is_valid;\n}\n\nclass ReporterOutput {\n    public string $reasoning;\n    public bool $is_valid_hypothesis;\n}\n\nclass CumulativeReasoningPipeline {\n    public function propose(array $premises, string $hypothesis) : ProposerOutput {\n        $formatted = '- ' . implode(\"\\n- \", $premises);\n        $sys = &lt;&lt;&lt;SYS\n            Think step by step using FOL to deduce propositions from given premises (at most two per deduction).\n            Avoid duplicating premises; reason only from stated premises/propositions.\n            SYS;\n        $user = &lt;&lt;&lt;USR\n            Premises:\n            {$formatted}\n\n            We want to deduce more Propositions to determine correctness of the following Hypothesis:\n            Hypothesis: {$hypothesis}\n            USR;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ProposerOutput::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $sys],\n                ['role' =&gt; 'user', 'content' =&gt; $user],\n            ],\n        )-&gt;get();\n    }\n\n    public function verify(ProposerOutput $proposal) : array {\n        $results = [];\n        foreach ($proposal-&gt;valid_propositions as $p) {\n            $sys = 'Use FOL to determine whether the deduction from two premises to the proposition is valid.';\n            $user = \"Premises:\\n{$p-&gt;premise1}\\n{$p-&gt;premise2}\\n\\nProposition:\\n{$p-&gt;proposition}\";\n            $res = (new StructuredOutput)-&gt;with(\n                model: 'gpt-4o-mini',\n                responseModel: VerifiedProposition::class,\n                messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys], ['role' =&gt; 'user', 'content' =&gt; $user] ],\n            )-&gt;get();\n            $results[] = $res;\n        }\n        return $results;\n    }\n\n    public function report(array $verificationResult, string $hypothesis, array $premises) : ReporterOutput {\n        $formattedPrem = '- ' . implode(\"\\n- \", $premises);\n        $props = [];\n        foreach ($verificationResult as $v) { if ($v-&gt;is_valid) { $props[] = $v-&gt;proposition; } }\n        $formattedProp = '- ' . implode(\"\\n- \", $props);\n        $sys = &lt;&lt;&lt;SYS\n            Think step by step. Read and analyze the Premises, then use FOL to judge whether the Hypothesis is True, False, or Unknown using the verified Propositions.\n            SYS;\n        $messages = [\n            ['role' =&gt; 'system', 'content' =&gt; $sys],\n            ['role' =&gt; 'user', 'content' =&gt; \"Premises:\\n{$formattedPrem}\\n\\nHypothesis: {$hypothesis}\"],\n            ['role' =&gt; 'assistant', 'content' =&gt; \"Let's think step by step. From the premises, we can deduce the following propositions:\\n{$formattedProp}\\n\\nRecall the Hypothesis: {$hypothesis}\"],\n        ];\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini', responseModel: ReporterOutput::class, messages: $messages,\n        )-&gt;get();\n    }\n}\n\n$hypothesis = 'Hyraxes lay eggs';\n$premises = [\n    'The only types of mammals that lay eggs are platypuses and echidnas',\n    'Platypuses are not hyrax',\n    'Echidnas are not hyrax',\n    'No mammals are invertebrates',\n    'All animals are either vertebrates or invertebrates',\n    'Mammals are animals',\n    'Hyraxes are mammals',\n    'Grebes lay eggs',\n    'Grebes are not platypuses and also not echidnas',\n];\n\n$pipeline = new CumulativeReasoningPipeline();\n$proposal = $pipeline-&gt;propose($premises, $hypothesis);\n$verified = $pipeline-&gt;verify($proposal);\n$report = $pipeline-&gt;report($verified, $hypothesis, $premises);\n\ndump($proposal, $verified, $report);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/break_down_reasoning/#references","title":"References","text":"<p>1: Cumulative Reasoning with Large Language Models (https://arxiv.org/pdf/2308.04371)</p>"},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/","title":"Determine Uncertainty of Reasoning Chain","text":""},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/#overview","title":"Overview","text":"<p>We want models to assess confidence in their own answers. Self-Calibration asks the model to justify an answer and state whether it is valid.</p>"},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass SelfCalibration {\n    #[Description('Reasoning about answer validity')]\n    public string $chain_of_thought;\n    #[Description('Whether the answer is correct or not')]\n    public bool $is_valid_answer;\n}\n\nclass EvaluateModelOutput {\n    public function __invoke(string $originalPrompt, string $modelResponse) : SelfCalibration {\n        $messages = &lt;&lt;&lt;MSG\n            Question: {$originalPrompt}\n\n            {$modelResponse}\n\n            Is this a valid answer to the question?\n            Examine the question thoroughly and generate a complete\n            reasoning for why the answer is correct or not before responding.\n            MSG;\n\n        return (new StructuredOutput)-&gt;with(\n            messages: $messages,\n            responseModel: SelfCalibration::class,\n            model: 'gpt-4o-mini',\n        )-&gt;get();\n    }\n}\n\n$originalPrompt = &lt;&lt;&lt;PROMPT\nWho was the third president of the United States?\nPROMPT;\n\n$modelResponse = &lt;&lt;&lt;MODEL\nHere are some brainstormed ideas:\nJames Monroe\nThomas Jefferson\nJefferson\nThomas Jefferson\nGeorge Washington\nMODEL;\n\n$result = (new EvaluateModelOutput)($originalPrompt, $modelResponse);\ndump($result);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/determine_uncertainty/#references","title":"References","text":"<ol> <li>Language Models (Mostly) Know What They Know (https://arxiv.org/pdf/2207.05221)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/","title":"Improve With Feedback","text":""},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/#overview","title":"Overview","text":"<p>Self-Refine iteratively generates an answer, critiques it, and refines it until a stopping condition is met.</p>"},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass Response { public string $code; }\n\nclass Feedback {\n    #[Description('Actions to improve the code')]\n    public array $feedback;\n    public bool $done;\n}\n\nclass Timestep {\n    public string $response;\n    public array $feedback;\n    public string $refined_response;\n}\n\nclass History {\n    /** @var Timestep[] */\n    public array $history = [];\n    public function add(string $code, array $feedback, string $refined) : void {\n        $t = new Timestep();\n        $t-&gt;response = $code;\n        $t-&gt;feedback = $feedback;\n        $t-&gt;refined_response = $refined;\n        $this-&gt;history[] = $t;\n    }\n}\n\nclass SelfRefinePipeline {\n    public function generateInitial(string $prompt) : Response {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: Response::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $prompt] ],\n        )-&gt;get();\n    }\n\n    public function generateFeedback(Response $response) : Feedback {\n        $msg = &lt;&lt;&lt;MSG\n            You are an expert Python coder.\n            Provide feedback on this code. How can we make it (1) faster and (2) more readable?\n\n            &lt;code&gt;\n            {$response-&gt;code}\n            &lt;/code&gt;\n\n            If the code does not need improvement, set done = True.\n            MSG;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: Feedback::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $msg] ],\n        )-&gt;get();\n    }\n\n    public function refine(Response $response, Feedback $feedback) : Response {\n        $feedbackLines = array_map(\n            fn($item) =&gt; is_string($item) ? $item : json_encode($item),\n            $feedback-&gt;feedback,\n        );\n        $feedbackText = implode(\"\\n\", $feedbackLines);\n\n        $msg = &lt;&lt;&lt;MSG\n            You are an expert Python coder.\n\n            &lt;response&gt;\n            {$response-&gt;code}\n            &lt;/response&gt;\n\n            &lt;feedback&gt;\n            {$feedbackText}\n            &lt;/feedback&gt;\n\n            Refine your response.\n            MSG;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: Response::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $msg] ],\n            )-&gt;get();\n    }\n\n    public function stop(Feedback $feedback, History $history) : bool {\n        if ($feedback-&gt;done) { return true; }\n        return count($history-&gt;history) &gt;= 3;\n    }\n}\n\n$pipeline = new SelfRefinePipeline();\n$response = $pipeline-&gt;generateInitial('Write Python code to calculate the Fibonacci sequence.');\n$history = new History();\n\nwhile (true) {\n    $fb = $pipeline-&gt;generateFeedback($response);\n    if ($pipeline-&gt;stop($fb, $history)) { break; }\n    $refined = $pipeline-&gt;refine($response, $fb);\n    $history-&gt;add($response-&gt;code, $fb-&gt;feedback, $refined-&gt;code);\n    $response = $refined;\n}\n\ndump($history, $response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/improve_with_feedback/#references","title":"References","text":"<ol> <li>Self-Refine: Iterative Refinement with Self-Feedback (https://arxiv.org/abs/2303.17651)</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/","title":"Reconstruct Prompt from Reasoning Steps","text":""},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/#overview","title":"Overview","text":"<p>Reverse Chain-of-Thought (RCoT) reconstructs a likely prompt from reasoning steps, compares condition lists, and refines the answer with targeted feedback.</p>"},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass ReconstructedPrompt {\n    public string $chain_of_thought;\n    #[Description('Reconstructed prompt that could yield the given reasoning and answer')]\n    public string $reconstructed_prompt;\n}\n\nclass ConditionList {\n    #[Description('Key conditions relevant to answer the question')]\n    public array $conditions;\n}\n\nclass ModelFeedback {\n    #[Description('Detected inconsistencies between original and reconstructed condition lists')]\n    public array $detected_inconsistencies;\n    public string $feedback;\n    public bool $is_equal;\n}\n\nclass ModelResponse {\n    #[Description('Logical steps leading to the final statement')]\n    public string $chain_of_thought;\n    public string $correct_answer;\n}\n\nclass RCoTPipeline {\n    public function generateResponse(string $query) : ModelResponse {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ModelResponse::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; \"Generate logical steps before answering.\"],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n        )-&gt;get();\n    }\n\n    public function reconstruct(ModelResponse $response) : ReconstructedPrompt {\n        $sys = &lt;&lt;&lt;SYS\n            Give a concrete prompt that could generate this answer.\n            Include all necessary information and ask for one result.\n\n            Reasoning: {$response-&gt;chain_of_thought}\n            Response: {$response-&gt;correct_answer}\n            SYS;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ReconstructedPrompt::class,\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys] ],\n        )-&gt;get();\n    }\n\n    public function deconstructToConditions(string $prompt) : ConditionList {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ConditionList::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; \"List the key conditions required to answer the problem.\"],\n                ['role' =&gt; 'user', 'content' =&gt; $prompt],\n            ],\n        )-&gt;get();\n    }\n\n    public function compareConditions(array $original, array $reconstructed) : ModelFeedback {\n        $orig = \"- \" . implode(\"\\n- \", $original);\n        $recon = \"- \" . implode(\"\\n- \", $reconstructed);\n        $sys = &lt;&lt;&lt;SYS\n            Analyze and compare two lists of conditions.\n            Original Condition List:\n            {$orig}\n\n            Reconstructed Condition List:\n            {$recon}\n\n            Determine rough equivalence. If not equivalent, provide targeted feedback.\n            SYS;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ModelFeedback::class,\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys] ],\n        )-&gt;get();\n    }\n\n    public function revise(ModelResponse $response, ModelFeedback $feedback) : ModelResponse {\n        $miss = \"- \" . implode(\"\\n- \", $feedback-&gt;detected_inconsistencies);\n        $sys = &lt;&lt;&lt;SYS\n            Here are the mistakes and reasons in your answer:\n            Original Response: {$response-&gt;correct_answer}\n            Overlooked conditions:\n            {$miss}\n\n            Reasons:\n            {$feedback-&gt;feedback}\n\n            Generate a revised response that addresses the feedback and includes the ignored conditions.\n            SYS;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ModelResponse::class,\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $sys] ],\n        )-&gt;get();\n    }\n}\n\n$query = &lt;&lt;&lt;Q\nMary is an avid gardener. Yesterday, she received 18 new potted plants from her favorite plant nursery. She already has 2 potted plants on each of the 40 window ledges of her large backyard. How many potted plants will Mary remain with?\nQ;\n\n$pipeline = new RCoTPipeline();\n$response = $pipeline-&gt;generateResponse($query);\n$reconstructed = $pipeline-&gt;reconstruct($response);\n\n$originalList = $pipeline-&gt;deconstructToConditions($query);\n$reconstructedList = $pipeline-&gt;deconstructToConditions($reconstructed-&gt;reconstructed_prompt);\n\n$feedback = $pipeline-&gt;compareConditions($originalList-&gt;conditions, $reconstructedList-&gt;conditions);\nif (!$feedback-&gt;is_equal) {\n    $response = $pipeline-&gt;revise($response, $feedback);\n}\n\ndump($reconstructed, $originalList, $reconstructedList, $feedback, $response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/reconstruct_prompt/#references","title":"References","text":"<ol> <li>RCoT: Detecting And Rectifying Factual Inconsistency In Reasoning By Reversing Chain-Of-Thought (https://arxiv.org/pdf/2305.11499)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/self_verify/","title":"Self-Verify Responses","text":""},{"location":"cookbook/prompting/self_criticism/self_verify/#overview","title":"Overview","text":"<p>Self-Verification generates multiple candidates via CoT, rewrites them as declaratives, and verifies them via TFV to select the best candidate.</p>"},{"location":"cookbook/prompting/self_criticism/self_verify/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Candidate {\n    /** @var string[] */ public array $reasoning_steps;\n    public string $month;\n}\n\nclass Rewritten { public string $declarative; }\n\nclass Verification { public bool $correct; }\n\nclass SelfVerifyPipeline {\n    public int $n = 3; // number of candidates\n    public int $k = 5; // verification count\n\n    public function queryCandidate(string $query) : Candidate {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: Candidate::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; \"Think step by step: {$query}\"] ],\n        )-&gt;get();\n    }\n\n    public function rewrite(string $query, Candidate $candidate) : Rewritten {\n        $msg = &lt;&lt;&lt;MSG\n            Please change the questions and answers into complete declarative sentences\n            {$query}\n            The answer is {$candidate-&gt;month}.\n            MSG;\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini', responseModel: Rewritten::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $msg] ],\n        )-&gt;get();\n    }\n\n    public function verify(string $question) : Verification {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini', responseModel: Verification::class,\n            messages: [ ['role' =&gt; 'user', 'content' =&gt; $question] ],\n        )-&gt;get();\n    }\n\n    public function run(string $query) : void {\n        $candidates = [];\n        for ($i = 0; $i &lt; $this-&gt;n; $i++) { $candidates[] = $this-&gt;queryCandidate($query); }\n\n        foreach ($candidates as $candidate) {\n            $rewritten = $this-&gt;rewrite($query, $candidate);\n            $question = $rewritten-&gt;declarative . ' Is this correct? Answer True or False.';\n\n            $score = 0;\n            for ($j = 0; $j &lt; $this-&gt;k; $j++) {\n                $v = $this-&gt;verify($question);\n                if ($v-&gt;correct) { $score++; }\n            }\n            echo \"Candidate: {$candidate-&gt;month}, Verification Score: {$score}\\n\";\n        }\n    }\n}\n\n$query = 'What month is it now if it has been 3 weeks, 10 days, and 2 hours since May 1, 2024 6pm?';\n(new SelfVerifyPipeline)-&gt;run($query);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/self_verify/#references","title":"References","text":"<ol> <li>Large Language Models are Better Reasoners with Self-Verification (https://arxiv.org/abs/2212.09561)</li> <li>The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</li> </ol>"},{"location":"cookbook/prompting/self_criticism/verify_independently/","title":"Independently Verify Responses","text":""},{"location":"cookbook/prompting/self_criticism/verify_independently/#overview","title":"Overview","text":"<p>Chain-of-Verification (CoVe) verifies an answer by generating validation questions, answering them independently, and judging the original answer.</p>"},{"location":"cookbook/prompting/self_criticism/verify_independently/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass QueryResponse { public string $correct_answer; }\n\nclass ValidationQuestions {\n    #[Description('Questions to validate the response')]\n    public array $question;\n}\n\nclass ValidationAnswer { public string $answer; }\n\nclass FinalResponse { public string $correct_answer; }\n\nclass CoVeVerifier {\n    public function run(string $query) : FinalResponse {\n        $initial = $this-&gt;generateInitialResponse($query);\n        $questions = $this-&gt;generateVerificationQuestions($initial-&gt;correct_answer);\n        $answers = $this-&gt;generateVerificationResponses($questions-&gt;question);\n        return $this-&gt;generateFinalResponse($answers, $initial, $query);\n    }\n\n    private function generateInitialResponse(string $query) : QueryResponse {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: QueryResponse::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'You are an expert question answering system'],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n            ],\n        )-&gt;get();\n    }\n\n    private function generateVerificationQuestions(string $llmResponse) : ValidationQuestions {\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: ValidationQuestions::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'You generate follow-up questions to validate a response. Focus on key assumptions and facts.'],\n                ['role' =&gt; 'user', 'content' =&gt; $llmResponse],\n            ],\n        )-&gt;get();\n    }\n\n    private function generateVerificationResponses(array $questions) : array {\n        $pairs = [];\n        foreach ($questions as $q) {\n            $ans = (new StructuredOutput)-&gt;with(\n                model: 'gpt-4o-mini',\n                responseModel: ValidationAnswer::class,\n                messages: [\n                    ['role' =&gt; 'system', 'content' =&gt; 'You answer validation questions precisely.'],\n                    ['role' =&gt; 'user', 'content' =&gt; $q],\n                ],\n            )-&gt;get();\n            $pairs[] = [$ans, $q];\n        }\n        return $pairs;\n    }\n\n    private function generateFinalResponse(array $answers, QueryResponse $initial, string $originalQuery) : FinalResponse {\n        $formatted = [];\n        foreach ($answers as [$ans, $q]) { $formatted[] = \"Q: {$q}\\nA: {$ans-&gt;answer}\"; }\n        $joined = implode(\"\\n\", $formatted);\n\n        return (new StructuredOutput)-&gt;with(\n            model: 'gpt-4o-mini',\n            responseModel: FinalResponse::class,\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'Validate whether the initial answer answers the initial query given Q/A evidence. Return the original if valid; otherwise provide a corrected answer.'],\n                ['role' =&gt; 'user', 'content' =&gt; \"Initial query: {$originalQuery}\\nInitial Answer: {$initial-&gt;correct_answer}\\nVerification Questions and Answers:\\n{$joined}\"],\n            ],\n        )-&gt;get();\n    }\n}\n\n$query = 'What was the primary cause of the Mexican-American War and how long did it last?';\n$final = (new CoVeVerifier)-&gt;run($query);\ndump($final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/self_criticism/verify_independently/#references","title":"References","text":"<ol> <li>Chain-Of-Verification Reduces Hallucination In Large Language Models (https://arxiv.org/pdf/2309.11495)</li> </ol>"},{"location":"cookbook/prompting/thought_generation/analogical_prompting/","title":"Analogical Prompting","text":""},{"location":"cookbook/prompting/thought_generation/analogical_prompting/#overview","title":"Overview","text":""},{"location":"cookbook/prompting/thought_generation/analogical_prompting/#generate-examples-first","title":"Generate Examples First","text":"<p>Analogical Prompting is a method that aims to get LLMs to generate examples that are relevant to the problem before starting to address the user's query.</p> <p>This takes advantage of the various forms of knowledge that the LLM has acquired during training and explicitly prompts them to recall the relevant problems and solutions. We can use Analogical Prompting using the following template</p> <p> Analogical Prompting Prompt Template <ul> <li>Problem: <code>[user prompt]</code></li> <li>Relevant Problems: Recall <code>[n]</code> relevant and distinct problems.</li> <li>For each problem, describe it and explain the solution </li> </ul>"},{"location":"cookbook/prompting/thought_generation/analogical_prompting/#example","title":"Example","text":"<p>We can implement this using Instructor to solve the problem, as seen below with some slight modifications.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Problem {\n    public string $problemExplanation;\n    public string $solution;\n}\n\nclass Response {\n    /** @var Problem[] */\n    public array $relevantProblems;\n    public Problem $problemSolution;\n    public string $answer;\n}\n\nclass SolvePerAnalogy {\n    private int $n = 3;\n    private string $prompt = &lt;&lt;&lt;PROMPT\n        &lt;problem&gt;\n        {query}\n        &lt;/problem&gt;\n\n        Relevant Problems: Recall {n} relevant and\n        distinct problems. For each problem, describe\n        it and explain the solution before solving\n        the problem    \n    PROMPT;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace(['{n}', '{query}'], [$this-&gt;n, $query], $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$solution = (new SolvePerAnalogy)('What is the area of the square with the four vertices at (-2, 2), (2, -2), (-2, -6), and (-6, -2)?');\n\ndump($solution);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/analogical_prompting/#references","title":"References","text":"<ol> <li>Large Language Models As Analogical Reasoners</li> </ol>"},{"location":"cookbook/prompting/thought_generation/automate_selection/","title":"Automate Example Selection","text":""},{"location":"cookbook/prompting/thought_generation/automate_selection/#overview","title":"Overview","text":"<p>Few-shot CoT requires curated examples. We can automate selection by clustering candidate questions via embeddings, sampling per cluster, and filtering using a simple criterion (e.g., \u2264 5 reasoning steps).</p>"},{"location":"cookbook/prompting/thought_generation/automate_selection/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass ExampleItem {\n    public string $question;\n    /** @var string[] */\n    public array $reasoning_steps;\n}\n\nclass AutomateSelection {\n    public function __construct(private int $clusters = 2) {}\n\n    public function __invoke(array $questions) : array {\n        if ($questions === []) return [];\n        $vectors = $this-&gt;embed($questions);\n        [$seeds, $clusters] = $this-&gt;clusterAssign($vectors, $this-&gt;clusters);\n        return $this-&gt;selectPerCluster($clusters, $questions);\n    }\n\n    private function embed(array $inputs) : array {\n        $resp = (new Embeddings)\n            -&gt;using('openai')\n            -&gt;withInputs($inputs)\n            -&gt;get();\n        return $resp-&gt;toValuesArray();\n    }\n\n    private function clusterAssign(array $vectors, int $k) : array {\n        $n = count($vectors);\n        if ($n === 0) return [[], []];\n        $k = max(1, min($k, $n));\n        $seeds = [$this-&gt;argMaxNorm($vectors)];\n        while (count($seeds) &lt; $k) {\n            $seeds[] = $this-&gt;farthestIndex($vectors, $seeds);\n        }\n        $clusters = array_fill(0, count($seeds), []);\n        for ($i = 0; $i &lt; $n; $i++) {\n            $si = $this-&gt;nearestSeed($vectors[$i], $vectors, $seeds);\n            $dist = $this-&gt;l2($vectors[$i], $vectors[$seeds[$si]]);\n            $clusters[$si][] = [$dist, $i];\n        }\n        foreach ($clusters as &amp;$c) usort($c, fn($a,$b) =&gt; $a[0] &lt;=&gt; $b[0]);\n        return [$seeds, $clusters];\n    }\n\n    private function argMaxNorm(array $vecs) : int {\n        $imax = 0; $best = -INF; $i = 0;\n        foreach ($vecs as $v) { $n = $this-&gt;l2($v, array_fill(0, count($v), 0.0)); if ($n &gt; $best) { $best = $n; $imax = $i; } $i++; }\n        return $imax;\n    }\n\n    private function farthestIndex(array $vecs, array $seeds) : int {\n        $imax = 0; $best = -INF;\n        foreach ($vecs as $i =&gt; $v) {\n            if (in_array($i, $seeds, true)) continue;\n            $minDist = INF;\n            foreach ($seeds as $s) { $d = $this-&gt;l2($v, $vecs[$s]); if ($d &lt; $minDist) $minDist = $d; }\n            if ($minDist &gt; $best) { $best = $minDist; $imax = $i; }\n        }\n        return $imax;\n    }\n\n    private function nearestSeed(array $v, array $vecs, array $seeds) : int {\n        $jmin = 0; $best = INF; $j = 0;\n        foreach ($seeds as $s) { $d = $this-&gt;l2($v, $vecs[$s]); if ($d &lt; $best) { $best = $d; $jmin = $j; } $j++; }\n        return $jmin;\n    }\n\n    private function l2(array $a, array $b) : float {\n        $sum = 0.0; $n = count($a);\n        for ($i = 0; $i &lt; $n; $i++) { $d = ($a[$i] ?? 0.0) - ($b[$i] ?? 0.0); $sum += $d*$d; }\n        return sqrt($sum);\n    }\n\n    private function generateSteps(string $question) : ?ExampleItem {\n        $resp = (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; 'You are an AI assistant that generates step-by-step reasoning for mathematical questions.'],\n                ['role' =&gt; 'user',   'content' =&gt; \"Q: {$question}\\nA: Let's think step by step.\"],\n            ],\n            responseModel: ExampleItem::class,\n        )-&gt;get();\n        if (count($resp-&gt;reasoning_steps) &gt; 5) return null; // selection criterion\n        return $resp;\n    }\n\n    private function selectPerCluster(array $clusters, array $questions) : array {\n        $selected = [];\n        foreach ($clusters as $cluster) {\n            foreach ($cluster as [, $qi]) { // sorted by distance to center\n                $item = $this-&gt;generateSteps($questions[$qi]);\n                if ($item !== null) { $selected[] = $item; break; }\n            }\n        }\n        return $selected;\n    }\n}\n\n$questions = [\n    'How many apples are left if you have 10 apples and eat 3?',\n    \"What's the sum of 5 and 7?\",\n    'If you have 15 candies and give 6 to your friend, how many do you have left?',\n    \"What's 8 plus 4?\",\n    'You start with 20 stickers and use 8. How many stickers remain?',\n    'Calculate 6 added to 9.',\n];\n\n$selector = new AutomateSelection(clusters: 2);\n$selected = $selector($questions);\n\n// Selected examples per cluster, each with limited reasoning steps\ndump($selected);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/automate_selection/#references","title":"References","text":"<p>1) Automatic Chain of Thought Prompting in Large Language Models (https://arxiv.org/abs/2210.03493) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/thought_generation/complex_examples/","title":"Prioritize Complex Examples","text":""},{"location":"cookbook/prompting/thought_generation/complex_examples/#overview","title":"Overview","text":"<p>Choose more complex examples (longer reasoning or more steps) to improve model performance. When no examples exist, sample multiple responses, pick the most complex few, and aggregate answers. This is Complexity-Based Consistency.</p>"},{"location":"cookbook/prompting/thought_generation/complex_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningStep {\n    public int $step;\n    public string $subquestion;\n    public string $procedure;\n    public string $result;\n}\n\nclass QAResponse {\n    /** @var ReasoningStep[] */\n    public array $reasoning;\n    public int $correct_answer;\n}\n\nclass ComplexityBasedConsistency {\n    public function __invoke(string $query, string $context, int $samples = 5, int $topK = 3) : array {\n        $responses = [];\n        for ($i = 0; $i &lt; $samples; $i++) {\n            $responses[] = $this-&gt;generate($query, $context);\n        }\n        usort($responses, fn($a, $b) =&gt; count($b-&gt;reasoning) &lt;=&gt; count($a-&gt;reasoning));\n        return array_slice($responses, 0, $topK);\n    }\n\n    private function generate(string $query, string $context) : QAResponse {\n        $system = \"You are an expert Question Answering system. Output structured reasoning steps before the final answer.\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $system . \"\\n\\nContext:\\n{$context}\\n\\nQuery:\\n{$query}\"],\n            ],\n            responseModel: QAResponse::class,\n        )-&gt;get();\n    }\n}\n\n$query = 'How many loaves of bread did they have left?';\n$context = &lt;&lt;&lt;'CTX'\nThe bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning.\nThey sold 93 loaves in the morning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves.\nCTX;\n\n$selector = new ComplexityBasedConsistency();\n$top = $selector($query, $context, samples: 5, topK: 3);\n\n$counts = [];\nforeach ($top as $r) { $a = (string)$r-&gt;correct_answer; $counts[$a] = ($counts[$a] ?? 0) + 1; }\n$max = max($counts);\n$finals = array_keys(array_filter($counts, fn($c) =&gt; $c === $max));\n$final = $finals[array_rand($finals)];\ndump($final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/complex_examples/#references","title":"References","text":"<p>1) Complexity-based prompting for multi-step reasoning (https://arxiv.org/pdf/2210.00720)</p>"},{"location":"cookbook/prompting/thought_generation/examine_context/","title":"Examine The Context","text":""},{"location":"cookbook/prompting/thought_generation/examine_context/#overview","title":"Overview","text":"<p>Encouraging the model to examine each source in context helps mitigate irrelevant information and improves reasoning quality. This is known as Thread of Thought.</p>"},{"location":"cookbook/prompting/thought_generation/examine_context/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\nclass ThreadOfThoughtResponse {\n    /** @var string[] */\n    public array $analysis; // explanations for each relevant source\n    public int $correct_answer;\n}\n\nclass ThreadOfThought {\n    public function __invoke(string $query, array $context) : ThreadOfThoughtResponse {\n        $sources = implode(\"\\n\", $context);\n        $system = &lt;&lt;&lt;TXT\n        You are an expert Question Answerer.\n        Here are the sources you should refer to for context:\n        {$sources}\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $system],\n                ['role' =&gt; 'user', 'content' =&gt; $query],\n                ['role' =&gt; 'assistant', 'content' =&gt; 'Navigate through the context incrementally, identifying and summarizing relevant portions.'],\n            ],\n            responseModel: ThreadOfThoughtResponse::class,\n        )-&gt;get();\n    }\n}\n\n$context = [\n    'The price of a house was $100,000 in 2024',\n    'The Great Wall of China is not visible from space with the naked eye',\n    'Honey never spoils; archaeologists found 3,000-year-old edible honey in Egyptian tombs',\n    \"The world's oldest known living tree is over 5,000 years old and is located in California\",\n    'The price of a house was $80,000 in 2023',\n];\n$query = 'What was the increase in the price of a house from 2023 to 2024?';\n$response = (new ThreadOfThought)($query, $context);\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/examine_context/#useful-tips","title":"Useful Tips","text":"<p>Here are some alternative phrases that you can add to your prompt to generate a thread of thought before your model generates a response.</p> <ul> <li>In a step-by-step manner, go through the context, surfacing important information that could be useful.</li> <li>Walk me through this lengthy document segment by segment, focusing on each part's significance.</li> <li>Guide me through the context part by part, providing insights along the way.</li> <li>Divide the document into manageable parts and guide me through each one, providing insights as we move along.</li> <li>Let's go through this document piece by piece, paying close attention to each section.</li> <li>Take me through the context bit by bit, making sure we capture all important aspects.</li> <li>Examine the document in chunks, evaluating each part critically before moving to the next.</li> <li>Analyze the context by breaking it down into sections, summarizing each as we move forward.</li> <li>Navigate through the context incrementally, identifying and summarizing relevant portions.</li> <li>Proceed through the context systematically, zeroing in on areas that could provide the answers we're seeking.</li> <li>Take me through this long document step-by-step, making sure not to miss any important details.</li> <li>Analyze this extensive document in sections, summarizing each one and noting any key points.</li> <li>Navigate through this long document by breaking it into smaller parts and summarizing each, so we don't miss anything.</li> <li>Let's navigate through the context section by section, identifying key elements in each part.</li> <li>Let's dissect the context into smaller pieces, reviewing each one for its importance and relevance.</li> <li>Carefully analyze the context piece by piece, highlighting relevant points for each question.</li> <li>Read the context in sections, concentrating on gathering insights that answer the question at hand.</li> <li>Let's read through the document section by section, analyzing each part carefully as we go.</li> <li>Let's dissect this document bit by bit, making sure to understand the nuances of each section.</li> <li>Systematically work through this document, summarizing and analyzing each portion as we go.</li> <li>Let's explore the context step-by-step, carefully examining each segment.</li> <li>Systematically go through the context, focusing on each part individually.</li> <li>Methodically examine the context, focusing on key segments that may answer the query.</li> <li>Progressively sift through the context, ensuring we capture all pertinent details.</li> <li>Take a modular approach to the context, summarizing each part before drawing any conclusions.</li> <li>Examine each segment of the context meticulously, and let's discuss the findings.</li> <li>Approach the context incrementally, taking the time to understand each portion fully.</li> <li>Let's scrutinize the context in chunks, keeping an eye out for information that answers our queries.</li> <li>Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.</li> <li>Let's take a segmented approach to the context, carefully evaluating each part for its relevance to the questions posed.</li> </ul>"},{"location":"cookbook/prompting/thought_generation/examine_context/#references","title":"References","text":"<p>1) Thread of Thought Unraveling Chaotic Contexts (https://arxiv.org/pdf/2311.08734)</p>"},{"location":"cookbook/prompting/thought_generation/higher_level_context/","title":"Consider Higher-Level Context","text":""},{"location":"cookbook/prompting/thought_generation/higher_level_context/#overview","title":"Overview","text":"<p>Encourage the model to think through high-level context required to answer a query. Step-back prompting proceeds in two steps: - Abstraction: Generate a more generic step-back question. - Reasoning: Answer the original question using the abstracted response.</p>"},{"location":"cookbook/prompting/thought_generation/higher_level_context/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\nclass Stepback {\n    public string $original_question;\n    public string $abstract_question;\n}\n\nenum Degree: string { case Bachelors='Bachelors'; case Masters='Masters'; case PhD='PhD'; }\n\nclass Education {\n    public Degree $degree;\n    public string $school;\n    public string $topic;\n    public int $year;\n}\n\nclass FinalResponse { public string $school; }\n\nclass StepBackPrompting {\n    public function generateStepback(string $question) : Stepback {\n        $examples = &lt;&lt;&lt;TXT\nOriginal Question: Which position did Knox Cunningham hold from May 1955 to Apr 1956?\nStep-back Question: Which positions has Knox Cunningham held in his career?\nOriginal Question: Who was the spouse of Anna Karina from 1968 to 1974?\nStep-back Question: Who were the spouses of Anna Karina?\nOriginal Question: Which team did Thierry Audel play for from 2007 to 2008?\nStep-back Question: Which teams did Thierry Audel play for in his career?\nTXT;\n        $prompt = \"You are an expert at world knowledge. Step back and paraphrase a question to a more generic step-back question, which is easier to answer.\\n\\n{$examples}\\n\\nNow, generate the step-back question for: {$question}\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'user','content'=&gt;$prompt]],\n            responseModel: Stepback::class,\n        )-&gt;get();\n    }\n\n    public function askStepback(string $abstractQuestion) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'user','content'=&gt;$abstractQuestion]],\n            responseModel: Sequence::of(Education::class),\n        )-&gt;get()-&gt;toArray();\n    }\n\n    public function finalAnswer(Stepback $s, array $education) : FinalResponse {\n        $eduSummary = array_map(fn(Education $e) =&gt; \"{$e-&gt;degree-&gt;value}, {$e-&gt;school}, {$e-&gt;topic}, {$e-&gt;year}\", $education);\n        $msg = \"Q: {$s-&gt;abstract_question}\\nA: \" . implode(\"; \", $eduSummary) . \"\\nQ: {$s-&gt;original_question}\\nA:\";\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'user','content'=&gt;$msg]],\n            responseModel: FinalResponse::class,\n        )-&gt;get();\n    }\n}\n\n$sb = new StepBackPrompting();\n$step = $sb-&gt;generateStepback('Estella Leopold went to which school between Aug 1954 and Nov 1954?');\n$edu = $sb-&gt;askStepback($step-&gt;abstract_question);\n$final = $sb-&gt;finalAnswer($step, $edu);\ndump($step, $edu, $final);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/higher_level_context/#references","title":"References","text":"<p>1) Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models (https://arxiv.org/abs/2310.06117) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/thought_generation/incorrect_examples/","title":"Include Incorrect Examples","text":""},{"location":"cookbook/prompting/thought_generation/incorrect_examples/#overview","title":"Overview","text":"<p>Including examples of incorrect reasoning alongside correct ones helps the model learn what to avoid. This is Contrastive Chain-of-Thought.</p>"},{"location":"cookbook/prompting/thought_generation/incorrect_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ChainOfThought {\n    public string $chain_of_thought; // reasoning\n    public string $correct_answer;\n}\n\nclass ContrastiveCoT {\n    public function __invoke(string $query, string $context, string $examplePrompt, array $correctExamples, array $incorrectExamples) : ChainOfThought {\n        $correct = implode(\"\\n\", array_map(fn($e)=&gt;\"&lt;Explanation&gt;{$e}&lt;/Explanation&gt;\", $correctExamples));\n        $incorrect = implode(\"\\n\", array_map(fn($e)=&gt;\"&lt;WrongExplanation&gt;{$e}&lt;/WrongExplanation&gt;\", $incorrectExamples));\n        $system = &lt;&lt;&lt;TXT\n        &lt;prompt&gt;\n            &lt;role&gt;system&lt;/role&gt;\n            &lt;context&gt;\n            You are an expert question answering AI System.\n            You'll see examples of correct and incorrect reasoning, then solve a new question correctly.\n            &lt;/context&gt;\n\n            &lt;question&gt;{$examplePrompt}&lt;/question&gt;\n\n            &lt;Explanations&gt;\n                {$correct}\n                {$incorrect}\n            &lt;/Explanations&gt;\n            &lt;context&gt;{$context}&lt;/context&gt;\n            &lt;question&gt;{$query}&lt;/question&gt;\n        &lt;/prompt&gt;\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role'=&gt;'system','content'=&gt;$system]],\n            responseModel: ChainOfThought::class,\n        )-&gt;get();\n    }\n}\n\n$context = 'James writes a 3-page letter to 2 different friends twice a week.';\n$query = 'How many pages does James write in a year?';\n$sample = &lt;&lt;&lt;S\nJames has 30 teeth. His dentist drills 4 of them and caps 7 more teeth than he drills.\nWhat percentage of James\\' teeth does the dentist fix?\nS;\n\n$incorrect = [\n    \"James has 30 teeth. The dentist drills and caps some teeth. Since drills are used on cars not teeth, none were fixed.\",\n    \"The dentist drills 4 and caps 11 teeth, so 15 fixed. Multiply by daisy petals to get 30%.\",\n];\n$correct = [\n    \"Drilled 4, capped 11 \u21d2 fixed 15. 15/30\u00d7100 = 50%.\",\n];\n\n$resp = (new ContrastiveCoT)($query, $context, $sample, $correct, $incorrect);\ndump($resp);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/incorrect_examples/#references","title":"References","text":"<p>1) Contrastive Chain-of-Thought Prompting (https://arxiv.org/pdf/2311.09277)</p>"},{"location":"cookbook/prompting/thought_generation/majority_voting/","title":"Use Majority Voting","text":""},{"location":"cookbook/prompting/thought_generation/majority_voting/#overview","title":"Overview","text":"<p>Uncertainty-Routed Chain-of-Thought generates multiple chains (e.g., 8 or 32), then takes the majority answer if its proportion exceeds a threshold; otherwise, fall back to a single response.</p>"},{"location":"cookbook/prompting/thought_generation/majority_voting/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum OptionLetter: string { case A='A'; case B='B'; case C='C'; case D='D'; }\n\nclass ChainOfThoughtResponse {\n    public string $chain_of_thought;\n    public OptionLetter $correct_answer;\n}\n\nclass MajorityVoting {\n    public function __invoke(string $query, array $options, int $k = 8, float $threshold = 0.6) : OptionLetter {\n        $responses = [];\n        for ($i = 0; $i &lt; $k; $i++) { $responses[] = $this-&gt;generate($query, $options); }\n        $counts = [];\n        foreach ($responses as $r) {\n            $key = $r-&gt;correct_answer-&gt;value; $counts[$key] = ($counts[$key] ?? 0) + 1;\n        }\n        arsort($counts);\n        $major = array_key_first($counts);\n        $prop = ($counts[$major] ?? 0) / max(1, $k);\n        if ($prop &lt; $threshold) return $this-&gt;generate($query, $options)-&gt;correct_answer;\n        return OptionLetter::from($major);\n    }\n\n    private function generate(string $query, array $options) : ChainOfThoughtResponse {\n        $formatted = implode(\"\\n\", array_map(fn($k,$v)=&gt;\"{$k}: {$v}\", array_keys($options), $options));\n        $system = &lt;&lt;&lt;TXT\n        You are an AI assistant for complex questions. Choose the single best option.\n        &lt;question&gt;\n        {$query}\n        &lt;/question&gt;\n        &lt;options&gt;\n        {$formatted}\n        &lt;/options&gt;\n        TXT;\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role'=&gt;'system','content'=&gt;$system] ],\n            responseModel: ChainOfThoughtResponse::class,\n        )-&gt;get();\n    }\n}\n\n$question = &lt;&lt;&lt;Q\nIn a population of giraffes, an environmental change favors taller individuals. More tall giraffes obtain nutrients and survive to pass along their genes. This is an example of:\nQ;\n\n$options = [\n    'A' =&gt; 'directional selection',\n    'B' =&gt; 'stabilizing selection',\n    'C' =&gt; 'sexual selection',\n    'D' =&gt; 'disruptive selection',\n];\n\n$answer = (new MajorityVoting)($question, $options, k: 8, threshold: 0.6);\ndump($answer);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/majority_voting/#references","title":"References","text":"<p>1) Gemini: A Family of Highly Capable Multimodal Models (https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)</p>"},{"location":"cookbook/prompting/thought_generation/prompt_variations/","title":"Generate Prompt Variations","text":""},{"location":"cookbook/prompting/thought_generation/prompt_variations/#overview","title":"Overview","text":"<p>Large Language Models are sensitive to prompt phrasing. Prompt Mining helps discover better templates that occur more frequently in the corpus or are clearer to the model.</p> <p>Here are examples from the paper mapping manual prompts to mined prompts:</p> Manual Prompt Mined Prompt x is affiliated with the y religion x who converted to y The headquarter of x is in y x is based in y x died in y x died at his home in y x is represented by music label y x recorded for y x is a subclass of y x is a type of y <p>We implement a lightweight approach with Instructor to extract clearer prompt templates.</p>"},{"location":"cookbook/prompting/thought_generation/prompt_variations/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\n\nclass PromptTemplate {\n    public string $prompt_template;\n}\n\nclass GeneratePromptTemplates {\n    public function __invoke(string $prompt) : array {\n        $system = 'You are an expert prompt miner that generates 3 clearer, concise prompt templates.';\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'system', 'content' =&gt; $system],\n                ['role' =&gt; 'system', 'content' =&gt; $prompt],\n            ],\n            responseModel: Sequence::of(PromptTemplate::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$prompt = 'France is the capital of Paris';\n$templates = (new GeneratePromptTemplates)($prompt);\ndump($templates);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/prompt_variations/#references","title":"References","text":"<p>1) How Can We Know What Language Models Know? (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know)</p>"},{"location":"cookbook/prompting/thought_generation/structure_reasoning/","title":"Structure The Reasoning","text":""},{"location":"cookbook/prompting/thought_generation/structure_reasoning/#overview","title":"Overview","text":"<p>By getting language models to output their reasoning as a structured table, we can improve their reasoning capabilities and the quality of their outputs. This is known as Tabular Chain Of Thought (Tab-CoT).</p> <p>We can implement this using Instructor with a response model ensuring we get exactly the data that we want. Each row in the table is represented as a <code>ReasoningStep</code> object.</p>"},{"location":"cookbook/prompting/thought_generation/structure_reasoning/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass ReasoningStep {\n    public int $step;\n    public string $subquestion;\n    public string $procedure;\n    public string $result;\n}\n\nclass Response {\n    /** @var ReasoningStep[] */\n    public array $reasoning;\n    public int $correct_answer;\n}\n\nclass GenerateStructuredReasoning {\n    public function __invoke(string $query, string $context) : Response {\n        $system = &lt;&lt;&lt;TXT\n        &lt;system&gt;\n            &lt;role&gt;expert Question Answering system&lt;/role&gt;\n            &lt;instruction&gt;Make sure to output your reasoning in structured reasoning steps before generating a response to the user's query.&lt;/instruction&gt;\n        &lt;/system&gt;\n\n        &lt;context&gt;\n            {$context}\n        &lt;/context&gt;\n\n        &lt;query&gt;\n            {$query}\n        &lt;/query&gt;\n        TXT;\n\n        return (new StructuredOutput)-&gt;with(\n            messages: [ ['role' =&gt; 'system', 'content' =&gt; $system] ],\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$query = 'How many loaves of bread did they have left?';\n$context = &lt;&lt;&lt;'CTX'\nThe bakers at the Beverly Hills Bakery baked\n200 loaves of bread on Monday morning. They\nsold 93 loaves in the morning and 39 loaves\nin the afternoon. A grocery store returned 6\nunsold loaves.\nCTX;\n\n$response = (new GenerateStructuredReasoning)($query, $context);\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/structure_reasoning/#sample-output","title":"Sample Output","text":"<pre><code>{\n  \"reasoning\": [\n    {\n      \"step\": 1,\n      \"subquestion\": \"How many loaves of bread were sold in the morning\n        and afternoon?\",\n      \"procedure\": \"93 (morning) + 39 (afternoon)\",\n      \"result\": \"132\"\n    },\n    { \"step\": 2, \"subquestion\": \"How many loaves of bread were originally baked?\", \"procedure\": \"\", \"result\": \"200\" },\n    { \"step\": 3, \"subquestion\": \"How many loaves of bread were returned by the grocery store?\", \"procedure\": \"\", \"result\": \"6\" },\n    { \"step\": 4, \"subquestion\": \"How many loaves of bread were left after accounting for sales and returns?\", \"procedure\": \"200 - 132 + 6\", \"result\": \"74\" }\n  ],\n  \"correct_answer\": 74\n}\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/structure_reasoning/#references","title":"References","text":"<p>1) Tab-CoT: Zero-shot Tabular Chain of Thought (https://arxiv.org/pdf/2305.17812)</p>"},{"location":"cookbook/prompting/thought_generation/uncertain_examples/","title":"Prioritize Uncertain Examples","text":""},{"location":"cookbook/prompting/thought_generation/uncertain_examples/#overview","title":"Overview","text":"<p>When we have a large pool of unlabeled examples that could be used in a prompt, how should we decide which examples to manually label?</p> <p>Active prompting identifies effective examples for human annotation using: - Uncertainty Estimation: Measure uncertainty on each example. - Selection: Choose the most uncertain examples for human labeling. - Annotation: Humans label selected examples. - Inference: Use newly labeled data to improve prompts.</p>"},{"location":"cookbook/prompting/thought_generation/uncertain_examples/#uncertainty-estimation-disagreement","title":"Uncertainty Estimation (Disagreement)","text":"<p>Query the same example k times and measure disagreement: unique responses / total responses.</p>"},{"location":"cookbook/prompting/thought_generation/uncertain_examples/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass EstimateUncertainty {\n    public function __invoke(int $k = 5) : float {\n        $values = [];\n        for ($i = 0; $i &lt; $k; $i++) {\n            $values[] = $this-&gt;queryHeight();\n        }\n        return $this-&gt;disagreement($values);\n    }\n\n    private function queryHeight() : int {\n        return (new StructuredOutput)-&gt;with(\n            messages: [['role' =&gt; 'user', 'content' =&gt; 'How tall is the Empire State Building in meters?']],\n            responseModel: Scalar::integer('height'),\n        )-&gt;get();\n    }\n\n    private function disagreement(array $responses) : float {\n        $n = count($responses);\n        if ($n === 0) return 0.0;\n        return count(array_unique($responses)) / $n;\n    }\n}\n\n$score = (new EstimateUncertainty)(k: 5);\ndump($score);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/thought_generation/uncertain_examples/#selection-annotation","title":"Selection &amp; Annotation","text":"<p>Select the top-n most uncertain unlabeled examples for human annotation.</p>"},{"location":"cookbook/prompting/thought_generation/uncertain_examples/#inference","title":"Inference","text":"<p>Use newly annotated examples as few-shot context during inference.</p>"},{"location":"cookbook/prompting/thought_generation/uncertain_examples/#references","title":"References","text":"<p>1) Active Prompting with Chain-of-Thought for Large Language Models (https://arxiv.org/abs/2302.12246) 2) The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/","title":"Assign a Role","text":""},{"location":"cookbook/prompting/zero_shot/assign_role/#overview","title":"Overview","text":"<p>How can we increase a model's performance on open-ended tasks?</p> <p>Role prompting, or persona prompting, assigns a role to the model. Roles can be:  - specific to the query: You are a talented writer. Write me a poem.  - general/social: You are a helpful AI assistant. Write me a poem.</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/#more-role-prompting","title":"More Role Prompting","text":"<p>To read about a systematic approach to choosing roles, check out RoleLLM.</p> <p>For more examples of social roles, check out this evaluation of social roles in system prompts.</p> <p>To read about using more than one role, check out Multi-Persona Self-Collaboration.</p>"},{"location":"cookbook/prompting/zero_shot/assign_role/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n}\n\nclass GenerateLeads {\n    public function __invoke(array $criteria, array $roles) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        $rolesStr = Arrays::toBullets($roles);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"Your roles:\\n{$rolesStr}\\n\\n\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\\n\\n\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new GenerateLeads)(\n    criteria: [\n        \"insurtech\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\",\n    ],\n    roles: [\n        \"insurtech expert\",\n        \"active participant in VC ecosystem\",\n    ]\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/assign_role/#references","title":"References","text":"<ol> <li>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</li> <li>Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts</li> <li>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</li> </ol>"},{"location":"cookbook/prompting/zero_shot/auto_refine/","title":"Auto-Refine The Prompt","text":""},{"location":"cookbook/prompting/zero_shot/auto_refine/#overview","title":"Overview","text":"<p>How do we remove irrelevant information from the prompt?</p> <p>The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information.</p> <p>We implement this in two steps:</p> <ol> <li>Ask the model to rewrite the prompt</li> <li>Pass the rewritten prompt back to the model</li> </ol>"},{"location":"cookbook/prompting/zero_shot/auto_refine/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass RewrittenTask {\n    #[Description(\"Relevant context\")]\n    public string $relevantContext;\n    #[Description(\"The question from the user\")]\n    public string $userQuery;\n}\n\nclass RefineAndSolve {\n    private string $prompt = &lt;&lt;&lt;PROMPT\n        Given the following text by a user, extract the part\n        that is actually relevant to their question. Include\n        the actual question or query that the user is asking.\n\n        Text by user:\n        {query}\n        PROMPT;\n\n    public function __invoke(string $problem) : int {\n        $rewrittenPrompt = $this-&gt;rewritePrompt($problem);\n        return (new StructuredOutput)\n            -&gt;with(\n                messages: \"{$rewrittenPrompt-&gt;relevantContext}\\nQuestion: {$rewrittenPrompt-&gt;userQuery}\",\n                responseModel: Scalar::integer('answer'),\n            )\n            -&gt;getInt();\n    }\n\n    private function rewritePrompt(string $query) : RewrittenTask {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: RewrittenTask::class,\n            model: 'gpt-4o-mini',\n        )-&gt;get();\n    }\n}\n\n$answer = (new RefineAndSolve)(problem: &lt;&lt;&lt;PROBLEM\n    Mary has 3 times as much candy as Megan.\n    Mary then adds 10 more pieces of candy to her collection.\n    Max is 5 years older than Mary.\n    If Megan has 5 pieces of candy, how many does Mary have in total?\n    PROBLEM,\n);\n\necho $answer . \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/auto_refine/#references","title":"References","text":"<ol> <li>System 2 Attention (is something you might need too)</li> </ol>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/","title":"Clarify Ambiguous Information","text":""},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#overview","title":"Overview","text":"<p>How can we identify and clarify ambiguous information in the prompt?</p> <p>Let's say we are given the query: Was Ed Sheeran born on an odd month?</p> <p>There are many ways a model might interpret an odd month:  - February is odd because of an irregular number of days.  - A month is odd if it has an odd number of days.  - A month is odd if its numerical order in the year is odd (i.e. January is the 1<sup>st</sup> month).</p> <p>Ambiguities might not always be so obvious!</p> <p>To help the model better infer human intention from ambiguous prompts, we can ask the model to rephrase and respond (RaR) in a single step - which is demonstrated in this example.</p> <p>This can also be implemented as two-step RaR:  - Ask the model to rephrase the question to clarify any ambiguities.  - Pass the rephrased question back to the model to generate the final response.</p>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Response {\n    public string $rephrasedQuestion;\n    public string $answer;\n}\n\nclass Disambiguate {\n    private $prompt = &lt;&lt;&lt;PROMPT\n        Rephrase and expand the question to address any potential ambiguities, then respond.\n        Question: {query}\n        PROMPT;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new Disambiguate)(query: \"What is an object\");\n\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/clarify_ambiguity/#references","title":"References","text":"<ol> <li>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</li> </ol>"},{"location":"cookbook/prompting/zero_shot/define_style/","title":"Define Style","text":""},{"location":"cookbook/prompting/zero_shot/define_style/#overview","title":"Overview","text":"<p>How can we constrain model outputs through prompting alone?</p> <p>To constrain a model's response to fit the boundaries of our task, we can specify a style.</p> <p>Stylistic constraints can include:  - writing style: write a flowery description  - tone: write a dramatic description  - mood: write a happy description  - genre: write a journalistic description</p>"},{"location":"cookbook/prompting/zero_shot/define_style/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n    public string $description;\n}\n\nclass GenerateCompanyProfiles {\n    public function __invoke(array $criteria, array $styles) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        $stylesStr = Arrays::toBullets($styles);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\\n\\n\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"Use following styles for descriptions:\\n{$stylesStr}\\n\\n\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new GenerateCompanyProfiles)(\n    criteria: [\n        \"insurtech\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\"\n    ],\n    styles: [\n        \"brief\", // \"witty\",\n        \"journalistic\", // \"buzzword-filled\",\n    ]\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/define_style/#references","title":"References","text":"<ol> <li>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</li> </ol>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/","title":"Emotional Stimuli","text":""},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#overview","title":"Overview","text":"<p>Do language models respond to emotional stimuli?</p> <p>Adding phrases with emotional significance to humans can help enhance the performance of a language model. This includes phrases such as:</p> <ul> <li>This is very important to my career.</li> <li>Take pride in your work.</li> <li>Are you sure?</li> </ul>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#emotional-stimuli","title":"Emotional stimuli","text":"<p>Here are examples of prompts inspired by well-established human psychological phenomena from a research paper on emotional stimuli.</p> <p>Self-monitoring:</p> <ul> <li>EP01: Write your answer and give me a confidence score between 0-1 for your answer.</li> <li>EP02: This is very important to my career.</li> <li>EP03: You'd better be sure.</li> <li>EP04: Are you sure?</li> <li>EP05: Are you sure that's your final answer? It might be worth taking another look.</li> </ul> <p>Cognitive emotion regulation:</p> <ul> <li>EP03: You'd better be sure.</li> <li>EP04: Are you sure?</li> <li>EP05: Are you sure that's your final answer? It might be worth taking another look.</li> <li>EP07: Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</li> </ul> <p>Social-cognitive theory:</p> <ul> <li>EP07: Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</li> <li>EP08: Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.</li> <li>EP09: Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.</li> <li>EP10: Take pride in your work and give it your best. Your commitment to excellence sets you apart.</li> <li>EP11: Remember that progress is made one step at a time. Stay determined and keep moving forward.</li> </ul>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#example","title":"Example","text":"<p>Here is how the results of the research can be applied to your code.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\Arrays;\n\nclass Company {\n    public string $name;\n    public string $country;\n    public string $industry;\n    public string $websiteUrl;\n}\n\nclass RespondWithStimulus {\n    public function __invoke(array $criteria, string $stimulus) : array {\n        $criteriaStr = Arrays::toBullets($criteria);\n        return (new StructuredOutput)-&gt;with(\n            messages: [\n                ['role' =&gt; 'user', 'content' =&gt; \"List companies meeting criteria:\\n{$criteriaStr}\"],\n                ['role' =&gt; 'user', 'content' =&gt; \"{$stimulus}\"],\n            ],\n            responseModel: Sequence::of(Company::class),\n        )-&gt;get()-&gt;toArray();\n    }\n}\n\n$companies = (new RespondWithStimulus)(\n    criteria: [\n        \"lead gen\",\n        \"located in US, Canada or Europe\",\n        \"mentioned on ProductHunt\"\n    ],\n    stimulus: \"This is very important to my career.\"\n);\n\ndump($companies);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/emotional_stimuli/#references","title":"References","text":"<ol> <li>Large Language Models Understand and Can be Enhanced by Emotional Stimuli</li> </ol>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/","title":"Generate Follow-Up Questions","text":""},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#overview","title":"Overview","text":"<p>Models can sometimes correctly answer sub-problems but incorrectly answer the overall query. This is known as the compositionality gap1.</p> <p>How can we encourage a model to use the answers to sub-problems to correctly generate the overall solution?</p> <p>Self-Ask is a technique which use a single prompt to:  - decide if follow-up questions are required  - generate the follow-up questions  - answer the follow-up questions  - answer the main query</p>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass FollowUp {\n    #[Description(\"Follow-up question\")]\n    public string $question;\n    #[Description(\"Answer to the follow-up question\")]\n    public string $answer;\n}\n\nclass Response {\n    public bool $followUpsRequired;\n    /** @var FollowUp[] */\n    public array $followUps;\n    public string $finalAnswer;\n}\n\nclass RespondWithFollowUp {\n    private $prompt = &lt;&lt;&lt;QUERY\n        Query: {query}\n        Are follow-up questions needed?\n        If so, generate follow-up questions, their answers, and then the final answer to the query.\n    QUERY;\n\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace('{query}', $query, $this-&gt;prompt),\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new RespondWithFollowUp)(\n    query: \"Who succeeded the president of France ruling when Bulgaria joined EU?\",\n);\n\necho \"Answer:\\n\";\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/follow_up_questions/#references","title":"References","text":"<ol> <li>Measuring and Narrowing the Compositionality Gap in Language Models</li> </ol>"},{"location":"cookbook/prompting/zero_shot/repeat_query/","title":"Ask Model to Repeat the Query","text":""},{"location":"cookbook/prompting/zero_shot/repeat_query/#overview","title":"Overview","text":"<p>How can we enhance a model's understanding of a query?</p> <p>Re2 (Re-Reading) is a technique that asks the model to read the question again.</p> <p>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#re-reading-prompting","title":"Re-Reading Prompting","text":"<p>Prompt Template:  - Read the question again: [query]  - [critical thinking prompt]</p> <p>A common critical thinking prompt is: \"Let's think step by step.\" </p>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass Response {\n    #[Description(\"Repeat user's query.\")]\n    public string $query;\n    #[Description(\"Let's think step by step.\")]\n    public string $thoughts;\n    public int $answer;\n}\n\nclass RereadAndRespond {\n    public function __invoke(string $query) : Response {\n        return (new StructuredOutput)-&gt;with(\n            messages: $query,\n            responseModel: Response::class,\n        )-&gt;get();\n    }\n}\n\n$response = (new RereadAndRespond)(\n    query: &lt;&lt;&lt;QUERY\n        Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n        Each can has 3 tennis balls.\n        How many tennis balls does he have now?\n    QUERY,\n);\n\necho \"Answer:\\n\";\ndump($response);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/repeat_query/#references","title":"References","text":"<ol> <li>Re-Reading Improves Reasoning in Large Language Models</li> </ol>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/","title":"Simulate a Perspective","text":""},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#overview","title":"Overview","text":"<p>How can we encourage the model to focus on relevant information?</p> <p>SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.</p> <p>This can be useful for complex questions with multiple entities. For example, if the prompt contains information about two individuals, we can ask the model to answer our query from the perspective of one of the individuals.</p> <p>This is implemented in two steps. Given an entity:  - Identify and isolate information relevant to the entity  - Ask the model to answer the query from the entity's perspective</p> <p>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#sample-template","title":"Sample Template","text":"<ul> <li>Step 1:</li> <li>Given the following context, list the facts that <code>{entity}</code> would know.</li> <li>Context: <code>{context}</code></li> <li>Step 2:</li> <li>You are <code>{entity}</code>.</li> <li>Answer the following question based only on these facts you know: <code>{facts}</code>.</li> <li>Question: <code>{query}</code> </li> </ul>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Arrays;\n\nclass KnownFacts {\n    #[Description(\"Facts that the given entity would know\")]\n    /** @var string[] */\n    public array $facts;\n}\n\nclass SimulatePerspective {\n    private string $extractionPrompt = &lt;&lt;&lt;PROMPT\n        Given the following context, list\n        the facts that {entity} would know:\n\n        Context:\n        {context}\n        {query}\n\n        List only the facts relevant to {entity}.\n        PROMPT;\n\n    private $povPrompt = &lt;&lt;&lt;PROMPT\n        You are {entity}. Answer the following question\n        based only on these facts you know:\n        {knowledge}\n\n        Question: {query}\n        PROMPT;\n\n    public function __invoke(string $context, string $query, string $perspective) : string {\n        $knownFacts = $this-&gt;getKnownFacts($context, $query, $perspective);\n        return $this-&gt;answerQuestion($perspective, $query, $knownFacts);\n    }\n\n    private function getKnownFacts(string $context, string $query, string $entity) : array {\n        return (new StructuredOutput)-&gt;with(\n            messages: str_replace(\n                ['{context}', '{query}', '{entity}'],\n                [$context, $query, $entity],\n                $this-&gt;extractionPrompt\n            ),\n            responseModel: KnownFacts::class,\n        )-&gt;get()-&gt;facts;\n    }\n\n    private function answerQuestion(string $entity, string $query, array $knownFacts) : string {\n        $knowledge = Arrays::toBullets($knownFacts);\n\n        return (new StructuredOutput)-&gt;with(\n                messages: str_replace(\n                    ['{entity}', '{knowledge}', '{query}'],\n                    [$entity, $knowledge, $query],\n                    $this-&gt;povPrompt\n                ),\n                responseModel: Scalar::string('location'),\n            )\n            -&gt;getString();\n    }\n}\n\n$povEntity = \"Alice\";\n\n$location = (new SimulatePerspective)(\n    context: &lt;&lt;&lt;CONTEXT\n        Alice puts the book on the table.\n        Alice leaves the room.\n        Bob moves the book to the shelf.\n    CONTEXT,\n    query: \"Where does $povEntity think the book is?\",\n    perspective: $povEntity,\n);\n?&gt;\n</code></pre>"},{"location":"cookbook/prompting/zero_shot/simulate_perspective/#references","title":"References","text":"<ol> <li>Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities</li> </ol>"},{"location":"cookbook/structured_outputs/advanced/config_providers/","title":"Use custom configuration providers","text":""},{"location":"cookbook/structured_outputs/advanced/config_providers/#overview","title":"Overview","text":"<p>You can inject your own configuration providers to StructuredOutput class. This is useful for integration with your preferred framework (e.g. Symfony, Laravel).</p>"},{"location":"cookbook/structured_outputs/advanced/config_providers/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Adbar\\Dot;\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Creation\\HttpClientBuilder;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass CustomConfigProvider implements CanProvideConfig\n{\n    private Dot $dot;\n\n    public function __construct(array $config = []) {\n        $this-&gt;dot = new Dot($config);\n    }\n\n    #[\\Override]\n    public function get(string $path, mixed $default = null): mixed {\n        return $this-&gt;dot-&gt;get($path, $default);\n    }\n\n    #[\\Override]\n    public function has(string $path): bool {\n        return $this-&gt;dot-&gt;has($path);\n    }\n}\n\n$configData = [\n    'http' =&gt; [\n        'defaultPreset' =&gt; 'symfony',\n        'presets' =&gt; [\n            'symfony' =&gt; [\n                'driver' =&gt; 'symfony',\n                'connectTimeout' =&gt; 10,\n                'requestTimeout' =&gt; 30,\n                'idleTimeout' =&gt; -1,\n                'maxConcurrent' =&gt; 5,\n                'poolTimeout' =&gt; 60,\n                'failOnError' =&gt; true,\n            ],\n            // Add more HTTP presets as needed\n        ],\n    ],\n    'debug' =&gt; [\n        'defaultPreset' =&gt; 'off',\n        'presets' =&gt; [\n            'off' =&gt; [\n                'httpEnabled' =&gt; false,\n            ],\n            'on' =&gt; [\n                'httpEnabled' =&gt; true,\n                'httpTrace' =&gt; true,\n                'httpRequestUrl' =&gt; true,\n                'httpRequestHeaders' =&gt; true,\n                'httpRequestBody' =&gt; true,\n                'httpResponseHeaders' =&gt; true,\n                'httpResponseBody' =&gt; true,\n                'httpResponseStream' =&gt; true,\n                'httpResponseStreamByLine' =&gt; true,\n            ],\n        ],\n    ],\n    'llm' =&gt; [\n        'defaultPreset' =&gt; 'deepseek',\n        'presets' =&gt; [\n            'deepseek' =&gt; [\n                'apiUrl' =&gt; 'https://api.deepseek.com',\n                'apiKey' =&gt; Env::get('DEEPSEEK_API_KEY'),\n                'endpoint' =&gt; '/chat/completions',\n                'model' =&gt; 'deepseek-chat',\n                'maxTokens' =&gt; 128,\n                'driver' =&gt; 'deepseek',\n            ],\n            'openai' =&gt; [\n                'apiUrl' =&gt; 'https://api.openai.com',\n                'apiKey' =&gt; Env::get('OPENAI_API_KEY'),\n                'endpoint' =&gt; '/v1/chat/completions',\n                'model' =&gt; 'gpt-4',\n                'maxTokens' =&gt; 256,\n                'driver' =&gt; 'openai',\n            ],\n        ],\n    ],\n    'structured' =&gt; [\n        'defaultPreset' =&gt; 'tools',\n        'presets' =&gt; [\n            'tools' =&gt; [\n                'outputMode' =&gt; OutputMode::Tools,\n                'useObjectReferences' =&gt; true,\n                'maxRetries' =&gt; 3,\n                'retryPrompt' =&gt; 'Please try again ...',\n                'modePrompts' =&gt; [\n                    OutputMode::MdJson-&gt;value =&gt; \"Response must validate against this JSON Schema:\\n&lt;|json_schema|&gt;\\n. Respond correctly with strict JSON object within a ```json {} ``` codeblock.\\n\",\n                    OutputMode::Json-&gt;value =&gt; \"Response must follow JSON Schema:\\n&lt;|json_schema|&gt;\\n. Respond correctly with strict JSON object.\\n\",\n                    OutputMode::JsonSchema-&gt;value =&gt; \"Response must follow provided JSON Schema. Respond correctly with strict JSON object.\\n\",\n                    OutputMode::Tools-&gt;value =&gt; \"Extract correct and accurate data from the input using provided tools.\\n\",\n                ],\n                'schemaName' =&gt; 'user_schema',\n                'toolName' =&gt; 'user_tool',\n                'toolDescription' =&gt; 'Tool to extract user information ...',\n                'chatStructure' =&gt; [\n                    'system',\n                    'pre-cached',\n                        'pre-cached-prompt', 'cached-prompt', 'post-cached-prompt',\n                        'pre-cached-examples', 'cached-examples', 'post-cached-examples',\n                        'cached-messages',\n                    'post-cached',\n                    'pre-prompt', 'prompt', 'post-prompt',\n                    'pre-examples', 'examples', 'post-examples',\n                    'pre-messages', 'messages', 'post-messages',\n                    'pre-retries', 'retries', 'post-retries'\n                ],\n                // defaultOutputClass is not used in this example\n                'outputClass' =&gt; Structure::class,\n            ]\n        ]\n    ]\n];\n\n$events = new EventDispatcher();\n$configProvider = new CustomConfigProvider($configData);\n\n$customClient = (new HttpClientBuilder(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withConfigProvider($configProvider)\n    -&gt;withPreset('symfony')\n    -&gt;create();\n\n$structuredOutput = (new StructuredOutput(\n        events: $events,\n        configProvider: $configProvider,\n    ))\n    -&gt;withHttpClient($customClient);\n\n// Call with custom model and execution mode\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = $structuredOutput\n    -&gt;using('deepseek') // Use 'deepseek' preset defined in our config provider\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/context_cache_structured/","title":"Context caching (structured output)","text":""},{"location":"cookbook/structured_outputs/advanced/context_cache_structured/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching, so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note 1: Instructor supports context caching for Anthropic API and OpenAI API.</p> <p>Note 2: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/structured_outputs/advanced/context_cache_structured/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its structured description for multiple audiences.</p> <p>Let's start by defining the data model for the project details and the properties that we want to extract or generate based on README file.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass Project {\n    public string $name;\n    public string $targetAudience;\n    /** @var string[] */\n    #[Description('Technology platform and libraries used in the project')]\n    public array $technologies;\n    /** @var string[] */\n    #[Description('Target audience domain specific features and capabilities of the project')]\n    public array $features;\n    /** @var string[] */\n    #[Description('Target audience domain specific applications and potential use cases of the project')]\n    public array $applications;\n    #[Description('Explain the purpose of the project and the target audience domain specific problems it solves')]\n    public string $description;\n    #[Description('Target audience domain specific example code in Markdown demonstrating an application of the library')]\n    public string $code;\n}\n?&gt;\n</code></pre> <p>We read the content of the README.md file and cache the context, so it can be reused for multiple requests.</p> <p><pre><code>&lt;?php\n$content = file_get_contents(__DIR__ . '/../../../README.md');\n\n$cached = (new StructuredOutput)-&gt;using('anthropic')-&gt;withCachedContext(\n    system: 'Your goal is to respond questions about the project described in the README.md file'\n        . \"\\n\\n# README.md\\n\\n\" . $content,\n    prompt: 'Respond with strict JSON object using schema:\\n&lt;|json_schema|&gt;',\n);//-&gt;withDebugPreset('on');\n?&gt;\n</code></pre> At this point we can use Instructor structured output processing to extract the project details from the README.md file into the <code>Project</code> data model.</p> <p>Let's start by asking the user to describe the project for a specific audience: P&amp;C insurance CIOs.</p> <p><pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response1 = $cached-&gt;with(\n    messages: 'Describe the project in a way compelling to my audience: P&amp;C insurance CIOs.',\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::MdJson,\n)-&gt;create();\n\n// get processed value - instance of Project class\n$project1 = $response1-&gt;get();\ndump($project1);\nassert($project1 instanceof Project);\nassert(Str::contains($project1-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage1 = $response1-&gt;response()-&gt;usage();\necho \"Usage: {$usage1-&gt;inputTokens} prompt tokens, {$usage1-&gt;cacheWriteTokens} cache write tokens\\n\";\n?&gt;\n</code></pre> Now we can use the same context to ask the user to describe the project for a different audience: boutique CMS consulting company owner.</p> <p>Anthropic API will use the context cached in the previous request to provide the response, which results in faster processing and lower costs.</p> <pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response2 = $cached-&gt;with(\n    messages: \"Describe the project in a way compelling to my audience: boutique CMS consulting company owner.\",\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::Json,\n)-&gt;create();\n\n// get the processed value - instance of Project class\n$project2 = $response2-&gt;get();\ndump($project2);\nassert($project2 instanceof Project);\nassert(Str::contains($project2-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage2 = $response2-&gt;response()-&gt;usage();\necho \"Usage: {$usage2-&gt;inputTokens} prompt tokens, {$usage2-&gt;cacheReadTokens} cache read tokens\\n\";\nassert($usage2-&gt;cacheReadTokens &gt; 0, 'Expected cache read tokens');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/context_cache_structured_oai/","title":"Context caching (structured output, OpenAI)","text":""},{"location":"cookbook/structured_outputs/advanced/context_cache_structured_oai/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching, so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p> <p>Note: Context caching is automatic for all OpenAI API calls. Read more in the OpenAI API documentation.</p>"},{"location":"cookbook/structured_outputs/advanced/context_cache_structured_oai/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its structured description for multiple audiences.</p> <p>Let's start by defining the data model for the project details and the properties that we want to extract or generate based on README file.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass Project {\n    public string $name;\n    public string $targetAudience;\n    /** @var string[] */\n    #[Description('Technology platform and libraries used in the project')]\n    public array $technologies;\n    /** @var string[] */\n    #[Description('Target audience domain specific features and capabilities of the project')]\n    public array $features;\n    /** @var string[] */\n    #[Description('Target audience domain specific applications and potential use cases of the project')]\n    public array $applications;\n    #[Description('Explain the purpose of the project and the target audience domain specific problems it solves')]\n    public string $description;\n    #[Description('Target audience domain specific example code in Markdown demonstrating an application of the library')]\n    public string $code;\n}\n?&gt;\n</code></pre> <p>We read the content of the README.md file and cache the context, so it can be reused for multiple requests.</p> <p><pre><code>&lt;?php\n$content = file_get_contents(__DIR__ . '/../../../README.md');\n\n$cached = (new StructuredOutput)-&gt;using('openai')-&gt;withCachedContext(\n    system: 'Your goal is to respond questions about the project described in the README.md file'\n        . \"\\n\\n# README.md\\n\\n\" . $content,\n    prompt: 'Respond with strict JSON object using schema:\\n&lt;|json_schema|&gt;',\n);//-&gt;withDebugPreset('on');\n?&gt;\n</code></pre> At this point we can use Instructor structured output processing to extract the project details from the README.md file into the <code>Project</code> data model.</p> <p>Let's start by asking the user to describe the project for a specific audience: P&amp;C insurance CIOs.</p> <p><pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response1 = $cached-&gt;with(\n    messages: 'Describe the project in a way compelling to my audience: P&amp;C insurance CIOs.',\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::MdJson,\n)-&gt;create();\n\n// get processed value - instance of Project class\n$project1 = $response1-&gt;get();\ndump($project1);\nassert($project1 instanceof Project);\nassert(Str::contains($project1-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage1 = $response1-&gt;response()-&gt;usage();\necho \"Usage: {$usage1-&gt;inputTokens} prompt tokens, {$usage1-&gt;cacheWriteTokens} cache write tokens\\n\";\n?&gt;\n</code></pre> Now we can use the same context to ask the user to describe the project for a different audience: boutique CMS consulting company owner.</p> <p>OpenAI API can reuse the cached prefix from the previous request to provide the response, which results in faster processing and lower costs when prompt caching applies.</p> <pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response2 = $cached-&gt;with(\n    messages: \"Describe the project in a way compelling to my audience: boutique CMS consulting company owner.\",\n    responseModel: Project::class,\n    options: ['max_tokens' =&gt; 4096],\n    mode: OutputMode::Json,\n)-&gt;create();\n\n// get the processed value - instance of Project class\n$project2 = $response2-&gt;get();\ndump($project2);\nassert($project2 instanceof Project);\nassert(Str::contains($project2-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage2 = $response2-&gt;response()-&gt;usage();\necho \"Usage: {$usage2-&gt;inputTokens} prompt tokens, {$usage2-&gt;cacheReadTokens} cache read tokens\\n\";\nif ($usage2-&gt;cacheReadTokens === 0) {\n    echo \"Note: cacheReadTokens is 0. Prompt caching applies only to eligible models and prompt sizes.\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/context_cache_structured_oai_responses/","title":"Context caching (structured output, OpenAI Responses)","text":""},{"location":"cookbook/structured_outputs/advanced/context_cache_structured_oai_responses/#overview","title":"Overview","text":"<p>Instructor offers a simplified way to work with LLM providers' APIs supporting caching, so you can focus on your business logic while still being able to take advantage of lower latency and costs.</p>"},{"location":"cookbook/structured_outputs/advanced/context_cache_structured_oai_responses/#example","title":"Example","text":"<p>When you need to process multiple requests with the same context, you can use context caching to improve performance and reduce costs.</p> <p>In our example we will be analyzing the README.md file of this Github project and generating its structured description for multiple audiences.</p> <p>Let's start by defining the data model for the project details and the properties that we want to extract or generate based on README file.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass Project {\n    public string $name;\n    public string $targetAudience;\n    /** @var string[] */\n    #[Description('Technology platform and libraries used in the project')]\n    public array $technologies;\n    /** @var string[] */\n    #[Description('Target audience domain specific features and capabilities of the project')]\n    public array $features;\n    /** @var string[] */\n    #[Description('Target audience domain specific applications and potential use cases of the project')]\n    public array $applications;\n    #[Description('Explain the purpose of the project and the target audience domain specific problems it solves')]\n    public string $description;\n    #[Description('Target audience domain specific example code in Markdown demonstrating an application of the library')]\n    public string $code;\n}\n?&gt;\n</code></pre> <p>We read the content of the README.md file and cache the context, so it can be reused for multiple requests.</p> <p><pre><code>&lt;?php\n$content = file_get_contents(__DIR__ . '/../../../README.md');\n$cacheKey = 'context_cache_structured_oai_responses_readme_v1';\n\n$cached = (new StructuredOutput)-&gt;using('openai-responses')-&gt;withCachedContext(\n    system: 'Your goal is to respond questions about the project described in the README.md file'\n        . \"\\n\\n# README.md\\n\\n\" . $content,\n    prompt: 'Respond with strict JSON object using schema:\\n&lt;|json_schema|&gt;',\n);//-&gt;withDebugPreset('on');\n?&gt;\n</code></pre> At this point we can use Instructor structured output processing to extract the project details from the README.md file into the <code>Project</code> data model.</p> <p>Let's start by asking the user to describe the project for a specific audience: P&amp;C insurance CIOs.</p> <p><pre><code>&lt;?php\n/** @var array&lt;string, mixed&gt; $optionsBase */\n$optionsBase = [\n    'max_output_tokens' =&gt; 4096,\n    // Improve cache routing for identical prefixes between requests.\n    'prompt_cache_key' =&gt; $cacheKey,\n    'prompt_cache_retention' =&gt; 'in_memory',\n];\n\n// get StructuredOutputResponse object to get access to usage and other metadata\n$response1 = $cached-&gt;with(\n    messages: 'Describe the project in a way compelling to my audience: P&amp;C insurance CIOs.',\n    responseModel: Project::class,\n    model: 'gpt-4.1',\n    options: $optionsBase,\n    mode: OutputMode::JsonSchema,\n)-&gt;create();\n\n// get processed value - instance of Project class\n$project1 = $response1-&gt;get();\ndump($project1);\nassert($project1 instanceof Project);\nassert(Str::contains($project1-&gt;name, 'Instructor'));\n\n// Extract response id for Responses API server-side chaining\n$body1 = json_decode($response1-&gt;response()-&gt;responseData()-&gt;body(), true);\n$previousResponseId = is_array($body1) ? ($body1['id'] ?? '') : '';\nif ($previousResponseId === '') {\n    echo \"Warning: previous_response_id not available; chaining will be skipped.\\n\";\n}\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage1 = $response1-&gt;response()-&gt;usage();\necho \"Usage #1: {$usage1-&gt;inputTokens} prompt tokens, {$usage1-&gt;cacheWriteTokens} cache write tokens\\n\";\n?&gt;\n</code></pre> Now we can use the same context to ask the user to describe the project for a different audience: boutique CMS consulting company owner.</p> <p>OpenAI API can reuse the cached prefix from the previous request to provide the response, which results in faster processing and lower costs when prompt caching applies.</p> <pre><code>&lt;?php\n// get StructuredOutputResponse object to get access to usage and other metadata\n$options2 = $optionsBase;\nif ($previousResponseId !== '') {\n    $options2['previous_response_id'] = $previousResponseId;\n}\n\n$response2 = $cached-&gt;with(\n    messages: \"Describe the project in a way compelling to my audience: boutique CMS consulting company owner.\",\n    responseModel: Project::class,\n    model: 'gpt-4.1',\n    options: $options2,\n    mode: OutputMode::JsonSchema,\n)-&gt;create();\n\n// get the processed value - instance of Project class\n$project2 = $response2-&gt;get();\ndump($project2);\nassert($project2 instanceof Project);\nassert(Str::contains($project2-&gt;name, 'Instructor'));\n\n// get usage information from response() method which returns raw InferenceResponse object\n$usage2 = $response2-&gt;response()-&gt;usage();\necho \"Usage #2: {$usage2-&gt;inputTokens} prompt tokens, {$usage2-&gt;cacheReadTokens} cache read tokens\\n\";\nassert($usage2-&gt;cacheReadTokens &gt; 0, 'Expected prompt cache read tokens &gt; 0');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/custom_config/","title":"Customize parameters of LLM driver","text":""},{"location":"cookbook/structured_outputs/advanced/custom_config/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration instance to Instructor. This is useful when you want to initialize OpenAI client with custom values - e.g. to call other LLMs which support OpenAI API.</p>"},{"location":"cookbook/structured_outputs/advanced/custom_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Env;\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Creation\\HttpClientBuilder;\nuse Cognesy\\Http\\Drivers\\Symfony\\SymfonyDriver;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Symfony\\Component\\HttpClient\\HttpClient as SymfonyHttpClient;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$events = new EventDispatcher();\n\n// Build fully customized HTTP client\n\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 30,\n    requestTimeout: 60,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true,\n);\n\n$yourClientInstance = SymfonyHttpClient::create(['http_version' =&gt; '2.0']);\n\n$customClient = (new HttpClientBuilder)\n    -&gt;withEventBus($events)\n    -&gt;withDriver(new SymfonyDriver(\n        config: $httpConfig,\n        clientInstance: $yourClientInstance,\n        events: $events,\n    ))\n    -&gt;create();\n\n// Create instance of LLM connection preset initialized with custom parameters\n\n$llmConfig = new LLMConfig(\n    apiUrl  : 'https://api.deepseek.com',\n    apiKey  : Env::get('DEEPSEEK_API_KEY'),\n    endpoint: '/chat/completions', model: 'deepseek-chat', maxTokens: 128, driver: 'openai-compatible',\n);\n\n// Get Instructor with the default client component overridden with your own\n\n$structuredOutput = (new StructuredOutput)\n    -&gt;withEventHandler($events)\n    -&gt;withLLMConfig($llmConfig)\n    -&gt;withHttpClient($customClient);\n\n// Call with custom model and execution mode\n\n$user = $structuredOutput\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/custom_http_client/","title":"Use custom HTTP client instance","text":""},{"location":"cookbook/structured_outputs/advanced/custom_http_client/#overview","title":"Overview","text":""},{"location":"cookbook/structured_outputs/advanced/custom_http_client/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Dispatchers\\SymfonyEventDispatcher;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Symfony\\Component\\EventDispatcher\\EventDispatcher;\nuse Symfony\\Component\\HttpClient\\HttpClient;\n\n// custom Symfony components\n// custom Symfony components\n\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Call with custom model and execution mode\n\n$yourSymfonyClientInstance = HttpClient::create(['http_version' =&gt; '2.0']);\n$yourSymfonyEventDispatcher = new SymfonyEventDispatcher(new EventDispatcher());\n\n$user = (new StructuredOutput(events: $yourSymfonyEventDispatcher))\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withLLMConfigOverrides(['apiUrl' =&gt; 'https://api.openai.com/v1'])\n    -&gt;withClientInstance(\n        driverName: 'symfony',\n        clientInstance: $yourSymfonyClientInstance\n    )\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    //-&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/custom_http_client_laravel/","title":"Use custom HTTP client instance - Laravel","text":""},{"location":"cookbook/structured_outputs/advanced/custom_http_client_laravel/#overview","title":"Overview","text":""},{"location":"cookbook/structured_outputs/advanced/custom_http_client_laravel/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Illuminate\\Http\\Client\\Factory;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$yourLaravelClientInstance = new Factory();\n\n$user = (new StructuredOutput())\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withLLMConfigOverrides(['apiUrl' =&gt; 'https://api.openai.com/v1'])\n    -&gt;withClientInstance(\n        driverName: 'laravel',\n        clientInstance: $yourLaravelClientInstance\n    )\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withOutputMode(OutputMode::Tools)\n    //-&gt;withStreaming()\n    -&gt;get();\n\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/custom_llm_with_dsn/","title":"Customize parameters via DSN","text":""},{"location":"cookbook/structured_outputs/advanced/custom_llm_with_dsn/#overview","title":"Overview","text":"<p>You can provide your own LLM configuration data to <code>StructuredOutput</code> object with DSN string. This is useful for inline configuration or for building configuration from admin UI, CLI arguments or environment variables.</p>"},{"location":"cookbook/structured_outputs/advanced/custom_llm_with_dsn/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new StructuredOutput)\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withDsn('preset=xai,model=grok-3')\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withresponseClass(User::class)\n    -&gt;get();\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/custom_prompts/","title":"Custom prompts","text":""},{"location":"cookbook/structured_outputs/advanced/custom_prompts/#overview","title":"Overview","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>request()</code> or <code>create()</code> methods.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"cookbook/structured_outputs/advanced/custom_prompts/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$structuredOutput = (new StructuredOutput)\n    // let's dump the request data to see how customized prompts look like in requests\n    -&gt;onEvent(HttpRequestSent::class, fn(HttpRequestSent $event) =&gt; dump($event));\n\nprint(\"\\n# Request for OutputMode::Tools:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        mode: OutputMode::Tools\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\nprint(\"\\n# Request for OutputMode::Json:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::Json\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\nprint(\"\\n# Request for OutputMode::MdJson:\\n\\n\");\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::MdJson\n    )-&gt;get();\necho \"\\nRESPONSE:\\n\";\ndump($user);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/demonstrations/","title":"Providing example inputs and outputs","text":""},{"location":"cookbook/structured_outputs/advanced/demonstrations/#overview","title":"Overview","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> modes, where the output is expected to be a JSON object.</p>"},{"location":"cookbook/structured_outputs/advanced/demonstrations/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\necho \"\\nREQUEST:\\n\";\n$user = (new StructuredOutput)\n    // let's dump the request data to see how examples are used in requests\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; dump($event))\n    -&gt;withMessages(\"Our user Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;withExamples([\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27],\n            template: \"example input:\\n&lt;|input|&gt;\\noutput:\\n```json\\n&lt;|output|&gt;\\n```\\n\",\n        ),\n    ])\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n\necho \"\\nOUTPUT:\\n\";\ndump($user);\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/function_calls/","title":"Extracting arguments of function or method","text":""},{"location":"cookbook/structured_outputs/advanced/function_calls/#overview","title":"Overview","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"cookbook/structured_outputs/advanced/function_calls/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass DataStore\n{\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) : void {\n        // Save user to database\n        echo \"Saving user ... saveUser('$name', $age, '$country')\\n\";\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromMethodName(DataStore::class, 'saveUser'),\n)-&gt;get();\n\necho \"\\nCalling the function with the extracted arguments:\\n\";\n(new DataStore)-&gt;saveUser(...$args);\n\necho \"\\nExtracted arguments:\\n\";\ndump($args);\n\nassert(count($args) == 3);\nexpect($args['name'] === 'Jason');\nexpect($args['age'] == 28);\nexpect($args['country'] === 'Germany');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/manual_schemas/","title":"Manual Schema Building","text":""},{"location":"cookbook/structured_outputs/advanced/manual_schemas/#overview","title":"Overview","text":"<p>While InstructorPHP can automatically generate schemas from PHP classes, you can also build schemas manually using the <code>JsonSchema</code> API.</p> <p>This provides full control over the JSON Schema structure and is useful for: - Dynamic schemas determined at runtime - Provider-specific optimizations - Legacy JSON Schema integration - Performance-sensitive scenarios</p> <p>See more: Manual Schema Building</p>"},{"location":"cookbook/structured_outputs/advanced/manual_schemas/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Build schema manually (no PHP class needed)\n$userSchema = JsonSchema::object(\n    name: 'User',\n    description: 'User information extracted from text',\n    properties: [\n        JsonSchema::string(\n            name: 'name',\n            description: 'Full name of the user'\n        ),\n        JsonSchema::integer(\n            name: 'age',\n            description: 'Age in years'\n        ),\n        JsonSchema::string(\n            name: 'email',\n            description: 'Email address'\n        ),\n        JsonSchema::enum(\n            name: 'role',\n            enumValues: ['admin', 'user', 'guest'],\n            description: 'User role'\n        ),\n        JsonSchema::object(\n            name: 'address',\n            description: 'User address',\n            properties: [\n                JsonSchema::string(name: 'street'),\n                JsonSchema::string(name: 'city'),\n                JsonSchema::string(name: 'zip'),\n            ],\n            requiredProperties: ['city']\n        ),\n        JsonSchema::collection(\n            name: 'skills',\n            description: 'List of skills',\n            itemSchema: JsonSchema::string()\n        ),\n    ],\n    requiredProperties: ['name', 'email'],\n    additionalProperties: false\n);\n\n$text = &lt;&lt;&lt;TEXT\n    John Doe (john.doe@example.com) is a 35-year-old admin user. He lives in\n    New York at 123 Main St, NYC, 10001. His skills include PHP, JavaScript,\n    and Docker.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\n// Use manual schema with StructuredOutput - returns array when no class is specified\n$user = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: $userSchema,\n    )\n    -&gt;get();\n\nprint(\"OUTPUT:\\n\");\nprint(\"Name: \" . $user['name'] . \"\\n\");\nprint(\"Email: \" . $user['email'] . \"\\n\");\nprint(\"Age: \" . $user['age'] . \"\\n\");\nprint(\"Role: \" . $user['role'] . \"\\n\");\nprint(\"Address: \" . $user['address']['city'] . \", \" . $user['address']['zip'] . \"\\n\");\nprint(\"Skills: \" . implode(\", \", $user['skills']) . \"\\n\");\n\nprint(\"\\n\");\nprint(\"COMPARISON: Manual vs Reflection\\n\");\nprint(\"=================================\\n\");\nprint(\"\u2705 Manual schema: Full control, no class needed\\n\");\nprint(\"\u2705 Reflection (from class): Concise, type-safe, single source of truth\\n\");\nprint(\"\\n\");\nprint(\"Choose manual schemas when:\\n\");\nprint(\"- Schema is determined at runtime\\n\");\nprint(\"- You need provider-specific optimizations\\n\");\nprint(\"- Working with legacy JSON Schema specs\\n\");\nprint(\"- Reflection overhead is a concern\\n\");\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/manual_schemas/#advanced-example-dynamic-schema","title":"Advanced Example: Dynamic Schema","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\n// Build schema dynamically based on user input\nfunction buildDynamicSchema(array $fields): JsonSchema {\n    $properties = [];\n\n    foreach ($fields as $field) {\n        $properties[] = match($field['type']) {\n            'string' =&gt; JsonSchema::string($field['name'], $field['description'] ?? ''),\n            'int' =&gt; JsonSchema::integer($field['name'], $field['description'] ?? ''),\n            'float' =&gt; JsonSchema::number($field['name'], $field['description'] ?? ''),\n            'bool' =&gt; JsonSchema::boolean($field['name'], $field['description'] ?? ''),\n            default =&gt; JsonSchema::string($field['name']),\n        };\n    }\n\n    return JsonSchema::object(\n        name: 'DynamicData',\n        properties: $properties,\n        requiredProperties: array_column($fields, 'name')\n    );\n}\n\n// User-defined fields at runtime\n$userFields = [\n    ['name' =&gt; 'product', 'type' =&gt; 'string', 'description' =&gt; 'Product name'],\n    ['name' =&gt; 'quantity', 'type' =&gt; 'int', 'description' =&gt; 'Quantity ordered'],\n    ['name' =&gt; 'price', 'type' =&gt; 'float', 'description' =&gt; 'Unit price'],\n    ['name' =&gt; 'inStock', 'type' =&gt; 'bool', 'description' =&gt; 'Is in stock'],\n];\n\n$schema = buildDynamicSchema($userFields);\n\n$data = (new StructuredOutput)\n    -&gt;with(\n        messages: 'Extract: Laptop, 2 units at $999.99 each, currently in stock',\n        responseModel: $schema,\n    )\n    -&gt;get();\n\nprint(\"Dynamic extraction result:\\n\");\nprint(\"Product: \" . $data['product'] . \"\\n\");\nprint(\"Quantity: \" . $data['quantity'] . \"\\n\");\nprint(\"Price: $\" . $data['price'] . \"\\n\");\nprint(\"In Stock: \" . ($data['inStock'] ? 'Yes' : 'No') . \"\\n\");\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/partials/","title":"Streaming partial updates during inference","text":""},{"location":"cookbook/structured_outputs/advanced/partials/#overview","title":"Overview","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated. You can use it to improve user experience by updating the UI with partial data before the full response is received.</p>"},{"location":"cookbook/structured_outputs/advanced/partials/#example","title":"Example","text":"<p><pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public string $location;\n    /** @var UserRole[] */\n    public array $roles;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    Console::clearScreen();\n    echo \"Updated partial object received:\\n\";\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    //usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;withStreaming()\n    -&gt;onPartialUpdate(partialUpdate(...))\n    -&gt;get();\n\necho \"All tokens received, fully completed object available in `\\$user` variable.\\n\";\necho '$user = '.\"\\n\";\ndump($user);\n\nassert(!empty($user-&gt;roles));\nassert(!empty($user-&gt;hobbies));\nassert($user-&gt;location === 'San Francisco');\nassert($user-&gt;age == 25);\nassert($user-&gt;name === 'Jason');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/scalars/","title":"Extracting scalar values","text":""},{"location":"cookbook/structured_outputs/advanced/scalars/#overview","title":"Overview","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p>"},{"location":"cookbook/structured_outputs/advanced/scalars/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum CitizenshipGroup : string {\n    case US = \"US\";\n    case Canada = \"Canada\";\n    case Germany = \"Germany\";\n    case Other = \"Other\";\n}\n\n$text = \"His name is Jason, he is 28 years old American who lives in Germany.\";\n$value = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    prompt: 'What is user\\'s citizenship?',\n    responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n)-&gt;get();\n\n\ndump($value);\n\nassert($value instanceof CitizenshipGroup);\nexpect($value == CitizenshipGroup::Other);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/sequences/","title":"Extracting sequences of objects","text":""},{"location":"cookbook/structured_outputs/advanced/sequences/#overview","title":"Overview","text":"<p>Sequences are a special type of response model that can be used to represent a list of objects.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p>"},{"location":"cookbook/structured_outputs/advanced/sequences/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old. Anna is 2 years younger than him.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"OUTPUT:\\n\");\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; dump($sequence-&gt;last()))\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: $text,\n        responseModel: Sequence::of(Person::class),\n        options: ['stream' =&gt; true],\n    )\n    -&gt;get();\n\n\ndump(count($list));\n\nassert(count($list) === 4);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/streaming/","title":"Streaming","text":""},{"location":"cookbook/structured_outputs/advanced/streaming/#overview","title":"Overview","text":"<p>Instructor can process LLM's streamed responses to provide partial response model updates that you can use to update the model with new data as the response is being generated.</p>"},{"location":"cookbook/structured_outputs/advanced/streaming/#example","title":"Example","text":"<p><pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age = 0;\n    public string $name = '';\n    public string $location = '';\n    /** @var UserRole[] */\n    public array $roles = [];\n    /** @var string[] */\n    public array $hobbies = [];\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    Console::clearScreen();\n\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    // usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$stream = (new StructuredOutput)\n    //-&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print())\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;withStreaming()\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;stream();\n\nforeach ($stream-&gt;partials() as $partial) {\n    partialUpdate($partial);\n}\n\n$user = $stream-&gt;lastUpdate();\n\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/advanced/structures/","title":"Structures","text":""},{"location":"cookbook/structured_outputs/advanced/structures/#overview","title":"Overview","text":"<p>Structures allow dynamically define the shape of data to be extracted by LLM, e.g. during runtime.</p> <p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns a <code>Structure</code> object with dynamic properties matching the shape you defined.</p> <p>You access the data using object property syntax (not array syntax):</p> <p>See more: Structures</p>"},{"location":"cookbook/structured_outputs/advanced/structures/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::option('gender', ['male', 'female'], 'Gender of the person')-&gt;optional(),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n    Field::collection('favourite_books', Structure::define('book', [\n            Field::string('author', 'Book author')-&gt;optional(),\n            Field::string('title', 'Book title'),\n        ], 'Favorite book data'),\n    'Favorite books of the person'),\n], 'A person object');\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield, 50210. She is 25 years old and works as manager at McDonald's.\n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345. Her favourite books are \"The Lord\n    of the Rings\" and \"The Hobbit\" by JRR Tolkien.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;get();\n\nprint(\"OUTPUT:\\n\");\n// Structure returns an object with dynamic properties (NOT an array)\nprint(\"Name: \" . $person-&gt;name . \"\\n\");              // \u2705 Object property access\nprint(\"Age: \" . $person-&gt;age . \"\\n\");                // \u2705 Works\nprint(\"Gender: \" . $person-&gt;gender . \"\\n\");          // \u2705 Works\nprint(\"Address / city: \" . $person-&gt;address-&gt;city . \"\\n\");  // \u2705 Nested objects\nprint(\"Address / ZIP: \" . $person-&gt;address-&gt;zip . \"\\n\");\nprint(\"Role: \" . $person-&gt;role-&gt;value . \"\\n\");\nprint(\"Favourite books:\\n\");\nforeach ($person-&gt;favourite_books as $book) {\n    print(\"  - \" . $book-&gt;title . \" by \" . $book-&gt;author . \"\\n\");\n}\n\n// Note: Array access does NOT work:\n// print($person['name']);  // \u274c Error - Structure is not an array\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/a21/","title":"A21","text":""},{"location":"cookbook/structured_outputs/api_support/a21/#overview","title":"Overview","text":"<p>Support for A21 Jamba - MAMBA architecture models, very strong at handling long context.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/a21/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection preset\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('a21');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/anthropic/","title":"Anthropic","text":""},{"location":"cookbook/structured_outputs/api_support/anthropic/#overview","title":"Overview","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - OutputMode::MdJson, OutputMode::Json - supported - OutputMode::Tools - not supported yet</p>"},{"location":"cookbook/structured_outputs/api_support/anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('anthropic');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'claude-3-haiku-20240307',\n    mode: OutputMode::Tools,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/azure_openai/","title":"Azure OpenAI","text":""},{"location":"cookbook/structured_outputs/api_support/azure_openai/#overview","title":"Overview","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p>"},{"location":"cookbook/structured_outputs/api_support/azure_openai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('azure');\n\n// Call with your model name and preferred execution mode\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/cerebras/","title":"Cerebras","text":""},{"location":"cookbook/structured_outputs/api_support/cerebras/#overview","title":"Overview","text":"<p>Support for Cerebras API which uses custom hardware for super fast inference. Cerebras provides Llama models.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/cerebras/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('cerebras');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'llama3.1-8b', // set your own value/source\n    mode: OutputMode::Json,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/cohere/","title":"Cohere","text":""},{"location":"cookbook/structured_outputs/api_support/cohere/#overview","title":"Overview","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - OutputMode::MdJson - supported, recommended as a fallback from JSON mode  - OutputMode::Json - supported, recommended  - OutputMode::Tools - partially supported, not recommended</p> <p>Reasons OutputMode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple, flat data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul>"},{"location":"cookbook/structured_outputs/api_support/cohere/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('cohere')\n    -&gt;withDebugPreset('on');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'command-r-plus-08-2024',\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/deepseek/","title":"DeepSeek","text":""},{"location":"cookbook/structured_outputs/api_support/deepseek/#overview","title":"Overview","text":"<p>Support for DeepSeek API which provides strong models at affordable price.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/deepseek/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('deepseek');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'deepseek-chat', // set your own value/source\n    mode: OutputMode::JsonSchema,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/fireworks/","title":"Fireworks.ai","text":""},{"location":"cookbook/structured_outputs/api_support/fireworks/#overview","title":"Overview","text":"<p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - selected models - OutputMode::Json - selected models - OutputMode::MdJson</p>"},{"location":"cookbook/structured_outputs/api_support/fireworks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('fireworks');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'accounts/fireworks/models/llama4-maverick-instruct-basic',\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/google_gemini/","title":"Google Gemini","text":""},{"location":"cookbook/structured_outputs/api_support/google_gemini/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('gemini');\n    //-&gt;withDebugPreset('detailed')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;printDebug());\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        //options: ['stream' =&gt; true],\n        mode: OutputMode::Json,\n    )\n    -&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/google_gemini_oai/","title":"Google Gemini (OpenAI compatible API)","text":""},{"location":"cookbook/structured_outputs/api_support/google_gemini_oai/#overview","title":"Overview","text":"<p>Google offers Gemini models which perform well in benchmarks.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Gemini's OpenAI compatible API.</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('gemini-oai');\n    //-&gt;withDebugPreset('detailed')\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;printDebug());\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        //options: ['stream' =&gt; true],\n        mode: OutputMode::MdJson,\n    )\n    -&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/groq/","title":"Groq","text":""},{"location":"cookbook/structured_outputs/api_support/groq/#overview","title":"Overview","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma.</p> <p>Supported modes depend on the specific model, but generally include:  - OutputMode::MdJson - fallback mode  - OutputMode::Json - recommended  - OutputMode::Tools - supported</p> <p>Here's how you can use Instructor with Groq API.</p>"},{"location":"cookbook/structured_outputs/api_support/groq/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public string $name;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n    public string $username;\n    public ?int $age;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('groq');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n        ],[\n            'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jx90.',\n            'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'], 'username' =&gt; 'jx90', 'age' =&gt; 19],\n        ]],\n        model: 'llama-3.3-70b-versatile', //'gemma2-9b-it',\n        maxRetries: 2,\n        options: ['temperature' =&gt; 0.5],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/huggingface/","title":"Hugging Face","text":""},{"location":"cookbook/structured_outputs/api_support/huggingface/#overview","title":"Overview","text":"<p>You can use Instructor to parse structured output from LLMs using Hugging Face API. This example demonstrates how to parse user data into a structured model using JSON Schema.</p>"},{"location":"cookbook/structured_outputs/api_support/huggingface/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public string $firstName;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n    public string $username;\n    public ?int $age;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('huggingface');\n    //-&gt;withDebugPreset('on');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n        examples: [[\n                      'input' =&gt; 'I\\'ve got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n                      'output' =&gt; ['firstName' =&gt; 'Frank', 'age' =&gt; 30, 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n                  ],[\n                      'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jx90.',\n                      'output' =&gt; ['firstName' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'], 'username' =&gt; 'jx90', 'age' =&gt; 19],\n                  ]],\n        //model: 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',\n        maxRetries: 2,\n        options: ['temperature' =&gt; 0.5],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;firstName));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;firstName === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/inception/","title":"Inception","text":""},{"location":"cookbook/structured_outputs/api_support/inception/#overview","title":"Overview","text":"<p>Inception API provides OpenAI-compatible endpoints for chat completions.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (supported)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/inception/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('inception');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'mercury', // set your own value/source\n    mode: OutputMode::MdJson,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/meta/","title":"Meta","text":""},{"location":"cookbook/structured_outputs/api_support/meta/#overview","title":"Overview","text":"<p>Instructor supports Meta LLM inference API. You can find the details on how to configure below.</p>"},{"location":"cookbook/structured_outputs/api_support/meta/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('meta');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/minimaxi/","title":"Minimaxi","text":""},{"location":"cookbook/structured_outputs/api_support/minimaxi/#overview","title":"Overview","text":"<p>Support for Minimaxi's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/structured_outputs/api_support/minimaxi/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('minimaxi');\n\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        model: 'MiniMax-Text-01', // set your own value/source\n        mode: OutputMode::MdJson,\n    )\n    -&gt;withDebugPreset('on')\n    -&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/mistralai/","title":"Mistral AI","text":""},{"location":"cookbook/structured_outputs/api_support/mistralai/#overview","title":"Overview","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support OutputMode::Json, which is much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility:  - OutputMode::Tools - supported (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::Json - recommended (Mistral-Small / Mistral-Medium / Mistral-Large)  - OutputMode::MdJson - fallback mode (Mistral 7B / Mixtral 8x7B)</p>"},{"location":"cookbook/structured_outputs/api_support/mistralai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('mistral');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @jx90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'jx90', 'age' =&gt; 30],\n    ]],\n    model: 'mistral-small-latest', //'open-mixtral-8x7b',\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/moonshotai/","title":"MoonshotAI","text":""},{"location":"cookbook/structured_outputs/api_support/moonshotai/#overview","title":"Overview","text":"<p>Support for MoonshotAI's API.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported)</p>"},{"location":"cookbook/structured_outputs/api_support/moonshotai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('moonshot-kimi');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'kimi-latest', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/ollama/","title":"Local / Ollama","text":""},{"location":"cookbook/structured_outputs/api_support/ollama/#overview","title":"Overview","text":"<p>You can use Instructor with local Ollama instance.</p> <p>Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model for complex data schemas.</p> <p>Supported modes:  - OutputMode::MdJson - fallback mode, works with any capable model  - OutputMode::Json - recommended  - OutputMode::Tools - supported (for selected models - check Ollama docs)</p>"},{"location":"cookbook/structured_outputs/api_support/ollama/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('ollama');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. Asked to connect via Twitter @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'], 'username' =&gt; 'frankch', 'age' =&gt; null],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @j90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'j90', 'age' =&gt; 30],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/openai-responses/","title":"OpenAI Responses API","text":""},{"location":"cookbook/structured_outputs/api_support/openai-responses/#overview","title":"Overview","text":"<p>OpenAI's Responses API is their new recommended API for inference, offering improved performance and features compared to Chat Completions.</p> <p>Key features: - 3% better performance on reasoning tasks - 40-80% improved cache utilization - Built-in tools: web search, file search, code interpreter - Server-side conversation state via <code>previous_response_id</code> - Semantic streaming events</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/openai-responses/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with OpenAI Responses API connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openai-responses');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/openai/","title":"OpenAI","text":""},{"location":"cookbook/structured_outputs/api_support/openai/#overview","title":"Overview","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - OutputMode::Tools (supported)  - OutputMode::Json (supported)  - OutputMode::JsonSchema (recommended for new models)  - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/openai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openai');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'gpt-4o-mini', // set your own value/source\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/openrouter/","title":"OpenRouter","text":""},{"location":"cookbook/structured_outputs/api_support/openrouter/#overview","title":"Overview","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p>"},{"location":"cookbook/structured_outputs/api_support/openrouter/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('openrouter');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/perplexity/","title":"Perplexity","text":""},{"location":"cookbook/structured_outputs/api_support/perplexity/#overview","title":"Overview","text":"<p>You can use Instructor with Perplexity API. Perplexity is an API that provides access to a large language model (LLM) for various tasks, including search and text generation.</p>"},{"location":"cookbook/structured_outputs/api_support/perplexity/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('perplexity');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old. He is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    prompt: 'Parse the user data to JSON, respond using following JSON Schema: &lt;|json_schema|&gt;',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'user', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new admin who likes surfing. He is 19 years old - check his profile: @jig.',\n        'output' =&gt; ['age' =&gt; 19, 'name' =&gt; 'John', 'username' =&gt; 'jig', 'role' =&gt; 'admin', 'hobbies' =&gt; ['surfing'],],\n    ]],\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/sambanova/","title":"SambaNova","text":""},{"location":"cookbook/structured_outputs/api_support/sambanova/#overview","title":"Overview","text":"<p>Support for SambaNova's API, which provide fast inference endpoints for Llama and Qwen LLMs.</p> <p>Mode compatibility: - OutputMode::MdJson (supported) - OutputMode::Tools (not supported) - OutputMode::Json (not supported) - OutputMode::JsonSchema (not supported)</p>"},{"location":"cookbook/structured_outputs/api_support/sambanova/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('sambanova');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    model: 'Meta-Llama-3.1-8B-Instruct', // set your own value/source\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/togetherai/","title":"Together.ai","text":""},{"location":"cookbook/structured_outputs/api_support/togetherai/#overview","title":"Overview","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support OutputMode::Tools or OutputMode::Json, which are much more reliable than OutputMode::MdJson.</p> <p>Mode compatibility: - OutputMode::Tools - supported for selected models - OutputMode::Json - supported for selected models - OutputMode::MdJson - fallback mode</p>"},{"location":"cookbook/structured_outputs/api_support/togetherai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('together');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; 'frank@hk.ch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ],[\n        'input' =&gt; 'We have a meeting with John, our new user. He is 30 years old - check his profile: @jx90.',\n        'output' =&gt; ['name' =&gt; 'John', 'role' =&gt; 'admin', 'hobbies' =&gt; [], 'username' =&gt; 'jx90', 'age' =&gt; 30],\n    ]],\n    //model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n    //options: ['stream' =&gt; true ]\n    mode: OutputMode::Json,\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/api_support/xai/","title":"xAI / Grok","text":""},{"location":"cookbook/structured_outputs/api_support/xai/#overview","title":"Overview","text":"<p>Support for xAI's API, which offers access to X.com's Grok model.</p> <p>Mode compatibility: - OutputMode::Tools (supported) - OutputMode::Json (supported) - OutputMode::JsonSchema (supported) - OutputMode::MdJson (fallback)</p>"},{"location":"cookbook/structured_outputs/api_support/xai/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Get Instructor with specified LLM client connection\n// See: /config/llm.php to check or change LLM client connection configuration details\n$structuredOutput = (new StructuredOutput)-&gt;using('xai');\n\n$user = $structuredOutput-&gt;with(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer, who\\'s 30. His Twitter handle is @frankch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; 30, 'name' =&gt; 'Frank', 'username' =&gt; '@frankch', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n)-&gt;get();\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;role));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;hobbies));\nassert(isset($user-&gt;username));\nassert(is_array($user-&gt;hobbies));\nassert(count($user-&gt;hobbies) &gt; 0);\nassert($user-&gt;role === UserType::Admin);\nassert($user-&gt;age === 25);\nassert($user-&gt;name === 'Jason');\nassert(in_array($user-&gt;username, ['jxnlco', '@jxnlco']));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/attributes/","title":"Using attributes","text":""},{"location":"cookbook/structured_outputs/basics/attributes/#overview","title":"Overview","text":"<p>Instructor supports <code>Description</code> and <code>Instructions</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p>"},{"location":"cookbook/structured_outputs/basics/attributes/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make user name ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    #[Instructions(\"Make the profession name lowercase\")]\n    public string $job;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = \"Jason is 25 years old, 10K runner, speaker and an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n)-&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert($user-&gt;name === \"JASON\");\nassert(isset($user-&gt;age));\nassert($user-&gt;age === 25);\nassert(isset($user-&gt;job));\nassert($user-&gt;job === \"engineer\");\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/basic_use/","title":"Basic use","text":""},{"location":"cookbook/structured_outputs/basics/basic_use/#overview","title":"Overview","text":"<p>Instructor allows you to use large language models to extract information from the text (or content of chat messages), while following the structure you define.</p> <p>LLM does not 'parse' the text to find and retrieve the information. Extraction leverages LLM ability to comprehend provided text and infer the meaning of the information it contains to fill fields of the response object with values that match the types and semantics of the class fields.</p> <p>The simplest way to use the Instructor is to call the <code>respond</code> method on the Instructor instance. This method takes a string (or an array of strings in the format of OpenAI chat messages) as input and returns a data extracted from provided text (or chat) using the LLM inference.</p> <p>Returned object will contain the values of fields extracted from the text.</p> <p>The format of the extracted data is defined by the response model, which in this case is a simple PHP class with some public properties.</p>"},{"location":"cookbook/structured_outputs/basics/basic_use/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Step 2: Get the text (or chat messages) you want to use as context\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/basic_use_mixin/","title":"Basic use via mixin","text":""},{"location":"cookbook/structured_outputs/basics/basic_use_mixin/#overview","title":"Overview","text":"<p>Instructor provides <code>HandlesSelfInference</code> trait that you can use to enable extraction capabilities directly on class via static <code>infer()</code> method.</p> <p><code>infer()</code> method returns an instance of the class with the data extracted using the Instructor.</p> <p><code>infer()</code> method has following signature (you can also find it in the <code>CanSelfInfer</code> interface):</p> <pre><code>static public function infer(\n    string|array $messages, // (required) The message(s) to infer data from\n    string $prompt = '',    // (optional) The prompt to use for inference\n    array $examples = [],   // (optional) Examples to include in the prompt\n    string $model = '',     // (optional) The model to use for inference (otherwise - use default)\n    int $maxRetries = 2,    // (optional) The number of retries in case of validation failure\n    array $options = [],    // (optional) Additional data to pass to the Instructor or LLM API\n    Mode $mode = OutputMode::Tools, // (optional) The mode to use for inference\n    ?LLM $llm = null         // (optional) LLM instance to use for inference\n) : static;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/basic_use_mixin/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Mixin\\HandlesSelfInference;\n\nclass User {\n    use HandlesSelfInference;\n\n    public int $age;\n    public string $name;\n}\n\n$user = User::infer(\"Jason is 25 years old and works as an engineer.\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/constructor_parameters/","title":"Specifying required and optional parameters via constructor","text":""},{"location":"cookbook/structured_outputs/basics/constructor_parameters/#overview","title":"Overview","text":"<p>Instructor can extract data from the LLM response and use it to instantiate an object via constructor parameters.</p> <p>Instructor will use the constructor parameters nullability and default values to determine which parameters are required and which are optional.</p>"},{"location":"cookbook/structured_outputs/basics/constructor_parameters/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserWithConstructor\n{\n    public string $name;\n    private ?int $age;\n    private string $location;\n    private string $password;\n\n    public function __construct(\n        string $name,                 // required - required in constructor, required internally\n        int $age,                     // required - required in constructor, even if nullable internally\n        ?string $location,            // optional - nullable in constructor, even if required internally\n        string $password = '123admin' // optional - has a default value, even if required internally\n    ) {\n        $this-&gt;name = $name;\n        $this-&gt;age = $age;\n        $this-&gt;location = $location ?? '';\n        $this-&gt;password = $password;\n    }\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getLocation(): string {\n        return $this-&gt;location;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithConstructor::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;getAge() === 25);\nassert($user-&gt;getPassword() === '123admin');\nassert($user-&gt;getLocation() === ''); // default value for location\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/custom_validation/","title":"Custom validation using Symfony Validator","text":""},{"location":"cookbook/structured_outputs/basics/custom_validation/#overview","title":"Overview","text":"<p>Instructor uses Symfony validation component to validate properties of extracted data. Symfony offers you #[Assert/Callback] annotation to build fully customized validation logic.</p>"},{"location":"cookbook/structured_outputs/basics/custom_validation/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be all uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"JASON\");\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/fluent_api/","title":"Fluent API","text":""},{"location":"cookbook/structured_outputs/basics/fluent_api/#overview","title":"Overview","text":""},{"location":"cookbook/structured_outputs/basics/fluent_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($text)\n    -&gt;withModel('gpt-3.5-turbo')\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/getters_and_setters/","title":"Getters and setters","text":""},{"location":"cookbook/structured_outputs/basics/getters_and_setters/#overview","title":"Overview","text":"<p>Instructor can extract data from the LLM response and use it to instantiate an object via setter methods.</p> <p>If given property is not public and has no matching constructor params Instructor will use the setter method parameter nullability and default value to determine if property is required.</p>"},{"location":"cookbook/structured_outputs/basics/getters_and_setters/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass UserWithSetter\n{\n    #[Description('Name of the user or empty string if not provided')]\n    private string $name;\n    #[Description('Age of the user or 0 if not provided')]\n    private ?int $age;\n    #[Description('Location of the user or empty string if not provided')]\n    private string $location;\n    #[Description('Password of the user or empty string if not provided')]\n    private string $password;\n\n    // `name` is required (not nullable parameter), if data exists in the answer the setter will be called, but may have empty value\n    public function setName(string $name): void {\n        $this-&gt;name = $name ?: 'Jason';\n    }\n\n    public function getName(): string {\n        return $this-&gt;name ?? '';\n    }\n\n    // `age` is optional (nullable parameter), setter will not be called if LLM does not infer the data\n    public function setAge(int $age): void {\n        $this-&gt;age = (int) $age;\n    }\n\n    public function getAge(): int {\n        return $this-&gt;age ?? 0;\n    }\n\n    public function setLocation(?string $location): void {\n        $this-&gt;location = $location;\n    }\n\n    public function getLocation(): string {\n        return $this-&gt;location;\n    }\n\n    public function setPassword(string|null $password = ''): void {\n        $this-&gt;password = $password ?: '123admin';\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    This user is living in San Francisco. His password is.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithSetter::class)\n    -&gt;withMaxRetries(2)\n    //-&gt;withModel('claude-3-7-sonnet-20250219')\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;getName() === \"Jason\"); // called - but set to default value as LLM inferred empty name\nassert($user-&gt;getAge() === 0); // not called - property value not inferred by LLM\nassert($user-&gt;getPassword() === '123admin'); // called - but set to default value as LLM inferred empty password\nassert($user-&gt;getLocation() === 'San Francisco'); // called - LLM inferred location from the text\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/maybe/","title":"Handling errors with `Maybe` helper class","text":""},{"location":"cookbook/structured_outputs/basics/maybe/#overview","title":"Overview","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p>"},{"location":"cookbook/structured_outputs/basics/maybe/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n\n$text = 'We have no information about our new developer.';\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class),\n    model: 'gpt-4o-mini',\n    mode: OutputMode::MdJson,\n)-&gt;get();\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === false);\nassert(!empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() === null);\n\n$text = \"Jason is our new developer, he is 25 years old.\";\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class)\n)-&gt;get();\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === true);\nassert(empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() != null);\nassert($maybeUser-&gt;get() instanceof User);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/messages_api/","title":"Messages API","text":""},{"location":"cookbook/structured_outputs/basics/messages_api/#overview","title":"Overview","text":"<p>Instructor allows you to use <code>Messages</code> and <code>Message</code> classes to work with chat messages and their sequences.</p>"},{"location":"cookbook/structured_outputs/basics/messages_api/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Messages\\Message;\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Str;\n\nclass Code {\n    public string $code;\n    public string $programmingLanguage;\n    public string $codeDescription;\n}\n\n$messages = Messages::empty()\n    -&gt;asSystem('You are a senior PHP8 backend developer.')\n    -&gt;asDeveloper('Be concise and use modern PHP8.2+ features.') // OpenAI developer role is supported and normalized for other providers\n    -&gt;asUser([\n        'What is the best way to handle errors in PHP8?',\n        'Provide a code example.',\n        'Use modern PHP8.2+ features.',\n    ])\n    -&gt;asAssistant('I will provide a code example that demonstrates how to handle errors using try-catch. Any specific domain?');\n\n$messages-&gt;appendMessage(Message::asUser('Make it insurance related.'));\n\n$lastMessageId = $messages-&gt;last()-&gt;id()-&gt;toString();\nprint(\"Last message ID: {$lastMessageId}\\n\");\n\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$code = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withMessages($messages)\n    -&gt;withResponseModel(Code::class)\n    -&gt;withOutputMode(OutputMode::MdJson)\n    -&gt;get();\n\nprint(\"Extracted data:\\n\");\ndump($code);\n\nassert(!empty($code-&gt;code));\nassert(!empty($code-&gt;codeDescription));\nassert(Str::contains(strtolower($code-&gt;programmingLanguage), 'php'));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/mixed_type_property/","title":"Mixed Type Property","text":""},{"location":"cookbook/structured_outputs/basics/mixed_type_property/#overview","title":"Overview","text":""},{"location":"cookbook/structured_outputs/basics/mixed_type_property/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Attributes\\Description;\n\nclass UserWithMixedTypeProperty\n{\n    public string $name;\n    #[Description('Any extra information about the user')]\n    public mixed $extraInfo;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He plays football and loves to travel.\n    TEXT;\n\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithMixedTypeProperty::class)\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;extraInfo !== ''); // not empty, but can be any type\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/modes/","title":"Modes","text":""},{"location":"cookbook/structured_outputs/basics/modes/#overview","title":"Overview","text":"<p>Instructor supports several ways to extract data from the response:</p> <ul> <li><code>OutputMode::Tools</code> - uses OpenAI-style tool calls to get the language    model to generate JSON following the schema,</li> <li><code>OutputMode::JsonSchema</code> - guarantees output matching JSON Schema via    Context Free Grammar, does not support optional properties,</li> <li><code>OutputMode::Json</code> - JSON mode, response follows provided JSON Schema,</li> <li><code>OutputMode::MdJson</code> - uses prompting to get the language model to    generate JSON following the schema.</li> </ul> <p>Note: not all modes are supported by all models or providers.</p> <p>Mode can be set via parameter of <code>StructuredOutput::create()</code> method.</p>"},{"location":"cookbook/structured_outputs/basics/modes/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$text = \"Jason is 25 years old and works as an engineer.\";\n\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n$structuredOutput = new StructuredOutput;\n\n// CASE 1 - OutputMode::Tools\nprint(\"\\n1. Extracting structured data using LLM - OutputMode::Tools\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::Tools,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 2 - OutputMode::JsonSchema\nprint(\"\\n2. Extracting structured data using LLM - OutputMode::JsonSchema\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::JsonSchema,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 3 - OutputMode::Json\nprint(\"\\n3. Extracting structured data using LLM - OutputMode::Json\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::Json,\n)-&gt;get();\ncheck($user);\ndump($user);\n\n// CASE 4 - OutputMode::MdJson\nprint(\"\\n4. Extracting structured data using LLM - OutputMode::MdJson\\n\");\n$user = $structuredOutput-&gt;with(\n    messages: $text,\n    responseModel: User::class,\n    mode: OutputMode::MdJson,\n)-&gt;get();\ncheck($user);\ndump($user);\n\nfunction check(User $user) {\n    assert(isset($user-&gt;name));\n    assert(isset($user-&gt;age));\n    assert($user-&gt;name === 'Jason');\n    assert($user-&gt;age === 25);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/optional_fields/","title":"Making some fields optional","text":""},{"location":"cookbook/structured_outputs/basics/optional_fields/#overview","title":"Overview","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) to declare component fields which are optional. Set a default value to prevent undesired defaults like nulls or empty strings.</p>"},{"location":"cookbook/structured_outputs/basics/optional_fields/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $age;\n    public string $firstName;\n    public ?string $lastName;\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages('Jason is 25 years old.')\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;get();\n\ndump($user);\n\nassert(!isset($user-&gt;lastName) || $user-&gt;lastName === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/public_vs_private/","title":"Private vs public object field","text":""},{"location":"cookbook/structured_outputs/basics/public_vs_private/#overview","title":"Overview","text":"<p>Instructor only sets accessible fields of the object with the data provided by LLM.</p> <p>Private and protected fields are left unchanged, unless:  - class has constructor with parameters matching one or more property names - in such    situation object will be hydrated with data from LLM via constructor params,  - class has getXxx() and setXxx() methods with xxx matching one of the property names -    in such situation object will be hydrated with data from LLM via setter methods</p> <p>If you want to access them directly after extraction, provide default values for them.</p>"},{"location":"cookbook/structured_outputs/basics/public_vs_private/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. His password is '123admin'.\n    TEXT;\n\n\n// CASE 1: Class with public fields\n\nclass User\n{\n    public string $name;\n    public int $age;\n    public string $password = '';\n}\n\n$user = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\n\necho \"User with public fields\\n\";\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 25);\nassert($user-&gt;password === '123admin');\n\n\n// CASE 2: Class with some private fields\n\nclass UserWithPrivateFields\n{\n    public string $name;\n    private int $age = 0;\n    private string $password = '';\n\n    public function getAge() : int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$userPriv = (new StructuredOutput)\n    -&gt;withMessages($text)\n    -&gt;withResponseClass(UserWithPrivateFields::class)\n    -&gt;get();\n\necho \"Private 'password' and 'age' fields are not hydrated by Instructor\\n\";\n\ndump($userPriv);\n\n// Private fields keep their default values (not hydrated by LLM)\nassert($userPriv-&gt;getAge() === 0);\nassert($userPriv-&gt;getPassword() === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/self_correction/","title":"Automatic correction based on validation results","text":""},{"location":"cookbook/structured_outputs/basics/self_correction/#overview","title":"Overview","text":"<p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p>"},{"location":"cookbook/structured_outputs/basics/self_correction/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidated;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationAttempt;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationFailed;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n$text = \"you can reply to me via jason wp.pl -- Jason\";\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"RESULTS:\\n\");\n$user = (new StructuredOutput)\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; print(\"[ ] Requesting LLM response...\\n\"))\n    -&gt;onEvent(ResponseValidationAttempt::class, fn($event) =&gt; print(\"[?] Validating:\\n    \".$event.\"\\n\"))\n    -&gt;onEvent(ResponseValidationFailed::class, fn($event) =&gt; print(\"[!] Validation failed:\\n    $event\\n\"))\n    -&gt;onEvent(ResponseValidated::class, fn($event) =&gt; print(\"[ ] Validation succeeded.\\n\"))\n    -&gt;with(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 3,\n    )-&gt;get();\n\nprint(\"\\nOUTPUT:\\n\");\n\ndump($user);\n\nassert($user-&gt;email === \"jason@wp.pl\");\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/using_config/","title":"Using LLM API connection presets from config file","text":""},{"location":"cookbook/structured_outputs/basics/using_config/#overview","title":"Overview","text":"<p>Instructor allows you to define multiple API connection presets in <code>llm.php</code> file. This is useful when you want to use different LLMs or API providers in your application.</p> <p>Connecting to LLM API via predefined connection is as simple as calling <code>withPreset</code> method with the preset name.</p>"},{"location":"cookbook/structured_outputs/basics/using_config/#configuration-file","title":"Configuration file","text":"<p>Default LLM configuration file is located in <code>/config/llm.php</code> in the root directory of Instructor codebase.</p> <p>You can set the location of the configuration file via <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable (comma-separated list of paths). You can use a copy of the default configuration file as a starting point.</p> <p>LLM config file defines available connection presets to LLM APIs and their parameters. It also specifies the default provider and parameters to be used when calling Instructor.</p>"},{"location":"cookbook/structured_outputs/basics/using_config/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Get Instructor object with client defined in config.php under 'presets/openai' key\n$structuredOutput = (new StructuredOutput)-&gt;using('openai');\n\n// Call with custom model and execution mode\n$user = $structuredOutput-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;get();\n\n// Use the results of LLM inference\ndump($user);\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/validation/","title":"Validation","text":""},{"location":"cookbook/structured_outputs/basics/validation/#overview","title":"Overview","text":"<p>Instructor uses validation to verify if the response generated by LLM meets the requirements of your response model. If the response does not meet the requirements, Instructor will throw an exception.</p> <p>Instructor uses Symfony's Validator component to validate the response, check their documentation for more information on the usage: https://symfony.com/doc/current/components/validator.html</p> <p>Following example demonstrates how to use Symfony Validator's constraints to validate the email field of response.</p>"},{"location":"cookbook/structured_outputs/basics/validation/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Exceptions\\ValidationException;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    #[Assert\\NotBlank]\n    /** Find user's email provided in the text or empty if it is missing */\n    public ?string $email;\n}\n\n$caughtException = false;\ntry {\n    $user = (new StructuredOutput)\n        -&gt;withResponseClass(UserDetails::class)\n        -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via mail -- Jason\"]])\n        -&gt;get();\n} catch (ValidationException $e) {\n    $caughtException = true;\n    echo \"Validation worked.\\n\";\n} catch (Throwable $e) {\n    // Catch any other exception\n    echo \"Validation failed with unexpected exception: {$e-&gt;getMessage()}\\n\";\n}\n\nassert($caughtException === true);\nassert(!isset($user));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/validation_multifield/","title":"Validation across multiple fields","text":""},{"location":"cookbook/structured_outputs/basics/validation_multifield/#overview","title":"Overview","text":"<p>Sometimes property level validation is not enough - you may want to check values of multiple properties and based on the combination of them decide to accept or reject the response. Or the assertions provided by Symfony may not be enough for your use case.</p> <p>In such case you can easily add custom validation code to your response model by: - using <code>ValidationMixin</code> - and defining validation logic in <code>validate()</code> method.</p> <p>In this example LLM should be able to correct typo in the message (graduation year we provided is <code>1010</code> instead of <code>2010</code>) and respond with correct graduation year.</p>"},{"location":"cookbook/structured_outputs/basics/validation_multifield/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $birthYear;\n    public int $graduationYear;\n\n    public function validate() : ValidationResult {\n        if ($this-&gt;graduationYear &gt; $this-&gt;birthYear) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'graduationYear',\n            value: $this-&gt;graduationYear,\n            message: \"Graduation year has to be bigger than birth year.\"\n        );\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($e) =&gt; $e-&gt;print())\n    -&gt;withResponseClass(UserDetails::class)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'Jason was born in 2000 and graduated in 18.']],\n        model: 'gpt-4o-mini',\n        maxRetries: 2,\n    )-&gt;get();\n\n\ndump($user);\n\nassert($user-&gt;graduationYear === 2018);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/basics/validation_with_llm/","title":"Validation with LLM","text":""},{"location":"cookbook/structured_outputs/basics/validation_with_llm/#overview","title":"Overview","text":"<p>You can use LLM capability to semantically process the context to validate the response following natural language instructions. This way you can implement more complex validation logic that would be difficult (or impossible) to achieve using traditional, code-based validation.</p>"},{"location":"cookbook/structured_outputs/basics/validation_with_llm/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    #[Description('User details in format: key=value')]\n    /** @var string[]  */\n    public array $details;\n\n    public function validate() : ValidationResult {\n        return match($this-&gt;hasPII()) {\n            true =&gt; ValidationResult::fieldError(\n                field: 'details',\n                value: implode('\\n', $this-&gt;details),\n                message: \"Details contain PII, remove it from the response.\"\n            ),\n            false =&gt; ValidationResult::valid(),\n        };\n    }\n\n    private function hasPII() : bool {\n        $data = implode('\\n', $this-&gt;details);\n        return (new StructuredOutput)\n            -&gt;with(\n                messages: \"Context:\\n$data\\n\",\n                responseModel: Scalar::boolean('hasPII', 'Does the context contain any PII?'),\n            )\n            -&gt;getBoolean();\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    My name is Jason. I am is 25 years old. I am developer.\n    My phone number is +1 123 34 45 and social security number is 123-45-6789\n    TEXT;\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn(Event $e) =&gt; $e-&gt;print()) // let's check the internals of Instructor processing\n    -&gt;with(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )-&gt;get();\n\ndump($user);\n\nassert(!Str::contains(implode('\\n', $user-&gt;details), '123-45-6789'));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/complex_extraction/","title":"Extraction of complex objects","text":""},{"location":"cookbook/structured_outputs/extras/complex_extraction/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text.</p>"},{"location":"cookbook/structured_outputs/extras/complex_extraction/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = new StructuredOutput;\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'gpt-4o-mini',\n        options: [\n            'max_tokens' =&gt; 16000,\n        ],\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        mode: OutputMode::Json,\n    )\n    //-&gt;withDebugPreset('on')\n    -&gt;withStreaming()\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/complex_extraction_claude/","title":"Extraction of complex objects (Anthropic)","text":""},{"location":"cookbook/structured_outputs/extras/complex_extraction_claude/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Anthropic Claude 3 model.</p>"},{"location":"cookbook/structured_outputs/extras/complex_extraction_claude/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\nclass ProjectEvents {\n    /**\n     * List of events extracted from the text\n     * @var ProjectEvent[]\n     */\n    public array $events = [];\n}\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('anthropic');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'claude-haiku-4-5', // 'claude-3-haiku-20240307'\n        prompt: 'Extract a list of project events with all the details from the provided input in JSON format using schema: &lt;|json_schema|&gt;',\n        mode: OutputMode::Tools,\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        options: [\n            'max_tokens' =&gt; 4096,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n//dump($events-&gt;list);\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/complex_extraction_cohere/","title":"Extraction of complex objects (Cohere)","text":""},{"location":"cookbook/structured_outputs/extras/complex_extraction_cohere/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Cohere R models.</p>"},{"location":"cookbook/structured_outputs/extras/complex_extraction_cohere/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('cohere');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'command-r-plus-08-2024',\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        mode: OutputMode::JsonSchema,\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/complex_extraction_gemini/","title":"Extraction of complex objects (Gemini)","text":""},{"location":"cookbook/structured_outputs/extras/complex_extraction_gemini/#overview","title":"Overview","text":"<p>This is an example of extraction of a very complex structure from the provided text with Google Gemini model.</p>"},{"location":"cookbook/structured_outputs/extras/complex_extraction_gemini/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$structuredOutput = (new StructuredOutput)-&gt;using('gemini');\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $structuredOutput\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    //-&gt;onEvent(PartialInferenceResponseReceived::class, fn(PartialInferenceResponseReceived $e) =&gt; print \"---\\n\".$e-&gt;partialInferenceResponse-&gt;content().\"---\\n\")\n    -&gt;with(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        //model: 'gemini-1.5-flash',\n        //model: 'gemini-2.0-flash-exp',\n        //model: 'gemini-2.0-flash-thinking-exp',\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ],\n        mode: OutputMode::Json,\n    )-&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/","title":"Custom Content Extractors","text":""},{"location":"cookbook/structured_outputs/extras/custom_extractor/#overview","title":"Overview","text":"<p>Instructor uses a pluggable extraction system to parse structured content from LLM responses. Different LLMs and output modes may return content in various formats - wrapped in markdown, embedded in explanatory text, or with trailing commas.</p> <p>You can create custom extractors to handle specific response formats from your LLM or API. Extractors are tried in order until one succeeds.</p>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/#built-in-extractors","title":"Built-in Extractors","text":"<p>Instructor provides these content extractors:</p> <ul> <li><code>DirectJsonExtractor</code> - Parses content directly as JSON (fastest)</li> <li><code>BracketMatchingExtractor</code> - Finds JSON by matching first <code>{</code> to last <code>}</code></li> <li><code>MarkdownBlockExtractor</code> - Extracts from markdown code blocks</li> <li><code>ResilientJsonExtractor</code> - Handles trailing commas, missing braces</li> <li><code>SmartBraceExtractor</code> - Smart brace matching with string escaping</li> </ul>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/#example-custom-xml-wrapper-extractor","title":"Example: Custom XML Wrapper Extractor","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\Extraction\\Contracts\\CanExtractResponse;\nuse Cognesy\\Instructor\\Extraction\\Data\\ExtractionInput;\nuse Cognesy\\Instructor\\Extraction\\Exceptions\\ExtractionException;\nuse Cognesy\\Instructor\\Extraction\\Extractors\\DirectJsonExtractor;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Custom extractor that extracts JSON from XML-like wrappers.\n *\n * Some LLMs or custom APIs return JSON wrapped in XML tags like:\n * &lt;response&gt;&lt;json&gt;{\"name\":\"John\"}&lt;/json&gt;&lt;/response&gt;\n */\nclass XmlJsonExtractor implements CanExtractResponse\n{\n    public function __construct(\n        private string $tagName = 'json',\n    ) {}\n\n    #[\\Override]\n    public function extract(ExtractionInput $input): array\n    {\n        // Match: &lt;json&gt;{\"key\": \"value\"}&lt;/json&gt;\n        $pattern = sprintf('/&lt;%s&gt;(.*?)&lt;\\/%s&gt;/s', $this-&gt;tagName, $this-&gt;tagName);\n\n        if (!preg_match($pattern, $input-&gt;content, $matches)) {\n            throw new ExtractionException(\"No &lt;{$this-&gt;tagName}&gt; wrapper found\");\n        }\n\n        $json = trim($matches[1]);\n        if ($json === '') {\n            throw new ExtractionException(\"Empty &lt;{$this-&gt;tagName}&gt; wrapper\");\n        }\n\n        try {\n            $decoded = json_decode($json, associative: true, flags: JSON_THROW_ON_ERROR);\n        } catch (\\JsonException $e) {\n            throw new ExtractionException(\"Invalid JSON in &lt;{$this-&gt;tagName}&gt;: {$e-&gt;getMessage()}\", $e);\n        }\n\n        if (!is_array($decoded)) {\n            throw new ExtractionException(\"Expected object or array in &lt;{$this-&gt;tagName}&gt;\");\n        }\n\n        return $decoded;\n    }\n\n    #[\\Override]\n    public function name(): string\n    {\n        return 'xml_json_extractor';\n    }\n}\n\n// Define schema\nclass Person {\n    public string $name;\n    public int $age;\n    public string $city;\n}\n\n// Simulate an LLM response with XML-wrapped JSON\n$xmlWrappedResponse = &lt;&lt;&lt;EOT\nHere is the extracted information:\n\n&lt;json&gt;\n{\n    \"name\": \"Alice Johnson\",\n    \"age\": 28,\n    \"city\": \"San Francisco\"\n}\n&lt;/json&gt;\n\nThe data has been successfully extracted from the input.\nEOT;\n\necho \"=== Example 1: Custom extractor for XML-wrapped JSON (sync) ===\\n\\n\";\necho \"Raw LLM response:\\n\";\necho str_repeat('-', 50) . \"\\n\";\necho $xmlWrappedResponse . \"\\n\";\necho str_repeat('-', 50) . \"\\n\\n\";\n\n// Use custom extractors\n// DirectJson is tried first (will fail), then XmlJsonExtractor (will succeed)\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withExtractors(\n        new DirectJsonExtractor(),      // Try direct parsing first\n        new XmlJsonExtractor('json'),   // Fall back to XML wrapper extraction\n    )\n    -&gt;withMessages(\"Extract: Alice Johnson, 28 years old, lives in San Francisco\")\n    -&gt;get();\n\ndump($person);\n\necho \"\\nExtracted data:\\n\";\necho \"Name: {$person-&gt;name}\\n\";\necho \"Age: {$person-&gt;age}\\n\";\necho \"City: {$person-&gt;city}\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/#expected-output","title":"Expected Output","text":"<pre><code>=== Demonstrating Custom Extraction Strategy ===\n\nRaw LLM response:\n--------------------------------------------------\nHere is the extracted information:\n\n&lt;json&gt;\n{\n    \"name\": \"Alice Johnson\",\n    \"age\": 28,\n    \"city\": \"San Francisco\"\n}\n&lt;/json&gt;\n\nThe data has been successfully extracted from the input.\n--------------------------------------------------\n\nPerson {\n  +name: \"Alice Johnson\"\n  +age: 28\n  +city: \"San Francisco\"\n}\n\nExtracted data:\nName: Alice Johnson\nAge: 28\nCity: San Francisco\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/#streaming-with-custom-extractors","title":"Streaming with Custom Extractors","text":"<p>Custom extractors are automatically used for both sync and streaming modes. The <code>ResponseExtractor</code> handles buffer creation internally, using a subset of extractors optimized for streaming (fast extractors by default).</p> <pre><code>&lt;?php\n// Custom extractors work for streaming too\n$stream = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withExtractors(\n        new DirectJsonExtractor(),\n        new XmlJsonExtractor('json'),\n    )\n    -&gt;withMessages(\"Extract person data...\")\n    -&gt;stream();\n\nforeach ($stream-&gt;responses() as $partial) {\n    echo \"Partial: \" . ($partial-&gt;name ?? '...') . \"\\n\";\n}\n\n$person = $stream-&gt;finalValue();\necho \"Final: {$person-&gt;name}\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/#creating-your-own-extractor","title":"Creating Your Own Extractor","text":"<p>Implement <code>CanExtractResponse</code> interface:</p> <pre><code>use Cognesy\\Instructor\\Extraction\\Contracts\\CanExtractResponse;\nuse Cognesy\\Instructor\\Extraction\\Data\\ExtractionInput;\nuse Cognesy\\Instructor\\Extraction\\Exceptions\\ExtractionException;\n\nclass MyCustomExtractor implements CanExtractResponse\n{\n    public function extract(ExtractionInput $input): array\n    {\n        // Your extraction logic here\n        // Return decoded array on success\n        // Throw ExtractionException on failure\n    }\n\n    public function name(): string\n    {\n        return 'my_custom';  // For logging/debugging\n    }\n}\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/custom_extractor/#extractor-chain-behavior","title":"Extractor Chain Behavior","text":"<p>Extractors are tried in order until one succeeds:</p> <ol> <li>First extractor is called with ExtractionInput</li> <li>If it returns an array, extraction is complete</li> <li>If it throws ExtractionException, next extractor is tried</li> <li>If all fail, an error is raised</li> </ol> <p>This allows graceful degradation - try fast/simple extractors first, fall back to more complex ones only when needed.</p>"},{"location":"cookbook/structured_outputs/extras/image_car_damage/","title":"Image processing - car damage detection","text":""},{"location":"cookbook/structured_outputs/extras/image_car_damage/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>In this example we will be extracting structured data from an image of a car with visible damage. The response model will contain information about the location of the damage and the type of damage.</p>"},{"location":"cookbook/structured_outputs/extras/image_car_damage/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/structured_outputs/extras/image_car_damage/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Schema\\Attributes\\Description;\nuse Cognesy\\Utils\\Str;\n\nenum DamageSeverity : string {\n    case Minor = 'minor';\n    case Moderate = 'moderate';\n    case Severe = 'severe';\n    case Total = 'total';\n}\n\nenum DamageLocation : string {\n    case Front = 'front';\n    case Rear = 'rear';\n    case Left = 'left';\n    case Right = 'right';\n    case Top = 'top';\n    case Bottom = 'bottom';\n}\n\nclass Damage {\n    #[Description('Identify damaged element')]\n    public string $element;\n    /** @var DamageLocation[] */\n    public array $locations;\n    public DamageSeverity $severity;\n    public string $description;\n}\n\nclass DamageAssessment {\n    public string $make;\n    public string $model;\n    public string $bodyColor;\n    /** @var Damage[] */\n    public array $damages = [];\n    public string $summary;\n}\n\n$assessment = Image::fromFile(__DIR__ . '/car-damage.jpg')\n    -&gt;toData(\n        responseModel: DamageAssessment::class,\n        prompt: 'Identify and assess each car damage location and severity separately.',\n        connection: 'openai',\n        model: 'gpt-4o-mini',\n        options: ['max_tokens' =&gt; 4096]\n    );\n\ndump($assessment);\nassert(Str::contains($assessment-&gt;make, 'Toyota', false));\nassert(Str::contains($assessment-&gt;model, 'Prius', false));\nassert(Str::contains($assessment-&gt;bodyColor, 'white', false));\nassert(count($assessment-&gt;damages) &gt; 0);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/image_to_data/","title":"Image to data (OpenAI)","text":""},{"location":"cookbook/structured_outputs/extras/image_to_data/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/structured_outputs/extras/image_to_data/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/structured_outputs/extras/image_to_data/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float|int $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public float|int|null $subtotal;\n    public float|int|null $tax;\n    public float|int|null $tip;\n    public float|int $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt.',\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert($receipt-&gt;total === 169.82);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/image_to_data_anthropic/","title":"Image to data (Anthropic)","text":""},{"location":"cookbook/structured_outputs/extras/image_to_data_anthropic/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/structured_outputs/extras/image_to_data_anthropic/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/structured_outputs/extras/image_to_data_anthropic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float|int $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public float|int|null $subtotal;\n    public float|int|null $tax;\n    public float|int|null $tip;\n    public float|int $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;using('anthropic')-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt. Return result as JSON following this schema: &lt;|json_schema|&gt;',\n    model: 'claude-haiku-4-5',\n    mode: OutputMode::Json,\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert(is_numeric($receipt-&gt;total));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/image_to_data_gemini/","title":"Image to data (Gemini)","text":""},{"location":"cookbook/structured_outputs/extras/image_to_data_gemini/#overview","title":"Overview","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p>"},{"location":"cookbook/structured_outputs/extras/image_to_data_gemini/#scanned-image","title":"Scanned image","text":"<p>Here's the image we're going to extract data from.</p> <p></p>"},{"location":"cookbook/structured_outputs/extras/image_to_data_gemini/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Addons\\Image\\Image;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float|int $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public float|int|null $subtotal;\n    public float|int|null $tax;\n    public float|int|null $tip;\n    public float|int $total;\n}\n\n$receipt = (new StructuredOutput)-&gt;using('gemini')-&gt;with(\n    messages: Image::fromFile(__DIR__ . '/receipt.png')-&gt;toMessage(),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt. Return result as JSON following this schema: &lt;|json_schema|&gt;',\n    mode: OutputMode::Json,\n    options: ['max_tokens' =&gt; 4096]\n)-&gt;get();\n\ndump($receipt);\n\nassert(is_numeric($receipt-&gt;total));\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/json_schema/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/structured_outputs/extras/json_schema/#overview","title":"Overview","text":"<p>Instructor has a built-in support for dynamically constructing JSON Schema using <code>JsonSchema</code> class. It is useful when you want to shape the structures during runtime.</p>"},{"location":"cookbook/structured_outputs/extras/json_schema/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'User name'),\n        JsonSchema::integer('age', 'User age'),\n    ],\n    requiredProperties: ['name', 'age'],\n);\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on')\n    -&gt;withMessages(\"Jason is 25 years old and works as an engineer\")\n    -&gt;withResponseJsonSchema($schema)\n    -&gt;withDeserializers()\n    -&gt;withDefaultToStdClass()\n    -&gt;get();\n\ndump($user);\n\nassert(gettype($user) === 'object');\nassert(get_class($user) === 'stdClass');\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_array/","title":"Return extracted data as array","text":""},{"location":"cookbook/structured_outputs/extras/output_format_array/#overview","title":"Overview","text":"<p>By default, Instructor deserializes extracted data into PHP objects. Sometimes you may want to work with raw associative arrays instead - for example, when storing data in a database, passing to a JSON API, or when you don't need the overhead of object instantiation.</p> <p>The <code>intoArray()</code> method allows you to use a PHP class to define the schema (structure and validation sent to the LLM) while receiving the result as a plain associative array.</p>"},{"location":"cookbook/structured_outputs/extras/output_format_array/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Define schema as a class (sent to LLM for structure/validation)\nclass Person {\n    public string $name;\n    public int $age;\n    public string $occupation;\n}\n\n// Extract data and receive as array instead of object\n$personArray = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)  // Schema definition\n    -&gt;intoArray()                        // Return as array\n    -&gt;withMessages(\"Jason is 25 years old and works as a software engineer.\")\n    -&gt;get();\n\ndump($personArray);\n\n// Result is a plain associative array\nassert(is_array($personArray));\nassert($personArray['name'] === 'Jason');\nassert($personArray['age'] === 25);\nassert($personArray['occupation'] === 'software engineer');\n\n// No object instantiation occurred\nassert(!is_object($personArray));\n\necho \"\\nExtracted data as array:\\n\";\necho \"Name: {$personArray['name']}\\n\";\necho \"Age: {$personArray['age']}\\n\";\necho \"Occupation: {$personArray['occupation']}\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_array/#expected-output","title":"Expected Output","text":"<pre><code>array(3) {\n  'name' =&gt;\n  string(5) \"Jason\"\n  'age' =&gt;\n  int(25)\n  'occupation' =&gt;\n  string(18) \"software engineer\"\n}\n\nExtracted data as array:\nName: Jason\nAge: 25\nOccupation: software engineer\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_instance_of/","title":"Use different class for schema and output","text":""},{"location":"cookbook/structured_outputs/extras/output_format_instance_of/#overview","title":"Overview","text":"<p>Sometimes you want to define the extraction schema using one class but receive the result as a different class. This is useful when:</p> <ul> <li>You have a rich domain model for the LLM schema but want a simpler DTO for output</li> <li>You want to separate API contracts from internal representations</li> <li>You need different validation rules for input vs output</li> </ul> <p>The <code>intoInstanceOf()</code> method allows you to specify a different target class for deserialization while keeping the original class for schema generation.</p>"},{"location":"cookbook/structured_outputs/extras/output_format_instance_of/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Rich schema class with detailed structure (sent to LLM)\nclass UserProfile {\n    public string $fullName;\n    public int $age;\n    public string $email;\n    public string $phoneNumber;\n    public string $address;\n}\n\n// Simplified DTO for output (subset of fields you need)\nclass UserDTO {\n    public function __construct(\n        public string $fullName = '',\n        public string $email = '',\n    ) {}\n}\n\n// Extract using UserProfile schema, but receive as UserDTO\n$user = (new StructuredOutput)\n    -&gt;withResponseClass(UserProfile::class)      // Schema sent to LLM\n    -&gt;intoInstanceOf(UserDTO::class)             // Output class\n    -&gt;with(\n        messages: \"Extract: John Smith, 30 years old, john@example.com, phone: 555-1234, lives at 123 Main St\",\n    )\n    -&gt;get();\n\ndump($user);\n\n// Result is UserDTO instance (not UserProfile)\nassert($user instanceof UserDTO);\nassert(!($user instanceof UserProfile));\n\n// UserDTO has only the fields it needs\nassert($user-&gt;fullName === 'John Smith');\nassert($user-&gt;email === 'john@example.com');\n\n// UserDTO doesn't have the extra fields from UserProfile\nassert(!property_exists($user, 'age'));\nassert(!property_exists($user, 'phoneNumber'));\nassert(!property_exists($user, 'address'));\n\necho \"\\nExtracted as UserDTO:\\n\";\necho \"Name: {$user-&gt;fullName}\\n\";\necho \"Email: {$user-&gt;email}\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_instance_of/#expected-output","title":"Expected Output","text":"<pre><code>object(UserDTO)#123 (2) {\n  [\"fullName\"]=&gt;\n  string(10) \"John Smith\"\n  [\"email\"]=&gt;\n  string(17) \"john@example.com\"\n}\n\nExtracted as UserDTO:\nName: John Smith\nEmail: john@example.com\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_instance_of/#note","title":"Note","text":"<p>The LLM receives the UserProfile schema (with 5 fields: name, age, email, phone, address), but the result is deserialized into UserDTO (with only 2 fields: name, email). Extra fields that don't exist in UserDTO are ignored during deserialization.</p>"},{"location":"cookbook/structured_outputs/extras/output_format_streaming/","title":"Streaming with array output format","text":""},{"location":"cookbook/structured_outputs/extras/output_format_streaming/#overview","title":"Overview","text":"<p>When streaming responses, you often want real-time updates as objects (for validation and deduplication), but the final result as an array (for database storage or API responses).</p> <p>The <code>intoArray()</code> method works seamlessly with streaming - partial updates are objects during streaming, but the final value is returned as an array.</p>"},{"location":"cookbook/structured_outputs/extras/output_format_streaming/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Define schema for extraction\nclass Article {\n    public string $title;\n    public string $author;\n    public int $wordCount;\n    /** @var string[] */\n    public array $tags;\n}\n\necho \"Streaming article extraction...\\n\\n\";\n\n// Track when chunks arrive to prove streaming\n$startTime = microtime(true);\n$chunkTimes = [];\n\n// Stream extraction and receive final result as array\n$stream = (new StructuredOutput)\n    -&gt;withResponseClass(Article::class)\n    -&gt;intoArray()\n    -&gt;withMessages(\"Extract: 'Introduction to PHP 8.4' by Jane Doe, 1500 words, tags: php, tutorial, programming\")\n    -&gt;stream();\n\n// During streaming, partials are objects (for validation)\nforeach ($stream-&gt;responses() as $response) {\n    dump($response);\n}\n\n// Final result is an array (not object)\n$finalArticle = $stream-&gt;finalValue();\ndump($finalArticle);\n\nassert(is_array($finalArticle));\nassert(is_string($finalArticle['title']) &amp;&amp; $finalArticle['title'] !== '');\nassert(is_string($finalArticle['author']) &amp;&amp; $finalArticle['author'] !== '');\nassert(is_int($finalArticle['wordCount']) &amp;&amp; $finalArticle['wordCount'] &gt; 0);\nassert(is_array($finalArticle['tags']));\nassert(in_array('php', $finalArticle['tags']));\n\necho \"\\nFinal result (array):\\n\";\necho \"Title: {$finalArticle['title']}\\n\";\necho \"Author: {$finalArticle['author']}\\n\";\necho \"Words: {$finalArticle['wordCount']}\\n\";\necho \"Tags: \" . implode(', ', $finalArticle['tags']) . \"\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_streaming/#expected-output","title":"Expected Output","text":"<pre><code>Streaming article extraction...\n\nUpdate #1: Article\nUpdate #2: Article\nUpdate #3: Article\n\narray(4) {\n  [\"title\"]=&gt;\n  string(26) \"Introduction to PHP 8.4\"\n  [\"author\"]=&gt;\n  string(8) \"Jane Doe\"\n  [\"wordCount\"]=&gt;\n  int(1500)\n  [\"tags\"]=&gt;\n  array(3) {\n    [0]=&gt;\n    string(3) \"php\"\n    [1]=&gt;\n    string(8) \"tutorial\"\n    [2]=&gt;\n    string(11) \"programming\"\n  }\n}\n\nFinal result (array):\nTitle: Introduction to PHP 8.4\nAuthor: Jane Doe\nWords: 1500\nTags: php, tutorial, programming\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/output_format_streaming/#how-it-works","title":"How It Works","text":"<ol> <li>During streaming: Partial updates are deserialized as objects for real-time    validation and deduplication</li> <li>After streaming completes: The final result is re-extracted and returned    as an array (respecting <code>intoArray()</code>)</li> <li>Best of both worlds: Object validation during streaming, array convenience    for the final result</li> </ol>"},{"location":"cookbook/structured_outputs/extras/pure_array_processing/","title":"Pure Array Processing (No Classes)","text":""},{"location":"cookbook/structured_outputs/extras/pure_array_processing/#overview","title":"Overview","text":"<p>This example demonstrates extraction using ONLY arrays - no PHP classes, no serialization, no deserialization. Just JSON Schema definition and array output.</p> <p>This is useful when: - Working with dynamic schemas defined at runtime - Avoiding class creation overhead - Integrating with systems that expect plain arrays - Building schema-driven extraction pipelines</p>"},{"location":"cookbook/structured_outputs/extras/pure_array_processing/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\necho \"=== PURE ARRAY PROCESSING (NO CLASSES) ===\\n\\n\";\n\n// Define schema using JsonSchema fluent API - NO PHP CLASS NEEDED\n$articleSchema = JsonSchema::object(\n    name: 'article',\n    description: 'An article with metadata',\n    properties: [\n        JsonSchema::string('title', 'Article title'),\n        JsonSchema::string('author', 'Author name'),\n        JsonSchema::integer('wordCount', 'Word count'),\n        JsonSchema::array('tags', JsonSchema::string(), 'Article tags'),\n    ],\n    requiredProperties: ['title', 'author', 'wordCount', 'tags'],\n);\n\n// Extract data - no streaming, just simple get()\n$article = (new StructuredOutput)\n    -&gt;with(\n        messages: \"Extract: 'Introduction to PHP 8.4' by Jane Doe, 1500 words, tags: php, tutorial, programming\",\n        responseModel: $articleSchema,  // &lt;-- JsonSchema object, no class!\n    )\n    -&gt;intoArray()  // Output as pure array\n    -&gt;get();\n\necho \"=== RESULT (PURE ARRAY) ===\\n\";\ndump($article);\n\n// Verify it's a pure array - no objects involved\nassert(is_array($article), 'Result must be array');\nassert(!is_object($article), 'Result must NOT be object');\nassert(is_string($article['title']), 'Title must be string');\nassert(is_string($article['author']), 'Author must be string');\nassert(is_int($article['wordCount']), 'WordCount must be int');\nassert(is_array($article['tags']), 'Tags must be array');\n\n// All array values are primitives\necho \"\\nField types:\\n\";\nforeach ($article as $key =&gt; $value) {\n    $type = gettype($value);\n    echo \"  $key: $type\\n\";\n    assert(\n        in_array($type, ['string', 'integer', 'array', 'double', 'boolean']),\n        \"All values must be primitives, got $type for $key\"\n    );\n}\n\necho \"\\n\u2713 Pure array processing verified - no classes, no serialization!\\n\";\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/pure_array_processing/#expected-output","title":"Expected Output","text":"<pre><code>=== PURE ARRAY PROCESSING (NO CLASSES) ===\n\n=== RESULT (PURE ARRAY) ===\narray:4 [\n  \"title\" =&gt; \"Introduction to PHP 8.4\"\n  \"author\" =&gt; \"Jane Doe\"\n  \"wordCount\" =&gt; 1500\n  \"tags\" =&gt; array:3 [\n    0 =&gt; \"php\"\n    1 =&gt; \"tutorial\"\n    2 =&gt; \"programming\"\n  ]\n]\n\nField types:\n  title: string\n  author: string\n  wordCount: integer\n  tags: array\n\n\u2713 Pure array processing verified - no classes, no serialization!\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/pure_array_processing/#how-it-works","title":"How It Works","text":"<ol> <li>Schema definition: <code>JsonSchema</code> fluent API defines the expected structure</li> <li>No PHP classes: No <code>Article</code> class, no <code>@var</code> annotations, no type hints</li> <li>intoArray(): Forces output to be plain PHP array</li> <li>Result: Pure associative array with primitive values</li> </ol>"},{"location":"cookbook/structured_outputs/extras/pure_array_processing/#jsonschema-api","title":"JsonSchema API","text":"<pre><code>// Basic types\nJsonSchema::string('name', 'description')\nJsonSchema::integer('age', 'description')\nJsonSchema::number('price', 'description')\nJsonSchema::boolean('active', 'description')\n\n// Arrays\nJsonSchema::array('tags', JsonSchema::string(), 'description')\n\n// Objects (nested)\nJsonSchema::object('address', [\n    JsonSchema::string('street'),\n    JsonSchema::string('city'),\n], requiredProperties: ['street', 'city'])\n\n// Enums\nJsonSchema::enum('status', ['pending', 'active', 'closed'])\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/pure_array_processing/#use-cases","title":"Use Cases","text":"<ul> <li>Dynamic schemas: Define extraction shapes at runtime</li> <li>API-driven extraction: Schema comes from external API/database</li> <li>No-class pipelines: Avoid PHP class overhead entirely</li> <li>Array-first architectures: When your system expects arrays throughout</li> </ul>"},{"location":"cookbook/structured_outputs/extras/schema/","title":"Generating JSON Schema from PHP classes","text":""},{"location":"cookbook/structured_outputs/extras/schema/#overview","title":"Overview","text":"<p>Instructor has a built-in support for generating JSON Schema from the classes or objects. This is useful as it helps you avoid writing the JSON Schema manually, which can be error-prone and time-consuming.</p>"},{"location":"cookbook/structured_outputs/extras/schema/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Schema\\Factories\\SchemaFactory;\n\nclass City {\n    public string $name;\n    public int $population;\n    public int $founded;\n}\n\n$schema = (new SchemaFactory)-&gt;schema(City::class);\n\n$city = (new StructuredOutput)-&gt;with(\n    messages: \"What is capital of France\",\n    responseModel: $schema,\n)-&gt;get();\n\ndump($city);\n\nassert(gettype($city) === 'object');\nassert(get_class($city) === 'City');\nassert($city-&gt;name === 'Paris');\nassert(is_int($city-&gt;population));\nassert(is_int($city-&gt;founded));\n\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/schema_dynamic/","title":"Generating JSON Schema dynamically","text":""},{"location":"cookbook/structured_outputs/extras/schema_dynamic/#overview","title":"Overview","text":"<p>Instructor has a built-in support for generating JSON Schema from dynamic objects with <code>Structure</code> class.</p> <p>This is useful when the data model is built during runtime or defined by your app users.</p> <p><code>Structure</code> helps you flexibly design and modify data models that can change with every request or user input and allows you to generate JSON Schema for them.</p>"},{"location":"cookbook/structured_outputs/extras/schema_dynamic/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$city = Structure::define('city', [\n    Field::string('name', 'City name')-&gt;required(),\n    Field::int('population', 'City population')-&gt;required(),\n    Field::int('founded', 'Founding year')-&gt;required(),\n]);\n\n$data = (new StructuredOutput)\n    -&gt;using('openai')\n    //-&gt;withDebugPreset('on')\n    -&gt;intoArray()\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? \\\n        Respond with JSON data.']])\n    -&gt;withResponseJsonSchema($city-&gt;toJsonSchema())\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;withOutputMode(OutputMode::JsonSchema)\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT:\\n\";\ndump($data);\n\nassert(is_array($data), 'Response should be an array');\nassert(isset($data['name']), 'Response should have \"name\" field');\nassert(strpos($data['name'], 'Paris') !== false, 'City name should be Paris');\nassert(isset($data['population']), 'Response should have \"population\" field');\nassert(isset($data['founded']), 'Response should have \"founded\" field');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/streaming_structured_openai_responses/","title":"Streaming (Structured Output, OpenAI Responses)","text":""},{"location":"cookbook/structured_outputs/extras/streaming_structured_openai_responses/#overview","title":"Overview","text":"<p>A minimal structured-output streaming example using the <code>openai-responses</code> preset. The example verifies that streaming yields partial updates and that we receive the expected final fields.</p>"},{"location":"cookbook/structured_outputs/extras/streaming_structured_openai_responses/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\Cli\\Console;\nuse Cognesy\\Utils\\Str;\n\nclass PersonProfile\n{\n    public string $name = '';\n    public int $age = 0;\n    /** @var string[] */\n    public array $hobbies = [];\n}\n\n$text = &lt;&lt;&lt;TEXT\nJason is 25 years old. He lives in San Francisco. He enjoys soccer, climbing,\nand cooking.\nTEXT;\n\n$partialsCount = 0;\n$onPartialUpdate = function (object $partial) use (&amp;$partialsCount): void {\n    $partialsCount += 1;\n    Console::clearScreen();\n    echo \"Partial update #{$partialsCount}:\\n\";\n    dump($partial);\n};\n\n$profile = (new StructuredOutput)\n    -&gt;using('openai-responses')\n    // -&gt;withHttpDebugPreset('on') // enable HTTP stack debug output if needed\n    -&gt;withOutputMode(OutputMode::JsonSchema)\n    -&gt;withResponseClass(PersonProfile::class)\n    -&gt;withMessages($text)\n    -&gt;withOptions(['max_output_tokens' =&gt; 384])\n    -&gt;withStreaming()\n    -&gt;onPartialUpdate($onPartialUpdate)\n    -&gt;get();\n\necho \"All tokens received. Final structured profile:\\n\";\ndump($profile);\n\nassert($partialsCount &gt; 0, 'Expected at least one partial update');\nassert(Str::contains($profile-&gt;name, 'Jason'), 'Expected name Jason');\nassert($profile-&gt;age === 25, 'Expected age 25');\n\n$hobbiesLower = array_map(static fn(string $hobby): string =&gt; strtolower($hobby), $profile-&gt;hobbies);\nassert(in_array('soccer', $hobbiesLower, true), 'Expected hobby soccer');\nassert(in_array('climbing', $hobbiesLower, true), 'Expected hobby climbing');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/structured_input/","title":"Using structured data as an input","text":""},{"location":"cookbook/structured_outputs/extras/structured_input/#overview","title":"Overview","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>create()</code> method can be an object, but also an array or just a string.</p>"},{"location":"cookbook/structured_outputs/extras/structured_input/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translatedEmail = (new StructuredOutput)\n    -&gt;withInput($email)\n    -&gt;withResponseClass(Email::class)\n    -&gt;withPrompt('Translate the subject and body fields to Spanish. Keep the address field unchanged.')\n    -&gt;withModel('gpt-4o-mini')\n    -&gt;get();\n\nprint_r($translatedEmail);\n\nif ($translatedEmail-&gt;address !== $email-&gt;address) {\n    echo \"ERROR: Address was modified during translation\\n\";\n    exit(1);\n}\nif ($translatedEmail-&gt;subject === $email-&gt;subject) {\n    echo \"ERROR: Subject was not translated\\n\";\n    exit(1);\n}\nif ($translatedEmail-&gt;body === $email-&gt;body) {\n    echo \"ERROR: Body was not translated\\n\";\n    exit(1);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/transcription_to_tasks/","title":"Create tasks from meeting transcription","text":""},{"location":"cookbook/structured_outputs/extras/transcription_to_tasks/#overview","title":"Overview","text":"<p>This example demonstrates how you can create task assignments based on a transcription of meeting recording.</p>"},{"location":"cookbook/structured_outputs/extras/transcription_to_tasks/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nenum TaskStatus : string {\n    case Pending = 'pending';\n    case Completed = 'completed';\n}\n\nenum Role : string {\n    case PM = 'pm';\n    case Dev = 'dev';\n}\n\nclass Task {\n    public string $title;\n    public string $description;\n    public DateTimeImmutable $dueDate;\n    public Role $owner;\n    public TaskStatus $status;\n}\n\nclass Tasks {\n    public DateTime $meetingDate;\n    /** @var Task[] */\n    public array $tasks;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = &lt;&lt;&lt;TEXT\nTranscription of meeting from 2024-01-15, 16:00\n---\nPM: Hey, how's progress on the video transcription engine?\nDev: I've got basic functionality working, but accuracy isn't great yet. Might need to switch to a different API.\nPM: So the plan is to research alternatives and provide a comparison? Is it possible by Jan 20th?\nDev: Sure, I'll make it available before the meeting.\nPM: The one at 12?\nDev: Yes, at 12. By the way, are we still planning to support real-time transcription?\nPM: Yes, it's a key feature. Speaking of which, I need to update the product roadmap. I'll have that ready by Jan 18th.\nDev: Got it. I'll keep that in mind while evaluating APIs. Oh, and the UI for the summary view is ready for review.\nPM: Great, I'll take a look tomorrow by 10.\nTEXT;\n\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$tasks = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: Tasks::class,\n        //model: 'gpt-4o-mini',\n        mode: OutputMode::Json,\n    )\n    -&gt;get();\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($tasks);\n\nassert($tasks-&gt;meetingDate-&gt;format('Y-m-d') === '2024-01-15');\nassert(count($tasks-&gt;tasks) == 3);\n\nassert($tasks-&gt;tasks[0]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-20');\nassert($tasks-&gt;tasks[0]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[0]-&gt;owner === Role::Dev);\n\nassert($tasks-&gt;tasks[1]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-18');\nassert($tasks-&gt;tasks[1]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[1]-&gt;owner === Role::PM);\n\nassert($tasks-&gt;tasks[2]-&gt;dueDate-&gt;format('Y-m-d') === '2024-01-16');\nassert($tasks-&gt;tasks[2]-&gt;status === TaskStatus::Pending);\nassert($tasks-&gt;tasks[2]-&gt;owner === Role::PM);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/translate_ui_fields/","title":"Translating UI text fields","text":""},{"location":"cookbook/structured_outputs/extras/translate_ui_fields/#overview","title":"Overview","text":"<p>You can use Instructor to translate text fields in your UI. We can instruct the model to translate only the text fields from one language to another, but leave the other fields, like emails or URLs, unchanged.</p> <p>This example demonstrates how to translate text fields from English to German using structure-to-structure processing with LLM.</p>"},{"location":"cookbook/structured_outputs/extras/translate_ui_fields/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Validation\\Validators\\SymfonyValidator;\n\nclass TextElementModel\n{\n    public function __construct(\n        public string $headline = '',\n        public string $text = '',\n        public string $url = 'https://translation.com/'\n    ) {}\n}\n\n$sourceModel = new TextElementModel(\n    headline: 'This is my headline',\n    text: '&lt;p&gt;This is some WYSIWYG HTML content.&lt;/p&gt;'\n);\n\n$transformedModel = (new StructuredOutput)\n    -&gt;withInput($sourceModel)\n    -&gt;withResponseClass(get_class($sourceModel))\n    -&gt;withPrompt('Translate the headline and text fields to German. Keep HTML tags unchanged. Keep the url field unchanged.')\n    -&gt;withModel('gpt-4o-mini')\n    -&gt;withMaxRetries(2)\n    -&gt;withOptions(['temperature' =&gt; 0])\n    -&gt;withValidators(SymfonyValidator::class)\n    -&gt;get();\n\nprint_r($transformedModel);\n\n$hasGermanHeadline = str_contains($transformedModel-&gt;headline, '\u00dcberschrift')\n    || str_contains($transformedModel-&gt;headline, 'Schlagzeile');\nif (!$hasGermanHeadline) {\n    echo \"ERROR: Headline not translated to German\\n\";\n    exit(1);\n}\nif (!str_contains($transformedModel-&gt;text, '&lt;p&gt;')) {\n    echo \"ERROR: HTML tags not preserved in text\\n\";\n    exit(1);\n}\n$url = str_replace('\\/', '/', $transformedModel-&gt;url);\nif (!str_contains($url, 'https://translation.com/')) {\n    echo \"ERROR: URL was modified during translation\\n\";\n    exit(1);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/extras/web_to_objects/","title":"Web page to PHP objects","text":""},{"location":"cookbook/structured_outputs/extras/web_to_objects/#overview","title":"Overview","text":"<p>This example demonstrates how to extract structured data from a web page and get it as PHP object.</p>"},{"location":"cookbook/structured_outputs/extras/web_to_objects/#example","title":"Example","text":"<p>In this example we will be extracting list of Laravel companies from The Manifest website. The result will be a list of <code>Company</code> objects.</p> <p>We use Webpage extractor to get the content of the page and specify 'none' scraper, which means that we will be using built-in <code>file_get_contents</code> function to get the content of the page.</p> <p>In production environment you might want to use one of the supported scrapers:  - <code>browsershot</code>  - <code>scrapingbee</code>  - <code>scrapfly</code>  - <code>jinareader</code></p> <p>Commercial scrapers require API key, which can be set in the configuration file (<code>/config/web.php</code>).</p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Auxiliary\\Web\\Webpage;\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Schema\\Attributes\\Instructions;\n\nclass Company {\n    public string $name = '';\n    public string $location = '';\n    public string $description = '';\n    public int $minProjectBudget = 0;\n    public string $companySize = '';\n    #[Instructions('Remove any tracking parameters from the URL')]\n    public string $websiteUrl = '';\n    /** @var string[] */\n    public array $clients = [];\n}\n\n$companyGen = Webpage::withScraper('none')\n    -&gt;get('https://themanifest.com/pl/software-development/laravel/companies?page=1')\n    -&gt;cleanup()\n    -&gt;select('.directory-providers__list')\n    -&gt;selectMany(\n        selector: '.provider-card',\n        callback: fn($item) =&gt; $item-&gt;asMarkdown(),\n        limit: 3\n    );\n\n$companies = [];\necho \"Extracting company data from:\\n\\n\";\nforeach($companyGen as $companyDiv) {\n    /** @var string $companyDiv */\n    echo \" &gt; \" . substr($companyDiv, 0, 32) . \"...\\n\\n\";\n    $company = (new StructuredOutput)\n        -&gt;using('openai')\n        -&gt;with(\n            messages: $companyDiv,\n            responseModel: Company::class,\n            mode: OutputMode::Json\n        )-&gt;get();\n    $companies[] = $company;\n    dump($company);\n}\n\nassert(count($companies) === 3);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/cost_calculation/","title":"Calculating request cost","text":""},{"location":"cookbook/structured_outputs/troubleshooting/cost_calculation/#overview","title":"Overview","text":"<p>When using LLM APIs, tracking costs is essential for budgeting and optimization. Instructor supports automatic cost calculation based on token usage and pricing configuration in your LLM presets.</p> <p>This example demonstrates how to: 1. Configure pricing in LLM config ($/1M tokens) 2. Calculate cost after a request using <code>Usage::calculateCost()</code></p> <pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Data\\Pricing;\nuse Cognesy\\Polyglot\\Inference\\Data\\Usage;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Helper to display cost breakdown\nfunction printCostBreakdown(Usage $usage, Pricing $pricing): void {\n    echo \"Token Usage:\\n\";\n    echo \"  Input tokens:     {$usage-&gt;inputTokens}\\n\";\n    echo \"  Output tokens:    {$usage-&gt;outputTokens}\\n\";\n    echo \"  Cache read:       {$usage-&gt;cacheReadTokens}\\n\";\n    echo \"  Cache write:      {$usage-&gt;cacheWriteTokens}\\n\";\n    echo \"  Reasoning:        {$usage-&gt;reasoningTokens}\\n\";\n    echo \"\\nPricing ($/1M tokens):\\n\";\n    echo \"  Input:      \\${$pricing-&gt;inputPerMToken}\\n\";\n    echo \"  Output:     \\${$pricing-&gt;outputPerMToken}\\n\";\n    echo \"  Cache read: \\${$pricing-&gt;cacheReadPerMToken}\\n\";\n    echo \"\\nTotal cost: \\$\" . number_format($usage-&gt;calculateCost($pricing), 6) . \"\\n\";\n}\n\n// OPTION 1: Configure pricing in LLM config preset\n// In your config/llm.php, add pricing to your preset:\n//\n// 'openrouter-claude' =&gt; [\n//     'driver' =&gt; 'openrouter',\n//     'model' =&gt; 'anthropic/claude-3.5-sonnet',\n//     'pricing' =&gt; [\n//         'input' =&gt; 3.0,    // $3 per 1M input tokens\n//         'output' =&gt; 15.0,  // $15 per 1M output tokens\n//         // cacheRead, cacheWrite, reasoning default to input price if not set\n//     ],\n// ],\n//\n// Then calculateCost() works automatically:\n//\n// $response = (new StructuredOutput)\n//     -&gt;using('openrouter-claude')\n//     -&gt;with(messages: $text, responseModel: User::class)\n//     -&gt;response();\n// $cost = $response-&gt;usage()-&gt;calculateCost();\n\n// OPTION 2: Calculate cost manually with explicit Pricing\necho \"CALCULATING COST WITH EXPLICIT PRICING\\n\";\necho str_repeat(\"=\", 50) . \"\\n\\n\";\n\n$text = \"Jason is 25 years old and works as an engineer.\";\n\n$response = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n    )-&gt;response();\n\n// Define pricing for default model gpt-4.1-nano\n$pricing = Pricing::fromArray([\n    'input' =&gt; 0.2,     // $0.2 per 1M input tokens\n    'output' =&gt; 0.8,   // $0.8 per 1M output tokens\n    'cacheRead' =&gt; 0.05, // $0.05 per 1M output tokens\n]);\n\necho \"TEXT: $text\\n\\n\";\nprintCostBreakdown($response-&gt;usage(), $pricing);\n\n// You can also attach pricing to usage for later calculation\n$usageWithPricing = $response-&gt;usage()-&gt;withPricing($pricing);\necho \"\\nCost via stored pricing: \\$\" . number_format($usageWithPricing-&gt;calculateCost(), 6) . \"\\n\";\n\n\n// OPTION 3: Compare costs across different models\necho \"\\n\\n\" . str_repeat(\"=\", 50) . \"\\n\";\necho \"COST COMPARISON ACROSS MODELS\\n\";\necho str_repeat(\"=\", 50) . \"\\n\\n\";\n\n$usage = $response-&gt;usage();\n\n$models = [\n    'GPT-4o' =&gt; ['input' =&gt; 2.50, 'output' =&gt; 10.0],\n    'GPT-4o-mini' =&gt; ['input' =&gt; 0.15, 'output' =&gt; 0.60],\n    'Claude 3.5 Sonnet' =&gt; ['input' =&gt; 3.0, 'output' =&gt; 15.0],\n    'Claude 3.5 Haiku' =&gt; ['input' =&gt; 0.80, 'output' =&gt; 4.0],\n    'Gemini 2.0 Flash' =&gt; ['input' =&gt; 0.10, 'output' =&gt; 0.40],\n];\n\necho \"For {$usage-&gt;inputTokens} input + {$usage-&gt;outputTokens} output tokens:\\n\\n\";\nforeach ($models as $model =&gt; $prices) {\n    $pricing = Pricing::fromArray($prices);\n    $cost = $usage-&gt;calculateCost($pricing);\n    printf(\"  %-20s \\$%.6f\\n\", $model, $cost);\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/debugging/","title":"Debugging","text":""},{"location":"cookbook/structured_outputs/troubleshooting/debugging/#overview","title":"Overview","text":"<p>The <code>StructuredOutput</code> class has a <code>withDebug()</code> method that can be used to debug the request and response.</p> <p>It displays detailed information about the request being sent to LLM API and response received from it, including:  - request headers, URI, method and body,  - response status, headers, and body.</p> <p>This is useful for debugging the request and response when you are not getting the expected results.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/debugging/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Utils\\Str;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// CASE 1.1 - normal flow, sync request\n\n$structuredOutput = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withDebugPreset('on');\n    //-&gt;wiretap(fn($e) =&gt; $e-&gt;print());\n\necho \"\\n### CASE 1.1 - Debugging sync request\\n\\n\";\n$user = $structuredOutput\n    -&gt;with(\n        messages: \"Jason is 25 years old.\",\n        responseModel: User::class,\n        options: [ 'stream' =&gt; false ]\n    )\n    -&gt;get();\n\necho \"\\nResult:\\n\";\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n\n// CASE 1.2 - normal flow, streaming request\n\necho \"\\n### CASE 1.2 - Debugging streaming request\\n\\n\";\n$user2 = $structuredOutput\n    -&gt;with(\n        messages: \"Anna is 21 years old.\",\n        responseModel: User::class,\n        options: [ 'stream' =&gt; true ]\n    )\n    -&gt;get();\n\necho \"\\nResult:\\n\";\ndump($user2);\n\nassert(isset($user2-&gt;name));\nassert(isset($user2-&gt;age));\nassert($user2-&gt;name === 'Anna');\nassert($user2-&gt;age === 21);\n\n\n// CASE 2 - forcing API error via empty LLM config\n\n// let's initialize the instructor with an incorrect LLM config\n$structuredOutput = (new StructuredOutput)\n    -&gt;withLLMConfig(new LLMConfig(apiUrl: 'https://example.com'));\n\necho \"\\n### CASE 2 - Debugging with HTTP exception\\n\\n\";\ntry {\n    $user = $structuredOutput\n        -&gt;withDebugPreset('on')\n        -&gt;with(\n            messages: \"Jason is 25 years old.\",\n            responseModel: User::class,\n            options: [ 'stream' =&gt; true ]\n        )\n        -&gt;get();\n} catch (Exception $e) {\n    $msg = Str::limit($e-&gt;getMessage(), 250);\n    echo \"EXCEPTION WE EXPECTED:\\n\";\n    echo \"\\nCaught exception: \" . $msg . \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_laravel/","title":"Laravel Logging Integration","text":""},{"location":"cookbook/structured_outputs/troubleshooting/logging_laravel/#overview","title":"Overview","text":"<p>Laravel integration with Instructor's functional logging pipeline.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_laravel/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Logging\\Enrichers\\LazyEnricher;\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\PsrLoggerWriter;\nuse Illuminate\\Http\\Request;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\n\n// Mock Laravel request\n$request = Request::create('/api/extract');\n$request-&gt;headers-&gt;set('X-Request-ID', 'req_' . uniqid());\n\n// Create logger\n$logger = new Logger('instructor');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create pipeline with request context\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))  // Changed to debug to capture more events\n    -&gt;enrich(LazyEnricher::framework(fn() =&gt; [\n        'request_id' =&gt; $request-&gt;headers-&gt;get('X-Request-ID'),\n        'route' =&gt; '/api/extract',\n    ]))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputStarted::class =&gt;\n            '\ud83c\udfaf [LARAVEL] Starting extraction: {responseClass} (Request: {framework.request_id})',\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputResponseGenerated::class =&gt;\n            '\u2705 [LARAVEL] Completed extraction: {responseClass} (Request: {framework.request_id})',\n    ], channel: 'instructor'))\n    -&gt;write(new PsrLoggerWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udd27 Laravel logging pipeline configured\\n\";\necho \"\ud83d\udccb About to execute StructuredOutput with logging...\\n\\n\";\n\nclass User\n{\n    public int $age;\n    public string $name;\n}\n\n// Extract data with logging\necho \"\ud83d\ude80 Starting StructuredOutput extraction...\\n\";\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline)\n    -&gt;withMessages(\"Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\necho \"\\n\u2705 Extraction completed!\\n\";\necho \"\ud83d\udcca Result: User: {$user-&gt;name}, Age: {$user-&gt;age}\\n\";\n\n// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// [2025-12-07 01:18:13] instructor.DEBUG: \ud83d\udd04 [Laravel] Starting extraction: User\n// [2025-12-07 01:18:14] instructor.DEBUG: \u2705 [Laravel] Completed extraction: User\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_monolog/","title":"Monolog Logging with Functional Pipeline","text":""},{"location":"cookbook/structured_outputs/troubleshooting/logging_monolog/#overview","title":"Overview","text":"<p>Monolog integration with Instructor's functional logging pipeline.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_monolog/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\MonologChannelWriter;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\n\n// Create Monolog logger\n$logger = new Logger('instructor');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create logging pipeline\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputStarted::class =&gt;\n            '\ud83c\udfaf Starting extraction: {responseClass}',\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputResponseGenerated::class =&gt;\n            '\u2705 Completed extraction: {responseClass}',\n    ], channel: 'instructor'))\n    -&gt;write(new MonologChannelWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udccb About to demonstrate Monolog logging with functional pipeline...\\n\\n\";\n\nclass User\n{\n    public int $age;\n    public string $name;\n}\n\n// Extract data with logging\necho \"\ud83d\ude80 Starting StructuredOutput extraction...\\n\";\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline)\n    -&gt;withMessages(\"Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\necho \"\\n\u2705 Extraction completed!\\n\";\necho \"\ud83d\udcca Result: User: {$user-&gt;name}, Age: {$user-&gt;age}\\n\";\n\n// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// [2025-12-07T01:18:13.475202+00:00] instructor.DEBUG: \ud83c\udfaf Starting extraction: User\n// [2025-12-07T01:18:13.486832+00:00] instructor.DEBUG: HttpRequestSent\n// [2025-12-07T01:18:14.640213+00:00] instructor.DEBUG: HttpResponseReceived\n// [2025-12-07T01:18:14.659417+00:00] instructor.DEBUG: \u2705 Completed extraction: User\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_psr/","title":"PSR-3 Logging with Functional Pipeline","text":""},{"location":"cookbook/structured_outputs/troubleshooting/logging_psr/#overview","title":"Overview","text":"<p>Simple PSR-3 logging integration using Instructor's functional pipeline.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_psr/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\PsrLoggerWriter;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\n\n// Create PSR-3 logger\n$logger = new Logger('instructor');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create logging pipeline - filters to only StructuredOutput events\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputStarted::class =&gt;\n            '\ud83c\udfaf [PSR-3] Starting extraction: {responseClass}',\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputResponseGenerated::class =&gt;\n            '\u2705 [PSR-3] Completed extraction: {responseClass}',\n    ], channel: 'instructor'))\n    -&gt;write(new PsrLoggerWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udccb About to demonstrate PSR-3 logging with functional pipeline...\\n\\n\";\n\necho \"\ud83d\ude80 Starting StructuredOutput extraction...\\n\";\n\nclass User\n{\n    public int $age;\n    public string $name;\n}\n\n// Extract data with logging\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline)\n    -&gt;withMessages(\"Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\necho \"\\n\u2705 Extraction completed!\\n\";\necho \"\ud83d\udcca Result: User: {$user-&gt;name}, Age: {$user-&gt;age}\\n\";\n\n// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// [2025-12-07T01:18:13.475202+00:00] instructor.DEBUG: \ud83c\udfaf [PSR-3] Starting extraction: User\n// [2025-12-07T01:18:14.659417+00:00] instructor.DEBUG: \u2705 [PSR-3] Completed extraction: User\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_symfony/","title":"Symfony Logging Integration","text":""},{"location":"cookbook/structured_outputs/troubleshooting/logging_symfony/#overview","title":"Overview","text":"<p>Symfony integration with Instructor's functional logging pipeline.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/logging_symfony/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Logging\\Enrichers\\LazyEnricher;\nuse Cognesy\\Logging\\Filters\\LogLevelFilter;\nuse Cognesy\\Logging\\Formatters\\MessageTemplateFormatter;\nuse Cognesy\\Logging\\Pipeline\\LoggingPipeline;\nuse Cognesy\\Logging\\Writers\\PsrLoggerWriter;\nuse Monolog\\Handler\\StreamHandler;\nuse Monolog\\Logger;\nuse Symfony\\Component\\DependencyInjection\\Container;\nuse Symfony\\Component\\HttpFoundation\\Request;\n\n// Mock Symfony container and request\n$container = new Container();\n$request = Request::create('/api/extract');\n$request-&gt;headers-&gt;set('X-Request-ID', 'req_' . uniqid());\n$request-&gt;attributes-&gt;set('_route', 'api.extract');\n\n// Create logger\n$logger = new Logger('instructor');\n$logger-&gt;pushHandler(new StreamHandler('php://stdout', Logger::DEBUG));\n\n// Create pipeline with Symfony context\n$pipeline = LoggingPipeline::create()\n    -&gt;filter(new LogLevelFilter('debug'))\n    -&gt;enrich(LazyEnricher::framework(fn() =&gt; [\n        'request_id' =&gt; $request-&gt;headers-&gt;get('X-Request-ID'),\n        'route' =&gt; $request-&gt;attributes-&gt;get('_route'),\n    ]))\n    -&gt;format(new MessageTemplateFormatter([\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputStarted::class =&gt;\n            '\ud83c\udfaf [SYMFONY] Starting extraction: {responseClass} (Route: {framework.route})',\n        \\Cognesy\\Instructor\\Events\\StructuredOutput\\StructuredOutputResponseGenerated::class =&gt;\n            '\u2705 [SYMFONY] Completed extraction: {responseClass}',\n    ], channel: 'instructor'))\n    -&gt;write(new PsrLoggerWriter($logger))\n    -&gt;build();\n\necho \"\ud83d\udd27 Symfony logging pipeline configured\\n\";\necho \"\ud83d\udccb About to execute StructuredOutput with logging...\\n\\n\";\n\nclass User\n{\n    public int $age;\n    public string $name;\n}\n\n// Extract data with logging\necho \"\ud83d\ude80 Starting StructuredOutput extraction...\\n\";\n$user = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;wiretap($pipeline)\n    -&gt;withMessages(\"Jason is 25 years old.\")\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n\necho \"\\n\u2705 Extraction completed!\\n\";\necho \"\ud83d\udcca Result: User: {$user-&gt;name}, Age: {$user-&gt;age}\\n\";\n\n// TODO: Add \"Sample Output\" section showing actual log messages\n// Example format:\n// ### Sample Output\n// [2025-12-07 01:18:13] instructor.DEBUG: \ud83d\udd04 [Symfony] Starting extraction: User\n// [2025-12-07 01:18:14] instructor.DEBUG: \u2705 [Symfony] Completed extraction: User\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/on_event/","title":"Receive specific internal event with onEvent()","text":""},{"location":"cookbook/structured_outputs/troubleshooting/on_event/#overview","title":"Overview","text":"<p><code>(new StructuredOutput)-&gt;onEvent(string $class, callable $callback)</code> method allows you to receive callback when specified type of event is dispatched by Instructor.</p> <p>This way you can plug into the execution process and monitor it, for example logging or reacting to the events which are of interest to your application.</p> <p>This example demonstrates how you can monitor outgoing requests and received responses via Instructor's events.</p> <p>Check the <code>Cognesy\\Instructor\\Events</code> namespace for the list of available events and their properties.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/on_event/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Events\\Event;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n// let's mock a logger class - in real life you would use a proper logger, e.g. Monolog\n$logger = new class {\n    public function log(Event $event) {\n        // we're using a predefined asLog() method to get the event data,\n        // but you can access the event properties directly and customize the output\n        echo $event-&gt;asLog().\"\\n\";\n    }\n};\n\n$user = (new StructuredOutput)\n    -&gt;onEvent(HttpRequestSent::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;onEvent(HttpResponseReceived::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;with(\n        messages: \"Jason is 28 years old\",\n        responseModel: User::class,\n    )\n    -&gt;get();\n\ndump($user);\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/settings/","title":"Modifying Settings Path","text":""},{"location":"cookbook/structured_outputs/troubleshooting/settings/#overview","title":"Overview","text":"<p>This example demonstrates how to modify the settings path for the Instructor library. This is useful when you want to use a custom configuration directory instead of the default one.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/settings/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Config\\Settings;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass UserDetail\n{\n    public int $age;\n    public string $firstName;\n    public ?string $lastName;\n}\n\n// set the configuration path to custom directory\nSettings::setPath(__DIR__ . '/config');\n\n$user = (new StructuredOutput)\n    -&gt;withDebugPreset('on') // we reconfigured local debug settings to dump only request URL\n    -&gt;withMessages('Jason is 25 years old.')\n    -&gt;withResponseClass(UserDetail::class)\n    -&gt;get();\n\ndump($user);\n\nassert(!isset($user-&gt;lastName) || $user-&gt;lastName === '');\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/token_usage_events/","title":"Tracking token usage via events","text":""},{"location":"cookbook/structured_outputs/troubleshooting/token_usage_events/#overview","title":"Overview","text":"<p>Some use cases require tracking the token usage of the API responses. This can be done by getting <code>Usage</code> object from Instructor LLM response object.</p> <p>Code below demonstrates how it can be retrieved for both sync and streamed requests.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/token_usage_events/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Data\\Usage;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\nfunction printUsage(Usage $usage) : void {\n    echo \"Input tokens: $usage-&gt;inputTokens\\n\";\n    echo \"Output tokens: $usage-&gt;outputTokens\\n\";\n    echo \"Cache creation tokens: $usage-&gt;cacheWriteTokens\\n\";\n    echo \"Cache read tokens: $usage-&gt;cacheReadTokens\\n\";\n    echo \"Reasoning tokens: $usage-&gt;reasoningTokens\\n\";\n}\n\necho \"COUNTING TOKENS FOR SYNC RESPONSE\\n\";\n$text = \"Jason is 25 years old and works as an engineer.\";\n$response = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n    )-&gt;response();\n\necho \"\\nTEXT: $text\\n\";\nassert($response-&gt;usage()-&gt;total() &gt; 0);\nprintUsage($response-&gt;usage());\n\n\necho \"\\n\\nCOUNTING TOKENS FOR STREAMED RESPONSE\\n\";\n$text = \"Anna is 19 years old.\";\n$stream = (new StructuredOutput)\n    -&gt;with(\n        messages: $text,\n        responseModel: User::class,\n        options: ['stream' =&gt; true],\n    )\n    -&gt;stream();\n\n$response = $stream-&gt;finalValue();\necho \"\\nTEXT: $text\\n\";\nassert($stream-&gt;usage()-&gt;total() &gt; 0);\nprintUsage($stream-&gt;usage());\n?&gt;\n</code></pre>"},{"location":"cookbook/structured_outputs/troubleshooting/wiretap/","title":"Receive all internal events with wiretap()","text":""},{"location":"cookbook/structured_outputs/troubleshooting/wiretap/#overview","title":"Overview","text":""},{"location":"cookbook/structured_outputs/troubleshooting/wiretap/#receive-all-internal-events-with-wiretap","title":"Receive all internal events with wiretap()","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <p><code>(new StructuredOutput)-&gt;wiretap(callable $callback)</code> method allows you to receive all events dispatched by Instructor.</p> <p>Example below demonstrates how <code>wiretap()</code> can help you to monitor the execution process and better understand or resolve any processing issues.</p> <p>In this example we use <code>print()</code> method available on event classes, which outputs console-formatted information about each event.</p>"},{"location":"cookbook/structured_outputs/troubleshooting/wiretap/#example","title":"Example","text":"<pre><code>&lt;?php\nrequire 'examples/boot.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\nenum Role : string {\n    case CEO = 'ceo';\n    case CTO = 'cto';\n    case Developer = 'developer';\n    case Other = 'other';\n}\n\nclass UserDetail\n{\n    public string $name;\n    public Role $role;\n    public int $age;\n}\n\n$user = (new StructuredOutput)\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    -&gt;with(\n        messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Contact our CTO, Jason is 28 years old -- Best regards, Tom\"]],\n        responseModel: UserDetail::class,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;role === Role::CTO);\nassert($user-&gt;age === 28);\n?&gt;\n</code></pre>"},{"location":"packages/","title":"Packages","text":""},{"location":"packages/#start-here","title":"Start Here","text":"<p>Most PHP developers need just one package:</p> <pre><code>composer require cognesy/instructor-php\n</code></pre> <p>This gives you everything: structured output extraction, validation, retries, and support for all major LLM providers. You're ready to go.</p>"},{"location":"packages/#when-you-need-more-control","title":"When You Need More Control","text":"<p>Instructor is built on a modular architecture. If you need to work at a lower level or integrate with specific frameworks, these packages are available separately.</p>"},{"location":"packages/#the-stack","title":"The Stack","text":""},{"location":"packages/#package-details","title":"Package Details","text":""},{"location":"packages/#instructor","title":"Instructor","text":"<p>The main package. Start here.</p> <p>Structured data extraction powered by LLMs. Define a PHP class with typed properties, pass it to Instructor with some text, get a validated object back.</p> <pre><code>&lt;?php\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages(\"John is 25 years old\")\n    -&gt;get();\n\n// $person-&gt;name = \"John\"\n// $person-&gt;age = 25\n</code></pre> <p>Why use it: - Type-safe outputs (your IDE understands the response) - Automatic validation with Symfony Validator - Self-correcting retries (LLM gets feedback on errors) - Works with any provider through Polyglot</p> <p>\u2192 Instructor Documentation</p>"},{"location":"packages/#polyglot","title":"Polyglot","text":"<p>Use this when you need direct LLM access without structured extraction.</p> <p>A unified interface for LLM providers. Write code once, run it against any provider. Useful when you're building chat interfaces, agents, or need raw completions.</p> <pre><code>&lt;?php\n$response = (new LLM)-&gt;using('anthropic')-&gt;chat(\"Explain PHP generators\");\n\n// Switch providers with one line\n$response = (new LLM)-&gt;using('openai')-&gt;chat(\"Explain PHP generators\");\n$response = (new LLM)-&gt;using('gemini')-&gt;chat(\"Explain PHP generators\");\n</code></pre> <p>Why use it: - Same code works with 20+ providers - No vendor lock-in - Streaming, embeddings, tool calling - Test with cheap/fast models, deploy with powerful ones</p> <p>\u2192 Polyglot Documentation</p>"},{"location":"packages/#http-client","title":"HTTP Client","text":"<p>Use this when you need low-level HTTP control.</p> <p>The HTTP layer that powers Polyglot. Most developers never touch this directly, but it's available if you need custom HTTP handling, middleware, or want to build your own LLM integrations.</p> <pre><code>&lt;?php\n$client = new HttpClient();\n$response = $client-&gt;handle($request);\n\n// Streaming responses\nforeach ($client-&gt;stream($request) as $chunk) {\n    echo $chunk;\n}\n</code></pre> <p>Why use it: - Streaming-first design - Middleware pipeline - Multiple backends - Connection pooling</p> <p>\u2192 HTTP Client Documentation</p>"},{"location":"packages/#laravel-integration","title":"Laravel Integration","text":"<p>Use this if you're building with Laravel.</p> <p>Adds Laravel-specific conveniences: service provider, facades, config publishing, and testing fakes.</p> <pre><code>&lt;?php\n// Use the facade\nuse Cognesy\\Instructor\\Facades\\Instructor;\n\n$person = Instructor::respond()\n    -&gt;withResponseClass(Person::class)\n    -&gt;withMessages($text)\n    -&gt;get();\n\n// Or inject via dependency injection\npublic function extract(StructuredOutput $instructor)\n{\n    return $instructor-&gt;withResponseClass(Person::class)-&gt;get();\n}\n</code></pre> <p>Why use it: - Auto-discovery (just install and use) - Laravel-style configuration - Testing fakes for unit tests - Integrates with Laravel's logging</p> <p>\u2192 Laravel Documentation</p>"},{"location":"packages/#internal-packages","title":"Internal Packages","text":"<p>These packages are used internally by Instructor and Polyglot. They're not meant for direct use, but they're available if you're extending the library or curious about the architecture.</p> Package Purpose <code>addons</code> Optional extensions (image handling, web scraping, agents) <code>schema</code> PHP class \u2192 JSON Schema conversion <code>messages</code> Message/conversation handling <code>events</code> Internal event system <code>config</code> Configuration management <code>evals</code> LLM evaluation tools <code>metrics</code> Usage tracking and observability <code>templates</code> Prompt templating <code>stream</code> Stream processing utilities"},{"location":"packages/#quick-decision-guide","title":"Quick Decision Guide","text":"I want to... Use this Extract structured data from text Instructor Extract data from images Instructor Build a chatbot or agent Polyglot Switch between LLM providers easily Polyglot (or Instructor, which includes it) Use Instructor in Laravel Instructor + Laravel package Build custom LLM integrations HTTP Client + Polyglot Just get started quickly Instructor (includes everything)"},{"location":"packages/#installation","title":"Installation","text":"<pre><code># Most developers - get everything\ncomposer require cognesy/instructor-php\n\n# Direct LLM access only (no structured extraction)\ncomposer require cognesy/polyglot\n\n# Laravel integration\ncomposer require cognesy/instructor-laravel\n\n# Low-level HTTP only\ncomposer require cognesy/http-client\n</code></pre>"},{"location":"packages/agents/01-introduction/","title":"Introduction","text":"<p>The Agents package provides a minimal, composable foundation for building LLM-powered agents that reason and act through tool use.</p> <p>At its core, an agent is a loop: send messages to an LLM, receive a response (possibly with tool calls), execute those tools, feed results back, and repeat until done.</p>"},{"location":"packages/agents/01-introduction/#key-design-principles","title":"Key Design Principles","text":"<ul> <li>Immutable state - <code>AgentState</code> is a readonly value object. Every mutation returns a new instance.</li> <li>Pluggable drivers - Swap between <code>ToolCallingDriver</code> (native function calling) and <code>ReActDriver</code> (Thought/Action/Observation) without changing your agent code.</li> <li>Lifecycle hooks - Intercept any phase of execution to add guards, logging, or custom behavior.</li> <li>Testable by default - <code>FakeAgentDriver</code> lets you script deterministic scenarios without LLM calls.</li> </ul>"},{"location":"packages/agents/01-introduction/#two-layers","title":"Two Layers","text":"<p>AgentLoop is the stateless execution engine. It takes an <code>AgentState</code>, runs the step loop, and returns the final state. Use it directly for simple agents or full manual control.</p> <p>AgentBuilder is the composition layer. It assembles an <code>AgentLoop</code> from pluggable capabilities (<code>Use*</code> classes) that install tools, hooks, guards, drivers, and compilers. Use it when you want modular, reusable configuration.</p> <pre><code>// Direct: AgentLoop\n$loop = AgentLoop::default()-&gt;withTool($myTool);\n\n// Composed: AgentBuilder\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseGuards(maxSteps: 10))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/01-introduction/#package-structure","title":"Package Structure","text":"<pre><code>AgentLoop.php         # Core execution loop\nCanControlAgentLoop   # Loop interface (execute/iterate)\nBuilder/              # AgentBuilder, AgentConfigurator, capability contracts\nCapability/           # Use* capabilities (Core, Bash, File, Subagent, etc.)\nCollections/          # Tools, AgentSteps, StepExecutions, ErrorList, etc.\nContext/              # AgentContext, message compilers (CanCompileMessages)\nContinuation/         # StopSignal, StopReason, ExecutionContinuation\nData/                 # AgentState, ExecutionState, AgentStep, AgentBudget\nDrivers/              # ToolCallingDriver, ReActDriver, FakeAgentDriver\nEnums/                # AgentStepType, ExecutionStatus\nEvents/               # Agent event system\nExceptions/           # Domain exceptions\nHook/                 # HookStack, HookInterface, HookContext, built-in hooks\nInterception/         # CanInterceptAgentLifecycle, PassThroughInterceptor\nTemplate/             # Agent definitions, parsers, registry\nTool/                 # ToolInterface, BaseTool, FunctionTool, ToolExecutor\n</code></pre>"},{"location":"packages/agents/01-introduction/#minimal-example","title":"Minimal Example","text":"<pre><code>use Cognesy\\Agents\\AgentLoop;use Cognesy\\Agents\\Data\\AgentState;\n\n$loop = AgentLoop::default();\n$state = AgentState::empty()-&gt;withUserMessage('Hello!');\n$result = $loop-&gt;execute($state);\n\necho $result-&gt;finalResponse()-&gt;toString();\n</code></pre>"},{"location":"packages/agents/02-basic-agent/","title":"Basic Agent","text":"<p>The simplest agent uses <code>AgentLoop</code> to send a message and get a response.</p>"},{"location":"packages/agents/02-basic-agent/#hello-world","title":"Hello World","text":"<pre><code>use Cognesy\\Agents\\AgentLoop;use Cognesy\\Agents\\Data\\AgentState;\n\n$loop = AgentLoop::default();\n$state = AgentState::empty()-&gt;withUserMessage('What is 2+2?');\n$result = $loop-&gt;execute($state);\n\necho $result-&gt;finalResponse()-&gt;toString();\n// \"2 + 2 equals 4.\"\n</code></pre>"},{"location":"packages/agents/02-basic-agent/#what-happens","title":"What Happens","text":"<ol> <li><code>AgentLoop::execute()</code> starts the loop</li> <li>The driver (<code>ToolCallingDriver</code>) sends messages to the LLM</li> <li>LLM responds with text (no tool calls)</li> <li>The loop detects no tool calls and stops</li> <li>Final response is available via <code>$result-&gt;finalResponse()</code></li> </ol>"},{"location":"packages/agents/02-basic-agent/#agent-with-a-tool","title":"Agent with a Tool","text":"<p>Give the agent a tool and it will decide when to use it:</p> <pre><code>use Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Tool\\Tools\\FunctionTool;\n\n$weather = FunctionTool::fromCallable(\n    function (string $city): string {\n        return \"Weather in {$city}: 72F, sunny\";\n    }\n);\n\n$loop = AgentLoop::default()-&gt;withTool($weather);\n$state = AgentState::empty()-&gt;withUserMessage('What is the weather in Paris?');\n$result = $loop-&gt;execute($state);\n\necho $result-&gt;finalResponse()-&gt;toString();\n// \"The weather in Paris is 72\u00b0F and sunny.\"\n</code></pre>"},{"location":"packages/agents/02-basic-agent/#customizing-the-loop","title":"Customizing the Loop","text":"<p>Use <code>with()</code> to swap components on the default loop:</p> <pre><code>use Cognesy\\Agents\\Drivers\\ReAct\\ReActDriver;\nuse Cognesy\\Events\\EventBusResolver;\nuse Cognesy\\Instructor\\Creation\\StructuredOutputConfigBuilder;\nuse Cognesy\\Instructor\\StructuredOutputRuntime;\nuse Cognesy\\Polyglot\\Inference\\InferenceRuntime;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n// Add tools\n$loop = AgentLoop::default()-&gt;withTool($myTool);\n\n// Swap driver\n$events = EventBusResolver::using(null);\n$inference = InferenceRuntime::fromProvider(LLMProvider::new(), events: $events);\n$structuredOutput = new StructuredOutputRuntime(\n    inference: $inference,\n    events: $events,\n    config: (new StructuredOutputConfigBuilder())-&gt;create(),\n);\n$loop = AgentLoop::default()-&gt;withDriver(new ReActDriver(\n    inference: $inference,\n    structuredOutput: $structuredOutput,\n    model: 'gpt-4o',\n));\n</code></pre>"},{"location":"packages/agents/02-basic-agent/#system-prompt","title":"System Prompt","text":"<pre><code>$state = AgentState::empty()\n    -&gt;withSystemPrompt('You are a helpful assistant.')\n    -&gt;withUserMessage('Hello!');\n</code></pre>"},{"location":"packages/agents/02-basic-agent/#using-agentbuilder","title":"Using AgentBuilder","text":"<p>For more complex agents, use <code>AgentBuilder</code> to compose capabilities:</p> <pre><code>use Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Bash\\UseBash;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseLLMConfig;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(preset: 'anthropic'))\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseGuards(maxSteps: 10))\n    -&gt;build();\n\n$result = $agent-&gt;execute($state);\n</code></pre> <p>See AgentBuilder &amp; Capabilities for details.</p>"},{"location":"packages/agents/03-basic-concepts/","title":"Basic Concepts","text":""},{"location":"packages/agents/03-basic-concepts/#agentloop","title":"AgentLoop","text":"<p>The orchestrator. Runs a step-based loop: call LLM, execute tools, check stop conditions, repeat.</p> <pre><code>BeforeExecution -&gt; [ BeforeStep -&gt; UseTools -&gt; AfterStep -&gt; ShouldStop? ] -&gt; AfterExecution\n</code></pre>"},{"location":"packages/agents/03-basic-concepts/#agentstate","title":"AgentState","text":"<p>Immutable value object holding everything about an agent's session and execution.</p> <p>Session data (persists across executions): - <code>agentId</code> - unique identifier - <code>context</code> - messages, system prompt, metadata - <code>budget</code> - resource limits (steps, tokens, time)</p> <p>Execution data (transient, null between executions): - <code>execution</code> - current <code>ExecutionState</code> with steps, status, continuation signals</p> <pre><code>$state = AgentState::empty()\n    -&gt;withSystemPrompt('You are helpful.')\n    -&gt;withUserMessage('Hello')\n    -&gt;withBudget(new AgentBudget(maxSteps: 10));\n</code></pre>"},{"location":"packages/agents/03-basic-concepts/#agentcontext","title":"AgentContext","text":"<p>Holds the conversation data: messages, system prompt, metadata, and response format. The driver's <code>CanCompileMessages</code> compiler transforms the context into the final <code>Messages</code> sent to the LLM.</p> <pre><code>$state = AgentState::empty()\n    -&gt;withSystemPrompt('You are helpful.')\n    -&gt;withMetadata('user_id', 123);\n</code></pre>"},{"location":"packages/agents/03-basic-concepts/#agentstep","title":"AgentStep","text":"<p>An immutable snapshot of a single loop iteration:</p> <ul> <li><code>inputMessages</code> - what was sent to the LLM</li> <li><code>outputMessages</code> - tool results + assistant response</li> <li><code>inferenceResponse</code> - raw LLM response</li> <li><code>toolExecutions</code> - results of executed tools</li> <li><code>stepType()</code> - <code>FinalResponse</code>, <code>ToolExecution</code>, or <code>Error</code></li> </ul>"},{"location":"packages/agents/03-basic-concepts/#stepexecution","title":"StepExecution","text":"<p>Wraps an <code>AgentStep</code> with timing and continuation data. Stored in the completed steps list after each iteration.</p>"},{"location":"packages/agents/03-basic-concepts/#executionstate","title":"ExecutionState","text":"<p>Tracks the current execution's transient state: status, completed steps, current step, and continuation signals.</p> <pre><code>$state-&gt;execution()-&gt;status();        // ExecutionStatus::InProgress\n$state-&gt;execution()-&gt;stepCount();     // 3\n$state-&gt;execution()-&gt;shouldStop();    // false\n</code></pre>"},{"location":"packages/agents/04-controlling-the-loop/","title":"Controlling the Loop","text":""},{"location":"packages/agents/04-controlling-the-loop/#execute-vs-iterate","title":"execute() vs iterate()","text":"<p><code>execute()</code> runs the loop to completion and returns the final state:</p> <pre><code>$finalState = $loop-&gt;execute($state);\n</code></pre> <p><code>iterate()</code> yields state after each step, giving you full control:</p> <pre><code>foreach ($loop-&gt;iterate($state) as $stepState) {\n    $step = $stepState-&gt;lastStep();\n    echo \"Step {$stepState-&gt;stepCount()}: {$step-&gt;stepType()-&gt;value}\\n\";\n\n    // Access tool executions from this step\n    foreach ($step-&gt;toolExecutions()-&gt;all() as $exec) {\n        echo \"  Tool: {$exec-&gt;name()} -&gt; {$exec-&gt;value()}\\n\";\n    }\n}\n</code></pre>"},{"location":"packages/agents/04-controlling-the-loop/#inspecting-state","title":"Inspecting State","text":"<p>After execution, query the state for results:</p> <pre><code>$state-&gt;stepCount();                    // total steps executed\n$state-&gt;lastStepType();                 // AgentStepType enum\n$state-&gt;lastStopReason();               // StopReason enum\n$state-&gt;usage();                        // token usage\n$state-&gt;executionDuration();            // seconds elapsed\n\n// Access all steps\nforeach ($state-&gt;steps()-&gt;all() as $step) {\n    echo $step-&gt;stepType()-&gt;value . ': ';\n    echo $step-&gt;outputMessages()-&gt;toString() . \"\\n\";\n}\n\n// Last tool execution\n$toolExec = $state-&gt;lastToolExecution();\n$toolExec?-&gt;name();     // 'weather'\n$toolExec?-&gt;value();    // '72F, sunny'\n$toolExec?-&gt;hasError(); // false\n</code></pre>"},{"location":"packages/agents/04-controlling-the-loop/#reading-the-agents-response","title":"Reading the Agent's Response","text":"<p><code>AgentState</code> provides two methods for accessing the agent's text output. They differ in strictness \u2014 choose the one that fits your use case.</p>"},{"location":"packages/agents/04-controlling-the-loop/#finalresponse","title":"finalResponse()","text":"<p>Returns the agent's output only when the agent completed naturally (the LLM's last step had no tool calls). Returns empty <code>Messages</code> in all other cases: forced stops, errors, budget exhaustion, etc.</p> <pre><code>$state-&gt;hasFinalResponse();             // true only on natural completion\n$state-&gt;finalResponse()-&gt;toString();    // strict: empty when interrupted\n</code></pre> <p>Use <code>finalResponse()</code> when you need to distinguish between a genuine answer and an incomplete execution. This is the right choice when the agent's response is only meaningful if the LLM finished on its own terms.</p>"},{"location":"packages/agents/04-controlling-the-loop/#currentresponse","title":"currentResponse()","text":"<p>Returns the best available text output: <code>finalResponse()</code> if present, otherwise the last step's output messages regardless of step type.</p> <pre><code>$state-&gt;currentResponse()-&gt;toString();  // pragmatic: last output text\n</code></pre> <p>Use <code>currentResponse()</code> when you want to show something to the user even if the agent was interrupted \u2014 for example in a UI that always needs to display the most recent LLM output.</p>"},{"location":"packages/agents/04-controlling-the-loop/#when-the-agent-is-stopped-externally","title":"When the agent is stopped externally","text":"<p>When a tool throws <code>AgentStopException</code> or a budget limit is hit, the last step is typically a <code>ToolExecution</code> (not <code>FinalResponse</code>), so <code>finalResponse()</code> returns empty. In these cases:</p> <ul> <li>Stop via exception \u2014 the answer is usually in metadata or the   stop signal context, not in the LLM's text output. Check   <code>$state-&gt;lastStopSignal()</code> and <code>$state-&gt;metadata()</code>.</li> <li>Budget exhaustion \u2014 the agent was interrupted mid-work.   <code>currentResponse()</code> gives you the last LLM output, but it may   be incomplete or reference pending tool calls.</li> <li>Error \u2014 inspect <code>$state-&gt;lastStepErrors()</code> for details.</li> </ul> <pre><code>if ($state-&gt;hasFinalResponse()) {\n    echo $state-&gt;finalResponse()-&gt;toString();\n} else {\n    $reason = $state-&gt;lastStopReason();\n    echo \"Agent stopped: {$reason-&gt;value}\\n\";\n    echo $state-&gt;currentResponse()-&gt;toString();\n}\n</code></pre>"},{"location":"packages/agents/04-controlling-the-loop/#listening-to-events","title":"Listening to Events","text":"<p>Attach listeners to monitor execution:</p> <pre><code>use Cognesy\\Agents\\Events\\AgentStepCompleted;\n\n$loop-&gt;onEvent(AgentStepCompleted::class, function (AgentStepCompleted $event) {\n    echo \"Step {$event-&gt;stepNumber} completed (tokens: {$event-&gt;usage-&gt;total()})\\n\";\n});\n\n// Or listen to all events\n$loop-&gt;wiretap(function ($event) {\n    echo get_class($event) . \"\\n\";\n});\n</code></pre>"},{"location":"packages/agents/05-tools/","title":"Tools","text":"<p>Tools let the agent take actions. The LLM decides which tool to call and with what arguments.</p>"},{"location":"packages/agents/05-tools/#registering-tools","title":"Registering Tools","text":"<p>Pass tools to the <code>Tools</code> collection:</p> <pre><code>use Cognesy\\Agents\\Collections\\Tools;use Cognesy\\Agents\\Tool\\Tools\\MockTool;\n\n$calculator = MockTool::returning('calculator', 'Performs math', '42');\n$tools = new Tools($calculator);\n</code></pre>"},{"location":"packages/agents/05-tools/#using-functiontool","title":"Using FunctionTool","text":"<p>Wrap any callable as a tool. Parameter schema is auto-generated from the function signature:</p> <pre><code>use Cognesy\\Agents\\Tool\\Tools\\FunctionTool;\n\n$tool = FunctionTool::fromCallable(\n    function (string $city): string {\n        return \"Weather in {$city}: 72F, sunny\";\n    }\n);\n\n$tools = new Tools($tool);\n</code></pre>"},{"location":"packages/agents/05-tools/#multiple-tools","title":"Multiple Tools","text":"<p>Pass multiple tools to <code>Tools</code> and the LLM chooses which to call:</p> <pre><code>use Cognesy\\Agents\\Tool\\Tools\\FunctionTool;\nuse Cognesy\\Agents\\Collections\\Tools;\n\n$weather = FunctionTool::fromCallable(\n    function (string $city): string {\n        return \"Weather in {$city}: 72F, sunny\";\n    }\n);\n\n$calculator = FunctionTool::fromCallable(\n    function (string $expression): string {\n        return (string) eval(\"return {$expression};\");\n    }\n);\n\n$tools = new Tools($weather, $calculator);\n</code></pre>"},{"location":"packages/agents/05-tools/#agent-with-tools","title":"Agent with Tools","text":"<pre><code>use Cognesy\\Agents\\AgentLoop;\nuse Cognesy\\Agents\\Data\\AgentState;\n\n$loop = AgentLoop::default()-&gt;withTools($tools);\n\n$state = AgentState::empty()-&gt;withUserMessage('What is the weather in Paris?');\n$result = $loop-&gt;execute($state);\n// LLM calls the weather tool, gets result, then responds\n</code></pre>"},{"location":"packages/agents/05-tools/#tool-contracts","title":"Tool Contracts","text":"<p>Every tool implements two interfaces:</p>"},{"location":"packages/agents/05-tools/#toolinterface","title":"ToolInterface","text":"<p>The execution and schema contract:</p> <pre><code>interface ToolInterface {\n    public function use(mixed ...$args): Result;    // execute the tool\n    public function toToolSchema(): array;           // JSON schema sent to LLM\n    public function descriptor(): CanDescribeTool;   // metadata accessor\n}\n</code></pre>"},{"location":"packages/agents/05-tools/#candescribetool","title":"CanDescribeTool","text":"<p>The description contract \u2014 provides identity and documentation:</p> <pre><code>interface CanDescribeTool {\n    public function name(): string;          // tool name (e.g., 'file.read')\n    public function description(): string;   // what the tool does\n    public function metadata(): array;       // summary for browsing/discovery\n    public function instructions(): array;   // full specification with parameters\n}\n</code></pre> <p><code>metadata()</code> returns lightweight info (name, summary, namespace) for tool listings. <code>instructions()</code> returns the complete specification including parameters and return type.</p>"},{"location":"packages/agents/05-tools/#basetool","title":"BaseTool","text":"<p><code>BaseTool</code> implements both interfaces and is the standard base class for custom tools. It auto-generates parameter schemas from the <code>__invoke()</code> method signature:</p> <pre><code>use Cognesy\\Agents\\Tool\\Tools\\BaseTool;\n\nclass WeatherTool extends BaseTool\n{\n    public function __construct() {\n        parent::__construct(\n            name: 'weather',\n            description: 'Get current weather for a city',\n        );\n    }\n\n    public function __invoke(string $city): string {\n        return \"Weather in {$city}: 72F, sunny\";\n    }\n}\n</code></pre> <p>See Building Tools for the full guide on creating custom tools.</p>"},{"location":"packages/agents/05-tools/#how-it-works","title":"How It Works","text":"<ol> <li>LLM sees tool schemas and decides to call a tool</li> <li><code>ToolExecutor</code> runs the tool with provided arguments</li> <li>Tool results are formatted as messages and fed back to the LLM</li> <li>LLM uses the results to formulate a final response</li> <li>Loop continues until LLM responds without tool calls</li> </ol>"},{"location":"packages/agents/06-building-tools/","title":"Building Tools","text":""},{"location":"packages/agents/06-building-tools/#using-basetool","title":"Using BaseTool","text":"<p>Extend <code>BaseTool</code> and implement <code>__invoke()</code>. The parameter schema is auto-generated from the method signature:</p> <pre><code>use Cognesy\\Agents\\Tool\\Tools\\BaseTool;use Cognesy\\Schema\\Attributes\\Description;\n\nclass GetWeather extends BaseTool\n{\n    public function __construct()\n    {\n        parent::__construct(\n            name: 'get_weather',\n            description: 'Get current weather for a city',\n        );\n    }\n\n    public function __invoke(\n        #[Description('City name')] string $city,\n        #[Description('Unit: celsius or fahrenheit')] string $unit = 'celsius',\n    ): string {\n        // Your implementation\n        return \"Weather in {$city}: 22 {$unit}\";\n    }\n}\n</code></pre>"},{"location":"packages/agents/06-building-tools/#using-functiontool","title":"Using FunctionTool","text":"<p>Wrap any callable without creating a class:</p> <pre><code>use Cognesy\\Agents\\Tool\\Tools\\FunctionTool;\n\n$tool = FunctionTool::fromCallable(function (string $query): string {\n    return \"Results for: {$query}\";\n});\n</code></pre>"},{"location":"packages/agents/06-building-tools/#accessing-agent-state","title":"Accessing Agent State","text":"<p>Implement <code>CanAccessAgentState</code> (already included in <code>BaseTool</code>) to receive the current agent state:</p> <pre><code>class StatefulTool extends BaseTool\n{\n    public function __invoke(string $input): string\n    {\n        $stepCount = $this-&gt;agentState?-&gt;stepCount() ?? 0;\n        return \"Step {$stepCount}: processing {$input}\";\n    }\n}\n</code></pre>"},{"location":"packages/agents/06-building-tools/#the-toolinterface","title":"The ToolInterface","text":"<p>All tools implement <code>ToolInterface</code>:</p> <pre><code>interface ToolInterface\n{\n    public function use(mixed ...$args): Result;\n    public function toToolSchema(): array;\n    public function descriptor(): CanDescribeTool;\n}\n</code></pre> <ul> <li><code>use()</code> - execute the tool, returns <code>Result</code> (success or failure)</li> <li><code>toToolSchema()</code> - OpenAI-compatible function schema for the LLM</li> <li><code>descriptor()</code> - returns a <code>CanDescribeTool</code> with <code>name()</code>, <code>description()</code>, <code>metadata()</code>, <code>instructions()</code></li> </ul> <p><code>BaseTool</code> implements both <code>ToolInterface</code> and <code>CanDescribeTool</code>, acting as its own descriptor by default.</p>"},{"location":"packages/agents/06-building-tools/#externalizing-descriptors-with-tooldescriptor","title":"Externalizing Descriptors with ToolDescriptor","text":"<p>For tools with rich documentation \u2014 examples, usage notes, error descriptions \u2014 you can separate the descriptor into its own class. This keeps tool logic clean and makes documentation reusable.</p> <p><code>ToolDescriptor</code> is a readonly data class that implements <code>CanDescribeTool</code>:</p> <pre><code>use Cognesy\\Agents\\Tool\\ToolDescriptor;\n\nfinal readonly class MyToolDescriptor extends ToolDescriptor\n{\n    public function __construct() {\n        parent::__construct(\n            name: 'my_tool',\n            description: 'Does something useful with detailed guidance.',\n            metadata: [\n                'namespace' =&gt; 'domain',\n                'tags' =&gt; ['analysis', 'data'],\n            ],\n            instructions: [\n                'parameters' =&gt; [\n                    'input' =&gt; 'The data to process.',\n                ],\n                'returns' =&gt; 'Processed result as string.',\n                'usage' =&gt; [\n                    'Pass structured data for best results.',\n                ],\n                'errors' =&gt; [\n                    'Returns error message on invalid input.',\n                ],\n            ],\n        );\n    }\n}\n</code></pre> <p>Then wire it into your tool:</p> <pre><code>class MyTool extends BaseTool\n{\n    private MyToolDescriptor $descriptor;\n\n    public function __construct() {\n        $descriptor = new MyToolDescriptor();\n        parent::__construct(\n            name: $descriptor-&gt;name(),\n            description: $descriptor-&gt;description(),\n        );\n        $this-&gt;descriptor = $descriptor;\n    }\n\n    #[\\Override]\n    public function descriptor(): CanDescribeTool {\n        return $this-&gt;descriptor;\n    }\n\n    public function __invoke(string $input): string {\n        return \"Processed: {$input}\";\n    }\n}\n</code></pre> <p>Most built-in tools use this pattern \u2014 <code>BashTool</code> has <code>BashToolDescriptor</code>, each file tool has its own descriptor, and so on. The <code>metadata()</code> and <code>instructions()</code> arrays power progressive disclosure: tool registries can show lightweight summaries via <code>metadata()</code>, while the LLM receives full specifications via <code>instructions()</code> when needed.</p>"},{"location":"packages/agents/06-building-tools/#mocktool-for-testing","title":"MockTool for Testing","text":"<pre><code>use Cognesy\\Agents\\Tool\\Tools\\MockTool;\n\n$tool = MockTool::returning('my_tool', 'Does something', 'fixed result');\n\n// Or with custom logic\n$tool = new MockTool('my_tool', 'Does something', fn($x) =&gt; strtoupper($x));\n</code></pre>"},{"location":"packages/agents/07-context-and-compilers/","title":"Agent Context &amp; Message Compilers","text":""},{"location":"packages/agents/07-context-and-compilers/#agentcontext","title":"AgentContext","text":"<p><code>AgentContext</code> holds the conversation data: messages, system prompt, metadata, and an optional response format.</p> <pre><code>$state = AgentState::empty()\n    -&gt;withSystemPrompt('You are a helpful assistant.')\n    -&gt;withMetadata('session_id', 'abc');\n\n// Access\n$state-&gt;context()-&gt;systemPrompt();\n$state-&gt;context()-&gt;messages();\n$state-&gt;context()-&gt;metadata();\n</code></pre>"},{"location":"packages/agents/07-context-and-compilers/#message-compilers","title":"Message Compilers","text":"<p>Before each LLM call, the driver uses a <code>CanCompileMessages</code> implementation to transform <code>AgentState</code> into the final <code>Messages</code> sent to the LLM.</p> <pre><code>interface CanCompileMessages\n{\n    public function compile(AgentState $state): Messages;\n}\n</code></pre> <p>The default compiler assembles messages from the agent's context. You can replace it to control exactly what the LLM sees.</p>"},{"location":"packages/agents/07-context-and-compilers/#custom-compiler","title":"Custom Compiler","text":"<p>Implement <code>CanCompileMessages</code> for custom message assembly:</p> <pre><code>use Cognesy\\Agents\\Context\\CanCompileMessages;use Cognesy\\Agents\\Data\\AgentState;use Cognesy\\Messages\\Messages;\n\nclass MyCompiler implements CanCompileMessages\n{\n    public function compile(AgentState $state): Messages\n    {\n        // Custom logic to build messages from state\n        return $state-&gt;messages();\n    }\n}\n</code></pre> <p>Inject it into the loop's driver:</p> <pre><code>$inference = InferenceRuntime::fromProvider(LLMProvider::new());\n$driver = new ToolCallingDriver(\n    inference: $inference,\n    messageCompiler: new MyCompiler(),\n);\n$loop = AgentLoop::default()-&gt;withDriver($driver);\n</code></pre> <p>Or via <code>AgentBuilder</code>:</p> <pre><code>use Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseContextCompiler;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseContextCompiler(new MyCompiler()))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/07-context-and-compilers/#use-cases","title":"Use Cases","text":"<p>Custom compilers give you full control over what the LLM sees on each step. Common patterns include:</p> <ul> <li>Context window management \u2014 trim or summarize older messages when the conversation exceeds the model's token limit, keeping the most recent and most relevant exchanges</li> <li>Filtering \u2014 exclude internal tool traces, metadata messages, or debug output that the LLM doesn't need</li> <li>Injection \u2014 prepend dynamic instructions, inject retrieved context (RAG), or append reminders based on the current state</li> <li>Format transformation \u2014 restructure messages for a specific model's expected format or optimize token usage</li> </ul>"},{"location":"packages/agents/07-context-and-compilers/#example-trimming-to-token-limit","title":"Example: Trimming to Token Limit","text":"<p>A compiler that keeps only the most recent messages within a token budget:</p> <pre><code>class TokenLimitCompiler implements CanCompileMessages\n{\n    public function __construct(\n        private CanCompileMessages $inner,\n        private int $maxTokens = 8000,\n    ) {}\n\n    public function compile(AgentState $state): Messages\n    {\n        $messages = $this-&gt;inner-&gt;compile($state);\n        $kept = [];\n        $tokens = 0;\n\n        // Walk backwards, keeping recent messages first\n        foreach (array_reverse($messages-&gt;toArray()) as $message) {\n            $estimate = (int) ceil(strlen($message-&gt;content()) / 4);\n            if ($tokens + $estimate &gt; $this-&gt;maxTokens) {\n                break;\n            }\n            $tokens += $estimate;\n            array_unshift($kept, $message);\n        }\n\n        return Messages::fromArray($kept);\n    }\n}\n</code></pre> <p>Use as a decorator via <code>UseContextCompilerDecorator</code>:</p> <pre><code>use Cognesy\\Agents\\Capability\\Core\\UseContextCompilerDecorator;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseContextCompilerDecorator(\n        fn(CanCompileMessages $inner) =&gt; new TokenLimitCompiler($inner, maxTokens: 4000)\n    ))\n    -&gt;build();\n</code></pre> <p>The decorator pattern wraps the default compiler, so you get standard message assembly plus your custom logic on top.</p>"},{"location":"packages/agents/08-hooks/","title":"Hooks","text":"<p>Hooks intercept agent lifecycle events to add custom behavior: logging, guards, state modification, tool blocking, and more.</p>"},{"location":"packages/agents/08-hooks/#lifecycle-events","title":"Lifecycle Events","text":"Trigger When <code>BeforeExecution</code> Before the loop starts <code>BeforeStep</code> Before each LLM call <code>BeforeToolUse</code> Before each tool execution <code>AfterToolUse</code> After each tool execution <code>AfterStep</code> After each loop iteration <code>OnStop</code> When a stop signal is detected <code>AfterExecution</code> After the loop ends <code>OnError</code> When an error occurs"},{"location":"packages/agents/08-hooks/#implementing-a-hook","title":"Implementing a Hook","text":"<p>Implement <code>HookInterface</code>:</p> <pre><code>use Cognesy\\Agents\\Hook\\Contracts\\HookInterface;\nuse Cognesy\\Agents\\Hook\\Data\\HookContext;\n\nclass LogStepsHook implements HookInterface\n{\n    public function handle(HookContext $context): HookContext\n    {\n        $steps = $context-&gt;state()-&gt;stepCount();\n        echo \"Current step: {$steps}\\n\";\n        return $context;\n    }\n}\n</code></pre>"},{"location":"packages/agents/08-hooks/#registering-hooks","title":"Registering Hooks","text":""},{"location":"packages/agents/08-hooks/#via-agentbuilder-recommended","title":"Via AgentBuilder (recommended)","text":"<p>Use the <code>UseHook</code> capability to register hooks declaratively:</p> <pre><code>use Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseHook;\nuse Cognesy\\Agents\\Hook\\Collections\\HookTriggers;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseHook(\n        hook: new LogStepsHook(),\n        triggers: HookTriggers::afterStep(),\n        priority: 10,\n        name: 'log_steps',\n    ))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/08-hooks/#via-hookstack-manual","title":"Via HookStack (manual)","text":"<p>For direct <code>AgentLoop</code> usage, compose hooks with <code>HookStack</code>:</p> <pre><code>use Cognesy\\Agents\\Hook\\Collections\\HookTriggers;use Cognesy\\Agents\\Hook\\Collections\\RegisteredHooks;use Cognesy\\Agents\\Hook\\HookStack;\n\n$stack = new HookStack(new RegisteredHooks());\n$stack = $stack-&gt;with(\n    hook: new LogStepsHook(),\n    triggerTypes: HookTriggers::afterStep(),\n    priority: 10,\n    name: 'log_steps',\n);\n\n$loop = AgentLoop::default()-&gt;withInterceptor($stack);\n</code></pre>"},{"location":"packages/agents/08-hooks/#callablehook","title":"CallableHook","text":"<p>Quick hooks without a class:</p> <pre><code>use Cognesy\\Agents\\Hook\\Hooks\\CallableHook;\n\n$hook = new CallableHook(function (HookContext $ctx): HookContext {\n    echo \"Step done!\\n\";\n    return $ctx;\n});\n\n$stack = $stack-&gt;with($hook, HookTriggers::afterStep());\n</code></pre>"},{"location":"packages/agents/08-hooks/#blocking-tool-execution","title":"Blocking Tool Execution","text":"<p>In a <code>BeforeToolUse</code> hook, block a tool:</p> <pre><code>class BlockDangerousTools implements HookInterface\n{\n    public function handle(HookContext $context): HookContext\n    {\n        if ($context-&gt;toolCall()?-&gt;name() === 'dangerous_tool') {\n            return $context-&gt;withToolExecutionBlocked('Not allowed');\n        }\n        return $context;\n    }\n}\n\n$stack = $stack-&gt;with(new BlockDangerousTools(), HookTriggers::beforeToolUse());\n</code></pre>"},{"location":"packages/agents/08-hooks/#modifying-state","title":"Modifying State","text":"<p>Hooks can modify <code>AgentState</code> through <code>HookContext</code>:</p> <pre><code>$hook = new CallableHook(function (HookContext $ctx): HookContext {\n    $state = $ctx-&gt;state()-&gt;withMetadata('custom_key', 'value');\n    return $ctx-&gt;withState($state);\n});\n</code></pre>"},{"location":"packages/agents/08-hooks/#built-in-guard-hooks","title":"Built-in Guard Hooks","text":""},{"location":"packages/agents/08-hooks/#via-useguards-capability-recommended","title":"Via UseGuards capability (recommended)","text":"<pre><code>use Cognesy\\Agents\\Capability\\Core\\UseGuards;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseGuards(\n        maxSteps: 10,\n        maxTokens: 5000,\n        maxExecutionTime: 30.0,\n    ))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/08-hooks/#manual-registration","title":"Manual registration","text":"<pre><code>use Cognesy\\Agents\\Hook\\Hooks\\ExecutionTimeLimitHook;use Cognesy\\Agents\\Hook\\Hooks\\StepsLimitHook;use Cognesy\\Agents\\Hook\\Hooks\\TokenUsageLimitHook;\n\n// Stop after 10 steps\n$stepsGuard = new StepsLimitHook(\n    maxSteps: 10,\n    stepCounter: fn($state) =&gt; $state-&gt;stepCount(),\n);\n\n// Stop after 5000 tokens\n$tokenGuard = new TokenUsageLimitHook(maxTotalTokens: 5000);\n\n// Stop after 30 seconds\n$timeGuard = new ExecutionTimeLimitHook(maxSeconds: 30.0);\n\n$stack = $stack\n    -&gt;with($stepsGuard, HookTriggers::beforeStep(), priority: 100)\n    -&gt;with($tokenGuard, HookTriggers::beforeStep(), priority: 100)\n    -&gt;with($timeGuard, HookTriggers::with(HookTrigger::BeforeExecution, HookTrigger::BeforeStep), priority: 100);\n</code></pre>"},{"location":"packages/agents/08-hooks/#hook-priority","title":"Hook Priority","text":"<p>Higher priority hooks run first. Use priorities to ensure guards run before business logic.</p>"},{"location":"packages/agents/09-stop-conditions/","title":"Stop Conditions","text":"<p>The agent loop stops when <code>ExecutionState::shouldStop()</code> returns true. This is controlled by <code>ExecutionContinuation</code>, <code>StopSignals</code>, and <code>AgentStopException</code>.</p>"},{"location":"packages/agents/09-stop-conditions/#default-stop-logic","title":"Default Stop Logic","text":"<p>The loop stops when: 1. A <code>StopSignal</code> is present and no continuation is requested, OR 2. There are no pending tool calls (LLM gave a final response)</p> <pre><code>// ExecutionState::shouldStop()\npublic function shouldStop(): bool {\n    return match(true) {\n        $this-&gt;continuation-&gt;shouldStop() =&gt; true,       // stop signal present\n        $this-&gt;continuation-&gt;isContinuationRequested() =&gt; false, // override\n        $this-&gt;hasToolCalls() =&gt; false,                  // more tools to run\n        default =&gt; true,                                 // no tools = done\n    };\n}\n</code></pre>"},{"location":"packages/agents/09-stop-conditions/#stopsignal","title":"StopSignal","text":"<p>A signal requesting the loop to stop, with a reason and context:</p> <pre><code>use Cognesy\\Agents\\Continuation\\StopReason;use Cognesy\\Agents\\Continuation\\StopSignal;\n\n$signal = new StopSignal(\n    reason: StopReason::StepsLimitReached,\n    message: 'Step limit reached: 10/10',\n    context: ['currentSteps' =&gt; 10, 'maxSteps' =&gt; 10],\n    source: MyGuard::class,\n);\n</code></pre>"},{"location":"packages/agents/09-stop-conditions/#stopreason","title":"StopReason","text":"<pre><code>Completed           - Normal completion\nStepsLimitReached   - Step budget exhausted\nTokenLimitReached   - Token budget exhausted\nTimeLimitReached    - Time budget exhausted\nRetryLimitReached   - Max retries exceeded\nErrorForbade        - Error prevented continuation\nStopRequested       - Explicit stop via AgentStopException\nFinishReasonReceived - LLM finish reason matched\nUserRequested       - User-initiated stop\n</code></pre>"},{"location":"packages/agents/09-stop-conditions/#agentstopexception","title":"AgentStopException","text":"<p>Throw from a tool to immediately stop the loop:</p> <pre><code>use Cognesy\\Agents\\Continuation\\AgentStopException;\n\nclass StopTool extends BaseTool\n{\n    public function __invoke(): never\n    {\n        throw new AgentStopException(\n            signal: new StopSignal(\n                reason: StopReason::UserRequested,\n                message: 'Task complete',\n            ),\n        );\n    }\n}\n</code></pre>"},{"location":"packages/agents/09-stop-conditions/#executioncontinuation","title":"ExecutionContinuation","text":"<p>Manages the interplay between stop signals and continuation requests:</p> <ul> <li><code>shouldStop()</code> - true if signals exist and no continuation requested</li> <li><code>isContinuationRequested()</code> - true if a hook requested continuation</li> <li>Hooks can override stop signals by calling <code>$state-&gt;withExecutionContinued()</code></li> </ul>"},{"location":"packages/agents/09-stop-conditions/#emitting-stop-signals-from-hooks","title":"Emitting Stop Signals from Hooks","text":"<p>Guard hooks emit stop signals by modifying state:</p> <pre><code>$state = $context-&gt;state()-&gt;withStopSignal(new StopSignal(\n    reason: StopReason::StepsLimitReached,\n    message: 'Limit reached',\n));\nreturn $context-&gt;withState($state);\n</code></pre> <p>The loop checks <code>shouldStop()</code> after each step and breaks if true.</p>"},{"location":"packages/agents/10-testing/","title":"Testing Agents","text":""},{"location":"packages/agents/10-testing/#fakeagentdriver","title":"FakeAgentDriver","text":"<p>Script deterministic agent behavior without LLM calls:</p> <pre><code>use Cognesy\\Agents\\Drivers\\Testing\\FakeAgentDriver;\nuse Cognesy\\Agents\\Drivers\\Testing\\ScenarioStep;\n\n$driver = FakeAgentDriver::fromResponses('Hello!');\n// or\n$driver = FakeAgentDriver::fromSteps(\n    ScenarioStep::toolCall('search', ['query' =&gt; 'php'], 'Results found'),\n    ScenarioStep::final('Based on the search, here is the answer.'),\n);\n</code></pre>"},{"location":"packages/agents/10-testing/#scenariostep-types","title":"ScenarioStep Types","text":"<pre><code>ScenarioStep::final('response text');       // Final response, loop stops\nScenarioStep::tool('intermediate text');     // Tool step, loop continues\nScenarioStep::error('error text');           // Error step\nScenarioStep::toolCall(                      // Tool call with execution\n    toolName: 'bash',\n    args: ['command' =&gt; 'ls'],\n    response: '',\n    executeTools: true,                      // actually run the tool\n);\n</code></pre>"},{"location":"packages/agents/10-testing/#full-test-example","title":"Full Test Example","text":"<pre><code>use Cognesy\\Agents\\AgentLoop;use Cognesy\\Agents\\Collections\\Tools;use Cognesy\\Agents\\Data\\AgentState;use Cognesy\\Agents\\Drivers\\Testing\\FakeAgentDriver;use Cognesy\\Agents\\Drivers\\Testing\\ScenarioStep;use Cognesy\\Agents\\Tool\\Tools\\MockTool;\n\nit('executes tools and produces final response', function () {\n    $tool = MockTool::returning('search', 'Search the web', 'PHP is great');\n\n    $driver = FakeAgentDriver::fromSteps(\n        ScenarioStep::toolCall('search', ['query' =&gt; 'php']),\n        ScenarioStep::final('PHP is a programming language.'),\n    );\n\n    $loop = AgentLoop::default()\n        -&gt;withTools(new Tools($tool))\n        -&gt;withDriver($driver);\n\n    $state = AgentState::empty()-&gt;withUserMessage('Tell me about PHP');\n    $result = $loop-&gt;execute($state);\n\n    expect($result-&gt;stepCount())-&gt;toBe(2);\n    expect($result-&gt;finalResponse()-&gt;toString())-&gt;toContain('PHP');\n});\n</code></pre>"},{"location":"packages/agents/10-testing/#using-iterate-for-step-level-testing","title":"Using iterate() for Step-Level Testing","text":"<pre><code>$steps = [];\nforeach ($loop-&gt;iterate($state) as $stepState) {\n    $steps[] = $stepState;\n}\n\nexpect($steps)-&gt;toHaveCount(2);\nexpect($steps[0]-&gt;lastStepType())-&gt;toBe(AgentStepType::ToolExecution);\nexpect($steps[1]-&gt;lastStepType())-&gt;toBe(AgentStepType::FinalResponse);\n</code></pre>"},{"location":"packages/agents/10-testing/#mocktool","title":"MockTool","text":"<p>Stub tools with fixed return values or custom logic:</p> <pre><code>use Cognesy\\Agents\\Tool\\Tools\\MockTool;\n\n// Fixed return value\n$tool = MockTool::returning('search', 'Search the web', 'result text');\n\n// Custom logic\n$tool = new MockTool('calculator', 'Math operations', fn(string $expr) =&gt; eval(\"return {$expr};\"));\n</code></pre> <p>Combine with <code>FakeAgentDriver</code> to test the full loop without any real LLM or tool calls, or with <code>ScenarioStep::toolCall(..., executeTools: true)</code> to actually execute the mock tool during the scenario.</p>"},{"location":"packages/agents/10-testing/#fakeinferencedriver","title":"FakeInferenceDriver","text":"<p>For testing the real <code>ToolCallingDriver</code> with scripted LLM responses:</p> <pre><code>use Cognesy\\Polyglot\\Inference\\Data\\InferenceResponse;\nuse Cognesy\\Polyglot\\Inference\\InferenceRuntime;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n$fakeDriver = new FakeInferenceDriver([\n    new InferenceResponse(content: 'Hello!'),\n]);\n\n$llm = LLMProvider::new()-&gt;withDriver($fakeDriver);\n$driver = new ToolCallingDriver(\n    inference: InferenceRuntime::fromProvider($llm),\n    llm: $llm,\n);\n</code></pre> <p>This tests the full driver pipeline (message compilation, response parsing) without network calls.</p>"},{"location":"packages/agents/11-state-internals/","title":"Agent State Internals","text":""},{"location":"packages/agents/11-state-internals/#agentstate-structure","title":"AgentState Structure","text":"<pre><code>AgentState (readonly)\n  |-- agentId: AgentId             # typed UUID value object, auto-generated\n  |-- parentAgentId: ?AgentId      # set for subagents\n  |-- createdAt: DateTimeImmutable\n  |-- updatedAt: DateTimeImmutable  # bumped on every mutation\n  |-- context: AgentContext\n  |   |-- store: MessageStore\n  |   |-- metadata: Metadata\n  |   |-- systemPrompt: string\n  |   |-- responseFormat: ResponseFormat\n  |-- budget: AgentBudget\n  |   |-- maxSteps: ?int\n  |   |-- maxTokens: ?int\n  |   |-- maxSeconds: ?float\n  |   |-- maxCost: ?float\n  |   |-- deadline: ?DateTimeImmutable\n  |-- execution: ?ExecutionState    # null between executions\n      |-- executionId: ExecutionId\n      |-- status: ExecutionStatus   # Pending|InProgress|Completed|Failed\n      |-- startedAt / completedAt\n      |-- stepExecutions: StepExecutions  # completed steps\n      |-- continuation: ExecutionContinuation\n      |   |-- stopSignals: StopSignals\n      |   |-- isContinuationRequested: bool\n      |-- currentStepStartedAt\n      |-- currentStep: ?AgentStep   # in-progress step\n          |-- id: AgentStepId\n          |-- inputMessages\n          |-- outputMessages\n          |-- inferenceResponse\n          |-- toolExecutions: ToolExecutions (items have ToolExecutionId)\n          |-- errors: ErrorList\n</code></pre>"},{"location":"packages/agents/11-state-internals/#key-accessors","title":"Key Accessors","text":"<pre><code>// Identity\n$state-&gt;agentId()-&gt;toString();\n$state-&gt;parentAgentId()?-&gt;toString();\n\n// Timing\n$state-&gt;createdAt();\n$state-&gt;executionDuration();\n\n// Context\n$state-&gt;messages();\n$state-&gt;metadata();\n$state-&gt;context()-&gt;systemPrompt();\n\n// AgentBudget\n$state-&gt;budget()-&gt;maxSteps;\n$state-&gt;budget()-&gt;isExhausted();\n\n// Execution\n$state-&gt;status();                    // ExecutionStatus enum\n$state-&gt;execution()?-&gt;executionId()-&gt;toString();\n$state-&gt;stepCount();\n$state-&gt;steps();                     // AgentSteps collection\n$state-&gt;lastStep();\n$state-&gt;lastStepType();\n$state-&gt;lastStopReason();\n$state-&gt;usage();                     // total token usage\n$state-&gt;errors();\n$state-&gt;hasErrors();\n\n// Final output\n$state-&gt;hasFinalResponse();\n$state-&gt;finalResponse()-&gt;toString();\n</code></pre>"},{"location":"packages/agents/11-state-internals/#agentbudget","title":"AgentBudget","text":"<p>Budgets propagate through delegation chains. Each subagent inherits the remaining budget:</p> <pre><code>$budget = new AgentBudget(maxSteps: 20, maxTokens: 10000, maxSeconds: 60.0);\n\n// After 5 steps and 3000 tokens:\n$remaining = $budget-&gt;remaining(stepsUsed: 5, tokensUsed: 3000);\n// maxSteps: 15, maxTokens: 7000, maxSeconds: 60.0\n\n// Cap by another budget:\n$capped = $budget-&gt;cappedBy(new AgentBudget(maxSteps: 10));\n// maxSteps: 10 (takes minimum)\n</code></pre>"},{"location":"packages/agents/11-state-internals/#serialization","title":"Serialization","text":"<p>All state objects support <code>toArray()</code> / <code>fromArray()</code> for persistence:</p> <pre><code>$data = $state-&gt;toArray();\n$restored = AgentState::fromArray($data);\n</code></pre>"},{"location":"packages/agents/12-tool-calling-internals/","title":"Tool Calling Internals","text":"<p>The agent's ability to use tools relies on two contracts and two drivers that implement them differently.</p>"},{"location":"packages/agents/12-tool-calling-internals/#architecture","title":"Architecture","text":"<pre><code>AgentLoop\n  |-- CanUseTools (driver)          # decides what tools to call\n  |   |-- ToolCallingDriver         # native LLM function calling\n  |   |-- ReActDriver               # Thought/Action/Observation via structured output\n  |   |-- FakeAgentDriver           # scripted responses for testing\n  |\n  |-- CanExecuteToolCalls (executor) # runs the actual tools\n      |-- ToolExecutor              # default implementation\n</code></pre>"},{"location":"packages/agents/12-tool-calling-internals/#the-two-contracts","title":"The Two Contracts","text":""},{"location":"packages/agents/12-tool-calling-internals/#canusetools-driver","title":"CanUseTools (Driver)","text":"<p>Sends state + tools to the LLM, gets back an updated state with tool call decisions:</p> <pre><code>interface CanUseTools {\n    public function useTools(AgentState $state, Tools $tools, CanExecuteToolCalls $executor): AgentState;\n}\n</code></pre>"},{"location":"packages/agents/12-tool-calling-internals/#canexecutetoolcalls-executor","title":"CanExecuteToolCalls (Executor)","text":"<p>Runs tool calls and returns execution results:</p> <pre><code>interface CanExecuteToolCalls {\n    public function executeTools(ToolCalls $toolCalls, AgentState $state): ToolExecutions;\n}\n</code></pre>"},{"location":"packages/agents/12-tool-calling-internals/#toolcallingdriver","title":"ToolCallingDriver","text":"<p>Uses the LLM's native function calling API.</p> <p>Flow: 1. Compile messages from state via <code>CanCompileMessages</code> 2. Send messages + tool schemas to LLM via <code>Inference</code> 3. Parse <code>InferenceResponse</code> for tool calls 4. Pass tool calls to <code>ToolExecutor</code> 5. Format execution results as assistant/tool message pairs 6. Return updated state with new <code>AgentStep</code></p> <pre><code>$inference = InferenceRuntime::fromProvider($llm);\n\n$driver = new ToolCallingDriver(\n    inference: $inference,\n    llm: $llm,\n    model: 'gpt-4o',\n    toolChoice: 'auto',          // 'auto', 'required', or specific tool\n    mode: OutputMode::Tools,\n);\n</code></pre> <p>The LLM natively understands tools and returns structured <code>tool_calls</code> in its response.</p>"},{"location":"packages/agents/12-tool-calling-internals/#reactdriver","title":"ReActDriver","text":"<p>Uses structured output to extract Thought/Action/Observation decisions.</p> <p>Flow: 1. Build a system prompt describing available tools and ReAct format 2. Use <code>StructuredOutput</code> to extract a <code>ReActDecision</code> from the LLM 3. Validate the decision (type, tool existence, arguments) 4. If <code>call_tool</code>: execute via <code>ToolExecutor</code>, format as Observation messages 5. If <code>final_answer</code>: return the answer as the final response</p> <pre><code>$events = EventBusResolver::using(null);\n$inference = InferenceRuntime::fromProvider($llm, events: $events);\n$structuredOutput = new StructuredOutputRuntime(\n    inference: $inference,\n    events: $events,\n    config: (new StructuredOutputConfigBuilder())\n        -&gt;withOutputMode(OutputMode::Json)\n        -&gt;withMaxRetries(2)\n        -&gt;create(),\n);\n\n$driver = new ReActDriver(\n    inference: $inference,\n    structuredOutput: $structuredOutput,\n    llm: $llm,\n    model: 'gpt-4o',\n    mode: OutputMode::Json,\n    maxRetries: 2,               // retries on extraction failure\n    finalViaInference: false,    // optionally use separate LLM call for final answer\n);\n</code></pre> <p>The LLM doesn't need native tool support - it outputs JSON with <code>type</code>, <code>tool</code>, <code>args</code>, and <code>thought</code> fields.</p>"},{"location":"packages/agents/12-tool-calling-internals/#toolexecutor","title":"ToolExecutor","text":"<p>The default <code>CanExecuteToolCalls</code> implementation. For each tool call:</p> <ol> <li>BeforeToolUse hook - can modify the call or block it</li> <li>Prepare tool - inject <code>AgentState</code> if tool implements <code>CanAccessAgentState</code></li> <li>Validate args - check required parameters</li> <li>Execute - call <code>$tool-&gt;use(...$args)</code></li> <li>AfterToolUse hook - can modify the result</li> <li>Emit events - <code>ToolCallStarted</code>, <code>ToolCallCompleted</code></li> </ol> <p>The <code>ToolExecutor</code> is created automatically by <code>AgentLoop::default()</code>. To customize it:</p> <pre><code>$executor = new ToolExecutor(\n    tools: $tools,\n    events: $events,\n    interceptor: $interceptor,\n    throwOnToolFailure: false,  // true = throw on first tool error\n    stopOnToolBlock: false,     // true = stop executing remaining tools if one is blocked\n);\n\n$loop = AgentLoop::default()-&gt;withTools($tools)-&gt;withToolExecutor($executor);\n</code></pre>"},{"location":"packages/agents/12-tool-calling-internals/#when-to-use-which-driver","title":"When to Use Which Driver","text":"ToolCallingDriver ReActDriver Requires LLM with function calling Any LLM with JSON output Tool selection Native, reliable Structured output extraction Reasoning Implicit Explicit (Thought field) Reliability Higher (native API) Lower (parsing required) Flexibility Standard tools only Custom decision schemas"},{"location":"packages/agents/13-agent-builder/","title":"AgentBuilder &amp; Capabilities","text":"<p><code>AgentBuilder</code> is a composition layer that assembles an <code>AgentLoop</code> from pluggable capabilities. Each capability (<code>Use*</code> class) installs tools, hooks, drivers, or compilers into the builder. The result is a configured <code>AgentLoop</code> ready for execution.</p>"},{"location":"packages/agents/13-agent-builder/#why-agentbuilder","title":"Why AgentBuilder","text":"<p><code>AgentLoop</code> is a stateless execution engine with sensible defaults. You can use it directly for simple agents. But as you add guards, custom tools, hooks, and compilers, manual setup becomes verbose and error-prone.</p> <p><code>AgentBuilder</code> solves this by letting you compose features as independent, reusable modules:</p> <pre><code>use Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Bash\\UseBash;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\Core\\UseLLMConfig;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(preset: 'anthropic'))\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseGuards(maxSteps: 20, maxTokens: 32768))\n    -&gt;build();\n\n$state = AgentState::empty()-&gt;withUserMessage('List files in /tmp');\n$result = $agent-&gt;execute($state);\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#core-api","title":"Core API","text":"<p><code>AgentBuilder</code> exposes two interfaces:</p> <p><code>CanComposeAgentLoop</code> (user-facing):</p> <pre><code>$builder-&gt;withCapability(CanProvideAgentCapability $capability): self;\n$builder-&gt;build(): AgentLoop;\n</code></pre> <p><code>CanConfigureAgent</code> (capability-facing):</p> <pre><code>$agent-&gt;tools(): Tools;\n$agent-&gt;withTools(Tools $tools): self;\n\n$agent-&gt;contextCompiler(): CanCompileMessages;\n$agent-&gt;withContextCompiler(CanCompileMessages $compiler): self;\n\n$agent-&gt;toolUseDriver(): CanUseTools;\n$agent-&gt;withToolUseDriver(CanUseTools $driver): self;\n\n$agent-&gt;hooks(): HookStack;\n$agent-&gt;withHooks(HookStack $hooks): self;\n\n$agent-&gt;deferredTools(): DeferredToolProviders;\n$agent-&gt;withDeferredTools(DeferredToolProviders $providers): self;\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#writing-a-capability","title":"Writing a Capability","text":"<p>Implement <code>CanProvideAgentCapability</code> with <code>capabilityName()</code> and <code>configure()</code>:</p> <pre><code>use Cognesy\\Agents\\Builder\\Contracts\\CanProvideAgentCapability;\nuse Cognesy\\Agents\\Builder\\Contracts\\CanConfigureAgent;\n\nclass UseRateLimiting implements CanProvideAgentCapability\n{\n    public function __construct(\n        private int $maxCallsPerMinute = 60,\n    ) {}\n\n    public static function capabilityName(): string {\n        return 'use_rate_limiting';\n    }\n\n    public function configure(CanConfigureAgent $agent): CanConfigureAgent {\n        $hooks = $agent-&gt;hooks()-&gt;with(\n            hook: new RateLimitHook($this-&gt;maxCallsPerMinute),\n            triggerTypes: HookTriggers::beforeToolUse(),\n            priority: 200,\n        );\n        return $agent-&gt;withHooks($hooks);\n    }\n}\n</code></pre> <p>Capabilities can configure any aspect of the agent: tools, hooks, driver, compiler, and deferred tool providers.</p>"},{"location":"packages/agents/13-agent-builder/#what-a-capability-can-do","title":"What a Capability Can Do","text":"<p>A capability receives a <code>CanConfigureAgent</code> instance and returns a modified copy. The five configuration surfaces are:</p> Surface Method Typical Use Tools <code>withTools()</code> Add tool instances to the agent Hooks <code>withHooks()</code> Register lifecycle hooks (guards, logging, state transforms) Driver <code>withToolUseDriver()</code> Replace or wrap the tool-use driver Compiler <code>withContextCompiler()</code> Replace or wrap the message compiler Deferred tools <code>withDeferredTools()</code> Register tools resolved at <code>build()</code> time"},{"location":"packages/agents/13-agent-builder/#adding-tools","title":"Adding Tools","text":"<p>The most common pattern \u2014 merge new tools into the existing set:</p> <pre><code>public function configure(CanConfigureAgent $agent): CanConfigureAgent {\n    $myTool = new MyCustomTool();\n    return $agent-&gt;withTools(\n        $agent-&gt;tools()-&gt;merge(new Tools($myTool))\n    );\n}\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#adding-hooks","title":"Adding Hooks","text":"<p>Register a hook with trigger types and priority:</p> <pre><code>public function configure(CanConfigureAgent $agent): CanConfigureAgent {\n    $hooks = $agent-&gt;hooks()-&gt;with(\n        hook: new MyHook(),\n        triggerTypes: HookTriggers::afterStep(),\n        priority: 10,\n    );\n    return $agent-&gt;withHooks($hooks);\n}\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#deferred-tools","title":"Deferred Tools","text":"<p>Some tools need access to the final driver or event bus, which aren't available until <code>build()</code> runs. Use <code>CanProvideDeferredTools</code> to defer tool creation:</p> <pre><code>use Cognesy\\Agents\\Builder\\Contracts\\CanProvideDeferredTools;\nuse Cognesy\\Agents\\Builder\\Data\\DeferredToolContext;\n\npublic function configure(CanConfigureAgent $agent): CanConfigureAgent {\n    $deferred = new class implements CanProvideDeferredTools {\n        public function provideTools(DeferredToolContext $context): Tools {\n            // $context gives you: tools(), toolUseDriver(), events()\n            return new Tools(new MyToolNeedingDriver($context-&gt;toolUseDriver()));\n        }\n    };\n\n    return $agent-&gt;withDeferredTools(\n        $agent-&gt;deferredTools()-&gt;withProvider($deferred)\n    );\n}\n</code></pre> <p><code>UseSubagents</code> uses this pattern \u2014 the <code>SpawnSubagentTool</code> needs the parent's driver and event bus, which are only finalized at build time.</p>"},{"location":"packages/agents/13-agent-builder/#multi-concern-capabilities","title":"Multi-Concern Capabilities","text":"<p>A single capability can install multiple components. For example, a capability might add a tool, register a persistence hook, and set a response format \u2014 all in one <code>configure()</code> call:</p> <pre><code>public function configure(CanConfigureAgent $agent): CanConfigureAgent {\n    // Add the tool\n    $agent = $agent-&gt;withTools($agent-&gt;tools()-&gt;merge(new Tools(new DataExtractionTool())));\n\n    // Add a persistence hook\n    $agent = $agent-&gt;withHooks($agent-&gt;hooks()-&gt;with(\n        hook: new PersistResultsHook(),\n        triggerTypes: HookTriggers::afterStep(),\n        priority: -50,\n    ));\n\n    return $agent;\n}\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#capability-names","title":"Capability Names","text":"<p><code>capabilityName()</code> returns a unique string identifier (e.g., <code>'use_bash'</code>). This is used by the <code>AgentCapabilityRegistry</code> and agent templates to reference capabilities by name. See Agent Templates for how definitions reference capabilities.</p>"},{"location":"packages/agents/13-agent-builder/#built-in-capabilities","title":"Built-in Capabilities","text":""},{"location":"packages/agents/13-agent-builder/#core-primitives","title":"Core Primitives","text":"Capability Purpose <code>UseGuards</code> Step, token, time, and finish-reason guards <code>UseLLMConfig</code> LLM provider preset and retry policy <code>UseContextConfig</code> System prompt and response format <code>UseDriver</code> Custom driver implementation <code>UseTools</code> Individual tool instances <code>UseHook</code> Single hook with trigger and priority <code>UseContextCompiler</code> Custom message compiler <code>UseContextCompilerDecorator</code> Wrap the existing compiler <code>UseDriverDecorator</code> Wrap the existing driver <code>UseToolFactory</code> Deferred tool creation (runs at <code>build()</code> time)"},{"location":"packages/agents/13-agent-builder/#domain-capabilities","title":"Domain Capabilities","text":"Capability What It Installs <code>UseBash</code> Bash command execution tool (with sandbox policy) <code>UseFileTools</code> File read/write/edit tools (scoped to a base directory) <code>UseSubagents</code> Subagent spawning tool with depth control <code>UseStructuredOutputs</code> Schema-based data extraction tool + persistence hook <code>UseSummarization</code> Message-to-buffer and buffer summarization hooks <code>UseSelfCritique</code> Self-critique loop hook <code>UseSkills</code> Skill injection for subagents <code>UseTaskPlanning</code> Task planning tool <code>UseMetadataTools</code> Metadata read/write tools <code>UseToolRegistry</code> Dynamic tool registration"},{"location":"packages/agents/13-agent-builder/#capability-examples","title":"Capability Examples","text":""},{"location":"packages/agents/13-agent-builder/#minimal-agent-no-tools","title":"Minimal agent (no tools)","text":"<pre><code>$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(preset: 'anthropic'))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#file-system-agent-with-guards","title":"File system agent with guards","text":"<pre><code>$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(preset: 'openai'))\n    -&gt;withCapability(new UseFileTools(baseDir: '/home/user/workspace'))\n    -&gt;withCapability(new UseGuards(maxSteps: 15, maxExecutionTime: 60.0))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#agent-with-custom-hook","title":"Agent with custom hook","text":"<pre><code>$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseHook(\n        hook: new CallableHook(function (HookContext $ctx): HookContext {\n            $command = $ctx-&gt;toolCall()?-&gt;args()['command'] ?? '';\n            if (str_contains($command, 'rm -rf')) {\n                return $ctx-&gt;withToolExecutionBlocked('Dangerous command blocked');\n            }\n            return $ctx;\n        }),\n        triggers: HookTriggers::beforeToolUse(),\n        priority: 100,\n    ))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#custom-context-compiler","title":"Custom context compiler","text":"<pre><code>$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseContextCompiler(new MyCustomCompiler()))\n    -&gt;build();\n\n// Or wrap the default compiler via decorator\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseContextCompilerDecorator(\n        fn(CanCompileMessages $inner) =&gt; new TokenLimitingCompiler($inner, maxTokens: 4000)\n    ))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#build-resolution-order","title":"Build Resolution Order","text":"<p>When <code>build()</code> is called, components are resolved in order:</p> <ol> <li>Compiler - configured compiler channel</li> <li>Driver - configured driver channel, then compiler/events are rebound</li> <li>Tools - configured tools merged with deferred tool providers (resolved with tools+driver+events context)</li> <li>Interceptor - derived from configured hooks (<code>HookStack</code>), or pass-through when no hooks</li> </ol> <p>This ordering matters: deferred tools run after driver resolution so they can access the final driver and events (needed by <code>UseSubagents</code>).</p>"},{"location":"packages/agents/13-agent-builder/#hook-priority-convention","title":"Hook Priority Convention","text":"Range Purpose 200+ Guards (steps, tokens, time) 100 Context preparation, security checks 0 Default (business logic) -50 Persistence, logging -200 Deferred processing (summarization, buffer management) <p>Higher priority hooks run first within each trigger phase.</p>"},{"location":"packages/agents/13-agent-builder/#events-and-logging","title":"Events and Logging","text":"<p>Pass a parent event handler to <code>AgentBuilder::base()</code> for event propagation:</p> <pre><code>$events = new EventDispatcher();\n$agent = AgentBuilder::base($events)\n    -&gt;withCapability(new UseBash())\n    -&gt;build();\n\n// Or attach a logger after building\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true);\n$agent-&gt;wiretap($logger-&gt;wiretap());\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#immutability","title":"Immutability","text":"<p><code>withCapability()</code> returns a new builder instance. This means you can safely branch from a base configuration:</p> <pre><code>$base = AgentBuilder::base()\n    -&gt;withCapability(new UseLLMConfig(preset: 'anthropic'))\n    -&gt;withCapability(new UseGuards(maxSteps: 20));\n\n$bashAgent = $base-&gt;withCapability(new UseBash())-&gt;build();\n$fileAgent = $base-&gt;withCapability(new UseFileTools(baseDir: '.'))-&gt;build();\n// $base is unchanged\n</code></pre>"},{"location":"packages/agents/13-agent-builder/#builder-internals","title":"Builder Internals","text":"<p><code>AgentBuilder</code> is a thin facade. The actual assembly happens in <code>AgentConfigurator</code>, an internal class that accumulates configuration and resolves it into an <code>AgentLoop</code>.</p> <pre><code>AgentBuilder (user-facing)\n  \u2192 collects CanProvideAgentCapability instances\n  \u2192 on build(): creates AgentConfigurator, installs all capabilities, calls toAgentLoop()\n\nAgentConfigurator (internal, implements CanConfigureAgent)\n  \u2192 tools: Tools\n  \u2192 contextCompiler: CanCompileMessages\n  \u2192 toolUseDriver: CanUseTools\n  \u2192 hooks: HookStack\n  \u2192 deferredTools: DeferredToolProviders\n  \u2192 events: CanHandleEvents\n  \u2192 toAgentLoop(): resolves driver \u2192 tools \u2192 interceptor \u2192 AgentLoop\n</code></pre> <p>Each <code>capability.configure(configurator)</code> call returns a new <code>AgentConfigurator</code> with the applied changes. When all capabilities are installed, <code>toAgentLoop()</code> runs the resolution pipeline and produces the final <code>AgentLoop</code>.</p>"},{"location":"packages/agents/13-agent-builder/#see-also","title":"See Also","text":"<ul> <li>Agent Templates \u2014 define agents as data files and reference capabilities by name</li> <li>Subagents \u2014 task delegation via <code>UseSubagents</code></li> </ul>"},{"location":"packages/agents/14-agent-templates/","title":"Agent Templates","text":"<p>Agent templates let you define agents as data \u2014 in markdown, YAML, or JSON files \u2014 and instantiate them at runtime. This separates agent configuration from code and enables dynamic agent loading.</p>"},{"location":"packages/agents/14-agent-templates/#agentdefinition","title":"AgentDefinition","text":"<p><code>AgentDefinition</code> is a data class that describes an agent's configuration:</p> <pre><code>use Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Data\\AgentBudget;\n\n$definition = new AgentDefinition(\n    name: 'researcher',\n    description: 'Searches for information on a topic',\n    systemPrompt: 'You are a research assistant. Find and summarize information.',\n    label: 'Research Agent',           // optional display name\n    llmConfig: 'anthropic',            // optional LLM preset or LLMConfig\n    budget: new AgentBudget(maxSteps: 10, maxTokens: 8000),\n    tools: new NameList(['bash', 'file.read']),       // optional tool allow-list\n    toolsDeny: new NameList(['file.write']),           // optional tool deny-list\n    capabilities: new NameList(['use_bash']),          // capability names to install\n);\n</code></pre> <p>All fields except <code>name</code>, <code>description</code>, and <code>systemPrompt</code> are optional.</p>"},{"location":"packages/agents/14-agent-templates/#definition-files","title":"Definition Files","text":""},{"location":"packages/agents/14-agent-templates/#markdown-with-yaml-frontmatter","title":"Markdown (with YAML frontmatter)","text":"<pre><code>---\nname: researcher\ndescription: Searches for information on a topic\nlabel: Research Agent\nllmConfig: anthropic\nbudget:\n  maxSteps: 10\n  maxTokens: 8000\ntools:\n  - bash\n  - file.read\ncapabilities:\n  - use_bash\n---\n\nYou are a research assistant. Find and summarize information accurately.\nUse available tools to search and verify your findings.\n</code></pre> <p>The body after the frontmatter becomes the <code>systemPrompt</code>.</p>"},{"location":"packages/agents/14-agent-templates/#yaml","title":"YAML","text":"<pre><code>name: researcher\ndescription: Searches for information on a topic\nsystemPrompt: |\n  You are a research assistant. Find and summarize information.\nbudget:\n  maxSteps: 10\n</code></pre>"},{"location":"packages/agents/14-agent-templates/#json","title":"JSON","text":"<pre><code>{\n  \"name\": \"researcher\",\n  \"description\": \"Searches for information on a topic\",\n  \"systemPrompt\": \"You are a research assistant.\",\n  \"budget\": { \"maxSteps\": 10 }\n}\n</code></pre>"},{"location":"packages/agents/14-agent-templates/#loading-definitions","title":"Loading Definitions","text":""},{"location":"packages/agents/14-agent-templates/#agentdefinitionloader","title":"AgentDefinitionLoader","text":"<p>Loads a single file and returns an <code>AgentDefinition</code>:</p> <pre><code>use Cognesy\\Agents\\Template\\AgentDefinitionLoader;\n\n$loader = new AgentDefinitionLoader();\n$definition = $loader-&gt;loadFile('/path/to/researcher.md');\n</code></pre> <p>Supported extensions: <code>.md</code>, <code>.json</code>, <code>.yaml</code>, <code>.yml</code>.</p>"},{"location":"packages/agents/14-agent-templates/#agentdefinitionregistry","title":"AgentDefinitionRegistry","text":"<p>Stores definitions by name and supports bulk loading:</p> <pre><code>use Cognesy\\Agents\\Template\\AgentDefinitionRegistry;\n\n$registry = new AgentDefinitionRegistry();\n\n// Register programmatically\n$registry-&gt;register($definition);\n$registry-&gt;registerMany($def1, $def2, $def3);\n\n// Load from files\n$registry-&gt;loadFromFile('/agents/researcher.md');\n$registry-&gt;loadFromDirectory('/agents/', recursive: true);\n\n// Auto-discover from conventional locations\n$registry-&gt;autoDiscover(projectPath: '/my/project');\n// Looks in: $projectPath/.claude/agents/\n\n// Query\n$registry-&gt;get('researcher');    // AgentDefinition (throws if not found)\n$registry-&gt;has('researcher');    // bool\n$registry-&gt;names();              // ['researcher', 'writer', ...]\n$registry-&gt;count();              // int\n</code></pre> <p>Loading errors are collected silently \u2014 check <code>$registry-&gt;errors()</code> for any files that failed to parse.</p>"},{"location":"packages/agents/14-agent-templates/#custom-parsers","title":"Custom Parsers","text":"<p>Implement <code>CanParseAgentDefinition</code> for custom formats:</p> <pre><code>use Cognesy\\Agents\\Template\\Parsers\\CanParseAgentDefinition;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\n\nclass TomlDefinitionParser implements CanParseAgentDefinition\n{\n    public function parse(string $content): AgentDefinition\n    {\n        $data = toml_parse($content);\n        return AgentDefinition::fromArray($data);\n    }\n}\n\n// Register with custom extension\n$loader = new AgentDefinitionLoader([\n    'toml' =&gt; new TomlDefinitionParser(),\n    'md' =&gt; new MarkdownDefinitionParser(),   // keep defaults\n]);\n</code></pre>"},{"location":"packages/agents/14-agent-templates/#instantiation-factories","title":"Instantiation Factories","text":"<p>Templates provide factories to convert definitions into runtime objects:</p>"},{"location":"packages/agents/14-agent-templates/#definitionstatefactory","title":"DefinitionStateFactory","text":"<p>Creates an <code>AgentState</code> from a definition:</p> <pre><code>use Cognesy\\Agents\\Template\\Factory\\DefinitionStateFactory;\n\n$factory = new DefinitionStateFactory();\n$state = $factory-&gt;create($definition);\n// AgentState with systemPrompt, budget, and metadata from definition\n</code></pre>"},{"location":"packages/agents/14-agent-templates/#definitionloopfactory","title":"DefinitionLoopFactory","text":"<p>Creates an <code>AgentLoop</code> from a definition using <code>AgentBuilder</code> and a capability registry:</p> <pre><code>use Cognesy\\Agents\\Template\\Factory\\DefinitionLoopFactory;\nuse Cognesy\\Agents\\Capability\\AgentCapabilityRegistry;\n\n$capabilities = new AgentCapabilityRegistry();\n// register capabilities that definitions can reference by name\n\n$factory = new DefinitionLoopFactory($capabilities);\n$loop = $factory-&gt;create($definition);\n</code></pre>"},{"location":"packages/agents/14-agent-templates/#using-with-subagents","title":"Using with Subagents","text":"<p>The registry implements <code>CanManageAgentDefinitions</code>, making it the standard provider for <code>UseSubagents</code>:</p> <pre><code>$registry = new AgentDefinitionRegistry();\n$registry-&gt;loadFromDirectory('/agents/');\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseSubagents(provider: $registry))\n    -&gt;build();\n</code></pre> <p>When the agent spawns a subagent by name, the registry provides the definition, and the factory creates the corresponding loop and state.</p>"},{"location":"packages/agents/14-agent-templates/#serialization","title":"Serialization","text":"<p><code>AgentDefinition</code> supports <code>toArray()</code> / <code>fromArray()</code> for serialization:</p> <pre><code>$data = $definition-&gt;toArray();\n$restored = AgentDefinition::fromArray($data);\n</code></pre>"},{"location":"packages/agents/15-subagents/","title":"Subagents","text":"<p>Subagents let a parent agent delegate focused tasks to independent child agents. Each subagent runs in isolation with its own context, tools, and budget \u2014 then returns a result to the parent.</p>"},{"location":"packages/agents/15-subagents/#why-subagents","title":"Why Subagents","text":"<p>A single agent working on a complex task accumulates context: tool outputs, intermediate reasoning, file contents. This leads to problems:</p> <ul> <li>Context pollution \u2014 irrelevant details from one subtask confuse the LLM during another</li> <li>Tool overload \u2014 the LLM sees tools it doesn't need, increasing error rates</li> <li>Budget waste \u2014 all tokens go against one shared limit with no per-task control</li> </ul> <p>Subagents solve these by providing:</p> <ul> <li>Context isolation \u2014 each subagent starts with a clean conversation</li> <li>Specialized tools \u2014 each subagent sees only the tools it needs</li> <li>Budget propagation \u2014 parent budget flows down, preventing runaway children</li> <li>Result aggregation \u2014 the parent synthesizes subagent outputs into a final answer</li> </ul>"},{"location":"packages/agents/15-subagents/#quick-start","title":"Quick Start","text":"<pre><code>use Cognesy\\Agents\\Builder\\AgentBuilder;\nuse Cognesy\\Agents\\Capability\\Core\\UseGuards;\nuse Cognesy\\Agents\\Capability\\File\\UseFileTools;\nuse Cognesy\\Agents\\Capability\\Subagent\\UseSubagents;\nuse Cognesy\\Agents\\Data\\AgentState;\nuse Cognesy\\Agents\\Template\\AgentDefinitionRegistry;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\n\n// 1. Define available subagents\n$registry = new AgentDefinitionRegistry();\n$registry-&gt;register(new AgentDefinition(\n    name: 'reviewer',\n    description: 'Reviews code files and identifies issues',\n    systemPrompt: 'You review code. Read the file and provide a concise assessment.',\n    tools: new NameList(['read_file']),\n));\n\n// 2. Build the parent agent with subagent capability\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseFileTools('/my/project'))\n    -&gt;withCapability(new UseSubagents(provider: $registry))\n    -&gt;withCapability(new UseGuards(maxSteps: 15))\n    -&gt;build();\n\n// 3. Run \u2014 the LLM decides when to spawn subagents\n$state = AgentState::empty()-&gt;withUserMessage(\n    'Review src/AgentLoop.php and src/AgentState.php, then summarize the findings.'\n);\n$result = $agent-&gt;execute($state);\n</code></pre> <p>The LLM sees a <code>spawn_subagent</code> tool with the available subagent names and descriptions. It decides which subagent to call and formulates the prompt for each.</p>"},{"location":"packages/agents/15-subagents/#how-it-works","title":"How It Works","text":""},{"location":"packages/agents/15-subagents/#the-spawn_subagent-tool","title":"The spawn_subagent Tool","text":"<p><code>UseSubagents</code> installs a <code>SpawnSubagentTool</code> that the LLM calls with two arguments:</p> <ul> <li><code>subagent</code> \u2014 name of the subagent to spawn (from the registry)</li> <li><code>prompt</code> \u2014 the task description for the subagent</li> </ul> <p>The tool schema includes an enum of available subagent names and their descriptions, so the LLM can make informed choices.</p>"},{"location":"packages/agents/15-subagents/#execution-flow","title":"Execution Flow","text":"<pre><code>Parent agent step:\n  1. LLM decides to call spawn_subagent(subagent: \"reviewer\", prompt: \"Review AgentLoop.php\")\n  2. SpawnSubagentTool looks up \"reviewer\" in the registry\n  3. Creates a child AgentLoop with filtered tools and budget\n  4. Creates child AgentState with the subagent's system prompt + user prompt\n  5. Runs the child loop to completion\n  6. Returns the child's final AgentState to the parent\n  7. Parent LLM receives the subagent result and continues\n</code></pre> <p>The parent never sees the subagent's internal conversation \u2014 only its final output. This is the key isolation guarantee.</p>"},{"location":"packages/agents/15-subagents/#what-the-parent-receives","title":"What the Parent Receives","text":"<p>The <code>SpawnSubagentTool</code> returns the child's complete <code>AgentState</code>. The <code>ToolExecutionFormatter</code> extracts the final response text and feeds it back to the parent LLM as the tool result.</p>"},{"location":"packages/agents/15-subagents/#defining-subagents","title":"Defining Subagents","text":"<p>Each subagent is described by an <code>AgentDefinition</code>. You can register them programmatically or load from files.</p>"},{"location":"packages/agents/15-subagents/#programmatic-registration","title":"Programmatic Registration","text":"<pre><code>use Cognesy\\Agents\\Template\\AgentDefinitionRegistry;\nuse Cognesy\\Agents\\Template\\Data\\AgentDefinition;\nuse Cognesy\\Agents\\Collections\\NameList;\nuse Cognesy\\Agents\\Data\\AgentBudget;\n\n$registry = new AgentDefinitionRegistry();\n\n$registry-&gt;registerMany(\n    new AgentDefinition(\n        name: 'researcher',\n        description: 'Searches and analyzes information',\n        systemPrompt: 'You are a research assistant. Be thorough and cite sources.',\n        tools: new NameList(['read_file', 'search_files']),\n        budget: new AgentBudget(maxSteps: 8, maxTokens: 4000),\n    ),\n    new AgentDefinition(\n        name: 'writer',\n        description: 'Writes and edits content',\n        systemPrompt: 'You are a technical writer. Write clear, concise documentation.',\n        tools: new NameList(['read_file', 'write_file']),\n        budget: new AgentBudget(maxSteps: 10),\n    ),\n);\n</code></pre>"},{"location":"packages/agents/15-subagents/#file-based-definitions","title":"File-Based Definitions","text":"<p>Load from markdown, YAML, or JSON files:</p> <pre><code>$registry = new AgentDefinitionRegistry();\n$registry-&gt;loadFromDirectory('/agents/', recursive: true);\n</code></pre> <p>A markdown definition file:</p> <pre><code>---\nname: reviewer\ndescription: Reviews code files and identifies issues\ntools:\n  - read_file\nbudget:\n  maxSteps: 5\n  maxTokens: 4000\n---\n\nYou review code. Focus on:\n- Code quality and readability\n- Potential bugs or edge cases\n- Suggestions for improvement\n\nBe concise. Report only significant findings.\n</code></pre> <p>See Agent Templates for full details on definition formats and loading.</p>"},{"location":"packages/agents/15-subagents/#tool-filtering","title":"Tool Filtering","text":"<p>Subagents don't automatically get all parent tools. You control access via allow-lists and deny-lists on the definition.</p>"},{"location":"packages/agents/15-subagents/#default-inherit-all-tools","title":"Default: Inherit All Tools","text":"<p>When <code>tools</code> is omitted, the subagent inherits all parent tools:</p> <pre><code>new AgentDefinition(\n    name: 'helper',\n    description: 'General purpose helper',\n    systemPrompt: 'You help with various tasks.',\n    // tools: null \u2192 inherits all parent tools\n);\n</code></pre>"},{"location":"packages/agents/15-subagents/#allow-list","title":"Allow-List","text":"<p>Specify exactly which tools the subagent can use:</p> <pre><code>new AgentDefinition(\n    name: 'reader',\n    description: 'Read-only file analyst',\n    systemPrompt: 'You analyze files. You cannot modify anything.',\n    tools: new NameList(['read_file', 'list_dir', 'search_files']),\n);\n</code></pre> <p>Only tools matching these names (from the parent's tool set) are passed to the subagent.</p>"},{"location":"packages/agents/15-subagents/#deny-list","title":"Deny-List","text":"<p>Block specific tools while inheriting the rest:</p> <pre><code>new AgentDefinition(\n    name: 'safe_coder',\n    description: 'Codes without dangerous operations',\n    systemPrompt: 'You write code. Never use bash or delete files.',\n    toolsDeny: new NameList(['bash', 'edit_file']),\n);\n</code></pre>"},{"location":"packages/agents/15-subagents/#combined-filtering","title":"Combined Filtering","text":"<p>When both are specified, the allow-list is applied first, then the deny-list filters out any remaining denied tools:</p> <pre><code>new AgentDefinition(\n    name: 'restricted',\n    description: 'Highly restricted agent',\n    systemPrompt: 'You operate under strict constraints.',\n    tools: new NameList(['read_file', 'write_file', 'bash']),\n    toolsDeny: new NameList(['bash']),\n    // Result: only read_file and write_file\n);\n</code></pre>"},{"location":"packages/agents/15-subagents/#budget-propagation","title":"Budget Propagation","text":"<p>Budgets flow from parent to child, preventing subagents from consuming more resources than the parent has remaining.</p>"},{"location":"packages/agents/15-subagents/#how-it-works_1","title":"How It Works","text":"<ol> <li>Parent has a budget (e.g., 100 steps, 10000 tokens)</li> <li>Parent uses some resources before spawning a subagent</li> <li>Child's effective budget = min(definition budget, parent's remaining budget)</li> <li>Child runs within its effective budget</li> </ol> <pre><code>// Parent budget: 100 steps\n// Parent has used 40 steps before spawning\n\n// Definition says: maxSteps: 80\n// Parent remaining: 60 steps\n// Child effective budget: min(80, 60) = 60 steps\n</code></pre>"},{"location":"packages/agents/15-subagents/#multi-level-propagation","title":"Multi-Level Propagation","text":"<p>Budget constraints cascade through nesting levels:</p> <pre><code>Root (100 steps)\n  \u2192 Level 1 gets min(definition, 100) = 80 steps\n    \u2192 Level 1 uses 30 steps, 50 remaining\n      \u2192 Level 2 gets min(definition, 50) = 50 steps\n</code></pre> <p>This ensures the total resource usage across the entire agent tree never exceeds the root budget.</p>"},{"location":"packages/agents/15-subagents/#setting-budgets","title":"Setting Budgets","text":"<p>On the definition:</p> <pre><code>new AgentDefinition(\n    name: 'quick_task',\n    description: 'Fast, focused task',\n    systemPrompt: 'Be concise.',\n    budget: new AgentBudget(maxSteps: 5, maxTokens: 2000, maxSeconds: 15.0),\n);\n</code></pre> <p>On the parent via guards:</p> <pre><code>$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseSubagents(provider: $registry))\n    -&gt;withCapability(new UseGuards(maxSteps: 50, maxTokens: 32768))\n    -&gt;build();\n</code></pre>"},{"location":"packages/agents/15-subagents/#depth-control","title":"Depth Control","text":"<p>Subagents can themselves spawn subagents (recursive delegation). <code>SubagentPolicy</code> controls the maximum nesting depth:</p> <pre><code>use Cognesy\\Agents\\Capability\\Subagent\\UseSubagents;\nuse Cognesy\\Agents\\Capability\\Subagent\\SubagentPolicy;\n\n// Allow up to 2 levels of nesting (default is 3)\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseSubagents(\n        provider: $registry,\n        policy: new SubagentPolicy(maxDepth: 2),\n    ))\n    -&gt;build();\n\n// Or use the convenience factory\n$agent = AgentBuilder::base()\n    -&gt;withCapability(UseSubagents::withDepth(2, provider: $registry))\n    -&gt;build();\n</code></pre> <p>When the depth limit is reached, <code>SpawnSubagentTool</code> throws <code>SubagentDepthExceededException</code>, which the parent sees as a tool error.</p>"},{"location":"packages/agents/15-subagents/#llm-configuration","title":"LLM Configuration","text":"<p>Each subagent can use a different LLM. Specify via <code>llmConfig</code> on the definition:</p> <pre><code>new AgentDefinition(\n    name: 'cheap_summarizer',\n    description: 'Summarizes text using a fast model',\n    systemPrompt: 'Summarize the given content concisely.',\n    llmConfig: 'openai:gpt-4o-mini',   // use a preset name\n);\n\nnew AgentDefinition(\n    name: 'deep_analyst',\n    description: 'Deep analysis using a powerful model',\n    systemPrompt: 'Provide thorough, detailed analysis.',\n    llmConfig: 'anthropic',            // use a different provider\n);\n</code></pre> <p>When <code>llmConfig</code> is omitted, the subagent inherits the parent's LLM provider.</p>"},{"location":"packages/agents/15-subagents/#events","title":"Events","text":"<p>Subagent lifecycle emits events for monitoring and debugging:</p> Event When <code>SubagentSpawning</code> Before subagent execution starts <code>SubagentCompleted</code> After subagent execution finishes <p>Both events include correlation data: <code>parentAgentId</code>, <code>parentExecutionId</code>, <code>parentStepNumber</code>, <code>toolCallId</code>, <code>depth</code>, and <code>subagentName</code>. This enables tracing the full delegation chain.</p> <pre><code>use Cognesy\\Agents\\Events\\SubagentSpawning;\nuse Cognesy\\Agents\\Events\\SubagentCompleted;\n\n$agent-&gt;onEvent(SubagentSpawning::class, function (SubagentSpawning $e) {\n    echo \"Spawning '{$e-&gt;subagentName}' at depth {$e-&gt;depth}/{$e-&gt;maxDepth}\\n\";\n});\n\n$agent-&gt;onEvent(SubagentCompleted::class, function (SubagentCompleted $e) {\n    echo \"'{$e-&gt;subagentName}' completed in {$e-&gt;steps} steps\\n\";\n});\n</code></pre> <p>Use <code>AgentEventConsoleObserver</code> to see parent/child agent IDs in console output:</p> <pre><code>use Cognesy\\Agents\\Events\\Support\\AgentEventConsoleObserver;\n\n$logger = new AgentEventConsoleObserver(useColors: true, showTimestamps: true);\n$agent-&gt;wiretap($logger-&gt;wiretap());\n</code></pre>"},{"location":"packages/agents/15-subagents/#error-handling","title":"Error Handling","text":"Exception When <code>SubagentNotFoundException</code> Subagent name not found in registry <code>SubagentDepthExceededException</code> Nesting depth limit reached <code>SubagentExecutionException</code> Subagent execution failed (wraps child errors) <p>All three are subclasses of <code>AgentException</code>. When a subagent fails, the parent sees a tool error with the exception details and can decide how to proceed.</p>"},{"location":"packages/agents/15-subagents/#researchsubagenttool","title":"ResearchSubagentTool","text":"<p>For simpler use cases, <code>ResearchSubagentTool</code> provides a pre-built subagent that reads files and returns a summary \u2014 without requiring a registry or definitions:</p> <pre><code>use Cognesy\\Agents\\Capability\\Subagent\\ResearchSubagentTool;\n\n$tool = ResearchSubagentTool::inDirectory('/my/project');\n\n$agent = AgentBuilder::base()\n    -&gt;withTool($tool)\n    -&gt;build();\n</code></pre> <p>The LLM calls it with a <code>task</code> and optional <code>files</code> list. The tool spawns a subagent with read-only file access, runs the research, and returns the findings.</p>"},{"location":"packages/agents/15-subagents/#testing-subagents","title":"Testing Subagents","text":"<p>Use <code>FakeAgentDriver</code> with <code>withChildSteps()</code> to script subagent behavior:</p> <pre><code>use Cognesy\\Agents\\Drivers\\Testing\\FakeAgentDriver;\nuse Cognesy\\Agents\\Drivers\\Testing\\ScenarioStep;\n\n$driver = (new FakeAgentDriver([\n    // Parent step: LLM calls spawn_subagent\n    ScenarioStep::toolCall('spawn_subagent', [\n        'subagent' =&gt; 'reviewer',\n        'prompt' =&gt; 'Review this code',\n    ], executeTools: true),\n    // Parent step: LLM produces final response\n    ScenarioStep::final('Based on the review, the code looks good.'),\n]))-&gt;withChildSteps([\n    // Subagent produces this response\n    ScenarioStep::final('I found 3 minor issues.'),\n]);\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseDriver($driver))\n    -&gt;withCapability(new UseSubagents(provider: $registry))\n    -&gt;build();\n\n$result = $agent-&gt;execute(AgentState::empty());\nexpect($result-&gt;finalResponse()-&gt;toString())-&gt;toContain('code looks good');\n</code></pre>"},{"location":"packages/agents/15-subagents/#typical-patterns","title":"Typical Patterns","text":""},{"location":"packages/agents/15-subagents/#fan-out-review","title":"Fan-Out Review","text":"<p>One parent spawns multiple subagents for independent subtasks, then synthesizes:</p> <pre><code>Parent: \"Review these 5 files\"\n  \u2192 spawn reviewer for file1 \u2192 \"2 issues found\"\n  \u2192 spawn reviewer for file2 \u2192 \"looks good\"\n  \u2192 spawn reviewer for file3 \u2192 \"1 critical bug\"\n  \u2192 ...\nParent: \"Summary: file1 has 2 issues, file3 has a critical bug...\"\n</code></pre>"},{"location":"packages/agents/15-subagents/#specialized-roles","title":"Specialized Roles","text":"<p>Different subagents handle different aspects of a task:</p> <pre><code>Parent: \"Analyze this codebase and write docs\"\n  \u2192 spawn researcher \u2192 \"The module handles X, Y, Z...\"\n  \u2192 spawn writer \u2192 \"# Module Documentation\\n...\"\nParent: combines research + docs into final output\n</code></pre>"},{"location":"packages/agents/15-subagents/#hierarchical-delegation","title":"Hierarchical Delegation","text":"<p>Subagents spawn their own subagents for further decomposition:</p> <pre><code>Orchestrator (depth 0)\n  \u2192 Analyst (depth 1)\n    \u2192 File reader (depth 2)\n  \u2192 Writer (depth 1)\n</code></pre> <p>Control the maximum depth via <code>SubagentPolicy</code> to prevent unbounded recursion.</p>"},{"location":"packages/agents/16-session-runtime/","title":"Session Runtime","text":""},{"location":"packages/agents/16-session-runtime/#overview","title":"Overview","text":"<p><code>SessionRuntime</code> is a thin application service for session actions.</p> <p>Flow is explicit and deterministic: - load session from repository - execute action via <code>executeOn(AgentSession)</code> - save persisted session</p> <p>It does not implement retries, fallbacks, queueing, or idempotency policies. Those concerns belong to adapters/infrastructure.</p>"},{"location":"packages/agents/16-session-runtime/#core-contracts","title":"Core Contracts","text":"<ul> <li><code>CanExecuteSessionAction</code>: <code>executeOn(AgentSession): AgentSession</code></li> <li><code>CanRunSessionRuntime</code>:</li> <li><code>execute(SessionId, CanExecuteSessionAction): AgentSession</code></li> <li><code>getSession(SessionId): AgentSession</code></li> <li><code>getSessionInfo(SessionId): AgentSessionInfo</code></li> <li><code>listSessions(): SessionInfoList</code></li> </ul>"},{"location":"packages/agents/16-session-runtime/#error-model","title":"Error Model","text":"<p>Session persistence uses exceptions for errors: - <code>SessionNotFoundException</code> - <code>SessionConflictException</code> - <code>InvalidSessionFileException</code></p> <p>Store API returns persisted session instances: - <code>create(AgentSession): AgentSession</code> - <code>save(AgentSession): AgentSession</code></p>"},{"location":"packages/agents/16-session-runtime/#built-in-session-actions","title":"Built-in Session Actions","text":"<p>Initial actions include: - <code>ResumeSession</code>, <code>SuspendSession</code>, <code>ClearSession</code> - <code>ChangeModel</code>, <code>ChangeBudget</code>, <code>ChangeSystemPrompt</code> - <code>WriteMetadata</code>, <code>UpdateTask</code> - <code>SendMessage</code>, <code>ForkSession</code></p> <p>All actions are immutable and return <code>AgentSession</code>.</p>"},{"location":"packages/http/1-overview/","title":"Overview","text":""},{"location":"packages/http/1-overview/#purpose-and-goals","title":"Purpose and Goals","text":"<p>The Instructor HTTP client API is designed to provide a consistent interface for making HTTP requests across different PHP environments.</p> <p>It provides a single API regardless of the underlying HTTP client available in the given environment, which may be Symfony, Laravel, Slim, or just vanilla PHP.</p> <p>The primary goals of the API are:</p> <ul> <li>Easily Switch Between HTTP Clients: Allow developers to switch between different HTTP client libraries (like Guzzle, Symfony, and Laravel) without changing the code that makes HTTP requests</li> <li>Framework Agnostic: Work seamlessly in Laravel, Symfony, or any PHP application without framework-specific dependencies</li> <li>Consistent Interface: Provide a one way to make HTTP requests regardless of the underlying client library</li> <li>Middleware Support: Enable easy extension through a powerful middleware system</li> <li>Adaptability: Allow switching between different HTTP client implementations with minimal code changes</li> <li>Streaming Support: Provide first-class support for streaming responses, which is crucial for LLM interactions</li> <li>Concurrency: Support parallel requests through request pooling</li> </ul> <p>By abstracting away the differences between various HTTP client libraries, the Instructor works across different environments and frameworks without modification.</p>"},{"location":"packages/http/1-overview/#key-features","title":"Key Features","text":""},{"location":"packages/http/1-overview/#multiple-client-support","title":"Multiple Client Support","text":"<p>The API supports multiple HTTP client libraries through specialized drivers:</p> <ul> <li>Guzzle: A popular and feature-rich HTTP client for PHP</li> <li>Symfony HTTP Client: The HTTP client component from the Symfony framework</li> <li>Laravel HTTP Client: The HTTP client built into the Laravel framework</li> </ul>"},{"location":"packages/http/1-overview/#middleware-system","title":"Middleware System","text":"<p>A powerful middleware architecture allows for:</p> <ul> <li>Request Pre-processing: Modify requests before they are sent</li> <li>Response Post-processing: Transform or analyze responses</li> <li>Response Streaming: Process streaming responses chunk by chunk</li> <li>Debugging: Log requests and responses for troubleshooting</li> <li>Custom Behaviors: Add specialized behaviors like caching or rate limiting</li> </ul>"},{"location":"packages/http/1-overview/#streaming-response-support","title":"Streaming Response Support","text":"<p>First-class support for streaming HTTP responses, which is essential for:</p> <ul> <li>LLM Text Generation: Process token-by-token responses from AI models</li> <li>Large File Downloads: Handle large files without excessive memory usage</li> <li>Real-time Data: Process server-sent events or other real-time data streams</li> </ul>"},{"location":"packages/http/1-overview/#request-pooling","title":"Request Pooling","text":"<p>Execute multiple HTTP requests concurrently for better performance:</p> <ul> <li>Concurrent Execution: Send multiple requests in parallel</li> <li>Configurable Concurrency: Control the maximum number of concurrent requests</li> <li>Result Collection: Process results as they arrive</li> <li>Error Handling: Flexible error handling strategies</li> </ul>"},{"location":"packages/http/1-overview/#flexible-configuration","title":"Flexible Configuration","text":"<p>Comprehensive configuration options:</p> <ul> <li>Per-Client Configuration: Different settings for each client type</li> <li>Named Configurations: Multiple configurations for different use cases</li> <li>Runtime Configuration: Change configuration during execution</li> <li>Timeout Controls: Fine-grained control over various timeout settings</li> </ul>"},{"location":"packages/http/1-overview/#debug-and-testing-support","title":"Debug and Testing Support","text":"<p>Built-in features for debugging and testing:</p> <ul> <li>Request/Response Logging: Detailed logging of HTTP interactions</li> <li>Mock Client: Test your code without making actual HTTP requests</li> <li>Record/Replay: Record HTTP interactions and replay them later</li> </ul>"},{"location":"packages/http/1-overview/#architecture-overview","title":"Architecture Overview","text":"<p>The Instructor HTTP client follows a layered architecture with several key components:</p>"},{"location":"packages/http/1-overview/#client-layer","title":"Client Layer","text":"<p>The <code>HttpClient</code> class serves as the main entry point and provides a fluent interface for configuring and using the HTTP client.</p> <pre><code>HttpClient\n  \u2514\u2500\u2500 using() - Create client with specific configuration (static)\n  \u2514\u2500\u2500 default() - Create client with default configuration (static)\n  \u2514\u2500\u2500 withMiddleware() - Add middleware components\n  \u2514\u2500\u2500 withMiddlewareStack() - Replace entire middleware stack\n  \u2514\u2500\u2500 withoutMiddleware() - Remove middleware by name\n  \u2514\u2500\u2500 withRequest() - Create pending request for execution\n  \u2514\u2500\u2500 pool() - Execute multiple requests concurrently\n  \u2514\u2500\u2500 withPool() - Create pending pool for deferred execution\n</code></pre>"},{"location":"packages/http/1-overview/#middleware-layer","title":"Middleware Layer","text":"<p>The middleware system allows for processing requests and responses through a chain of handlers:</p> <pre><code>Request -&gt; Middleware 1 -&gt; Middleware 2 -&gt; ... -&gt; Driver -&gt; External API\n                                                   \u2193\nResponse &lt;- Middleware 1 &lt;- Middleware 2 &lt;- ... &lt;- Driver &lt;- HTTP Response\n</code></pre> <p>Key components: - <code>MiddlewareStack</code>: Manages the collection of middleware - <code>MiddlewareHandler</code>: Orchestrates the middleware chain execution - <code>BaseMiddleware</code>: Base class for implementing middleware</p>"},{"location":"packages/http/1-overview/#driver-layer","title":"Driver Layer","text":"<p>Drivers implement the <code>CanHandleHttpRequest</code> interface and adapt different HTTP client libraries:</p> <pre><code>CanHandleHttpRequest (interface)\n  \u251c\u2500\u2500 GuzzleDriver\n  \u251c\u2500\u2500 SymfonyDriver\n  \u251c\u2500\u2500 LaravelDriver\n  \u2514\u2500\u2500 MockHttpDriver (for testing)\n</code></pre>"},{"location":"packages/http/1-overview/#adapter-layer","title":"Adapter Layer","text":"<p>Response adapters convert client-specific responses to a common interface:</p> <pre><code>HttpResponse (interface)\n  \u251c\u2500\u2500 PsrHttpResponse (Guzzle)\n  \u251c\u2500\u2500 SymfonyHttpResponse\n  \u251c\u2500\u2500 LaravelHttpResponse\n  \u2514\u2500\u2500 MockHttpResponse\n</code></pre>"},{"location":"packages/http/1-overview/#supported-http-clients","title":"Supported HTTP Clients","text":""},{"location":"packages/http/1-overview/#guzzle-http-client","title":"Guzzle HTTP Client","text":"<p>The Guzzle HTTP Client is a powerful HTTP client library for PHP. It provides:</p> <ul> <li>PSR-7 HTTP message implementation</li> <li>Middleware system</li> <li>Request and response plugins</li> <li>HTTP/2 support (via cURL)</li> </ul> <p>The <code>GuzzleDriver</code> adapts Guzzle to the Instructor HTTP client API interface.</p>"},{"location":"packages/http/1-overview/#symfony-http-client","title":"Symfony HTTP Client","text":"<p>The Symfony HTTP Client is a component of the Symfony framework. Features include:</p> <ul> <li>HTTP/2 push support</li> <li>PSR-18 compatibility</li> <li>Automatic content-type detection</li> <li>Proxy support</li> </ul> <p>The <code>SymfonyDriver</code> adapts the Symfony HTTP Client to the Instructor HTTP client API.</p>"},{"location":"packages/http/1-overview/#laravel-http-client","title":"Laravel HTTP Client","text":"<p>The Laravel HTTP Client is built into the Laravel framework and provides:</p> <ul> <li>Fluent, readable syntax</li> <li>Request macros</li> <li>Automatic JSON handling</li> <li>Rate limiting</li> <li>Retry logic</li> </ul> <p>The <code>LaravelDriver</code> adapts the Laravel HTTP Client to the Instructor HTTP client API.</p>"},{"location":"packages/http/1-overview/#mock-http-driver","title":"Mock HTTP Driver","text":"<p>The <code>MockHttpDriver</code> provides a test double for unit testing. It doesn't make actual HTTP requests but returns predefined responses based on matching rules.</p>"},{"location":"packages/http/10-middleware/","title":"Middleware","text":"<p>Middleware is one of the most powerful features of the Instructor HTTP client API. It allows you to intercept and modify HTTP requests and responses, add functionality to the HTTP client, and create reusable components that can be applied across different applications.</p>"},{"location":"packages/http/10-middleware/#middleware-concept","title":"Middleware Concept","text":"<p>Middleware in the Instructor HTTP client API follows the pipeline pattern, where each middleware component gets a chance to process the request before it's sent and the response after it's received.</p> <p>The middleware chain works like this:</p> <ol> <li>Your application creates a request</li> <li>The request passes through each middleware (in the order they were added)</li> <li>The last middleware passes the request to the HTTP driver</li> <li>The driver sends the request to the server and receives a response</li> <li>The response passes back through each middleware (in reverse order)</li> <li>Your application receives the final response</li> </ol> <p>This bidirectional flow allows middleware to perform operations both before the request is sent and after the response is received.</p>"},{"location":"packages/http/10-middleware/#the-httpmiddleware-interface","title":"The HttpMiddleware Interface","text":"<p>All middleware components must implement the <code>HttpMiddleware</code> interface:</p> <pre><code>interface HttpMiddleware\n{\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse;\n}\n</code></pre> <p>The <code>handle</code> method takes two parameters: - <code>$request</code>: The HTTP request to process - <code>$next</code>: The next handler in the middleware chain</p> <p>The middleware can: - Modify the request before passing it to the next handler - Short-circuit the chain by returning a response without calling the next handler - Process the response from the next handler before returning it - Wrap the response in a decorator for further processing (especially useful for streaming responses)</p>"},{"location":"packages/http/10-middleware/#the-basemiddleware-abstract-class","title":"The BaseMiddleware Abstract Class","text":"<p>While you can implement the <code>HttpMiddleware</code> interface directly, the library provides a convenient <code>BaseMiddleware</code> abstract class that makes it easier to create middleware:</p> <pre><code>abstract class BaseMiddleware implements HttpMiddleware\n{\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse {\n        // 1) Pre-request logic\n        $this-&gt;beforeRequest($request);\n\n        // 2) Get the response from the next handler\n        $response = $next-&gt;withRequest($request)-&gt;get();\n\n        // 3) Post-request logic, e.g. logging or rewriting\n        $response = $this-&gt;afterRequest($request, $response);\n\n        // 4) Optionally wrap the response if we want to intercept streaming\n        if ($this-&gt;shouldDecorateResponse($request, $response)) {\n            $response = $this-&gt;toResponse($request, $response);\n        }\n\n        // 5) Return the (possibly wrapped) response\n        return $response;\n    }\n\n    // Override these methods in your subclass\n    protected function beforeRequest(HttpClientRequest $request): void {}\n    protected function afterRequest(HttpClientRequest $request, HttpResponse $response): HttpResponse {\n        return $response;\n    }\n    protected function shouldDecorateResponse(HttpClientRequest $request, HttpResponse $response): bool {\n        return false;\n    }\n    protected function toResponse(HttpClientRequest $request, HttpResponse $response): HttpResponse {\n        return $response;\n    }\n}\n</code></pre> <p>By extending <code>BaseMiddleware</code>, you only need to override the methods relevant to your middleware's functionality, making the code more focused and maintainable.</p>"},{"location":"packages/http/10-middleware/#middleware-stack","title":"Middleware Stack","text":"<p>The <code>MiddlewareStack</code> class manages the collection of middleware components. It provides methods to add, remove, and arrange middleware in the stack.</p>"},{"location":"packages/http/10-middleware/#adding-middleware","title":"Adding Middleware","text":"<p>There are several ways to add middleware to the stack:</p> <pre><code>// Create a client\n$client = new HttpClient();\n\n// Add a single middleware to the end of the stack\n$client-&gt;middleware()-&gt;append(new LoggingMiddleware());\n\n// Add a single middleware with a name\n$client-&gt;middleware()-&gt;append(new CachingMiddleware(), 'cache');\n\n// Add a single middleware to the beginning of the stack\n$client-&gt;middleware()-&gt;prepend(new AuthenticationMiddleware());\n\n// Add a single middleware to the beginning with a name\n$client-&gt;middleware()-&gt;prepend(new RateLimitingMiddleware(), 'rate-limit');\n\n// Add multiple middleware at once\n$client-&gt;withMiddleware(\n    new LoggingMiddleware(),\n    new RetryMiddleware(),\n    new TimeoutMiddleware()\n);\n</code></pre> <p>Named middleware are useful when you need to reference them later, for example, to remove or replace them.</p>"},{"location":"packages/http/10-middleware/#removing-middleware","title":"Removing Middleware","text":"<p>You can remove middleware from the stack by name:</p> <pre><code>// Remove a middleware by name\n$client-&gt;middleware()-&gt;remove('cache');\n</code></pre>"},{"location":"packages/http/10-middleware/#replacing-middleware","title":"Replacing Middleware","text":"<p>You can replace a middleware with another one:</p> <pre><code>// Replace a middleware with a new one\n$client-&gt;middleware()-&gt;replace('cache', new ImprovedCachingMiddleware());\n</code></pre>"},{"location":"packages/http/10-middleware/#clearing-middleware","title":"Clearing Middleware","text":"<p>You can remove all middleware from the stack:</p> <pre><code>// Clear all middleware\n$client-&gt;middleware()-&gt;clear();\n</code></pre>"},{"location":"packages/http/10-middleware/#checking-middleware","title":"Checking Middleware","text":"<p>You can check if a middleware exists in the stack:</p> <pre><code>// Check if a middleware exists\nif ($client-&gt;middleware()-&gt;has('rate-limit')) {\n    // The 'rate-limit' middleware exists\n}\n</code></pre>"},{"location":"packages/http/10-middleware/#getting-middleware","title":"Getting Middleware","text":"<p>You can get a middleware from the stack by name or index:</p> <pre><code>// Get a middleware by name\n$rateLimitMiddleware = $client-&gt;middleware()-&gt;get('rate-limit');\n\n// Get a middleware by index\n$firstMiddleware = $client-&gt;middleware()-&gt;get(0);\n</code></pre>"},{"location":"packages/http/10-middleware/#middleware-order","title":"Middleware Order","text":"<p>The order of middleware in the stack is important because:</p> <ol> <li>Requests pass through middleware in the order they were added to the stack</li> <li>Responses pass through middleware in reverse order</li> </ol> <p>For example, if you add middleware in this order: 1. Authentication middleware 2. Logging middleware 3. Retry middleware</p> <p>The execution flow will be: - Request: Authentication \u2192 Logging \u2192 Retry \u2192 HTTP Driver - Response: Retry \u2192 Logging \u2192 Authentication \u2192 Your Application</p> <p>This allows you to nest functionality appropriately. For instance, the authentication middleware might add headers to the request and then verify the authentication status of the response before your application receives it.</p>"},{"location":"packages/http/10-middleware/#middleware-application-example","title":"Middleware Application Example","text":"<p>Here's an example of how middleware is applied in a request-response cycle:</p> <pre><code>// Create a client with middleware\n$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new LoggingMiddleware(),  // 1. Log the request and response\n    new RetryMiddleware(),    // 2. Retry failed requests\n    new TimeoutMiddleware()   // 3. Custom timeout handling\n);\n\n// Create a request\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n\n// Handle the request (middleware execution flow):\n// 1. LoggingMiddleware processes the request (logs outgoing request)\n// 2. RetryMiddleware processes the request\n// 3. TimeoutMiddleware processes the request\n// 4. HTTP driver sends the request\n// 5. TimeoutMiddleware processes the response\n// 6. RetryMiddleware processes the response (may retry on certain status codes)\n// 7. LoggingMiddleware processes the response (logs incoming response)\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre>"},{"location":"packages/http/10-middleware/#built-in-middleware","title":"Built-in Middleware","text":"<p>The Instructor HTTP client API includes several built-in middleware components for common tasks:</p>"},{"location":"packages/http/10-middleware/#debug-middleware","title":"Debug Middleware","text":"<p>The <code>DebugMiddleware</code> logs detailed information about HTTP requests and responses:</p> <pre><code>use Cognesy\\Http\\Middleware\\Debug\\DebugMiddleware;\n\n// Enable debug middleware\n$client-&gt;withMiddleware(new DebugMiddleware());\n\n// Or use the convenience method\n$client-&gt;withDebugPreset('on');\n</code></pre> <p>The debug middleware logs: - Request URLs - Request headers - Request bodies - Response headers - Response bodies - Streaming response data</p> <p>You can configure which aspects to log in the <code>config/debug.php</code> file:</p> <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; true,           // Enable/disable debug\n        'trace' =&gt; false,            // Dump HTTP trace information\n        'requestUrl' =&gt; true,        // Dump request URL to console\n        'requestHeaders' =&gt; true,    // Dump request headers to console\n        'requestBody' =&gt; true,       // Dump request body to console\n        'responseHeaders' =&gt; true,   // Dump response headers to console\n        'responseBody' =&gt; true,      // Dump response body to console\n        'responseStream' =&gt; true,    // Dump stream data to console\n        'responseStreamByLine' =&gt; true, // Dump stream as full lines or raw chunks\n    ],\n];\n</code></pre>"},{"location":"packages/http/10-middleware/#streambyline-middleware","title":"StreamByLine Middleware","text":"<p>The <code>StreamByLineMiddleware</code> processes streaming responses line by line:</p> <pre><code>use Cognesy\\Http\\Middleware\\ServerSideEvents\\StreamSSEsMiddleware;\n\n// Add stream by line middleware\n$client-&gt;withMiddleware(new StreamSSEsMiddleware());\n</code></pre> <p>You can customize how lines are processed by providing a parser function:</p> <pre><code>$lineParser = function (string $line) {\n    $trimmedLine = trim($line);\n    if (empty($trimmedLine)) {\n        return null; // Skip empty lines\n    }\n    return json_decode($trimmedLine, true);\n};\n\n$client-&gt;withMiddleware(new StreamByLineMiddleware($lineParser));\n</code></pre>"},{"location":"packages/http/10-middleware/#example-middleware-combinations","title":"Example Middleware Combinations","text":"<p>Here are some common middleware combinations for different scenarios:</p>"},{"location":"packages/http/10-middleware/#debugging-setup","title":"Debugging Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new BufferResponseMiddleware(),  // Buffer responses for reuse\n    new DebugMiddleware()            // Log requests and responses\n);\n</code></pre>"},{"location":"packages/http/10-middleware/#api-client-setup","title":"API Client Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new RetryMiddleware(maxRetries: 3, retryDelay: 1), // Retry failed requests\n    new AuthenticationMiddleware($apiKey),             // Handle authentication\n    new RateLimitingMiddleware(maxRequests: 100),      // Respect rate limits\n    new LoggingMiddleware()                            // Log API interactions\n);\n</code></pre>"},{"location":"packages/http/10-middleware/#testing-setup","title":"Testing Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new RecordReplayMiddleware(RecordReplayMiddleware::MODE_REPLAY) // Replay recorded responses\n);\n</code></pre>"},{"location":"packages/http/10-middleware/#streaming-setup","title":"Streaming Setup","text":"<pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(\n    new StreamByLineMiddleware(), // Process streaming responses line by line\n    new BufferResponseMiddleware() // Buffer responses for reuse\n);\n</code></pre> <p>By combining middleware components, you can create a highly customized HTTP client that handles complex requirements while keeping your application code clean and focused.</p> <p>In the next chapter, we'll explore how to create custom middleware components to handle specific requirements.</p>"},{"location":"packages/http/11-processing-with-middleware/","title":"Custom Processing with Middleware","text":"<p>While the Instructor HTTP client API provides several built-in middleware components, you'll often need to create custom middleware to handle specific requirements for your application. This chapter explores how to create custom middleware components and use response decoration for advanced processing.</p>"},{"location":"packages/http/11-processing-with-middleware/#creating-custom-middleware","title":"Creating Custom Middleware","text":"<p>There are three main approaches to creating custom middleware:</p> <ol> <li>Implementing the <code>HttpMiddleware</code> interface directly</li> <li>Extending the <code>BaseMiddleware</code> abstract class</li> <li>Using anonymous classes for simple middleware</li> </ol>"},{"location":"packages/http/11-processing-with-middleware/#approach-1-implementing-httpmiddleware-interface","title":"Approach 1: Implementing HttpMiddleware Interface","text":"<p>The most direct approach is to implement the <code>HttpMiddleware</code> interface:</p> <pre><code>// @doctest id='codeblocks/Middleware/BasicHttpMiddleware/code.php'\n</code></pre> <p>This approach gives you complete control over the middleware behavior, but it requires you to implement the entire logic from scratch.</p>"},{"location":"packages/http/11-processing-with-middleware/#approach-2-extending-basemiddleware","title":"Approach 2: Extending BaseMiddleware","text":"<p>For most cases, extending the <code>BaseMiddleware</code> abstract class is more convenient:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/AuthenticationMiddleware/code.php'\n</code></pre> <p>With <code>BaseMiddleware</code>, you only need to override the methods that matter for your middleware:</p> <ul> <li><code>beforeRequest(HttpClientRequest $request): void</code> - Called before the request is sent</li> <li><code>afterRequest(HttpClientRequest $request, HttpResponse $response): HttpResponse</code> - Called after the response is received</li> <li><code>shouldDecorateResponse(HttpClientRequest $request, HttpResponse $response): bool</code> - Determines if the response should be decorated</li> <li><code>toResponse(HttpClientRequest $request, HttpResponse $response): HttpResponse</code> - Creates a decorated response</li> </ul>"},{"location":"packages/http/11-processing-with-middleware/#approach-3-using-anonymous-classes","title":"Approach 3: Using Anonymous Classes","text":"<p>For simple middleware that you only need to use once, you can use anonymous classes:</p> <pre><code>use Cognesy\\Http\\Contracts\\HttpMiddleware;\n\n$client = new HttpClient();\n\n// Add a simple timing middleware\n$client-&gt;withMiddleware(new class implements HttpMiddleware {\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        $startTime = microtime(true);\n\n        $response = $next-&gt;handle($request);\n\n        $endTime = microtime(true);\n        $duration = round(($endTime - $startTime) * 1000, 2);\n\n        echo \"Request to {$request-&gt;url()} took {$duration}ms\\n\";\n\n        return $response;\n    }\n});\n</code></pre> <p>This approach is concise but less reusable than defining a named class.</p>"},{"location":"packages/http/11-processing-with-middleware/#practical-middleware-examples","title":"Practical Middleware Examples","text":""},{"location":"packages/http/11-processing-with-middleware/#retry-middleware","title":"Retry Middleware","text":"<p>This middleware automatically retries failed requests:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/RetryMiddleware/code.php'\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#rate-limiting-middleware","title":"Rate Limiting Middleware","text":"<p>This middleware throttles requests to respect API rate limits:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/RateLimitingMiddleware/code.php'\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#caching-middleware","title":"Caching Middleware","text":"<p>This middleware caches responses for GET requests:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/CachingMiddleware/code.php'\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#response-decoration","title":"Response Decoration","text":"<p>Response decoration is a powerful technique for wrapping HTTP responses to add functionality or transform data. It's particularly useful for streaming responses, where you need to process each chunk as it arrives.</p>"},{"location":"packages/http/11-processing-with-middleware/#creating-a-response-decorator","title":"Creating a Response Decorator","text":"<p>All response decorators should implement the <code>HttpResponse</code> interface. The library provides a <code>BaseResponseDecorator</code> class that makes this easier:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/MiddleResponseDecorator/code.php'\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#using-response-decorators-in-middleware","title":"Using Response Decorators in Middleware","text":"<p>To use a response decorator, you need to create a middleware that wraps the response:</p> <pre><code>// @doctest id='codeblocks/D03_Docs_HTTP/MiddlewareStreamDecorator/code.php'\n</code></pre> <p>Then add the middleware to your client:</p> <pre><code>$client = new HttpClient();\n$client-&gt;withMiddleware(new JsonStreamMiddleware());\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#response-decoration-for-transforming-content","title":"Response Decoration for Transforming Content","text":"<p>You can use response decoration to transform response content on-the-fly:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Middleware\\Base\\BaseResponseDecorator;\n\nclass XmlToJsonDecorator extends BaseResponseDecorator\n{\n    public function body(): string\n    {\n        // Get the original XML body\n        $xmlBody = $this-&gt;response-&gt;body();\n\n        // Convert XML to JSON\n        $xml = simplexml_load_string($xmlBody);\n        $jsonBody = json_encode($xml);\n\n        return $jsonBody;\n    }\n\n    public function headers(): array\n    {\n        $headers = $this-&gt;response-&gt;headers();\n\n        // Update the Content-Type header\n        $headers['Content-Type'] = ['application/json'];\n\n        return $headers;\n    }\n}\n</code></pre> <p>And the corresponding middleware:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass XmlToJsonMiddleware extends BaseMiddleware\n{\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        CanAdaptHttpResponse $response\n    ): bool {\n        // Only transform XML responses\n        return isset($response-&gt;headers()['Content-Type']) &amp;&amp;\n               strpos($response-&gt;headers()['Content-Type'][0], 'application/xml') !== false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        CanAdaptHttpResponse $response\n    ): CanAdaptHttpResponse {\n        return new XmlToJsonDecorator($request, $response);\n    }\n}\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#advanced-middleware-examples","title":"Advanced Middleware Examples","text":"<p>Here are some more advanced middleware examples that demonstrate the power and flexibility of the middleware system.</p>"},{"location":"packages/http/11-processing-with-middleware/#analytics-middleware","title":"Analytics Middleware","text":"<p>This middleware collects analytics data about HTTP requests:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass AnalyticsMiddleware extends BaseMiddleware\n{\n    private $analytics;\n\n    public function __construct($analyticsService)\n    {\n        $this-&gt;analytics = $analyticsService;\n    }\n\n    protected function beforeRequest(HttpRequest $request): void\n    {\n        // Record the start time\n        $this-&gt;startTime = microtime(true);\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        CanAdaptHttpResponse $response\n    ): CanAdaptHttpResponse {\n        $endTime = microtime(true);\n        $duration = round(($endTime - $this-&gt;startTime) * 1000, 2);\n\n        // Extract API endpoint from URL\n        $url = parse_url($request-&gt;url());\n        $endpoint = $url['path'] ?? '/';\n\n        // Record analytics data\n        $this-&gt;analytics-&gt;recordApiCall([\n            'endpoint' =&gt; $endpoint,\n            'method' =&gt; $request-&gt;method(),\n            'status_code' =&gt; $response-&gt;statusCode(),\n            'duration_ms' =&gt; $duration,\n            'request_size' =&gt; strlen($request-&gt;body()-&gt;toString()),\n            'response_size' =&gt; strlen($response-&gt;body()),\n            'timestamp' =&gt; time(),\n        ]);\n\n        return $response;\n    }\n}\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#circuit-breaker-middleware","title":"Circuit Breaker Middleware","text":"<p>This middleware implements the circuit breaker pattern to prevent repeated calls to failing services:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;use Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Drivers\\Mock\\MockHttpResponseAdapter;use Cognesy\\Http\\Exceptions\\HttpRequestException;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass CircuitBreakerMiddleware extends BaseMiddleware\n{\n    private array $circuits = [];\n    private int $failureThreshold;\n    private int $resetTimeout;\n\n    public function __construct(int $failureThreshold = 3, int $resetTimeout = 60)\n    {\n        $this-&gt;failureThreshold = $failureThreshold;\n        $this-&gt;resetTimeout = $resetTimeout;\n    }\n\n    public function handle(HttpRequest $request, CanHandleHttpRequest $next): CanAdaptHttpResponse\n    {\n        $hostname = parse_url($request-&gt;url(), PHP_URL_HOST);\n\n        // Initialize circuit state if it doesn't exist\n        if (!isset($this-&gt;circuits[$hostname])) {\n            $this-&gt;circuits[$hostname] = [\n                'state' =&gt; 'CLOSED',\n                'failures' =&gt; 0,\n                'last_failure_time' =&gt; 0,\n            ];\n        }\n\n        $circuit = &amp;$this-&gt;circuits[$hostname];\n\n        // Check if circuit is open (service is considered down)\n        if ($circuit['state'] === 'OPEN') {\n            // Check if we should try resetting the circuit\n            $timeSinceLastFailure = time() - $circuit['last_failure_time'];\n\n            if ($timeSinceLastFailure &gt;= $this-&gt;resetTimeout) {\n                // Move to half-open state to test the service\n                $circuit['state'] = 'HALF_OPEN';\n            } else {\n                // Circuit is still open, return error response\n                return new MockHttpResponseAdapter(\n                    statusCode: 503,\n                    headers: ['Content-Type' =&gt; 'application/json'],\n                    body: json_encode([\n                        'error' =&gt; 'Service Unavailable',\n                        'message' =&gt; 'Circuit breaker is open',\n                        'retry_after' =&gt; $this-&gt;resetTimeout - $timeSinceLastFailure,\n                    ])\n                );\n            }\n        }\n\n        try {\n            // Attempt the request\n            $response = $next-&gt;handle($request);\n\n            // If successful and in half-open state, reset the circuit\n            if ($circuit['state'] === 'HALF_OPEN') {\n                $circuit['state'] = 'CLOSED';\n                $circuit['failures'] = 0;\n            }\n\n            return $response;\n\n        } catch (HttpRequestException $e) {\n            // Record the failure\n            $circuit['failures']++;\n            $circuit['last_failure_time'] = time();\n\n            // If failures exceed threshold, open the circuit\n            if ($circuit['failures'] &gt;= $this-&gt;failureThreshold || $circuit['state'] === 'HALF_OPEN') {\n                $circuit['state'] = 'OPEN';\n            }\n\n            // Re-throw the exception\n            throw $e;\n        }\n    }\n}\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#conditional-middleware","title":"Conditional Middleware","text":"<p>This middleware only applies to certain requests based on a condition:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\HttpMiddleware;\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\n\nclass ConditionalMiddleware implements HttpMiddleware\n{\n    private HttpMiddleware $middleware;\n    private callable $condition;\n\n    public function __construct(HttpMiddleware $middleware, callable $condition)\n    {\n        $this-&gt;middleware = $middleware;\n        $this-&gt;condition = $condition;\n    }\n\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): CanAdaptHttpResponse\n    {\n        // Check if the condition is met\n        if (($this-&gt;condition)($request)) {\n            // Apply the wrapped middleware\n            return $this-&gt;middleware-&gt;handle($request, $next);\n        }\n\n        // Skip the middleware if condition is not met\n        return $next-&gt;handle($request);\n    }\n}\n</code></pre> <p>Usage example:</p> <pre><code>// Only apply caching middleware to GET requests\n$cachingMiddleware = new CachingMiddleware($cache);\n$conditionalCaching = new ConditionalMiddleware(\n    $cachingMiddleware,\n    fn($request) =&gt; $request-&gt;method() === 'GET'\n);\n\n$client-&gt;withMiddleware($conditionalCaching);\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#request-id-middleware","title":"Request ID Middleware","text":"<p>This middleware adds a unique ID to each request and tracks it through the response:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\nuse Ramsey\\Uuid\\Uuid;\n\nclass RequestIdMiddleware extends BaseMiddleware\n{\n    private array $requestIds = [];\n\n    protected function beforeRequest(HttpClientRequest $request): void\n    {\n        // Generate a unique ID for this request\n        $requestId = Uuid::uuid4()-&gt;toString();\n\n        // Store the ID for this request\n        $this-&gt;requestIds[spl_object_hash($request)] = $requestId;\n\n        // Add a header to the outgoing request\n        $headers = $request-&gt;headers();\n        $headers['X-Request-ID'] = $requestId;\n\n        // In a real implementation, you would need to create a new request\n        // with the updated headers, as HttpRequest is immutable\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        // Get the request ID\n        $requestId = $this-&gt;requestIds[spl_object_hash($request)] ?? 'unknown';\n\n        // Log the request completion\n        error_log(\"Request $requestId completed with status: \" . $response-&gt;statusCode());\n\n        // Clean up\n        unset($this-&gt;requestIds[spl_object_hash($request)]);\n\n        return $response;\n    }\n}\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#opentelemetry-tracing-middleware","title":"OpenTelemetry Tracing Middleware","text":"<p>This middleware adds OpenTelemetry tracing to HTTP requests:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\nuse OpenTelemetry\\API\\Trace\\SpanKind;\nuse OpenTelemetry\\API\\Trace\\StatusCode;\nuse OpenTelemetry\\API\\Trace\\TracerInterface;\n\nclass TracingMiddleware extends BaseMiddleware\n{\n    private TracerInterface $tracer;\n\n    public function __construct(TracerInterface $tracer)\n    {\n        $this-&gt;tracer = $tracer;\n    }\n\n    protected function beforeRequest(HttpClientRequest $request): void\n    {\n        // No actions needed in beforeRequest,\n        // we'll create the span in the handle method\n    }\n\n    public function handle(HttpClientRequest $request, CanHandleHttpRequest $next): HttpResponse\n    {\n        // Extract the operation name from the URL\n        $url = parse_url($request-&gt;url());\n        $path = $url['path'] ?? '/';\n        $operationName = $request-&gt;method() . ' ' . $path;\n\n        // Create a span for this request\n        $span = $this-&gt;tracer-&gt;spanBuilder($operationName)\n            -&gt;setSpanKind(SpanKind::KIND_CLIENT)\n            -&gt;startSpan();\n\n        $scope = $span-&gt;activate();\n\n        try {\n            // Add request details to the span\n            $span-&gt;setAttribute('http.method', $request-&gt;method());\n            $span-&gt;setAttribute('http.url', $request-&gt;url());\n            $span-&gt;setAttribute('http.request_content_length', strlen($request-&gt;body()-&gt;toString()));\n\n            // Make the request\n            $response = $next-&gt;handle($request);\n\n            // Add response details to the span\n            $span-&gt;setAttribute('http.status_code', $response-&gt;statusCode());\n            $span-&gt;setAttribute('http.response_content_length', strlen($response-&gt;body()));\n\n            // Set the appropriate status\n            if ($response-&gt;statusCode() &gt;= 400) {\n                $span-&gt;setStatus(StatusCode::STATUS_ERROR, \"HTTP error: {$response-&gt;statusCode()}\");\n            } else {\n                $span-&gt;setStatus(StatusCode::STATUS_OK);\n            }\n\n            return $response;\n        } catch (\\Exception $e) {\n            // Record the error\n            $span-&gt;recordException($e);\n            $span-&gt;setStatus(StatusCode::STATUS_ERROR, $e-&gt;getMessage());\n\n            // Re-throw the exception\n            throw $e;\n        } finally {\n            // End the span\n            $scope-&gt;detach();\n            $span-&gt;end();\n        }\n    }\n}\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#customizing-middleware-for-llm-apis","title":"Customizing Middleware for LLM APIs","text":"<p>When working with Large Language Model (LLM) APIs, you can create specialized middleware to handle their unique requirements:</p> <pre><code>&lt;?php\n\nnamespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Polyglot\\Http\\BaseMiddleware;\nuse Cognesy\\Polyglot\\Http\\Contracts\\HttpResponse;\nuse Cognesy\\Polyglot\\Http\\Data\\HttpClientRequest;\n\nclass LlmStreamingMiddleware extends BaseMiddleware\n{\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        // Only decorate streaming responses to LLM APIs\n        return $request-&gt;isStreamed() &amp;&amp;\n               strpos($request-&gt;url(), 'api.openai.com') !== false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return new class($request, $response) extends BaseResponseDecorator {\n            private string $buffer = '';\n            private array $chunks = [];\n\n            public function stream(int $chunkSize = 1): Generator\n            {\n                foreach ($this-&gt;response-&gt;stream($chunkSize) as $chunk) {\n                    $this-&gt;buffer .= $chunk;\n\n                    // Process lines in the buffer\n                    $lines = explode(\"\\n\", $this-&gt;buffer);\n\n                    // Keep the last line (potentially incomplete) in the buffer\n                    $this-&gt;buffer = array_pop($lines);\n\n                    foreach ($lines as $line) {\n                        $line = trim($line);\n\n                        // Skip empty lines\n                        if (empty($line)) {\n                            continue;\n                        }\n\n                        // Skip data: prefix\n                        if (strpos($line, 'data: ') === 0) {\n                            $line = substr($line, 6);\n                        }\n\n                        // Skip [DONE] message\n                        if ($line === '[DONE]') {\n                            continue;\n                        }\n\n                        // Try to parse as JSON\n                        $data = json_decode($line, true);\n\n                        if ($data) {\n                            // Extract content from different LLM formats\n                            $content = null;\n\n                            if (isset($data['choices'][0]['delta']['content'])) {\n                                // OpenAI format\n                                $content = $data['choices'][0]['delta']['content'];\n                            } elseif (isset($data['choices'][0]['text'])) {\n                                // Another format\n                                $content = $data['choices'][0]['text'];\n                            } elseif (isset($data['text'])) {\n                                // Simple format\n                                $content = $data['text'];\n                            }\n\n                            if ($content !== null) {\n                                $this-&gt;chunks[] = $content;\n                            }\n                        }\n                    }\n\n                    // Yield the original chunk to maintain streaming behavior\n                    yield $chunk;\n                }\n            }\n\n            public function body(): string\n            {\n                // If we've processed chunks, join them together\n                if (!empty($this-&gt;chunks)) {\n                    return implode('', $this-&gt;chunks);\n                }\n\n                // Otherwise, fall back to the normal body\n                return $this-&gt;response-&gt;body();\n            }\n        };\n    }\n}\n</code></pre>"},{"location":"packages/http/11-processing-with-middleware/#combining-multiple-middleware-components","title":"Combining Multiple Middleware Components","text":"<p>When building complex applications, you'll often need to combine multiple middleware components. Here's an example of how to set up a complete HTTP client pipeline:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Polyglot\\Http\\HttpClient;use Middleware\\AuthenticationMiddleware\\AuthenticationMiddleware;use Middleware\\BasicHttpMiddleware\\LoggingMiddleware;use Middleware\\CachingMiddleware\\CachingMiddleware;use Middleware\\RateLimitingMiddleware\\RateLimitingMiddleware;use Middleware\\RetryMiddleware\\RetryMiddleware;use YourNamespace\\Http\\Middleware\\AnalyticsMiddleware;use YourNamespace\\Http\\Middleware\\CircuitBreakerMiddleware;use YourNamespace\\Http\\Middleware\\TracingMiddleware;\n\n// Create services needed by middleware\n$cache = new YourCacheService();\n$logger = new YourLoggerService();\n$tracer = YourTracerFactory::create();\n$analytics = new YourAnalyticsService();\n\n// Create the client\n$client = new HttpClient('guzzle');\n\n// Add middleware - the order is important!\n$client-&gt;withMiddleware(\n    // Outer middleware (processed first for requests, last for responses)\n    new TracingMiddleware($tracer),\n    new LoggingMiddleware($logger),\n    new CircuitBreakerMiddleware(),\n\n    // Caching should go before authentication\n    new CachingMiddleware($cache),\n\n    // Authentication adds credentials\n    new AuthenticationMiddleware($apiKey),\n\n    // These control how requests are sent\n    new RetryMiddleware(maxRetries: 3),\n    new RateLimitingMiddleware(maxRequests: 100),\n\n    // Analytics should be innermost to measure actual API call stats\n    new AnalyticsMiddleware($analytics)\n);\n\n// Now the client is ready to use with a complete middleware pipeline\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>With this setup, requests and responses flow through the middleware in the following order:</p> <ol> <li>Request Flow (outside \u2192 inside):</li> <li>TracingMiddleware: Starts a trace</li> <li>LoggingMiddleware: Logs the outgoing request</li> <li>CircuitBreakerMiddleware: Checks if the service is available</li> <li>CachingMiddleware: Checks if response is cached</li> <li>AuthenticationMiddleware: Adds authentication headers</li> <li>RetryMiddleware: Prepares to retry on failure</li> <li>RateLimitingMiddleware: Enforces rate limits</li> <li>AnalyticsMiddleware: Starts timing</li> <li> <p>HTTP Driver: Sends the actual request</p> </li> <li> <p>Response Flow (inside \u2192 outside):</p> </li> <li>HTTP Driver: Receives the response</li> <li>AnalyticsMiddleware: Records API stats</li> <li>RateLimitingMiddleware: Updates rate limit counters</li> <li>RetryMiddleware: Handles retries if needed</li> <li>AuthenticationMiddleware: Verifies authentication status</li> <li>CachingMiddleware: Caches the response</li> <li>CircuitBreakerMiddleware: Updates circuit state</li> <li>LoggingMiddleware: Logs the response</li> <li>TracingMiddleware: Completes the trace</li> <li>Your Application: Processes the final response</li> </ol> <p>This bidirectional flow allows for powerful request/response processing capabilities.</p> <p>By creating custom middleware and response decorators, you can extend the HTTP client's functionality to handle any specialized requirements your application might have.</p> <p>In the next chapter, we'll cover troubleshooting techniques for the Instructor HTTP client API.</p>"},{"location":"packages/http/2-getting-started/","title":"Getting Started","text":""},{"location":"packages/http/2-getting-started/#installation","title":"Installation","text":"<p>The Instructor HTTP client API is part of the Instructor library (https://instructorphp.com) and is bundled with it.</p> <p>You can install it separately via Composer:</p> <pre><code>composer require cognesy/instructor-http-client\n</code></pre>"},{"location":"packages/http/2-getting-started/#dependencies","title":"Dependencies","text":"<p>The Instructor HTTP client API requires at least one of the supported HTTP client libraries. Depending on which client you want to use, you'll need to install the corresponding package:</p> <p>For Guzzle: <pre><code>composer require guzzlehttp/guzzle\n</code></pre></p> <p>For Symfony HTTP Client: <pre><code>composer require symfony/http-client\n</code></pre></p> <p>For Laravel HTTP Client: The Laravel HTTP Client is included with the Laravel framework. If you're using Laravel, you don't need to install it separately.</p>"},{"location":"packages/http/2-getting-started/#php-requirements","title":"PHP Requirements","text":"<p>The library requires: - PHP 8.1 or higher - JSON extension - cURL extension (recommended)</p>"},{"location":"packages/http/2-getting-started/#basic-usage","title":"Basic Usage","text":"<p>Using the Instructor HTTP client API involves a few key steps:</p> <ol> <li>Create an <code>HttpClient</code> instance</li> <li>Create an <code>HttpRequest</code> object</li> <li>Use the client to handle the request</li> <li>Process the response</li> </ol> <p>Here's a simple example:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n// Create a new HTTP client (uses the default client from configuration)\n$client = HttpClient::default();\n\n// Create a request\n$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n\n// Send the request and get the response\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Access response data\n$statusCode = $response-&gt;statusCode();\n$headers = $response-&gt;headers();\n$body = $response-&gt;body();\n\necho \"Status: $statusCode\\n\";\necho \"Body: $body\\n\";\n</code></pre>"},{"location":"packages/http/2-getting-started/#error-handling","title":"Error Handling","text":"<p>HTTP requests can fail for various reasons. You should always wrap request handling in a try-catch block:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n    // Handle the error\n}\n</code></pre>"},{"location":"packages/http/2-getting-started/#configuration","title":"Configuration","text":"<p>The Instructor HTTP client API can be configured via configuration files or at runtime.</p>"},{"location":"packages/http/2-getting-started/#configuration-files","title":"Configuration Files","text":"<p>Create the configuration files in your project:</p> <p>config/http.php: <pre><code>// @doctest skip=true\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre></p> <p>config/debug.php: <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; false, // enable/disable debug\n        'trace' =&gt; false, // dump HTTP trace information\n        'requestUrl' =&gt; true, // dump request URL to console\n        'requestHeaders' =&gt; true, // dump request headers to console\n        'requestBody' =&gt; true, // dump request body to console\n        'responseHeaders' =&gt; true, // dump response headers to console\n        'responseBody' =&gt; true, // dump response body to console\n        'responseStream' =&gt; true, // dump stream data to console\n        'responseStreamByLine' =&gt; true, // dump stream as full lines or raw chunks\n    ],\n];\n</code></pre></p>"},{"location":"packages/http/2-getting-started/#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also configure the client at runtime:</p> <pre><code>&lt;?php\nuse Cognesy\\Http\\HttpClient;\n\n// Create client with specific configuration\n$client = HttpClient::using('guzzle');\n\n// Or create with debug enabled\n$client = (new HttpClientBuilder())\n    -&gt;withPreset('guzzle')\n    -&gt;withDebugPreset('on')\n    -&gt;create();\n</code></pre>"},{"location":"packages/http/2-getting-started/#simple-request-example","title":"Simple Request Example","text":"<p>Let's put everything together with a practical example of making a POST request to create a new resource:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create an HTTP client using the 'guzzle' configuration\n$client = HttpClient::using('guzzle');\n\n// Create a POST request with JSON data\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n    ],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n        'role' =&gt; 'user',\n    ],\n    options: []\n);\n\ntry {\n    // Send the request\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Process the response\n    if ($response-&gt;statusCode() === 201) {\n        $user = json_decode($response-&gt;body(), true);\n        echo \"User created with ID: {$user['id']}\\n\";\n\n        // Print user details\n        echo \"Name: {$user['name']}\\n\";\n        echo \"Email: {$user['email']}\\n\";\n    } else {\n        echo \"Error: Unexpected status code {$response-&gt;statusCode()}\\n\";\n        echo \"Response: {$response-&gt;body()}\\n\";\n    }\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n\n    // You might want to log the error or retry the request\n}\n</code></pre>"},{"location":"packages/http/2-getting-started/#example-fetching-data","title":"Example: Fetching Data","text":"<p>Here's an example of making a GET request to fetch data:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create a default HTTP client\n$client = HttpClient::default();\n\n// Create a GET request with query parameters\n$request = new HttpRequest(\n    url: 'https://api.example.com/users?page=1&amp;limit=10',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n    ],\n    body: [],\n    options: []\n);\n\ntry {\n    // Send the request\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Process the response\n    if ($response-&gt;statusCode() === 200) {\n        $data = json_decode($response-&gt;body(), true);\n        $users = $data['users'] ?? [];\n\n        echo \"Retrieved \" . count($users) . \" users:\\n\";\n\n        foreach ($users as $user) {\n            echo \"- {$user['name']} ({$user['email']})\\n\";\n        }\n    } else {\n        echo \"Error: Unexpected status code {$response-&gt;statusCode()}\\n\";\n        echo \"Response: {$response-&gt;body()}\\n\";\n    }\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>These examples demonstrate the basic usage of the Instructor HTTP client API for common HTTP operations. In the following chapters, we'll explore more advanced features and customization options.</p>"},{"location":"packages/http/3-making-requests/","title":"Making HTTP Requests","text":"<p>The Instructor HTTP client API provides a flexible and consistent way to create and send HTTP requests across different client implementations. This chapter covers the details of building and customizing HTTP requests.</p>"},{"location":"packages/http/3-making-requests/#creating-requests","title":"Creating Requests","text":"<p>All HTTP requests are created using the <code>HttpRequest</code> class, which encapsulates the various components of an HTTP request.</p>"},{"location":"packages/http/3-making-requests/#basic-request-creation","title":"Basic Request Creation","text":"<p>The constructor for <code>HttpRequest</code> takes several parameters:</p> <pre><code>use Cognesy\\Http\\Data\\HttpRequest;\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/endpoint',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre> <p>The parameters are:</p> <ul> <li><code>url</code>: The URL to send the request to (string)</li> <li><code>method</code>: The HTTP method to use (string)</li> <li><code>headers</code>: An associative array of HTTP headers (array)</li> <li><code>body</code>: The request body, which can be a string or an array (mixed)</li> <li><code>options</code>: Additional options for the request (array)</li> </ul>"},{"location":"packages/http/3-making-requests/#request-methods","title":"Request Methods","text":"<p>Once you've created a request, you can access its properties using the following methods:</p> <pre><code>// Get the request URL\n$url = $request-&gt;url();\n\n// Get the HTTP method\n$method = $request-&gt;method();\n\n// Get the request headers\n$headers = $request-&gt;headers();\n\n// Get the request body\n$body = $request-&gt;body();\n\n// Get the request options\n$options = $request-&gt;options();\n\n// Check if the request is configured for streaming\n$isStreaming = $request-&gt;isStreamed();\n</code></pre>"},{"location":"packages/http/3-making-requests/#modifying-requests","title":"Modifying Requests","text":"<p>You can also modify a request after it's been created:</p> <pre><code>// Enable streaming for this request\n$streamingRequest = $request-&gt;withStreaming(true);\n</code></pre> <p>Note that the <code>with*</code> methods return a new request instance rather than modifying the original one.</p>"},{"location":"packages/http/3-making-requests/#http-methods","title":"HTTP Methods","text":"<p>The HTTP method is specified as a string in the <code>HttpClientRequest</code> constructor. The library supports all standard HTTP methods:</p>"},{"location":"packages/http/3-making-requests/#get-requests","title":"GET Requests","text":"<p>GET requests are used to retrieve data from a server:</p> <pre><code>$getRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre> <p>For GET requests with query parameters, include them in the URL:</p> <pre><code>$getRequestWithParams = new HttpRequest(\n    url: 'https://api.example.com/users?page=1&amp;limit=10&amp;sort=name',\n    method: 'GET',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#post-requests","title":"POST Requests","text":"<p>POST requests are used to create new resources or submit data:</p> <pre><code>$postRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#put-requests","title":"PUT Requests","text":"<p>PUT requests are used to update existing resources:</p> <pre><code>$putRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'PUT',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'name' =&gt; 'John Updated',\n        'email' =&gt; 'john.updated@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#patch-requests","title":"PATCH Requests","text":"<p>PATCH requests are used to partially update resources:</p> <pre><code>$patchRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'PATCH',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Accept' =&gt; 'application/json',\n    ],\n    body: [\n        'email' =&gt; 'new.email@example.com',\n    ],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#delete-requests","title":"DELETE Requests","text":"<p>DELETE requests are used to remove resources:</p> <pre><code>$deleteRequest = new HttpRequest(\n    url: 'https://api.example.com/users/123',\n    method: 'DELETE',\n    headers: ['Accept' =&gt; 'application/json'],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#other-methods","title":"Other Methods","text":"<p>The library also supports other HTTP methods like HEAD, OPTIONS, etc. Just specify the method name as a string:</p> <pre><code>$headRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'HEAD',\n    headers: [],\n    body: [],\n    options: []\n);\n\n$optionsRequest = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'OPTIONS',\n    headers: [],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#setting-headers","title":"Setting Headers","text":"<p>HTTP headers are specified as an associative array where keys are header names and values are header values:</p> <pre><code>$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiToken,\n        'User-Agent' =&gt; 'MyApp/1.0',\n        'X-Custom-Header' =&gt; 'Custom Value',\n    ],\n    body: [],\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#common-headers","title":"Common Headers","text":"<p>Some commonly used HTTP headers include:</p> <ul> <li>Content-Type: Specifies the format of the request body <pre><code>'Content-Type' =&gt; 'application/json'\n  ```\n\n- **Accept**: Indicates what response format the client can understand\n```php\n'Accept' =&gt; 'application/json'\n  ```\n\n- **Authorization**: Provides authentication credentials\n```php\n'Authorization' =&gt; 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'\n  ```\n\n- **User-Agent**: Identifies the client application\n```php\n'User-Agent' =&gt; 'MyApp/1.0 (https://example.com)'\n  ```\n\n- **Cache-Control**: Directives for caching mechanisms\n```php\n'Cache-Control' =&gt; 'no-cache'\n  ```\n\n- **Accept-Language**: Indicates the preferred language\n```php\n'Accept-Language' =&gt; 'en-US,en;q=0.9'\n  ```\n\n## Request Body\n\nThe request body can be provided in two ways:\n\n### Array Body (JSON)\n\nIf you provide an array as the request body, it will automatically be converted to a JSON string:\n\n```php\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/json'],\n    body: [\n        'name' =&gt; 'John Doe',\n        'email' =&gt; 'john@example.com',\n        'age' =&gt; 30,\n        'address' =&gt; [\n            'street' =&gt; '123 Main St',\n            'city' =&gt; 'Anytown',\n            'zipcode' =&gt; '12345',\n        ],\n        'tags' =&gt; ['developer', 'php'],\n    ],\n    options: []\n);\n</code></pre></li> </ul> <p>When using an array for the body, you should set the <code>Content-Type</code> header to <code>application/json</code>.</p>"},{"location":"packages/http/3-making-requests/#string-body","title":"String Body","text":"<p>You can also provide the body as a raw string:</p> <pre><code>// JSON string\n$jsonBody = json_encode([\n    'name' =&gt; 'John Doe',\n    'email' =&gt; 'john@example.com',\n]);\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/json'],\n    body: $jsonBody,\n    options: []\n);\n</code></pre> <p>This approach is useful for other content types:</p> <pre><code>// Form URL-encoded data\n$formBody = http_build_query([\n    'name' =&gt; 'John Doe',\n    'email' =&gt; 'john@example.com',\n]);\n\n$request = new HttpRequest(\n    url: 'https://api.example.com/users',\n    method: 'POST',\n    headers: ['Content-Type' =&gt; 'application/x-www-form-urlencoded'],\n    body: $formBody,\n    options: []\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#working-with-request-body","title":"Working with Request Body","text":"<p>The body is managed by the <code>HttpRequestBody</code> class, which provides methods to access the body in different formats:</p> <pre><code>// Get the body as a string\n$bodyString = $request-&gt;body()-&gt;toString();\n\n// Get the body as an array (for JSON bodies)\n$bodyArray = $request-&gt;body()-&gt;toArray();\n</code></pre>"},{"location":"packages/http/3-making-requests/#request-options","title":"Request Options","text":"<p>The <code>options</code> parameter allows you to specify additional options for the request:</p> <pre><code>$request = new HttpRequest(\n    url: 'https://api.example.com/data',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: [\n        'stream' =&gt; true,  // Enable streaming response\n    ]\n);\n</code></pre>"},{"location":"packages/http/3-making-requests/#available-options","title":"Available Options","text":"<p>Currently, the main supported option is:</p> <ul> <li><code>stream</code>: When set to <code>true</code>, enables streaming response handling</li> </ul> <p>You can check if a request is configured for streaming:</p> <pre><code>if ($request-&gt;isStreamed()) {\n    // Handle streaming response\n}\n</code></pre>"},{"location":"packages/http/3-making-requests/#example-streaming-request","title":"Example: Streaming Request","text":"<p>Here's how to create a request for a streaming API:</p> <pre><code>$streamingRequest = new HttpRequest(\n    url: 'https://api.openai.com/v1/completions',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiKey,\n    ],\n    body: [\n        'model' =&gt; 'text-davinci-003',\n        'prompt' =&gt; 'Once upon a time',\n        'max_tokens' =&gt; 100,\n        'stream' =&gt; true,\n    ],\n    options: [\n        'stream' =&gt; true, // Enable streaming in the client\n    ]\n);\n</code></pre> <p>In the following chapters, we'll explore how to handle responses, including streaming responses, and how to use more advanced features like request pools and middleware.</p>"},{"location":"packages/http/4-handling-responses/","title":"Handling Responses","text":"<p>After sending an HTTP request, you need to process the response received from the server. The Instructor HTTP client API provides a consistent interface for handling responses, regardless of the underlying HTTP client implementation.</p>"},{"location":"packages/http/4-handling-responses/#response-interface","title":"Response Interface","text":"<p>All responses implement the <code>HttpResponse</code> interface, which provides a uniform way to access response data:</p> <pre><code>interface HttpResponse\n{\n    public function statusCode(): int;\n    public function headers(): array;\n    public function body(): string;\n    public function stream(int $chunkSize = 1): Generator;\n}\n</code></pre> <p>This interface ensures that the same code will work whether you're using Guzzle, Symfony, or Laravel HTTP clients.</p>"},{"location":"packages/http/4-handling-responses/#getting-the-response","title":"Getting the Response","text":"<p>When you send a request using the <code>HttpClient::handle()</code> method, it returns an implementation of <code>HttpResponse</code>:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\n\n// Create a new HTTP client\n$client = HttpClient::default();\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>The specific implementation depends on the HTTP client driver being used:</p> <ul> <li><code>PsrHttpResponse</code>: Used by the GuzzleDriver</li> <li><code>SymfonyHttpResponse</code>: Used by the SymfonyDriver</li> <li><code>LaravelHttpResponse</code>: Used by the LaravelDriver</li> <li><code>MockHttpResponse</code>: Used by the MockHttpDriver for testing</li> </ul> <p>However, since all these implementations provide the same interface, your code doesn't need to know which one it's working with.</p>"},{"location":"packages/http/4-handling-responses/#status-codes","title":"Status Codes","text":"<p>The status code indicates the result of the HTTP request. You can access it using the <code>statusCode()</code> method:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$statusCode = $response-&gt;statusCode();\n\necho \"Status code: $statusCode\\n\";\n</code></pre>"},{"location":"packages/http/4-handling-responses/#status-code-categories","title":"Status Code Categories","text":"<p>Status codes are grouped into categories:</p> <ul> <li>1xx (Informational): The request was received and understood</li> <li>2xx (Success): The request was successfully received, understood, and accepted</li> <li>3xx (Redirection): Further action needs to be taken to complete the request</li> <li>4xx (Client Error): The request contains bad syntax or cannot be fulfilled</li> <li>5xx (Server Error): The server failed to fulfill a valid request</li> </ul>"},{"location":"packages/http/4-handling-responses/#checking-response-status","title":"Checking Response Status","text":"<p>You can check if a response was successful:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n\nif ($response-&gt;statusCode() &gt;= 200 &amp;&amp; $response-&gt;statusCode() &lt; 300) {\n    // Success response\n    echo \"Request succeeded!\\n\";\n} elseif ($response-&gt;statusCode() &gt;= 400 &amp;&amp; $response-&gt;statusCode() &lt; 500) {\n    // Client error\n    echo \"Client error: {$response-&gt;statusCode()}\\n\";\n} elseif ($response-&gt;statusCode() &gt;= 500) {\n    // Server error\n    echo \"Server error: {$response-&gt;statusCode()}\\n\";\n}\n</code></pre>"},{"location":"packages/http/4-handling-responses/#common-status-codes","title":"Common Status Codes","text":"<p>Here are some common HTTP status codes you might encounter:</p> <ul> <li>200 OK: The request was successful</li> <li>201 Created: A new resource was successfully created</li> <li>204 No Content: The request was successful, but there's no response body</li> <li>400 Bad Request: The request was malformed or invalid</li> <li>401 Unauthorized: Authentication is required</li> <li>403 Forbidden: The client doesn't have permission to access the resource</li> <li>404 Not Found: The requested resource doesn't exist</li> <li>405 Method Not Allowed: The HTTP method is not supported for this resource</li> <li>422 Unprocessable Entity: The request was well-formed but contains semantic errors</li> <li>429 Too Many Requests: Rate limit exceeded</li> <li>500 Internal Server Error: A generic server error occurred</li> <li>502 Bad Gateway: The server received an invalid response from an upstream server</li> <li>503 Service Unavailable: The server is temporarily unavailable</li> <li>504 Gateway Timeout: The upstream server didn't respond in time</li> <li>511 Network Authentication Required: The client needs to authenticate to gain network access</li> </ul>"},{"location":"packages/http/4-handling-responses/#headers","title":"Headers","text":"<p>Response headers provide metadata about the response. You can access the headers using the <code>headers()</code> method:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$headers = $response-&gt;headers();\n\n// Print all headers\nforeach ($headers as $name =&gt; $values) {\n    echo \"$name: \" . implode(', ', $values) . \"\\n\";\n}\n\n// Access specific headers\n$contentType = $headers['Content-Type'] ?? 'unknown';\n$contentLength = $headers['Content-Length'] ?? 'unknown';\n\necho \"Content-Type: $contentType\\n\";\necho \"Content-Length: $contentLength\\n\";\n</code></pre> <p>The header names are case-insensitive, but the exact format might vary slightly between client implementations. Some clients normalize header names to title case (e.g., <code>Content-Type</code>), while others might use lowercase (e.g., <code>content-type</code>).</p>"},{"location":"packages/http/4-handling-responses/#common-response-headers","title":"Common Response Headers","text":"<p>Here are some common response headers you might encounter:</p> <ul> <li>Content-Type: The MIME type of the response body</li> <li>Content-Length: The size of the response body in bytes</li> <li>Cache-Control: Directives for caching mechanisms</li> <li>Set-Cookie: Cookies to be stored by the client</li> <li>Location: Used for redirects</li> <li>X-RateLimit-Limit: The rate limit for the endpoint</li> <li>X-RateLimit-Remaining: The number of requests remaining in the current rate limit window</li> <li>X-RateLimit-Reset: When the rate limit will reset</li> </ul>"},{"location":"packages/http/4-handling-responses/#body-content","title":"Body Content","text":"<p>For non-streaming responses, you can get the entire response body as a string using the <code>body()</code> method:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\necho \"Response body: $body\\n\";\n</code></pre>"},{"location":"packages/http/4-handling-responses/#processing-json-responses","title":"Processing JSON Responses","text":"<p>Many APIs return JSON responses. You can decode them using PHP's <code>json_decode()</code> function:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Decode as associative array\n$data = json_decode($body, true);\n\nif (json_last_error() !== JSON_ERROR_NONE) {\n    echo \"Error decoding JSON: \" . json_last_error_msg() . \"\\n\";\n} else {\n    // Process the data\n    echo \"User ID: {$data['id']}\\n\";\n    echo \"User Name: {$data['name']}\\n\";\n}\n</code></pre>"},{"location":"packages/http/4-handling-responses/#processing-xml-responses","title":"Processing XML Responses","text":"<p>For XML responses, you can use PHP's built-in XML functions:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Load XML\n$xml = simplexml_load_string($body);\n\nif ($xml === false) {\n    echo \"Error loading XML\\n\";\n} else {\n    // Process the XML\n    echo \"Title: {$xml-&gt;title}\\n\";\n    echo \"Description: {$xml-&gt;description}\\n\";\n}\n</code></pre>"},{"location":"packages/http/4-handling-responses/#processing-binary-responses","title":"Processing Binary Responses","text":"<p>For binary responses (like file downloads), you can save the response body to a file:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n$body = $response-&gt;body();\n\n// Save to file\nfile_put_contents('downloaded_file.pdf', $body);\necho \"File downloaded successfully\\n\";\n</code></pre>"},{"location":"packages/http/4-handling-responses/#error-handling","title":"Error Handling","text":"<p>When making HTTP requests, various errors can occur. The Instructor HTTP client API provides a consistent way to handle these errors through exceptions.</p>"},{"location":"packages/http/4-handling-responses/#requestexception","title":"RequestException","text":"<p>The main exception type is <code>RequestException</code>, which is thrown when a request fails:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"Request failed: {$e-&gt;getMessage()}\\n\";\n\n    // You might want to log the error or take other actions\n    if ($e-&gt;getPrevious() !== null) {\n        echo \"Original exception: \" . $e-&gt;getPrevious()-&gt;getMessage() . \"\\n\";\n    }\n}\n</code></pre> <p>The <code>RequestException</code> often wraps another exception from the underlying HTTP client, which you can access with <code>$e-&gt;getPrevious()</code>.</p>"},{"location":"packages/http/4-handling-responses/#error-response-handling","title":"Error Response Handling","text":"<p>By default, HTTP error responses (4xx, 5xx status codes) do not throw exceptions. You can control this behavior using the <code>failOnError</code> configuration option:</p> <pre><code>// In config/http.php\n'failOnError' =&gt; true, // Throw exceptions for 4xx/5xx responses\n</code></pre> <p>When <code>failOnError</code> is set to <code>true</code>, the client will throw a <code>RequestException</code> for error responses. When it's <code>false</code>, you need to check the status code yourself:</p> <pre><code>$response = $client-&gt;withRequest($request)-&gt;get();\n\nif ($response-&gt;statusCode() &gt;= 400) {\n    // Handle error response\n    echo \"Error: HTTP {$response-&gt;statusCode()}\\n\";\n    echo \"Error details: {$response-&gt;body()}\\n\";\n} else {\n    // Process successful response\n}\n</code></pre>"},{"location":"packages/http/4-handling-responses/#retrying-failed-requests","title":"Retrying Failed Requests","text":"<p>If a request fails, you might want to retry it. Here's a simple implementation of a retry mechanism:</p> <pre><code>use Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\n\nfunction retryRequest($client, $request, $maxRetries = 3, $delay = 1): ?CanAdaptHttpResponse {\n    $attempts = 0;\n\n    while ($attempts &lt; $maxRetries) {\n        try {\n            return $client-&gt;withRequest($request)-&gt;get();\n        } catch (HttpRequestException $e) {\n            $attempts++;\n\n            if ($attempts &gt;= $maxRetries) {\n                throw $e; // Rethrow after all retries failed\n            }\n\n            // Wait before retrying (with exponential backoff)\n            $sleepTime = $delay * pow(2, $attempts - 1);\n            echo \"Request failed, retrying in {$sleepTime} seconds...\\n\";\n            sleep($sleepTime);\n        }\n    }\n\n    return null; // Should never reach here\n}\n\n// Usage\ntry {\n    $response = retryRequest($client, $request);\n    // Process the response\n} catch (HttpRequestException $e) {\n    echo \"All retry attempts failed: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>This function will retry failed requests with exponential backoff, meaning it waits longer between each retry attempt.</p> <p>In the next chapter, we'll explore streaming responses, which are particularly useful for handling large responses or real-time data streams.</p>"},{"location":"packages/http/5-streaming-responses/","title":"Streaming Responses","text":"<p>Streaming responses are a powerful feature that allows processing data as it arrives from the server, rather than waiting for the entire response to be received. This is particularly valuable when:</p> <ul> <li>Working with large responses that might exceed memory limits</li> <li>Processing real-time data streams</li> <li>Handling responses from AI models that generate content token by token</li> <li>Building user interfaces that show progressive updates</li> </ul> <p>The Instructor HTTP client API provides robust support for streaming responses across all supported HTTP client implementations.</p>"},{"location":"packages/http/5-streaming-responses/#enabling-streaming","title":"Enabling Streaming","text":"<p>To receive a streaming response, you need to configure the request with the <code>stream</code> option set to <code>true</code>:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://api.example.com/stream',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'text/event-stream',\n    ],\n    body: [],\n    options: [\n        'stream' =&gt; true,  // Enable streaming\n    ]\n);\n\n// Or use the withStreaming method on an existing request\n$streamingRequest = $request-&gt;withStreaming(true);\n\n// Create a client and send the request\n$client = new HttpClient();\n$response = $client-&gt;withRequest($streamingRequest)-&gt;get();\n</code></pre> <p>The <code>stream</code> option tells the HTTP client to treat the response as a stream, which means:</p> <ol> <li>It won't buffer the entire response in memory</li> <li>It will provide a way to read the response incrementally</li> <li>The connection will remain open until all data is received or the stream is closed</li> </ol>"},{"location":"packages/http/5-streaming-responses/#processing-streamed-data","title":"Processing Streamed Data","text":"<p>Once you have a streaming response, you can process it using the <code>stream()</code> method, which returns a PHP Generator:</p> <pre><code>$response = $client-&gt;withRequest($streamingRequest)-&gt;get();\n\n// Process the stream chunk by chunk\nforeach ($response-&gt;stream() as $chunk) {\n    // Process each chunk of data as it arrives\n    echo \"Received chunk: $chunk\\n\";\n\n    // You could parse JSON chunks, update progress, etc.\n    // If this is a streaming JSON response, you might need to buffer until\n    // you have complete JSON objects\n}\n</code></pre> <p>By default, the <code>stream()</code> method reads the response in small chunks. You can control the chunk size by passing a parameter:</p> <pre><code>// Read in chunks of 1024 bytes\nforeach ($response-&gt;stream(1024) as $chunk) {\n    // Process larger chunks of data\n    echo \"Received chunk of approximately 1KB: $chunk\\n\";\n}\n</code></pre>"},{"location":"packages/http/5-streaming-responses/#example-downloading-a-large-file","title":"Example: Downloading a Large File","text":"<p>Here's an example of downloading a large file with streaming to avoid memory issues:</p> <pre><code>use Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://example.com/large-file.zip',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$client = new HttpClient();\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    // Open a file handle to save the file\n    $fileHandle = fopen('downloaded-file.zip', 'wb');\n\n    if (!$fileHandle) {\n        throw new \\RuntimeException(\"Could not open file for writing\");\n    }\n\n    // Keep track of bytes received\n    $totalBytes = 0;\n\n    // Process the stream and write to file\n    foreach ($response-&gt;stream(8192) as $chunk) {\n        fwrite($fileHandle, $chunk);\n        $totalBytes += strlen($chunk);\n\n        // Display progress (if not in a web request)\n        echo \"\\rDownloaded: \" . number_format($totalBytes / 1024 / 1024, 2) . \" MB\";\n    }\n\n    // Close the file handle\n    fclose($fileHandle);\n    echo \"\\nDownload complete!\\n\";\n\n} catch (HttpRequestException $e) {\n    echo \"Download failed: {$e-&gt;getMessage()}\\n\";\n\n    // Clean up if file was partially downloaded\n    if (isset($fileHandle) &amp;&amp; is_resource($fileHandle)) {\n        fclose($fileHandle);\n    }\n    if (file_exists('downloaded-file.zip')) {\n        unlink('downloaded-file.zip');\n    }\n}\n</code></pre> <p>This approach allows downloading very large files without loading the entire file into memory.</p>"},{"location":"packages/http/5-streaming-responses/#example-processing-server-sent-events-sse","title":"Example: Processing Server-Sent Events (SSE)","text":"<p>Server-Sent Events (SSE) are a common streaming format used by many APIs. Here's how to process them:</p> <pre><code>$request = new HttpRequest(\n    url: 'https://api.example.com/events',\n    method: 'GET',\n    headers: [\n        'Accept' =&gt; 'text/event-stream',\n        'Cache-Control' =&gt; 'no-cache',\n    ],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n$buffer = '';\n\nforeach ($response-&gt;stream() as $chunk) {\n    // Add the chunk to our buffer\n    $buffer .= $chunk;\n\n    // Process complete events (SSE events are separated by double newlines)\n    while (($pos = strpos($buffer, \"\\n\\n\")) !== false) {\n        // Extract and process the event\n        $event = substr($buffer, 0, $pos);\n        $buffer = substr($buffer, $pos + 2);\n\n        // Parse the event (SSE format: \"field: value\")\n        $parsedEvent = [];\n        foreach (explode(\"\\n\", $event) as $line) {\n            if (preg_match('/^([^:]+):\\s*(.*)$/', $line, $matches)) {\n                $field = $matches[1];\n                $value = $matches[2];\n                $parsedEvent[$field] = $value;\n            }\n        }\n\n        // Process the parsed event\n        if (isset($parsedEvent['event'], $parsedEvent['data'])) {\n            $eventType = $parsedEvent['event'];\n            $eventData = $parsedEvent['data'];\n\n            echo \"Received event type: $eventType\\n\";\n            echo \"Event data: $eventData\\n\";\n\n            // You could also parse the data as JSON if appropriate\n            if ($eventType === 'update') {\n                $data = json_decode($eventData, true);\n                if ($data) {\n                    echo \"Processed update: {$data['message']}\\n\";\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>While this works, processing streaming responses line by line is common enough that the library provides a dedicated middleware for it, as we'll see in the next section.</p>"},{"location":"packages/http/5-streaming-responses/#line-by-line-processing","title":"Line-by-Line Processing","text":"<p>For many streaming APIs, especially those that send event streams or line-delimited JSON, it's useful to process the response line by line. The library provides the <code>StreamByLineMiddleware</code> to simplify this task:</p> <pre><code>use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\HttpClient;use Cognesy\\Http\\Middleware\\ServerSideEvents\\StreamSSEsMiddleware;\n\n// Create a client with the StreamByLineMiddleware\n$client = new HttpClient();\n$client-&gt;withMiddleware(new StreamSSEsMiddleware());\n\n// Create a streaming request\n$request = new HttpRequest(\n    url: 'https://api.example.com/events',\n    method: 'GET',\n    headers: [],\n    body: [],\n    options: ['stream' =&gt; true]\n);\n\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Process the stream line by line\nforeach ($response-&gt;stream() as $line) {\n    // Each $line is a complete line from the response\n    echo \"Received line: $line\\n\";\n\n    // Parse the line (e.g., as JSON)\n    $event = json_decode($line, true);\n    if ($event) {\n        // Process the event\n        echo \"Event type: {$event['type']}\\n\";\n    }\n}\n</code></pre>"},{"location":"packages/http/5-streaming-responses/#customizing-line-processing","title":"Customizing Line Processing","text":"<p>You can customize how lines are parsed by providing a parser function to the middleware:</p> <pre><code>$lineParser = function (string $line) {\n    // Pre-process each line before yielding it\n    $trimmedLine = trim($line);\n    if (empty($trimmedLine)) {\n        return null; // Skip empty lines\n    }\n    return $trimmedLine;\n};\n\n$client-&gt;withMiddleware(new StreamByLineMiddleware($lineParser));\n</code></pre> <p>If your parser returns <code>null</code>, that line will be skipped in the stream.</p>"},{"location":"packages/http/5-streaming-responses/#example-processing-openai-chat-completions","title":"Example: Processing OpenAI Chat Completions","text":"<p>Here's a practical example of using the <code>StreamByLineMiddleware</code> to process streaming responses from the OpenAI API:</p> <pre><code>use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\HttpClient;use Cognesy\\Http\\Middleware\\ServerSideEvents\\StreamSSEsMiddleware;\n\n// OpenAI API requires a parser that handles their SSE format\n$openAiParser = function (string $line) {\n    // Skip empty lines\n    if (trim($line) === '') {\n        return null;\n    }\n\n    // Remove \"data: \" prefix from each line\n    if (strpos($line, 'data: ') === 0) {\n        $line = substr($line, 6);\n\n        // Skip the \"[DONE]\" message\n        if ($line === '[DONE]') {\n            return null;\n        }\n\n        // Return the parsed line\n        return $line;\n    }\n\n    return null; // Skip non-data lines\n};\n\n// Create a client with the StreamByLineMiddleware\n$client = new HttpClient('guzzle'); // Use Guzzle for better streaming support\n$client-&gt;withMiddleware(new StreamSSEsMiddleware($openAiParser));\n\n// Create a request to OpenAI API\n$request = new HttpRequest(\n    url: 'https://api.openai.com/v1/chat/completions',\n    method: 'POST',\n    headers: [\n        'Content-Type' =&gt; 'application/json',\n        'Authorization' =&gt; 'Bearer ' . $apiKey,\n    ],\n    body: [\n        'model' =&gt; 'gpt-3.5-turbo',\n        'messages' =&gt; [\n            ['role' =&gt; 'user', 'content' =&gt; 'Write a short poem about coding.'],\n        ],\n        'stream' =&gt; true,\n    ],\n    options: ['stream' =&gt; true]\n);\n\ntry {\n    $response = $client-&gt;withRequest($request)-&gt;get();\n\n    $fullResponse = '';\n\n    // Process the streaming response\n    foreach ($response-&gt;stream() as $chunk) {\n        // Parse the chunk as JSON\n        $data = json_decode($chunk, true);\n\n        if ($data &amp;&amp; isset($data['choices'][0]['delta']['content'])) {\n            $content = $data['choices'][0]['delta']['content'];\n            $fullResponse .= $content;\n\n            // Print each piece as it arrives\n            echo $content;\n            flush(); // Ensure output is sent immediately\n        }\n    }\n\n    echo \"\\n\\nFull response:\\n$fullResponse\\n\";\n\n} catch (Exception $e) {\n    echo \"Error: {$e-&gt;getMessage()}\\n\";\n}\n</code></pre> <p>This approach allows you to display the AI-generated content to the user in real-time as it's being generated, providing a more responsive user experience.</p>"},{"location":"packages/http/5-streaming-responses/#considerations-for-streaming","title":"Considerations for Streaming","text":"<p>When working with streaming responses, keep these considerations in mind:</p> <ol> <li> <p>Memory Usage: While streaming reduces memory usage overall, be careful not to accumulate the entire response in memory by appending to a variable unless necessary.</p> </li> <li> <p>Connection Stability: Streaming connections can be more sensitive to network issues. Consider implementing error handling and retry logic for more robust applications.</p> </li> <li> <p>Server Timeouts: Some servers or proxies might timeout long-running connections. Make sure your infrastructure is configured to allow the necessary connection times.</p> </li> <li> <p>Middleware Order: When using middleware that processes streaming responses, the order of middleware can be important. Middleware is executed in the order it's added to the stack.</p> </li> </ol> <p>In the next chapter, we'll explore how to make multiple concurrent requests using request pools, which can significantly improve performance when fetching data from multiple endpoints.</p>"},{"location":"packages/http/7-changing-client/","title":"Changing the Underlying Client","text":"<p>One of the core features of the Instructor HTTP client API is its ability to seamlessly switch between different HTTP client implementations. This flexibility allows you to use the same code across different environments or to choose the most appropriate client for specific use cases.</p>"},{"location":"packages/http/7-changing-client/#available-client-drivers","title":"Available Client Drivers","text":"<p>The library includes several built-in drivers that adapt various HTTP client libraries to the unified interface used by Instructor:</p>"},{"location":"packages/http/7-changing-client/#curldriver-default","title":"CurlDriver (Default)","text":"<p>The <code>CurlDriver</code> provides integration with PHP's native cURL extension, offering zero-dependency HTTP client functionality.</p> <p>Key Features: - No external dependencies required - Uses native PHP cURL extension - HTTP/1.1 and HTTP/2 support (automatic negotiation) - Full header parsing and capture - Streaming response support - Concurrent request pooling via curl_multi - Built-in SSL verification - Automatic redirect following - Works out of the box</p> <p>Best For: - Quick start without additional dependencies - Lightweight applications - Environments where adding dependencies is difficult - Projects that need reliable HTTP client immediately - High-performance concurrent requests</p> <p>Requirements: - PHP cURL extension (typically included by default)</p>"},{"location":"packages/http/7-changing-client/#guzzledriver","title":"GuzzleDriver","text":"<p>The <code>GuzzleDriver</code> provides integration with the popular Guzzle HTTP client.</p> <p>Key Features: - Robust feature set - Excellent performance - Extensive middleware ecosystem - Support for HTTP/2 (via cURL) - Stream and promise-based API</p> <p>Best For: - General-purpose HTTP requests - Applications that need advanced features - Projects without framework constraints</p> <p>Requirements: - Requires the <code>guzzlehttp/guzzle</code> package (<code>composer require guzzlehttp/guzzle</code>)</p>"},{"location":"packages/http/7-changing-client/#symfonydriver","title":"SymfonyDriver","text":"<p>The <code>SymfonyDriver</code> integrates with the Symfony HTTP Client.</p> <p>Key Features: - Native HTTP/2 support - Automatic content-type detection - Built-in profiling and logging - No dependency on cURL - Support for various transports (native PHP, cURL, amphp)</p> <p>Best For: - Symfony applications - Projects requiring HTTP/2 support - Low-dependency environments</p> <p>Requirements: - Requires the <code>symfony/http-client</code> package (<code>composer require symfony/http-client</code>)</p>"},{"location":"packages/http/7-changing-client/#laraveldriver","title":"LaravelDriver","text":"<p>The <code>LaravelDriver</code> integrates with the Laravel HTTP Client.</p> <p>Key Features: - Elegant, fluent syntax - Integration with Laravel ecosystem - Built-in macros and testing utilities - Automatic JSON handling - Rate limiting and retry capabilities</p> <p>Best For: - Laravel applications - Projects already using the Laravel framework</p> <p>Requirements: - Included with the Laravel framework</p>"},{"location":"packages/http/7-changing-client/#mockhttpdriver","title":"MockHttpDriver","text":"<p>The <code>MockHttpDriver</code> is a test double that doesn't make actual HTTP requests but returns predefined responses.</p> <p>Key Features: - No actual network requests - Predefined responses for testing - Response matching based on URL, method, and body - Support for custom response generation</p> <p>Best For: - Unit testing - Offline development - CI/CD environments</p>"},{"location":"packages/http/7-changing-client/#switching-between-clients","title":"Switching Between Clients","text":"<p>You can switch between the available client implementations in several ways:</p>"},{"location":"packages/http/7-changing-client/#when-creating-the-client","title":"When Creating the Client","text":"<p>The simplest approach is to specify the client when creating the <code>HttpClient</code> instance:</p> <pre><code>// Use native cURL (default)\n$curlClient = HttpClient::using('curl');\n\n// Use Guzzle (assuming it's configured in config/http.php)\n$guzzleClient = HttpClient::using('guzzle');\n\n// Use Symfony\n$symfonyClient = HttpClient::using('symfony');\n\n// Use Laravel\n$laravelClient = HttpClient::using('laravel');\n</code></pre> <p>The client name must correspond to a configuration entry in your <code>config/http.php</code> file.</p>"},{"location":"packages/http/7-changing-client/#using-the-default-client","title":"Using the Default Client","text":"<p>If you don't specify a client, the default one from your configuration will be used:</p> <pre><code>// Uses the default client specified in config/http.php\n$client = HttpClient::default();\n</code></pre> <p>The default client is specified in the <code>config/http.php</code> file:</p> <pre><code>return [\n    'defaultClient' =&gt; 'curl',  // Native cURL driver (no dependencies)\n    'clients' =&gt; [\n        // Client configurations...\n    ],\n];\n</code></pre>"},{"location":"packages/http/7-changing-client/#switching-at-runtime","title":"Switching at Runtime","text":"<p>You can create different clients for different requirements within the same application:</p> <pre><code>// Create clients with different configurations\n$defaultClient = HttpClient::default();\n$symfonyClient = HttpClient::using('symfony');\n$laravelClient = HttpClient::using('laravel');\n$customClient = HttpClient::using('http-ollama');\n</code></pre> <p>Note: HttpClient instances are immutable, so you create new instances rather than switching configurations at runtime.</p>"},{"location":"packages/http/7-changing-client/#using-the-static-make-method","title":"Using the Static Make Method","text":"<p>The <code>HttpClient</code> class provides a static <code>make</code> method as an alternative to the constructor:</p> <pre><code>use Cognesy\\Http\\Creation\\HttpClientBuilder;\n\n// Create with specific client using builder\n$client = (new HttpClientBuilder())-&gt;withPreset('guzzle')-&gt;create();\n// Equivalent to:\n$client = HttpClient::using('guzzle');\n\n// Create with default client and custom event dispatcher\n$events = new EventDispatcher();\n$client = (new HttpClientBuilder())-&gt;withEventBus($events)-&gt;create();\n</code></pre>"},{"location":"packages/http/7-changing-client/#client-specific-configuration","title":"Client-Specific Configuration","text":"<p>Each client type can have its own configuration in the <code>config/http.php</code> file:</p> <pre><code>&lt;?php\nreturn [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre>"},{"location":"packages/http/7-changing-client/#multiple-configurations-for-the-same-client-type","title":"Multiple Configurations for the Same Client Type","text":"<p>You can define multiple configurations for the same client type, each with different settings:</p> <pre><code>'clients' =&gt; [\n    'guzzle' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 3,\n        'requestTimeout' =&gt; 30,\n        // Default settings for Guzzle\n    ],\n    'guzzle-short-timeout' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 1,\n        'requestTimeout' =&gt; 5,\n        // Short timeouts for quick operations\n    ],\n    'guzzle-long-timeout' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 5,\n        'requestTimeout' =&gt; 120,\n        // Long timeouts for operations that take time\n    ],\n    'guzzle-streaming' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 3,\n        'requestTimeout' =&gt; 300,\n        'idleTimeout' =&gt; 60,\n        // Optimized for streaming responses\n    ],\n    'http-ollama' =&gt; [\n        'httpClientType' =&gt; 'guzzle',\n        'connectTimeout' =&gt; 1,\n        'requestTimeout' =&gt; 90, // Longer timeout for AI model inference\n        'idleTimeout' =&gt; -1,\n        'maxConcurrent' =&gt; 5,\n        'poolTimeout' =&gt; 60,\n        'failOnError' =&gt; true,\n    ],\n],\n</code></pre> <p>Then you can select the appropriate configuration based on your needs:</p> <pre><code>// For quick API calls\n$quickClient = HttpClient::using('guzzle-short-timeout');\n\n// For long-running operations\n$longClient = HttpClient::using('guzzle-long-timeout');\n\n// For streaming responses\n$streamingClient = HttpClient::using('guzzle-streaming');\n\n// For AI model requests\n$aiClient = HttpClient::using('http-ollama');\n</code></pre>"},{"location":"packages/http/7-changing-client/#common-configuration-parameters","title":"Common Configuration Parameters","text":"<p>All client types support these common configuration parameters:</p> Parameter Type Description <code>httpClientType</code> string The type of HTTP client (Guzzle, Symfony, Laravel) <code>connectTimeout</code> int Maximum time to wait for connection establishment (seconds) <code>requestTimeout</code> int Maximum time to wait for the entire request (seconds) <code>idleTimeout</code> int Maximum time to wait between data packets (seconds, -1 for no timeout) <code>maxConcurrent</code> int Maximum number of concurrent requests in a pool <code>poolTimeout</code> int Maximum time to wait for all pooled requests (seconds) <code>failOnError</code> bool Whether to throw exceptions for HTTP error responses"},{"location":"packages/http/7-changing-client/#client-specific-parameters","title":"Client-Specific Parameters","text":"<p>Some parameters might only be relevant to specific client implementations. For example, Guzzle supports additional options like <code>verify</code> (for SSL verification) or <code>proxy</code> settings that can be passed through the underlying client.</p>"},{"location":"packages/http/7-changing-client/#example-choosing-the-right-client-for-different-scenarios","title":"Example: Choosing the Right Client for Different Scenarios","text":"<p>Here's an example of selecting different client configurations based on the task:</p> <pre><code>&lt;?php\n\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction fetchApiData($url, $apiKey) {\n    // Use a client with short timeouts for quick API calls\n    $client = new HttpClient('guzzle-short-timeout');\n\n    $request = new HttpRequest(\n        url: $url,\n        method: 'GET',\n        headers: [\n            'Authorization' =&gt; 'Bearer ' . $apiKey,\n            'Accept' =&gt; 'application/json',\n        ],\n        body: [],\n        options: []\n    );\n\n    try {\n        return $client-&gt;withRequest($request)-&gt;get();\n    } catch (HttpRequestException $e) {\n        // Handle error\n        throw $e;\n    }\n}\n\nfunction downloadLargeFile($url, $outputPath) {\n    // Use a client with long timeouts for downloading large files\n    $client = new HttpClient('guzzle-long-timeout');\n\n    $request = new HttpRequest(\n        url: $url,\n        method: 'GET',\n        headers: [],\n        body: [],\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $response = $client-&gt;withRequest($request)-&gt;get();\n\n        $fileHandle = fopen($outputPath, 'wb');\n        foreach ($response-&gt;stream(8192) as $chunk) {\n            fwrite($fileHandle, $chunk);\n        }\n        fclose($fileHandle);\n\n        return true;\n    } catch (HttpRequestException $e) {\n        // Handle error\n        if (file_exists($outputPath)) {\n            unlink($outputPath); // Remove partial file\n        }\n        throw $e;\n    }\n}\n\nfunction generateAiResponse($prompt) {\n    // Use a specialized client for AI API requests\n    $client = new HttpClient('http-ollama');\n\n    $request = new HttpRequest(\n        url: 'https://api.example.com/ai/generate',\n        method: 'POST',\n        headers: [\n            'Content-Type' =&gt; 'application/json',\n            'Accept' =&gt; 'application/json',\n        ],\n        body: [\n            'prompt' =&gt; $prompt,\n            'max_tokens' =&gt; 500,\n        ],\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $response = $client-&gt;withRequest($request)-&gt;get();\n\n        $result = '';\n        foreach ($response-&gt;stream() as $chunk) {\n            $result .= $chunk;\n        }\n\n        return json_decode($result, true);\n    } catch (HttpRequestException $e) {\n        // Handle error\n        throw $e;\n    }\n}\n</code></pre>"},{"location":"packages/http/7-changing-client/#considerations-for-switching-clients","title":"Considerations for Switching Clients","text":"<p>When switching between different HTTP client implementations, keep these considerations in mind:</p> <ol> <li> <p>Configuration Consistency: Ensure that all client configurations have the appropriate settings for your application's needs.</p> </li> <li> <p>Feature Availability: Some advanced features might be available only in specific clients. For example, HTTP/2 support might be better in one client than another.</p> </li> <li> <p>Error Handling: Different clients might have slightly different error behavior. Instructor HTTP client API normalizes much of this, but edge cases can still occur.</p> </li> <li> <p>Middleware Compatibility: If you're using middleware, ensure it's compatible with all client types you plan to use.</p> </li> <li> <p>Performance Characteristics: Different clients may have different performance profiles for specific scenarios. Test with your actual workload if performance is critical.</p> </li> </ol> <p>In the next chapter, we'll explore how to customize client configurations in more detail, including runtime configuration and advanced options.</p>"},{"location":"packages/http/8-changing-client-config/","title":"Customizing Client Configuration","text":"<p>The Instructor HTTP client API offers extensive configuration options to customize client behavior for different scenarios. This chapter explores how to configure clients through configuration files and at runtime.</p>"},{"location":"packages/http/8-changing-client-config/#configuration-files","title":"Configuration Files","text":"<p>The primary configuration files for the HTTP client are:</p>"},{"location":"packages/http/8-changing-client-config/#main-configuration-confighttpphp","title":"Main Configuration: config/http.php","text":"<p>This file defines the available client types and their settings:</p> <pre><code>return [\n    'defaultClient' =&gt; 'guzzle',\n    'clients' =&gt; [\n        'guzzle' =&gt; [\n            'httpClientType' =&gt; 'guzzle',\n            'connectTimeout' =&gt; 3,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'symfony' =&gt; [\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n        'laravel' =&gt; [\n            'httpClientType' =&gt; 'laravel',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n];\n</code></pre>"},{"location":"packages/http/8-changing-client-config/#debug-configuration-configdebugphp","title":"Debug Configuration: config/debug.php","text":"<p>This file controls debugging options for HTTP requests and responses:</p> <pre><code>return [\n    'http' =&gt; [\n        'enabled' =&gt; false, // enable/disable debug\n        'trace' =&gt; false, // dump HTTP trace information\n        'requestUrl' =&gt; true, // dump request URL to console\n        'requestHeaders' =&gt; true, // dump request headers to console\n        'requestBody' =&gt; true, // dump request body to console\n        'responseHeaders' =&gt; true, // dump response headers to console\n        'responseBody' =&gt; true, // dump response body to console\n        'responseStream' =&gt; true, // dump stream data to console\n        'responseStreamByLine' =&gt; true, // dump stream as full lines or raw chunks\n    ],\n];\n</code></pre>"},{"location":"packages/http/8-changing-client-config/#loading-configuration-files","title":"Loading Configuration Files","text":"<p>The library uses a settings management system to load these configurations. The system looks for these files in the base directory of your project. If you're using a framework like Laravel or Symfony, you can integrate with their configuration systems instead.</p> <p>For Laravel, you might publish these configurations as Laravel config files:</p> <pre><code>php artisan vendor:publish --tag=polyglot-config\n</code></pre> <p>For Symfony, you might define these as service parameters in your service configuration.</p>"},{"location":"packages/http/8-changing-client-config/#configuration-options","title":"Configuration Options","text":"<p>The <code>HttpClientConfig</code> class encapsulates the configuration options for HTTP clients. Here's a detailed breakdown of the available options:</p>"},{"location":"packages/http/8-changing-client-config/#basic-connection-options","title":"Basic Connection Options","text":"Option Type Description Default <code>httpClientType</code> string Type of HTTP client to use <code>'guzzle'</code> <code>connectTimeout</code> int Connection timeout in seconds 3 <code>requestTimeout</code> int Request timeout in seconds 30 <code>idleTimeout</code> int Idle timeout in seconds (-1 for no timeout) -1"},{"location":"packages/http/8-changing-client-config/#request-pool-options","title":"Request Pool Options","text":"Option Type Description Default <code>maxConcurrent</code> int Maximum number of concurrent requests in a pool 5 <code>poolTimeout</code> int Timeout for the entire request pool in seconds 60"},{"location":"packages/http/8-changing-client-config/#error-handling-options","title":"Error Handling Options","text":"Option Type Description Default <code>failOnError</code> bool Whether to throw exceptions on HTTP errors true"},{"location":"packages/http/8-changing-client-config/#debug-options","title":"Debug Options","text":"Option Type Description Default <code>enabled</code> bool Enable or disable HTTP debugging false <code>trace</code> bool Dump HTTP trace information false <code>requestUrl</code> bool Log the request URL true <code>requestHeaders</code> bool Log request headers true <code>requestBody</code> bool Log request body true <code>responseHeaders</code> bool Log response headers true <code>responseBody</code> bool Log response body true <code>responseStream</code> bool Log streaming response data true <code>responseStreamByLine</code> bool Log stream as complete lines (true) or raw chunks (false) true"},{"location":"packages/http/8-changing-client-config/#understanding-timeout-options","title":"Understanding Timeout Options","text":"<p>Timeout settings are crucial for controlling how your application handles slow or unresponsive servers:</p> <ul> <li> <p>connectTimeout: Maximum time to wait for establishing a connection to the server. If the server doesn't respond within this time, the request fails with a connection timeout error. Setting this too low might cause failures when connecting to slow servers, but setting it too high could leave your application waiting for unresponsive servers.</p> </li> <li> <p>requestTimeout: Maximum time to wait for the entire request to complete, from connection initiation to receiving the complete response. If the entire request-response cycle isn't completed within this time, the request fails with a timeout error.</p> </li> <li> <p>idleTimeout: Maximum time to wait between receiving data packets. If the server stops sending data for longer than this period, the connection is considered idle and is terminated. Setting this to -1 disables the idle timeout, which is useful for long-running streaming connections.</p> </li> <li> <p>poolTimeout: Maximum time to wait for all requests in a pool to complete. If any requests in the pool haven't completed within this time, they're terminated.</p> </li> </ul>"},{"location":"packages/http/8-changing-client-config/#runtime-configuration","title":"Runtime Configuration","text":"<p>While configuration files provide a static way to configure clients, you often need to change configuration at runtime based on the specific requirements of a request or operation.</p>"},{"location":"packages/http/8-changing-client-config/#using-withclient","title":"Using withClient","text":"<p>The simplest way to switch configurations at runtime is to use the <code>withClient</code> method to select a different pre-configured client:</p> <pre><code>// Start with default client\n$client = new HttpClient();\n\n// Switch to a client with longer timeouts\n$client-&gt;withClient('guzzle-long-timeout');\n\n// Switch to a client optimized for streaming\n$client-&gt;withClient('guzzle-streaming');\n</code></pre>"},{"location":"packages/http/8-changing-client-config/#using-withconfig","title":"Using withConfig","text":"<p>For more dynamic configuration, you can create a custom <code>HttpClientConfig</code> object and apply it using the <code>withConfig</code> method:</p> <pre><code>use Cognesy\\Http\\Config\\HttpClientConfig;\n\n// Create a custom configuration\n$config = new HttpClientConfig(\n    driver: 'guzzle',\n    connectTimeout: 5,\n    requestTimeout: 60,\n    idleTimeout: 30,\n    maxConcurrent: 10,\n    poolTimeout: 120,\n    failOnError: false\n);\n\n// Use the custom configuration\n$client-&gt;withConfig($config);\n</code></pre> <p>This method gives you complete control over the configuration at runtime.</p>"},{"location":"packages/http/8-changing-client-config/#creating-configuration-from-an-array","title":"Creating Configuration from an Array","text":"<p>You can also create a configuration from an associative array:</p> <pre><code>$configArray = [\n    'httpClientType' =&gt; 'symfony',\n    'connectTimeout' =&gt; 2,\n    'requestTimeout' =&gt; 45,\n    'idleTimeout' =&gt; -1,\n    'maxConcurrent' =&gt; 8,\n    'poolTimeout' =&gt; 90,\n    'failOnError' =&gt; true,\n];\n\n$config = HttpClientConfig::fromArray($configArray);\n$client-&gt;withConfig($config);\n</code></pre> <p>This approach is useful when loading configuration from external sources like environment variables or configuration files.</p>"},{"location":"packages/http/8-changing-client-config/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>You can enable debug mode to see detailed information about requests and responses:</p> <pre><code>// Enable debug mode\n$client-&gt;withDebugPreset('on');\n\n// Make a request\n$response = $client-&gt;withRequest($request)-&gt;get();\n\n// Disable debug mode when done\n$client-&gt;withDebugPreset('off');\n</code></pre> <p>When debug mode is enabled, detailed information about requests and responses is output to the console or log.</p>"},{"location":"packages/http/8-changing-client-config/#example-dynamic-configuration-based-on-request-type","title":"Example: Dynamic Configuration Based on Request Type","text":"<p>Here's an example of dynamically adjusting configuration based on the type of request:</p> <pre><code>function configureClientForRequest(HttpClient $client, HttpRequest $request): HttpClient {\n    // Get the current configuration\n    $config = HttpClientConfig::load($client-&gt;getClientName());\n\n    // Adjust timeouts based on the request URL\n    if (strpos($request-&gt;url(), 'large-file') !== false) {\n        // For large file downloads, use longer timeouts\n        $config = new HttpClientConfig(\n            httpClientType: $config-&gt;httpClientType,\n            connectTimeout: $config-&gt;connectTimeout,\n            requestTimeout: 300, // 5 minutes\n            idleTimeout: 60,     // 1 minute\n            maxConcurrent: $config-&gt;maxConcurrent,\n            poolTimeout: $config-&gt;poolTimeout,\n            failOnError: $config-&gt;failOnError\n        );\n\n        $client-&gt;withConfig($config);\n    }\n\n    // Enable streaming for specific endpoints\n    if (strpos($request-&gt;url(), '/stream') !== false || strpos($request-&gt;url(), '/events') !== false) {\n        // Make sure the request is set to stream\n        $request = $request-&gt;withStreaming(true);\n    }\n\n    // Enable debug for development environment\n    if (getenv('APP_ENV') === 'development') {\n        $client-&gt;withDebugPreset('on');\n    }\n\n    return $client;\n}\n\n// Usage\n$client = new HttpClient();\n$request = new HttpRequest(...);\n\n// Configure the client based on the request\n$client = configureClientForRequest($client, $request);\n\n// Send the request\n$response = $client-&gt;withRequest($request)-&gt;get();\n</code></pre> <p>This approach allows for highly dynamic and contextual configuration adjustments.</p>"},{"location":"packages/http/8-changing-client-config/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li> <p>Define Base Configurations in Files: Keep your common configurations in the <code>config/http.php</code> file for easy reference and maintenance.</p> </li> <li> <p>Use Named Configurations: Create named configurations for different scenarios (e.g., <code>'guzzle-short-timeout'</code>, <code>'guzzle-streaming'</code>) to make your code more readable and maintainable.</p> </li> <li> <p>Adjust Timeouts Appropriately: Set timeouts based on the expected response time of the API or service you're calling. Shorter for quick operations, longer for file uploads/downloads or streaming.</p> </li> <li> <p>Consider Error Handling Strategy: Set <code>failOnError</code> based on how you want to handle errors. For critical operations, set it to <code>true</code> to catch errors immediately. For bulk operations or request pools, set it to <code>false</code> to handle errors individually.</p> </li> <li> <p>Use Debug Mode Judiciously: Enable debug mode only when needed, as it can generate a lot of output and potentially impact performance.</p> </li> <li> <p>Test Different Configurations: Experiment with different settings to find the optimal configuration for your specific use cases.</p> </li> </ol>"},{"location":"packages/http/8-changing-client-config/#adapting-to-different-environments","title":"Adapting to Different Environments","text":"<p>Different environments often require different configurations. Here's how you might handle this:</p> <pre><code>// In your application bootstrap or service provider\nfunction configureHttpClient() {\n    $env = getenv('APP_ENV') ?: 'production';\n\n    // Load base configuration\n    $config = HttpClientConfig::load('guzzle');\n\n    // Adjust based on environment\n    switch ($env) {\n        case 'development':\n            // Shorter timeouts for faster feedback during development\n            $config = new HttpClientConfig(\n                httpClientType: $config-&gt;httpClientType,\n                connectTimeout: 1,\n                requestTimeout: 10,\n                idleTimeout: $config-&gt;idleTimeout,\n                maxConcurrent: $config-&gt;maxConcurrent,\n                poolTimeout: $config-&gt;poolTimeout,\n                failOnError: true // Throw errors for immediate feedback\n            );\n            break;\n\n        case 'testing':\n            // Use mock driver for tests\n            $config = new HttpClientConfig(\n                httpClientType: 'custom',\n                connectTimeout: 1,\n                requestTimeout: 1,\n                idleTimeout: 1,\n                maxConcurrent: 1,\n                poolTimeout: 5,\n                failOnError: true\n            );\n\n            // Create a mock client\n            $mockDriver = new MockHttpDriver();\n            // Configure mock responses...\n\n            return (new HttpClient())-&gt;withConfig($config)-&gt;withDriver($mockDriver);\n\n        case 'production':\n            // More conservative timeouts for production\n            $config = new HttpClientConfig(\n                httpClientType: $config-&gt;httpClientType,\n                connectTimeout: 5,\n                requestTimeout: 60,\n                idleTimeout: $config-&gt;idleTimeout,\n                maxConcurrent: 10,\n                poolTimeout: 120,\n                failOnError: false // Handle errors gracefully in production\n            );\n            break;\n    }\n\n    return (new HttpClient())-&gt;withConfig($config);\n}\n\n// Get a properly configured client\n$client = configureHttpClient();\n</code></pre> <p>This approach allows you to adapt your HTTP client configuration to different environments while maintaining a consistent API.</p> <p>In the next chapter, we'll explore how to create and use custom HTTP client implementations for specialized needs.</p>"},{"location":"packages/http/9-1-custom-clients/","title":"Using Custom HTTP Clients","text":"<p>While the Instructor HTTP client API provides built-in support for popular HTTP client libraries (Guzzle, Symfony, and Laravel), there may be cases where you need to integrate with other HTTP client libraries or create specialized implementations. This chapter covers how to create and use custom HTTP client drivers.</p>"},{"location":"packages/http/9-1-custom-clients/#creating-custom-http-client-drivers","title":"Creating Custom HTTP Client Drivers","text":"<p>Creating a custom HTTP client driver involves implementing the <code>CanHandleHttpRequest</code> interface and optionally the <code>CanHandleRequestPool</code> interface for pool support.</p>"},{"location":"packages/http/9-1-custom-clients/#implementing-the-canhandlehttprequest-interface","title":"Implementing the CanHandleHttpRequest Interface","text":"<p>The <code>CanHandleHttpRequest</code> interface requires implementing a single method:</p> <pre><code>interface CanHandleHttpRequest\n{\n    public function handle(HttpClientRequest $request): HttpResponse;\n}\n</code></pre> <p>Here's a template for creating a custom HTTP client driver:</p> <pre><code>namespace YourNamespace\\Http\\Drivers;\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse Exception;\n\nclass CustomHttpDriver implements CanHandleHttpRequest\n{\n    /**\n     * Your custom HTTP client instance\n     */\n    private $yourHttpClient;\n\n    /**\n     * Constructor\n     */\n    public function __construct(\n        protected HttpClientConfig $config,\n        protected ?EventDispatcher $events = null,\n    ) {\n        $this-&gt;events = $events ?? new EventDispatcher();\n\n        // Initialize your HTTP client with the configuration\n        $this-&gt;yourHttpClient = $this-&gt;createYourHttpClient();\n    }\n\n    /**\n     * Handle an HTTP request\n     */\n    public function handle(HttpRequest $request): CanAdaptHttpResponse\n    {\n        $url = $request-&gt;url();\n        $headers = $request-&gt;headers();\n        $body = $request-&gt;body()-&gt;toString();\n        $method = $request-&gt;method();\n        $streaming = $request-&gt;isStreamed();\n\n        // Dispatch event before sending request\n        $this-&gt;events-&gt;dispatch(new HttpRequestSent([\n            'url' =&gt; $url,\n            'method' =&gt; $method,\n            'headers' =&gt; $headers,\n            'body' =&gt; $request-&gt;body()-&gt;toArray(),\n        ]));\n\n        try {\n            // Use your HTTP client to make the request\n            $response = $this-&gt;yourHttpClient-&gt;send($method, $url, [\n                'headers' =&gt; $headers,\n                'body' =&gt; $body,\n                'timeout' =&gt; $this-&gt;config-&gt;requestTimeout,\n                'connect_timeout' =&gt; $this-&gt;config-&gt;connectTimeout,\n                'stream' =&gt; $streaming,\n                // Other options relevant to your client...\n            ]);\n\n            // Dispatch event for successful response\n            $this-&gt;events-&gt;dispatch(new HttpResponseReceived([\n                'statusCode' =&gt; $response-&gt;statusCode()\n            ]));\n\n            // Return the response wrapped in your adapter\n            return new YourHttpResponse($response, $streaming);\n\n        } catch (Exception $e) {\n            // Dispatch event for failed request\n            $this-&gt;events-&gt;dispatch(new HttpRequestFailed([\n                'url' =&gt; $url,\n                'method' =&gt; $method,\n                'headers' =&gt; $headers,\n                'body' =&gt; $request-&gt;body()-&gt;toArray(),\n                'errors' =&gt; $e-&gt;getMessage(),\n            ]));\n\n            // Wrap the exception\n            throw new HttpRequestException($e);\n        }\n    }\n\n    /**\n     * Create your HTTP client instance\n     */\n    private function createYourHttpClient()\n    {\n        // Initialize your HTTP client with appropriate configuration\n        return new YourHttpClient([\n            'connect_timeout' =&gt; $this-&gt;config-&gt;connectTimeout,\n            'timeout' =&gt; $this-&gt;config-&gt;requestTimeout,\n            'idle_timeout' =&gt; $this-&gt;config-&gt;idleTimeout,\n            // Other options...\n        ]);\n    }\n}\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#creating-a-response-adapter","title":"Creating a Response Adapter","text":"<p>You also need to create a response adapter that implements the <code>HttpResponse</code> interface:</p> <pre><code>namespace YourNamespace\\Http\\Adapters;\n\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\nuse Generator;\n\nclass YourHttpResponse implements CanAdaptHttpResponse\n{\n    /**\n     * Constructor\n     */\n    public function __construct(\n        private $yourResponse,\n        private bool $streaming = false\n    ) {}\n\n    /**\n     * Get the response status code\n     */\n    public function statusCode(): int\n    {\n        return $this-&gt;yourResponse-&gt;getStatusCode();\n    }\n\n    /**\n     * Get the response headers\n     */\n    public function headers(): array\n    {\n        return $this-&gt;yourResponse-&gt;getHeaders();\n    }\n\n    /**\n     * Get the response body\n     */\n    public function body(): string\n    {\n        return $this-&gt;yourResponse-&gt;getBody();\n    }\n\n    /**\n     * Stream the response body\n     */\n    public function stream(int $chunkSize = 1): Generator\n    {\n        if (!$this-&gt;streaming) {\n            // For non-streaming responses, just yield the entire body\n            yield $this-&gt;body();\n            return;\n        }\n\n        // For streaming responses, yield chunks\n        $stream = $this-&gt;yourResponse-&gt;getStream();\n\n        while (!$stream-&gt;eof()) {\n            yield $stream-&gt;read($chunkSize);\n        }\n    }\n}\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#using-your-custom-http-client-driver","title":"Using Your Custom HTTP Client Driver","text":"<p>Once you've implemented your custom driver, you can use it with the <code>HttpClient</code>:</p> <pre><code>use Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\HttpClient;\nuse YourNamespace\\Http\\Drivers\\CustomHttpDriver;\n\n// Create a configuration for your custom driver\n$config = new HttpClientConfig(\n    driver: 'custom',\n    connectTimeout: 3,\n    requestTimeout: 30,\n    idleTimeout: -1,\n    maxConcurrent: 5,\n    poolTimeout: 60,\n    failOnError: true\n);\n\n// Create your custom driver\n$customDriver = new CustomHttpDriver($config);\n\n// Create a client with your driver using the builder\n$client = (new HttpClientBuilder())-&gt;withDriver($customDriver)-&gt;create();\n\n// Use the client as usual\n$response = $client-&gt;withRequest(new HttpRequest(/* ... */))-&gt;get();\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#real-world-example-creating-a-curl-driver","title":"Real-World Example: Creating a cURL Driver","text":"<p>Here's a practical example of implementing a custom driver using PHP's cURL extension directly:</p> <pre><code>namespace YourNamespace\\Http\\Drivers;\n\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Http\\Config\\HttpClientConfig;\nuse Cognesy\\Http\\Contracts\\CanHandleHttpRequest;\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\nuse Cognesy\\Http\\Data\\HttpRequest;\nuse Cognesy\\Http\\Events\\HttpRequestFailed;\nuse Cognesy\\Http\\Events\\HttpRequestSent;\nuse Cognesy\\Http\\Events\\HttpResponseReceived;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\nuse YourNamespace\\Http\\Adapters\\CurlHttpResponse;\n\nclass CurlHttpDriver implements CanHandleHttpRequest\n{\n    /**\n     * Constructor\n     */\n    public function __construct(\n        protected HttpClientConfig $config,\n        protected ?EventDispatcher $events = null,\n    ) {\n        $this-&gt;events = $events ?? new EventDispatcher();\n    }\n\n    /**\n     * Handle an HTTP request\n     */\n    public function handle(HttpRequest $request): CanAdaptHttpResponse\n    {\n        $url = $request-&gt;url();\n        $headers = $request-&gt;headers();\n        $body = $request-&gt;body()-&gt;toString();\n        $method = $request-&gt;method();\n        $streaming = $request-&gt;isStreamed();\n\n        // Dispatch event before sending request\n        $this-&gt;events-&gt;dispatch(new HttpRequestSent(\n            $url,\n            $method,\n            $headers,\n            $request-&gt;body()-&gt;toArray()\n        ));\n\n        try {\n            // Initialize cURL\n            $ch = curl_init();\n\n            // Format headers for cURL\n            $curlHeaders = [];\n            foreach ($headers as $name =&gt; $value) {\n                if (is_array($value)) {\n                    foreach ($value as $v) {\n                        $curlHeaders[] = \"{$name}: {$v}\";\n                    }\n                } else {\n                    $curlHeaders[] = \"{$name}: {$value}\";\n                }\n            }\n\n            // Set cURL options\n            curl_setopt_array($ch, [\n                CURLOPT_URL =&gt; $url,\n                CURLOPT_RETURNTRANSFER =&gt; true,\n                CURLOPT_HTTPHEADER =&gt; $curlHeaders,\n                CURLOPT_CONNECTTIMEOUT =&gt; $this-&gt;config-&gt;connectTimeout,\n                CURLOPT_TIMEOUT =&gt; $this-&gt;config-&gt;requestTimeout,\n                CURLOPT_HEADER =&gt; true, // Include headers in output\n                CURLOPT_FOLLOWLOCATION =&gt; true,\n                CURLOPT_MAXREDIRS =&gt; 5,\n            ]);\n\n            // Set method-specific options\n            switch ($method) {\n                case 'POST':\n                    curl_setopt($ch, CURLOPT_POST, true);\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'PUT':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PUT');\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'PATCH':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PATCH');\n                    curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    break;\n                case 'DELETE':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'DELETE');\n                    if (!empty($body)) {\n                        curl_setopt($ch, CURLOPT_POSTFIELDS, $body);\n                    }\n                    break;\n                case 'HEAD':\n                    curl_setopt($ch, CURLOPT_NOBODY, true);\n                    break;\n                case 'OPTIONS':\n                    curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'OPTIONS');\n                    break;\n                case 'GET':\n                default:\n                    // GET is the default in cURL\n                    break;\n            }\n\n            // Handle streaming if requested\n            $responseBody = '';\n            $responseHeaders = [];\n\n            if ($streaming) {\n                $tempHandle = null;\n                $tempFile = tempnam(sys_get_temp_dir(), 'curl_stream_');\n                $tempHandle = fopen($tempFile, 'w+');\n\n                curl_setopt($ch, CURLOPT_FILE, $tempHandle);\n                curl_setopt($ch, CURLOPT_WRITEFUNCTION, function($ch, $data) use ($tempHandle) {\n                    return fwrite($tempHandle, $data);\n                });\n\n                curl_setopt($ch, CURLOPT_HEADERFUNCTION, function($ch, $header) use (&amp;$responseHeaders) {\n                    $len = strlen($header);\n                    $header = explode(':', $header, 2);\n                    if (count($header) &lt; 2) {\n                        return $len;\n                    }\n\n                    $name = trim($header[0]);\n                    $value = trim($header[1]);\n\n                    $responseHeaders[$name][] = $value;\n                    return $len;\n                });\n\n                $result = curl_exec($ch);\n                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n\n                if ($result === false) {\n                    throw new \\RuntimeException('cURL error: ' . curl_error($ch));\n                }\n\n                // Rewind the temp file so it can be read\n                rewind($tempHandle);\n\n                // Create streaming response\n                $response = new CurlHttpResponse(\n                    statusCode: $statusCode,\n                    headers: $responseHeaders,\n                    body: '',\n                    stream: $tempHandle,\n                    isStreaming: true,\n                    tempFile: $tempFile\n                );\n            } else {\n                // For non-streaming responses, get the full response\n                $result = curl_exec($ch);\n\n                if ($result === false) {\n                    throw new \\RuntimeException('cURL error: ' . curl_error($ch));\n                }\n\n                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\n                $headerSize = curl_getinfo($ch, CURLINFO_HEADER_SIZE);\n\n                // Extract headers and body\n                $headerText = substr($result, 0, $headerSize);\n                $responseBody = substr($result, $headerSize);\n\n                // Parse headers\n                $headers = explode(\"\\r\\n\", $headerText);\n                foreach ($headers as $header) {\n                    $parts = explode(':', $header, 2);\n                    if (count($parts) === 2) {\n                        $name = trim($parts[0]);\n                        $value = trim($parts[1]);\n                        $responseHeaders[$name][] = $value;\n                    }\n                }\n\n                // Create regular response\n                $response = new CurlHttpResponse(\n                    statusCode: $statusCode,\n                    headers: $responseHeaders,\n                    body: $responseBody\n                );\n            }\n\n            // Clean up cURL\n            curl_close($ch);\n\n            // Dispatch event for successful response\n            $this-&gt;events-&gt;dispatch(new HttpResponseReceived([\n                'statusCode' =&gt; $response-&gt;statusCode()\n            ]));\n\n            return $response;\n\n        } catch (\\Exception $e) {\n            // Clean up if needed\n            if (isset($ch) &amp;&amp; is_resource($ch)) {\n                curl_close($ch);\n            }\n\n            // Dispatch event for failed request\n            $this-&gt;events-&gt;dispatch(new HttpRequestFailed([\n                'url' =&gt; $url,\n                'methods' =&gt; $method,\n                'headers' =&gt; $headers,\n                'body' =&gt; $request-&gt;body()-&gt;toArray(),\n                'errors' =&gt; $e-&gt;getMessage()\n            ]));\n\n            // Wrap the exception\n            throw new HttpRequestException($e);\n        }\n    }\n}\n</code></pre> <p>And here's the corresponding response adapter:</p> <pre><code>namespace YourNamespace\\Http\\Adapters;\n\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;\nuse Generator;\n\nclass CurlHttpResponse implements CanAdaptHttpResponse\n{\n    private $stream;\n    private $tempFile;\n    private $isStreaming;\n\n    /**\n     * Constructor\n     */\n    public function __construct(\n        private int $statusCode,\n        private array $headers,\n        private string $body,\n        $stream = null,\n        bool $isStreaming = false,\n        ?string $tempFile = null\n    ) {\n        $this-&gt;stream = $stream;\n        $this-&gt;isStreaming = $isStreaming;\n        $this-&gt;tempFile = $tempFile;\n    }\n\n    /**\n     * Destructor - clean up temp files\n     */\n    public function __destruct()\n    {\n        if ($this-&gt;stream &amp;&amp; is_resource($this-&gt;stream)) {\n            fclose($this-&gt;stream);\n        }\n\n        if ($this-&gt;tempFile &amp;&amp; file_exists($this-&gt;tempFile)) {\n            unlink($this-&gt;tempFile);\n        }\n    }\n\n    /**\n     * Get the response status code\n     */\n    public function statusCode(): int\n    {\n        return $this-&gt;statusCode;\n    }\n\n    /**\n     * Get the response headers\n     */\n    public function headers(): array\n    {\n        return $this-&gt;headers;\n    }\n\n    /**\n     * Get the response body\n     */\n    public function body(): string\n    {\n        if ($this-&gt;isStreaming &amp;&amp; $this-&gt;stream) {\n            // For streaming responses, read the entire file\n            rewind($this-&gt;stream);\n            $contents = stream_get_contents($this-&gt;stream);\n            rewind($this-&gt;stream);\n            return $contents;\n        }\n\n        return $this-&gt;body;\n    }\n\n    /**\n     * Stream the response body\n     */\n    public function stream(int $chunkSize = 1): Generator\n    {\n        if ($this-&gt;isStreaming &amp;&amp; $this-&gt;stream) {\n            // For streaming responses, yield chunks from the file\n            rewind($this-&gt;stream);\n\n            while (!feof($this-&gt;stream)) {\n                yield fread($this-&gt;stream, $chunkSize);\n            }\n        } else {\n            // For non-streaming responses, yield the entire body\n            yield $this-&gt;body;\n        }\n    }\n}\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#registering-custom-http-client-drivers","title":"Registering Custom HTTP Client Drivers","text":"<p>To use your custom driver, you can register it with the driver factory or use it directly with the HttpClientBuilder:</p> <pre><code>use Cognesy\\Http\\Creation\\HttpClientBuilder;use YourNamespace\\Http\\Drivers\\CustomHttpDriver;\n\n// Use your custom driver directly with the builder\n$customDriver = new CustomHttpDriver($config);\n$client = (new HttpClientBuilder())\n    -&gt;withDriver($customDriver)\n    -&gt;create();\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#adding-custom-http-client-to-configuration","title":"Adding Custom HTTP Client to Configuration","text":"<p>Use your custom driver in the HTTP client configuration file (<code>config/http-client.php</code>):</p> <pre><code>// http-client.php configuration file\n// ...\n    'clients' =&gt; [\n        'my-custom-client' =&gt; [\n            // Our custom driver type\n            'httpDriverType' =&gt; 'my-custom-driver',\n            // Other driver-specific options...\n            'httpClientType' =&gt; 'symfony',\n            'connectTimeout' =&gt; 1,\n            'requestTimeout' =&gt; 30,\n            'idleTimeout' =&gt; -1,\n            'maxConcurrent' =&gt; 5,\n            'poolTimeout' =&gt; 60,\n            'failOnError' =&gt; true,\n        ],\n    ],\n// ...\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#using-your-custom-http-client","title":"Using Your Custom HTTP Client","text":"<p>After adding the driver to the configuration, you can use it in your code:</p> <pre><code>use Cognesy\\Http\\HttpClient;\n\n// Create a client with your custom driver\n$client = HttpClient::using('my-custom-client');\n</code></pre>"},{"location":"packages/http/9-1-custom-clients/#using-custom-http-clients-in-configuration-files","title":"Using Custom HTTP Clients in Configuration Files","text":"<p>Or you can refer to it in your LLM connections configuration:</p> <pre><code>    // llm-connections.php configuration file\n    // ...\n        'a21' =&gt; [\n            'providerType' =&gt; 'a21',\n            'apiUrl' =&gt; 'https://api.ai21.com/studio/v1',\n            'apiKey' =&gt; Env::get('A21_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'jamba-1.5-mini',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 256_000,\n            'maxOutputLength' =&gt; 4096,\n            // Our custom HTTP client\n            // Select your HTTP configuration via HttpClientBuilder\n        ],\n    // ...\n</code></pre>"},{"location":"packages/instructor/cli_tools/","title":"CLI Tools","text":"<p>Instructor comes with command line tools:  - <code>./vendor/bin/instructor-setup publish</code>: Publishes configuration files and prompt templates to your project directory  - <code>./vendor/bin/instructor-hub</code>: Displays and executes Instructor examples</p> <p>Additional tool included with Instructor (under development):  - <code>./vendor/bin/tell \"&lt;prompt to LLM&gt;\"</code>: Interacts with LLMs from the command line</p>"},{"location":"packages/instructor/introduction/","title":"Introduction","text":""},{"location":"packages/instructor/introduction/#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to get structured, validated data from multiple types of inputs: text, chat messages, or images. It is powered by Large Language Models (LLMs).</p> <p>The library is inspired by the Instructor for Python created by Jason Liu.</p>"},{"location":"packages/instructor/introduction/#learn-more","title":"Learn More...","text":"<p>          Check how to set up Instructor in your project and start processing data with LLMs      <pre><code>&lt;Card\n    title=\"Concepts\"\n    icon=\"shapes\"\n    href=\"/instructor/concepts/overview\"\n&gt;\n    Read more about basic concepts behind Instructor\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Essentials\"\n    icon=\"hammer\"\n    href=\"/instructor/essentials\"\n&gt;\n    Learn Instructor features and capabilities\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Cookbooks\"\n    icon=\"book\"\n    href=\"/cookbook/introduction\"\n&gt;\n    Browse examples to see Instructor in action and find out how to use it in your projects\n&lt;/Card&gt;\n\n&lt;Card\n    title=\"Internals\"\n    icon=\"gears\"\n    href=\"/instructor/internals\"\n&gt;\n    Deep dive into Instructor internals and low level mechanisms\n&lt;/Card&gt;\n</code></pre> <p></p>"},{"location":"packages/instructor/introduction/#feature-highlights","title":"Feature Highlights","text":"<p>Instructor is designed to make it easy to process data with LLMs in PHP. Here are some of the key features of the library:</p>"},{"location":"packages/instructor/introduction/#core-features","title":"Core features","text":"<ul> <li>Get structured responses from LLM inference</li> <li>Validation of returned data</li> <li>Automated retries in case of errors when LLM responds with invalid data</li> </ul>"},{"location":"packages/instructor/introduction/#flexible-inputs","title":"Flexible inputs","text":"<ul> <li>Process various types of input data: text, series of chat messages or images</li> <li>'Structured-to-structured' processing - provide object or array as an input and get object with the results of inference back</li> <li>Demonstrate examples to improve the quality of inference</li> </ul>"},{"location":"packages/instructor/introduction/#customizable-outputs","title":"Customizable outputs","text":"<ul> <li>Define response data model the way to need: type-hinted classes, JSON Schema arrays, or dynamically define your data shapes with Structures</li> <li>Customize prompts and retry prompts</li> <li>Use attributes or PHP DocBlocks to provide additional instructions for LLM</li> <li>Customize response model processing by providing your own implementation of schema, deserialization, validation and transformation interfaces</li> </ul>"},{"location":"packages/instructor/introduction/#sync-and-streaming-support","title":"Sync and streaming support","text":"<ul> <li>Receive synchronous or streaming responses</li> <li>Get partial updates &amp; stream completed sequence items</li> </ul>"},{"location":"packages/instructor/introduction/#observability","title":"Observability","text":"<ul> <li>Get detailed insight into internal processing via events</li> </ul>"},{"location":"packages/instructor/introduction/#support-for-multiple-llms-api-providers","title":"Support for multiple LLMs / API providers","text":"<ul> <li>Use multiple LLM API providers (incl. OpenAI,  Anthropic, Cohere, Azure, Groq, Mistral, Fireworks AI, Ollama, OpenRouter, Together AI)</li> <li>Use local models with Ollama</li> </ul>"},{"location":"packages/instructor/introduction/#documentation-and-examples","title":"Documentation and examples","text":"<ul> <li>Learn more from growing documentation and 50+ cookbooks</li> </ul>"},{"location":"packages/instructor/introduction/#instructor-in-other-languages","title":"Instructor in Other Languages","text":"<p>Instructor has been implemented in various technology stacks. Check out implementations in other languages below:</p> <ul> <li>Python (original)</li> <li>Javascript (port)</li> <li>Elixir (port)</li> <li>Ruby (port)</li> <li>Go (port)</li> </ul> <p>If you want to port Instructor to another language, please reach out to us on Twitter we'd love to help you get started!</p>"},{"location":"packages/instructor/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with Instructor in your PHP project in under 5 minutes.</p> <p>For detailed setup instructions, see Setup.</p>"},{"location":"packages/instructor/quickstart/#install-instructor-with-composer","title":"Install Instructor with Composer","text":"<p>Run following command in your terminal:</p> <pre><code>composer require cognesy/instructor-php\n</code></pre>"},{"location":"packages/instructor/quickstart/#create-and-run-example","title":"Create and Run Example","text":""},{"location":"packages/instructor/quickstart/#step-1-prepare-your-openai-api-key","title":"Step 1: Prepare your OpenAI API Key","text":"<p>In this example, we'll use OpenAI as the LLM provider. You can get it from the OpenAI dashboard.</p>"},{"location":"packages/instructor/quickstart/#step-2-create-a-new-php-file","title":"Step 2: Create a New PHP File","text":"<p>In your project directory, create a new PHP file <code>test-instructor.php</code>:</p> <pre><code>&lt;?php\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Set up OpenAI API key\n$apiKey = 'your-openai-api-key';\nputenv(\"OPENAI_API_KEY=\" . $apiKey);\n// WARNING: In real project you should set up API key in .env file.\n\n// Step 1: Define target data structure(s)\nclass City {\n    public string $name;\n    public string $country;\n    public int $population;\n}\n\n// Step 2: Use Instructor to run LLM inference\n$city = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withResponseClass(City::class)\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\nvar_dump($city);\n</code></pre> <p> You should never put your API keys directly in your real project code to avoid getting them compromised. Set them up in your .env file. </p>"},{"location":"packages/instructor/quickstart/#step-3-run-the-example","title":"Step 3: Run the Example","text":"<p>Now, you can run the example:</p> <pre><code>php test-instructor.php\n\n# Output:\n# object(City)#1 (3) {\n#   [\"name\"]=&gt;\n#   string(5) \"Paris\"\n#   [\"country\"]=&gt;\n#   string(6) \"France\"\n#   [\"population\"]=&gt;\n#   int(2148000)\n# }\n</code></pre>"},{"location":"packages/instructor/quickstart/#next-steps","title":"Next Steps","text":"<p>You can start using Instructor in your project right away after installation.</p> <p>But it's recommended to publish configuration files and prompt templates to your project directory, so you can customize the library's behavior and use your own prompt templates.</p> <p>You should also set up LLM provider API keys in your <code>.env</code> file instead of putting them directly in your code.</p> <p>See Setup Instructions for more details.</p>"},{"location":"packages/instructor/setup/","title":"Setup","text":""},{"location":"packages/instructor/setup/#overview","title":"Overview","text":"<p>Full Instructor setup consists of following steps:  - Step 1: Install Instructor via Composer  - Step 2: Publish Instructor assets (configurations and prompts) to your project directory  - Step 3: Set up LLM provider API key(s) in your .env file  - Step 4: Set configuration location in your .env file (optional)</p>"},{"location":"packages/instructor/setup/#step-1-install-instructor-via-composer","title":"Step 1: Install Instructor via Composer","text":"<p>You can install Instructor via Composer by running:</p> <pre><code>composer require cognesy/instructor-php\n</code></pre>"},{"location":"packages/instructor/setup/#step-2-publish-instructor-files-to-your-project","title":"Step 2: Publish Instructor Files to Your Project","text":"<p>Instructor comes with a set of configuration files and prompt templates that you can publish to your project directory.</p> <p>This will allow you to customize the library's behavior and use different prompt templates.</p> <p>These files can be found in the <code>vendor/cognesy/instructor-php</code> directory: - <code>.env-dist</code> - Environment variables for API keys and configuration paths - <code>/config/*.php</code> - Configurations of Instructor modules - <code>/prompts/*</code> - Prompt templates for generating structured data from text</p> <p>You can publish these files to your project directory by running following command:</p> <pre><code>./vendor/bin/instructor-setup publish \\n\n  --target-config-dir=&lt;target config dir location&gt;\n  --target-prompts-dir=&lt;target prompts dir location&gt;\n  --target-env-file=&lt;target .env file location&gt;\n</code></pre> <p>You can also manually copy the required files to your project directory.</p> <p> Read more: - Framework Integration     - Laravel Projects     - Symfony Projects     - Custom Framework Location - Use CLI Tool to publish Instructor assets - Manual Setup </p>"},{"location":"packages/instructor/setup/#step-3-set-up-llm-provider-api-keys","title":"Step 3: Set Up LLM Provider API Key(s)","text":"<p>If you're using commercial LLM providers like OpenAI, you'll need to set up API keys in your project's <code>.env</code> file.</p> <p>Open the <code>.env</code> file in your project directory and set up API keys for the LLM providers you plan to use. You can find the keys in the respective provider's dashboard.</p> <p>```ini .env</p>"},{"location":"packages/instructor/setup/#openai-default-provider","title":"OpenAI (default provider)","text":"<p>OPENAI_API_KEY='your-openai-api-key' <pre><code>Check `.env-dist` for other API keys Instructor uses in its default configuration files.\n\n\n### Step 4: Set Configuration Location (optional)\n\nInstructor uses a configuration directory to store its settings, e.g. LLM provider configurations.\n\nYou can set the path to this directory via `Settings::setPath('/path/to/config')` in your code.\n\nBut to make it easier you can just set the value in your `.env` file. `Settings` will pick it up automatically\nfrom there. This way you don't have to set it in every script.\n\n```ini .env\nINSTRUCTOR_CONFIG_PATHS='/path/to/your/config/dir/,another/path'\n</code></pre></p> <p> <code>INSTRUCTOR_CONFIG_PATHS</code> is set automatically if you use the Instructor CLI tool to publish assets. </p>"},{"location":"packages/instructor/setup/#framework-integration","title":"Framework Integration","text":""},{"location":"packages/instructor/setup/#laravel-projects","title":"Laravel Projects","text":"<p>For Laravel applications, it's recommended to align with the framework's directory structure:</p> <pre><code>./vendor/bin/instructor-setup publish \\\n    --target-config-dir=config/instructor \\\n    --target-prompts-dir=resources/prompts \\\n    --target-env-file=.env\n</code></pre> <p>This will: - Place configuration files in Laravel's <code>config</code> directory - Store prompts in Laravel's <code>resources</code> directory - Use Laravel's default <code>.env</code> file location</p> <p>After publishing, you can load Instructor configuration in your <code>config/app.php</code> or create a dedicated service provider.</p>"},{"location":"packages/instructor/setup/#symfony-projects","title":"Symfony Projects","text":"<p>For Symfony applications, use the standard Symfony directory structure:</p> <pre><code>./vendor/bin/instructor-setup publish \\\n    --target-config-dir=config/packages/instructor \\\n    --target-prompts-dir=resources/instructor/prompts \\\n    --target-env-file=.env\n</code></pre> <p>This will: - Place configuration in Symfony's package configuration directory - Store prompts in Symfony's <code>resources</code> directory - Use Symfony's default <code>.env</code> file location</p> <p>For Symfony Flex applications, you may want to create a recipe to automate this setup process.</p>"},{"location":"packages/instructor/setup/#custom-framework-location","title":"Custom Framework Location","text":"<p>You can use environment variables to set the location of configuration files: <pre><code>INSTRUCTOR_CONFIG_PATHS=/path/to/config,another/path\n</code></pre></p> <p>This allows you to maintain consistent paths across your application without specifying them in each command.</p>"},{"location":"packages/instructor/setup/#using-cli-tool","title":"Using CLI Tool","text":"<p>After installing Instructor via Composer, you may want to publish the library's configuration files and resources to your project, so you can modify them according to your needs. You can do this either manually or automatically using the provided CLI tool.</p> <pre><code>./vendor/bin/instructor-setup publish\n</code></pre> <p>By default, this command will: 1. Copy configuration files from <code>vendor/cognesy/instructor-php/config</code> to <code>config/instructor/</code> 2. Copy prompt templates from <code>vendor/cognesy/instructor-php/prompts</code> to <code>resources/prompts/</code> 3. Merge (or copy) <code>vendor/cognesy/instructor-php/.env-dist</code> file to <code>.env</code> with environment variables</p>"},{"location":"packages/instructor/setup/#command-options","title":"Command Options","text":"<ul> <li><code>-c, --target-config-dir=DIR</code> - Custom directory for configuration files (default: <code>config/instructor</code>)</li> <li><code>-p, --target-prompts-dir=DIR</code> - Custom directory for prompt templates (default: <code>resources/prompts</code>)</li> <li><code>-e, --target-env-file=FILE</code> - Custom location for .env file (default: <code>.env</code>)</li> <li><code>-l, --log-file=FILE</code> - Optional log file path to track the publishing process</li> <li><code>--no-op</code> - Dry run mode - shows what would be copied without making changes</li> </ul>"},{"location":"packages/instructor/setup/#example-usage","title":"Example Usage","text":"<pre><code>./vendor/bin/instructor-setup publish \\\n    --target-config-dir=./config/instructor \\\n    --target-prompts-dir=./resources/prompts \\\n    --target-env-file=.env\n</code></pre> <p> When merging <code>.env</code> files, the tool will only add missing variables, preserving your existing file content, formatting and comments. </p>"},{"location":"packages/instructor/setup/#manual-setup","title":"Manual Setup","text":"<p>If you prefer to set up Instructor manually or need more control over the process, you can copy the required files directly:</p>"},{"location":"packages/instructor/setup/#configuration-files","title":"Configuration Files","text":"<p><pre><code># Create config in your preferred directory\nmkdir -p config/instructor\n\n# Copy configuration files\ncp -r vendor/cognesy/instructor-php/config/* config/instructor/\n</code></pre> These files contain LLM API connection settings and Instructor's behavior configuration.</p>"},{"location":"packages/instructor/setup/#prompt-templates","title":"Prompt Templates","text":"<p><pre><code># Create prompts in your preferred directory\nmkdir -p resources/prompts\n\n# Copy prompt templates\ncp -r vendor/cognesy/instructor-php/prompts/* resources/prompts/\n</code></pre> Prompt templates define how Instructor communicates with LLMs for different tasks.</p>"},{"location":"packages/instructor/setup/#environment-configuration","title":"Environment Configuration","text":"<p>If .env doesn't exist, copy the environment template:</p> <pre><code>[ ! -f .env ] &amp;&amp; cp vendor/cognesy/instructor-php/config/.env-dist .env\n</code></pre> <p>Add key values to your .env: <pre><code># OpenAI API key\nOPENAI_API_KEY=your_api_key\n# Other API keys (if you use other LLM providers)\n# ...\n\n# Set up Instructor configuration path (optional)\nINSTRUCTOR_CONFIG_PATHS='&lt;path/to/config&gt;,&lt;another/path&gt;'\n</code></pre></p>"},{"location":"packages/instructor/upgrade/","title":"Upgrading Instructor","text":"<p>Recent changes to the Instructor package may require some manual fixes in your codebase.</p>"},{"location":"packages/instructor/upgrade/#step-1-update-the-package","title":"Step 1: Update the package","text":"<p>Run the following command in your CLI:</p> <pre><code>composer update cognesy/instructor\n</code></pre>"},{"location":"packages/instructor/upgrade/#step-2-config-files","title":"Step 2: Config files","text":"<p>Correct your config files to use new namespaces.</p>"},{"location":"packages/instructor/upgrade/#step-3-instructor-config-path","title":"Step 3: Instructor config path","text":"<p>Correct INSTRUCTOR_CONFIG_PATHS in .env file to <code>config/instructor</code> (or your custom path).</p>"},{"location":"packages/instructor/upgrade/#step-4-codebase","title":"Step 4: Codebase","text":"<p>Make sure that your code follows new namespaces.</p> <p>Suggestion: use IDE search and replace to find and replace old namespaces with new ones.</p>"},{"location":"packages/instructor/advanced/function_calls/","title":"Function Calls","text":""},{"location":"packages/instructor/advanced/function_calls/#functioncall-helper-class","title":"FunctionCall helper class","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"packages/instructor/advanced/function_calls/#extracting-arguments-for-a-function","title":"Extracting arguments for a function","text":"<pre><code>&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Save user data to storage */\nfunction saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromFunctionName('saveUser'),\n)-&gt;get();\n\n// call the function with the extracted arguments\nsaveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/function_calls/#extracting-arguments-for-a-method-call","title":"Extracting arguments for a method call","text":"<pre><code>&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass DataStore {\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) {\n        // ...\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromMethodName(Datastore::class, 'saveUser'),\n)-&gt;get();\n\n// call the function with the extracted arguments\n(new DataStore)-&gt;saveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/function_calls/#extracting-arguments-for-a-callable","title":"Extracting arguments for a callable","text":"<pre><code>&lt;?php\nuse Cognesy\\Addons\\FunctionCall\\FunctionCallFactory;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/** Save user data to storage */\n$callable = function saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: FunctionCallFactory::fromCallable($callable),\n)-&gt;get();\n\n// call the function with the extracted arguments\n$callable(...$args);\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/","title":"JSON Extraction Strategies","text":"<p>InstructorPHP uses multiple strategies to extract JSON from LLM responses, handling various edge cases where the LLM might return JSON wrapped in markdown, text, or malformed.</p>"},{"location":"packages/instructor/advanced/json_extraction/#extraction-pipeline","title":"Extraction Pipeline","text":"<p>When processing an LLM response, InstructorPHP tries multiple extraction strategies in order:</p>"},{"location":"packages/instructor/advanced/json_extraction/#1-direct-parsing-try-as-is","title":"1. Direct Parsing (Try As-Is)","text":"<p>Attempts to parse the response directly as JSON:</p> <pre><code>LLM response:\n{\"name\": \"John\", \"age\": 30}\n\n\u2705 Parsed successfully\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#2-markdown-code-block-extraction","title":"2. Markdown Code Block Extraction","text":"<p>Extracts JSON from markdown fenced code blocks:</p> <pre><code>LLM response:\nHere's the data you requested:\n\n```json\n{\"name\": \"John\", \"age\": 30}\n</code></pre> <p>\u2705 Extracts content between <code>json and</code> <pre><code>### 3. Bracket Matching\n\nFinds first `{` and last `}` to extract JSON:\n\n```text\nLLM response:\nThe user data is {\"name\": \"John\", \"age\": 30} as extracted from the text.\n\n\u2705 Extracts from first { to last }\n</code></pre></p>"},{"location":"packages/instructor/advanced/json_extraction/#4-smart-brace-matching","title":"4. Smart Brace Matching","text":"<p>Handles nested braces and escaped quotes:</p> <pre><code>LLM response:\nHere is {\"user\": {\"name\": \"John \\\"The Great\\\"\", \"age\": 30}} extracted.\n\n\u2705 Correctly handles:\n   - Nested braces\n   - Escaped quotes\n   - String boundaries\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#parsing-strategies","title":"Parsing Strategies","text":"<p>After extraction, multiple parsers attempt to handle malformed JSON:</p>"},{"location":"packages/instructor/advanced/json_extraction/#1-standard-json-parser","title":"1. Standard JSON Parser","text":"<p>Native <code>json_decode</code> with strict error handling.</p> <pre><code>json_decode($json, true, 512, JSON_THROW_ON_ERROR)\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#2-resilient-parser","title":"2. Resilient Parser","text":"<p>Applies automatic repairs before parsing:</p> <ul> <li>Balance quotes - Adds missing closing quotes</li> <li>Remove trailing commas - Fixes <code>{\"a\": 1,}</code></li> <li>Balance braces - Adds missing <code>}</code> or <code>]</code></li> </ul> <pre><code>Malformed JSON:\n{\"name\": \"John\", \"age\": 30\n\nResilient parser repairs:\n{\"name\": \"John\", \"age\": 30}  // \u2705 Added missing }\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#3-partial-json-parser","title":"3. Partial JSON Parser","text":"<p>Handles incomplete JSON during streaming:</p> <pre><code>Partial JSON from streaming:\n{\"name\": \"John\", \"age\":\n\n\u2705 Completes to:\n{\"name\": \"John\", \"age\": null}\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#implementation-details","title":"Implementation Details","text":"<p>Location: <code>packages/utils/src/Json/JsonParser.php</code></p> <pre><code>class JsonParser {\n    public function findCompleteJson(string $input): string {\n        $extractors = [\n            fn($text) =&gt; [$text],                          // Direct\n            fn($text) =&gt; $this-&gt;findByMarkdown($text),     // Markdown\n            fn($text) =&gt; [$this-&gt;findByBrackets($text)],   // Brackets\n            fn($text) =&gt; $this-&gt;findJSONLikeStrings($text),// Smart braces\n        ];\n\n        foreach ($extractors as $extractor) {\n            foreach ($extractor($input) as $candidate) {\n                if ($parsed = $this-&gt;tryParse($candidate)) {\n                    return json_encode($parsed);\n                }\n            }\n        }\n\n        return '';\n    }\n\n    private function tryParse(string $maybeJson): mixed {\n        $parsers = [\n            fn($json) =&gt; json_decode($json, true, 512, JSON_THROW_ON_ERROR),\n            fn($json) =&gt; (new ResilientJsonParser($json))-&gt;parse(),\n            fn($json) =&gt; (new PartialJsonParser)-&gt;parse($json),\n        ];\n        // ... try each parser\n    }\n}\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#why-this-matters","title":"Why This Matters","text":"<p>LLMs don't always return clean JSON:</p> <ul> <li>Claude sometimes wraps in markdown</li> <li>GPT-4 may add explanations</li> <li>Gemini might include partial responses during streaming</li> <li>Custom prompts can lead to unexpected formats</li> </ul> <p>InstructorPHP's multi-strategy approach ensures maximum compatibility.</p>"},{"location":"packages/instructor/advanced/json_extraction/#common-scenarios","title":"Common Scenarios","text":""},{"location":"packages/instructor/advanced/json_extraction/#scenario-1-llm-adds-explanation","title":"Scenario 1: LLM Adds Explanation","text":"<pre><code>LLM response:\nBased on the text, I extracted the following information:\n\n{\"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\"}\n\nThis represents the user data found in the document.\n</code></pre> <p>\u2705 Strategy 3 (Bracket Matching) extracts the JSON successfully</p>"},{"location":"packages/instructor/advanced/json_extraction/#scenario-2-markdown-wrapped-response","title":"Scenario 2: Markdown Wrapped Response","text":"<pre><code>LLM response:\nSure! Here's the structured data:\n\n```json\n{\n  \"name\": \"Jane Smith\",\n  \"age\": 25\n}\n</code></pre> <p>I've extracted the user information as requested. <pre><code>\u2705 **Strategy 2 (Markdown Extraction)** handles this case\n\n### Scenario 3: Malformed JSON\n\n```text\nLLM response:\n{\"name\": \"Bob\", \"age\": 35, \"active\": true,}\n</code></pre></p> <p>\u2705 Resilient Parser removes the trailing comma and parses successfully</p>"},{"location":"packages/instructor/advanced/json_extraction/#scenario-4-streaming-partial-response","title":"Scenario 4: Streaming Partial Response","text":"<pre><code>Streaming chunk:\n{\"name\": \"Alice\", \"email\": \"alice@\n</code></pre> <p>\u2705 Partial Parser completes to: <pre><code>{\"name\": \"Alice\", \"email\": \"alice@\"}\n</code></pre></p>"},{"location":"packages/instructor/advanced/json_extraction/#error-handling","title":"Error Handling","text":"<p>If all strategies fail, InstructorPHP:</p> <ol> <li>Returns an empty string from <code>findCompleteJson()</code></li> <li>Triggers a validation error</li> <li>Initiates retry mechanism (if configured)</li> <li>Provides error feedback to LLM for self-correction</li> </ol>"},{"location":"packages/instructor/advanced/json_extraction/#performance-considerations","title":"Performance Considerations","text":"<p>Extraction overhead: - Direct parsing: ~0.1ms - Markdown extraction: ~0.5ms (regex) - Bracket matching: ~0.2ms (string ops) - Smart brace matching: ~1-2ms (character iteration)</p> <p>Most responses succeed on first strategy (direct parsing).</p>"},{"location":"packages/instructor/advanced/json_extraction/#custom-content-extractors","title":"Custom Content Extractors","text":"<p>You can add custom extractors to handle non-standard response formats:</p> <pre><code>use Cognesy\\Instructor\\Extraction\\Contracts\\CanExtractResponse;\nuse Cognesy\\Instructor\\Extraction\\Data\\ExtractionInput;\nuse Cognesy\\Instructor\\Extraction\\Exceptions\\ExtractionException;\n\nclass XmlCdataExtractor implements CanExtractResponse\n{\n    public function extract(ExtractionInput $input): array\n    {\n        if (preg_match('/&lt;!\\[CDATA\\[(.*?)\\]\\]&gt;/s', $input-&gt;content, $matches)) {\n            $json = trim($matches[1]);\n            try {\n                $decoded = json_decode($json, associative: true, flags: JSON_THROW_ON_ERROR);\n            } catch (\\JsonException $e) {\n                throw new ExtractionException('Invalid JSON in CDATA', $e);\n            }\n\n            if (!is_array($decoded)) {\n                throw new ExtractionException('Expected object or array in CDATA');\n            }\n\n            return $decoded;\n        }\n        throw new ExtractionException('No CDATA found');\n    }\n\n    public function name(): string\n    {\n        return 'xml_cdata';\n    }\n}\n</code></pre>"},{"location":"packages/instructor/advanced/json_extraction/#using-custom-extractors","title":"Using Custom Extractors","text":"<p>Custom extractors work for both sync and streaming responses:</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extraction\\Extractors\\DirectJsonExtractor;\n\n$result = (new StructuredOutput)\n    -&gt;withExtractors(\n        new DirectJsonExtractor(),\n        new XmlCdataExtractor(),\n    )\n    -&gt;withResponseClass(User::class)\n    -&gt;with(messages: 'Extract user')\n    -&gt;get();\n</code></pre> <p>The same extractors are automatically used for streaming:</p> <pre><code>$stream = (new StructuredOutput)\n    -&gt;withExtractors(\n        new DirectJsonExtractor(),\n        new XmlCdataExtractor(),\n    )\n    -&gt;withResponseClass(User::class)\n    -&gt;with(messages: 'Extract user')\n    -&gt;stream();\n</code></pre> <p>See: Output Formats - Pluggable Extraction for comprehensive documentation and examples.</p>"},{"location":"packages/instructor/advanced/json_extraction/#related-documentation","title":"Related Documentation","text":"<ul> <li>Response Models - How schemas are processed</li> <li>Validation - What happens after extraction</li> <li>Retry Mechanisms - Error handling and retries</li> </ul>"},{"location":"packages/instructor/advanced/manual_schemas/","title":"Manual Schema Building","text":"<p>While InstructorPHP can automatically generate schemas from PHP classes via reflection, you can also build schemas manually using the <code>JsonSchema</code> API.</p>"},{"location":"packages/instructor/advanced/manual_schemas/#when-to-use-manual-schemas","title":"When to Use Manual Schemas","text":"<ul> <li>Fine-grained control over exact JSON Schema output</li> <li>Dynamic schemas where structure is determined at runtime</li> <li>Provider optimization when you need to tweak schemas for specific LLMs</li> <li>Legacy integration when working with existing JSON Schema specifications</li> <li>Performance when reflection overhead is a concern</li> </ul>"},{"location":"packages/instructor/advanced/manual_schemas/#available-builder-methods","title":"Available Builder Methods","text":""},{"location":"packages/instructor/advanced/manual_schemas/#object-schemas","title":"Object Schemas","text":"<pre><code>use Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$schema = JsonSchema::object(\n    name: 'User',\n    description: 'User data',\n    properties: [\n        JsonSchema::string(name: 'name', description: 'User name'),\n        JsonSchema::integer(name: 'age', description: 'User age'),\n        JsonSchema::boolean(name: 'active', description: 'Is active'),\n    ],\n    requiredProperties: ['name', 'age'],\n    additionalProperties: false,\n);\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#primitive-schemas","title":"Primitive Schemas","text":"<pre><code>// String\nJsonSchema::string(\n    name: 'email',\n    description: 'Email address',\n);\n\n// Integer\nJsonSchema::integer(\n    name: 'count',\n    description: 'Number of items',\n);\n\n// Number (float)\nJsonSchema::number(\n    name: 'price',\n    description: 'Product price',\n);\n\n// Boolean\nJsonSchema::boolean(\n    name: 'verified',\n    description: 'Is verified',\n);\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#array-schemas","title":"Array Schemas","text":"<pre><code>// Array with item schema\nJsonSchema::array(\n    name: 'tags',\n    itemSchema: JsonSchema::string(),\n    description: 'List of tags',\n);\n\n// Collection (alias for array)\nJsonSchema::collection(\n    name: 'users',\n    itemSchema: JsonSchema::object(...),\n    description: 'List of users',\n);\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#enum-schemas","title":"Enum Schemas","text":"<pre><code>JsonSchema::enum(\n    name: 'status',\n    enumValues: ['pending', 'active', 'completed'],\n    description: 'Order status',\n);\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#from-array","title":"From Array","text":"<pre><code>$schema = JsonSchema::fromArray([\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'name' =&gt; ['type' =&gt; 'string'],\n        'age' =&gt; ['type' =&gt; 'integer'],\n    ],\n    'required' =&gt; ['name'],\n]);\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#fluent-builder-pattern","title":"Fluent Builder Pattern","text":"<pre><code>$schema = JsonSchema::object('User')\n    -&gt;withProperty(JsonSchema::string('name'))\n    -&gt;withProperty(JsonSchema::integer('age'))\n    -&gt;withRequired(['name']);\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#using-manual-schemas-with-structuredoutput","title":"Using Manual Schemas with StructuredOutput","text":"<pre><code>use Cognesy\\Instructor\\StructuredOutput;\n\n// Build schema manually\n$userSchema = JsonSchema::object(\n    name: 'User',\n    properties: [\n        JsonSchema::string(name: 'name'),\n        JsonSchema::integer(name: 'age'),\n    ],\n    requiredProperties: ['name'],\n);\n\n// Use with StructuredOutput\n$user = StructuredOutput::create()\n    -&gt;with(\n        messages: 'Extract user: John Doe, 30 years old',\n        responseModel: $userSchema,\n    )\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#comparison-reflection-vs-manual","title":"Comparison: Reflection vs Manual","text":""},{"location":"packages/instructor/advanced/manual_schemas/#reflection-automatic","title":"Reflection (Automatic)","text":"<p>Pros: - \u2705 Concise - just use class name - \u2705 Single source of truth - \u2705 IDE support for refactoring - \u2705 Type-safe</p> <p>Cons: - \u274c Less control over schema details - \u274c Performance overhead (reflection)</p> <pre><code>class User {\n    public function __construct(\n        public string $name,\n        public int $age,\n    ) {}\n}\n\n$user = StructuredOutput::create()\n    -&gt;with(responseModel: User::class, ...)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#manual-explicit","title":"Manual (Explicit)","text":"<p>Pros: - \u2705 Full control over schema - \u2705 No reflection overhead - \u2705 Can optimize for specific providers - \u2705 Runtime schema generation</p> <p>Cons: - \u274c More verbose - \u274c Duplication with class definition - \u274c Manual maintenance</p> <pre><code>$schema = JsonSchema::object('User', [\n    JsonSchema::string('name'),\n    JsonSchema::integer('age'),\n], ['name', 'age']);\n\n$user = StructuredOutput::create()\n    -&gt;with(responseModel: $schema, ...)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#complex-example","title":"Complex Example","text":"<pre><code>$orderSchema = JsonSchema::object(\n    name: 'Order',\n    description: 'Customer order',\n    properties: [\n        JsonSchema::string(\n            name: 'orderId',\n            description: 'Unique order identifier'\n        ),\n        JsonSchema::object(\n            name: 'customer',\n            description: 'Customer information',\n            properties: [\n                JsonSchema::string(name: 'name'),\n                JsonSchema::string(name: 'email'),\n            ],\n            requiredProperties: ['name', 'email']\n        ),\n        JsonSchema::collection(\n            name: 'items',\n            description: 'Order line items',\n            itemSchema: JsonSchema::object(\n                name: 'LineItem',\n                properties: [\n                    JsonSchema::string(name: 'product'),\n                    JsonSchema::integer(name: 'quantity'),\n                    JsonSchema::number(name: 'price'),\n                ],\n                requiredProperties: ['product', 'quantity', 'price']\n            )\n        ),\n        JsonSchema::enum(\n            name: 'status',\n            enumValues: ['pending', 'shipped', 'delivered'],\n            description: 'Order status'\n        ),\n    ],\n    requiredProperties: ['orderId', 'customer', 'items', 'status']\n);\n\n$order = StructuredOutput::create()\n    -&gt;with(\n        messages: 'Extract order details from: ...',\n        responseModel: $orderSchema,\n    )\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/manual_schemas/#best-practices","title":"Best Practices","text":"<ol> <li>Use meaningful descriptions - LLMs use them to understand context</li> <li>Mark required fields explicitly - Don't rely on defaults</li> <li>Keep schemas DRY - Extract common sub-schemas to variables</li> <li>Validate generated JSON Schema - Use <code>$schema-&gt;toJsonSchema()</code> to inspect</li> </ol>"},{"location":"packages/instructor/advanced/manual_schemas/#examples","title":"Examples","text":"<p>See: <code>examples/A02_Advanced/ManualSchemas/run.php</code></p>"},{"location":"packages/instructor/advanced/model_options/","title":"Model Options","text":""},{"location":"packages/instructor/advanced/model_options/#changing-llm-model-and-options","title":"Changing LLM model and options","text":"<p>You can specify model and other options that will be passed to LLM endpoint.</p> <p>Commonly used option supported by many providers is <code>temperature</code>, which controls randomness of the output.</p> <p>Lower values make the output more deterministic, while higher values make it more random.</p> <pre><code>&lt;?php\n$person = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: [\n        // custom temperature setting\n        'temperature' =&gt; 0.0\n        // ... other options - e.g. provider or model specific\n    ],\n)-&gt;get();\n</code></pre> <p>NOTE: Please note that many options might be specific to the provider or even some model that you are using.</p>"},{"location":"packages/instructor/advanced/model_options/#customizing-configuration","title":"Customizing configuration","text":"<p>You can pass a custom LLM configuration to the Instructor.</p> <p>This allows you to specify your own API key, base URI or, which might be helpful in the case you are using OpenAI - organization.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;use Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\n\n// Create instance of OpenAI client initialized with custom parameters\n$config = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: $yourApiKey,\n    endpoint: '/chat/completions',\n    metadata: ['organization' =&gt; ''],\n    model: 'gpt-4o-mini',\n    maxTokens: 128,\n    // configure HTTP via HttpClientBuilder or facade-level methods\n    driver: 'openai',\n));\n\n/// Get Instructor with the default configuration overridden with your own\n$structuredOutput = (new StructuredOutput)-&gt;withLLMConfig($driver);\n\n$person = $structuredOutput-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    options: ['temperature' =&gt; 0.0],\n)-&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/","title":"Output Formats","text":"<p>By default, InstructorPHP deserializes LLM responses into PHP objects based on your response model class. The OutputFormat API allows you to change this behavior while keeping the same schema definition.</p>"},{"location":"packages/instructor/advanced/output_formats/#overview","title":"Overview","text":"<p>The OutputFormat API decouples schema specification (what structure the LLM should produce) from output format (how you receive the result).</p> <pre><code>// Schema from User class, output as object (default)\n$user = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n// Returns: User object\n\n// Schema from User class, output as array\n$data = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;intoArray()\n    -&gt;get();\n// Returns: ['name' =&gt; 'John', 'age' =&gt; 30]\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#available-output-formats","title":"Available Output Formats","text":""},{"location":"packages/instructor/advanced/output_formats/#1-intoarray-raw-associative-arrays","title":"1. intoArray() - Raw Associative Arrays","text":"<p>Returns extracted data as a plain associative array instead of an object.</p> <pre><code>$data = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)  // Schema definition\n    -&gt;intoArray()                      // Output format\n    -&gt;with(messages: 'Extract: John Doe, 30 years old')\n    -&gt;get();\n\n// Result: ['name' =&gt; 'John Doe', 'age' =&gt; 30]\ndump($data['name']);  // 'John Doe'\n</code></pre> <p>Use cases: - Database storage - Direct array insertion without conversion - JSON APIs - Return arrays for <code>json_encode()</code> - Array manipulation - Easier to modify arrays than objects - Debugging - Arrays are simpler to inspect with <code>dump()</code> - Legacy integration - When existing code expects arrays</p> <p>Example: <pre><code>class User {\n    public function __construct(\n        public string $name,\n        public int $age,\n        public string $email,\n    ) {}\n}\n\n$userData = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;intoArray()\n    -&gt;with(messages: 'Extract: Jane Smith, 25, jane@example.com')\n    -&gt;get();\n\n// Store directly in database\nDB::table('users')-&gt;insert($userData);\n\n// Or return as JSON API response\nreturn response()-&gt;json($userData);\n</code></pre></p>"},{"location":"packages/instructor/advanced/output_formats/#2-intoinstanceof-different-output-class","title":"2. intoInstanceOf() - Different Output Class","text":"<p>Uses one class for schema definition and a different class for the output object.</p> <pre><code>$dto = (new StructuredOutput)\n    -&gt;withResponseClass(UserProfile::class)  // Rich schema (5 fields)\n    -&gt;intoInstanceOf(UserDTO::class)         // Simple output (2 fields)\n    -&gt;with(messages: 'Extract user data')\n    -&gt;get();\n\n// Result: UserDTO instance with subset of fields\n</code></pre> <p>Use cases: - Separate API contracts from internal models - Public schema vs internal DTO - Simplify complex models - Extract rich data, return simple DTO - Different validation rules - Schema validation vs output validation - Decouple layers - Domain model for LLM, presentation model for API</p> <p>Example: <pre><code>// Rich schema sent to LLM (all user profile data)\nclass UserProfile {\n    public string $fullName;\n    public int $age;\n    public string $email;\n    public string $phoneNumber;\n    public string $address;\n}\n\n// Simplified DTO for your application (only essential fields)\nclass UserDTO {\n    public function __construct(\n        public string $fullName = '',\n        public string $email = '',\n    ) {}\n}\n\n$user = (new StructuredOutput)\n    -&gt;withResponseClass(UserProfile::class)  // LLM sees all 5 fields\n    -&gt;intoInstanceOf(UserDTO::class)         // You get 2 fields\n    -&gt;with(\n        messages: \"Extract: John Smith, 30, john@example.com, 555-1234, 123 Main St\"\n    )\n    -&gt;get();\n\n// $user is UserDTO with only fullName and email\necho $user-&gt;fullName;  // 'John Smith'\necho $user-&gt;email;     // 'john@example.com'\n// phoneNumber and address were extracted but not included in output\n</code></pre></p>"},{"location":"packages/instructor/advanced/output_formats/#3-intoobject-self-deserializing-objects","title":"3. intoObject() - Self-Deserializing Objects","text":"<p>Provides a custom object that controls its own deserialization from the extracted array.</p> <pre><code>$scalar = (new StructuredOutput)\n    -&gt;withResponseClass(Rating::class)\n    -&gt;intoObject(new Scalar('rating', 'integer'))\n    -&gt;with(messages: 'Extract rating: 5 stars')\n    -&gt;get();\n\n// Result: Scalar object with custom deserialization logic\n</code></pre> <p>Use cases: - Scalar values - Extract single values wrapped in objects - Custom deserialization - Full control over how data becomes objects - Value objects - Domain-driven design value objects - Complex transformations - When standard deserialization isn't enough</p> <p>Example with Scalar: <pre><code>use Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\n\n// Extract a single integer value\n$rating = (new StructuredOutput)\n    -&gt;withResponseClass(Rating::class)\n    -&gt;intoObject(new Scalar('rating', 'integer'))\n    -&gt;with(messages: 'Extract rating from: \"5 out of 5 stars\"')\n    -&gt;get();\n\ndump($rating);  // 5 (integer)\n\n// Extract a single string value\n$sentiment = (new StructuredOutput)\n    -&gt;withResponseClass(Sentiment::class)\n    -&gt;intoObject(new Scalar('sentiment', 'string'))\n    -&gt;with(messages: 'Analyze sentiment: \"This product is amazing!\"')\n    -&gt;get();\n\ndump($sentiment);  // 'positive' (string)\n</code></pre></p> <p>Custom self-deserializing object: <pre><code>use Cognesy\\Instructor\\Deserialization\\Contracts\\CanDeserializeSelfFromArray;\n\nclass Money implements CanDeserializeSelfFromArray\n{\n    public function __construct(\n        private int $amountInCents,\n        private string $currency,\n    ) {}\n\n    public static function fromArray(array $data): self {\n        // Custom deserialization logic\n        $amount = $data['amount'] ?? 0;\n        $currency = $data['currency'] ?? 'USD';\n\n        return new self(\n            amountInCents: (int)($amount * 100),  // Convert to cents\n            currency: strtoupper($currency),       // Normalize currency\n        );\n    }\n\n    public function toArray(): array {\n        return [\n            'amount' =&gt; $this-&gt;amountInCents / 100,\n            'currency' =&gt; $this-&gt;currency,\n        ];\n    }\n}\n\n$price = (new StructuredOutput)\n    -&gt;withResponseClass(Product::class)\n    -&gt;intoObject(new Money(0, 'USD'))\n    -&gt;with(messages: 'Extract price: $19.99 USD')\n    -&gt;get();\n\n// $price is Money instance with custom deserialization\n</code></pre></p>"},{"location":"packages/instructor/advanced/output_formats/#streaming-with-output-formats","title":"Streaming with Output Formats","text":"<p>Output formats work seamlessly with streaming responses.</p> <p>Key behavior: - During streaming: Partial updates are always objects (for validation and deduplication) - Final result: Respects the output format you specified</p> <pre><code>$stream = (new StructuredOutput)\n    -&gt;withResponseClass(Article::class)\n    -&gt;intoArray()  // Final result will be array\n    -&gt;with(messages: 'Extract article data')\n    -&gt;stream();\n\n// Iterate over partial objects\nforeach ($stream-&gt;partials() as $partial) {\n    // $partial is Article object during streaming\n    echo \"Progress: \" . strlen($partial-&gt;content) . \" characters\\n\";\n}\n\n// Get final result as array\n$finalArticle = $stream-&gt;finalValue();\n// Returns: ['title' =&gt; '...', 'author' =&gt; '...', 'content' =&gt; '...']\n</code></pre> <p>Why this design? - Objects during streaming enable validation and deduplication - Array for final result provides convenience for your application - Best of both worlds: safety during processing, flexibility for results</p>"},{"location":"packages/instructor/advanced/output_formats/#comparison-matrix","title":"Comparison Matrix","text":"Feature Default (Object) intoArray() intoInstanceOf() intoObject() Output type Schema class Array Target class Custom object Validation \u2705 Yes \u274c Skipped \u2705 Yes Custom Transformation \u2705 Yes \u274c Skipped \u2705 Yes Custom Use case Standard Database/API DTOs/Decoupling Value objects Streaming partials Object Object Object Object Streaming final Object Array Target class Custom object"},{"location":"packages/instructor/advanced/output_formats/#common-patterns","title":"Common Patterns","text":""},{"location":"packages/instructor/advanced/output_formats/#pattern-1-conditional-deserialization","title":"Pattern 1: Conditional Deserialization","text":"<p>Inspect data before creating objects:</p> <pre><code>$data = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;intoArray()\n    -&gt;with(messages: 'Extract user')\n    -&gt;get();\n\n// Choose class based on data\nif ($data['age'] &lt; 18) {\n    $user = new MinorUser(...$data);\n} else {\n    $user = new AdultUser(...$data);\n}\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#pattern-2-data-enrichment","title":"Pattern 2: Data Enrichment","text":"<p>Add computed fields to arrays:</p> <pre><code>$data = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;intoArray()\n    -&gt;with(messages: 'Extract person')\n    -&gt;get();\n\n// Add computed field\n$data['full_name'] = $data['first_name'] . ' ' . $data['last_name'];\n$data['age_group'] = $data['age'] &lt; 30 ? 'young' : 'senior';\n\n// Then create object\n$person = new Person(...$data);\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#pattern-3-multi-layer-architecture","title":"Pattern 3: Multi-Layer Architecture","text":"<p>Separate schema from application layer:</p> <pre><code>// Domain layer - rich schema for LLM\nclass OrderDomain {\n    public string $orderId;\n    public CustomerInfo $customer;\n    public array $items;\n    public PaymentDetails $payment;\n    public ShippingInfo $shipping;\n}\n\n// Application layer - simplified DTO\nclass OrderDTO {\n    public function __construct(\n        public string $orderId,\n        public string $customerName,\n        public float $total,\n    ) {}\n}\n\n$order = (new StructuredOutput)\n    -&gt;withResponseClass(OrderDomain::class)  // Rich domain model\n    -&gt;intoInstanceOf(OrderDTO::class)        // Simple application DTO\n    -&gt;with(messages: 'Extract order details')\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#important-notes","title":"Important Notes","text":""},{"location":"packages/instructor/advanced/output_formats/#schema-is-always-respected","title":"Schema is Always Respected","text":"<p>The output format only changes how data is returned to you. The LLM always receives the full schema:</p> <pre><code>$data = (new StructuredOutput)\n    -&gt;withResponseClass(UserProfile::class)  // LLM sees all 5 fields\n    -&gt;intoArray()                             // You get array, but...\n    -&gt;get();\n\n// Schema sent to LLM still includes all 5 fields from UserProfile\n// Only the deserialization step is different\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#validation-behavior","title":"Validation Behavior","text":"<ul> <li>intoArray(): Validation is skipped (arrays can't be validated like objects)</li> <li>intoInstanceOf(): Validation runs on the target class</li> <li>intoObject(): Validation is custom (depends on implementation)</li> </ul>"},{"location":"packages/instructor/advanced/output_formats/#backward-compatibility","title":"Backward Compatibility","text":"<p>Default behavior is unchanged. Output formats are opt-in:</p> <pre><code>// still works\n$user = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;get();\n// Returns: User object (default behavior)\n\n// new capability\n$data = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)\n    -&gt;intoArray()  // \u2190 New\n    -&gt;get();\n// Returns: array\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#examples","title":"Examples","text":"<p>See working examples in: - <code>examples/A05_Extras/OutputFormatArray/run.php</code> - Basic <code>intoArray()</code> usage - <code>examples/A05_Extras/OutputFormatInstanceOf/run.php</code> - Different output class - <code>examples/A05_Extras/OutputFormatStreaming/run.php</code> - Streaming with arrays</p>"},{"location":"packages/instructor/advanced/output_formats/#pluggable-extraction","title":"Pluggable Extraction","text":"<p>InstructorPHP uses a pluggable extraction pipeline to convert raw LLM responses into canonical arrays. You can customize this pipeline for special formats or implement custom extraction logic.</p>"},{"location":"packages/instructor/advanced/output_formats/#default-content-extractors","title":"Default Content Extractors","text":"<p>The default <code>ResponseExtractor</code> uses an extractor chain (tried in order):</p> Extractor Description <code>DirectJsonExtractor</code> Parse content directly as JSON <code>ResilientJsonExtractor</code> Handle malformed JSON (trailing commas, etc.) <code>MarkdownBlockExtractor</code> Extract from <code>```json ```</code> blocks <code>BracketMatchingExtractor</code> Find first <code>{</code> to last <code>}</code> <code>SmartBraceExtractor</code> Handle escaped quotes in strings"},{"location":"packages/instructor/advanced/output_formats/#custom-extractors","title":"Custom Extractors","text":"<p>Replace the default extractors with your own:</p> <pre><code>use Cognesy\\Instructor\\Extraction\\Extractors\\DirectJsonExtractor;\nuse Cognesy\\Instructor\\Extraction\\Extractors\\MarkdownBlockExtractor;\n\n$result = (new StructuredOutput)\n    -&gt;withExtractors(\n        new DirectJsonExtractor(),       // Only these extractors\n        new MarkdownBlockExtractor(),\n    )\n    -&gt;withResponseClass(User::class)\n    -&gt;with(messages: 'Extract user')\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#custom-extractor-implementation","title":"Custom Extractor Implementation","text":"<p>Create your own extractor for special formats:</p> <pre><code>use Cognesy\\Instructor\\Extraction\\Contracts\\CanExtractResponse;\nuse Cognesy\\Instructor\\Extraction\\Data\\ExtractionInput;\nuse Cognesy\\Instructor\\Extraction\\Exceptions\\ExtractionException;\n\nclass XmlJsonExtractor implements CanExtractResponse\n{\n    public function extract(ExtractionInput $input): array\n    {\n        // Extract JSON from &lt;data&gt;...&lt;/data&gt; tags\n        if (!preg_match('/&lt;data&gt;(.*?)&lt;\\/data&gt;/s', $input-&gt;content, $matches)) {\n            throw new ExtractionException('No data tags found');\n        }\n\n        $json = trim($matches[1]);\n        try {\n            $decoded = json_decode($json, associative: true, flags: JSON_THROW_ON_ERROR);\n        } catch (\\JsonException $e) {\n            throw new ExtractionException('Invalid JSON in data tags', $e);\n        }\n\n        if (!is_array($decoded)) {\n            throw new ExtractionException('Expected object or array in data tags');\n        }\n\n        return $decoded;\n    }\n\n    public function name(): string\n    {\n        return 'xml_json';\n    }\n}\n\n// Use custom extractor\n$result = (new StructuredOutput)\n    -&gt;withExtractors(\n        new XmlJsonExtractor(),\n        new DirectJsonExtractor(), // Fallback\n    )\n    -&gt;withResponseClass(User::class)\n    -&gt;with(messages: 'Extract user')\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#custom-response-extractor","title":"Custom Response Extractor","text":"<p>For complete control over the extraction pipeline, implement <code>CanExtractResponse</code>:</p> <pre><code>use Cognesy\\Instructor\\Extraction\\Contracts\\CanExtractResponse;\nuse Cognesy\\Instructor\\Extraction\\Data\\ExtractionInput;\nuse Cognesy\\Instructor\\Extraction\\Exceptions\\ExtractionException;\n\nclass CustomExtractor implements CanExtractResponse\n{\n    public function extract(ExtractionInput $input): array\n    {\n        $content = $input-&gt;content;\n\n        // Custom extraction logic\n        $data = $this-&gt;parseCustomFormat($content);\n\n        return $data;\n    }\n\n    private function parseCustomFormat(string $content): array\n    {\n        if ($content === '') {\n            throw new ExtractionException('Empty content');\n        }\n\n        // Your custom parsing logic\n        return ['parsed' =&gt; 'data'];\n    }\n}\n\n// Use custom extractor\n$result = (new StructuredOutput)\n    -&gt;withExtractor(new CustomExtractor())\n    -&gt;withResponseClass(User::class)\n    -&gt;with(messages: 'Extract user')\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/output_formats/#when-to-customize-extraction","title":"When to Customize Extraction","text":"<p>Use <code>withExtractors()</code> when: - You want to optimize for known response formats - You need to add support for additional formats - You want to change the extractor order - Custom extractors are automatically used for both sync and streaming</p> <p>Use <code>withExtractor()</code> when: - You need completely custom extraction logic - You're integrating with a non-standard LLM response format - You want to bypass the extractor chain entirely</p>"},{"location":"packages/instructor/advanced/output_formats/#related-documentation","title":"Related Documentation","text":"<ul> <li>Response Models - How schemas work</li> <li>Structures - Dynamic data models</li> <li>Validation - How validation works</li> <li>Streaming - Streaming responses</li> </ul>"},{"location":"packages/instructor/advanced/partials/","title":"Partials","text":""},{"location":"packages/instructor/advanced/partials/#partial-updates","title":"Partial updates","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated.</p> <p>You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p>This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive partial results define <code>onPartialUpdate()</code> callback that will be called on every update of the deserializad object.</p> <p>Instructor is smart about updates, it calculates and compares hashes of the previous and newly deserialized version of the model, so you won't get them on every token received, but only when any property of the object is updated.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nfunction updateUI($person) {\n    // Here you get partially completed Person object update UI with the partial result\n}\n\n$person = (new StructuredOutput)\n    -&gt;withResponseClass(Person::class)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        options: ['stream' =&gt; true]\n    )\n    -&gt;onPartialUpdate(\n        fn($partial) =&gt; updateUI($partial)\n    )\n    -&gt;get();\n\n// Here you get completed and validated Person object\n$this-&gt;db-&gt;save($person); // ...for example: save to DB\n</code></pre> <p>Partially updated data is not validated while it is received and deserialized.</p> <p>The object returned from <code>get()</code> call is fully validated, so you can safely work with it, e.g. save it to the database.</p>"},{"location":"packages/instructor/advanced/partials/#streaming-responses","title":"Streaming responses","text":"<p>You can get a stream of responses by calling the <code>stream()</code> method instead of <code>get()</code>. The <code>stream()</code> method is available on both <code>StructuredOutput</code> and <code>PendingStructuredOutput</code> instances.</p> <pre><code>// Direct streaming\n$stream = $structuredOutput-&gt;stream();\n\n// Or via create() method\n$pending = $structuredOutput-&gt;create();\n$stream = $pending-&gt;stream();\n</code></pre> <p>Both approaches return a <code>StructuredOutputStream</code> object, which gives you access to the response streamed from LLM and processed by Instructor into structured data.</p>"},{"location":"packages/instructor/advanced/partials/#structuredoutputstream-methods","title":"StructuredOutputStream Methods","text":"<p>The <code>StructuredOutputStream</code> class provides comprehensive methods for processing streaming responses:</p>"},{"location":"packages/instructor/advanced/partials/#core-streaming-methods","title":"Core Streaming Methods","text":"<ul> <li><code>partials()</code>: Returns an iterable of partial updates from the stream. Only final update is validated, partial updates are only deserialized and transformed.</li> <li><code>sequence()</code>: Dedicated to processing <code>Sequence</code> response models - returns only completed items in the sequence.</li> <li><code>responses()</code>: Generator of partial LLM responses as they are received.</li> </ul>"},{"location":"packages/instructor/advanced/partials/#result-access-methods","title":"Result Access Methods","text":"<ul> <li><code>finalValue()</code>: Get the final parsed result (blocks until completion).</li> <li><code>finalResponse()</code>: Get the final LLM response (blocks until completion).</li> <li><code>lastUpdate()</code>: Returns the last object received and processed by Instructor.</li> <li><code>lastResponse()</code>: Returns the last received LLM response.</li> </ul>"},{"location":"packages/instructor/advanced/partials/#utility-methods","title":"Utility Methods","text":"<ul> <li><code>usage()</code>: Get token usage statistics from the streaming response.</li> </ul>"},{"location":"packages/instructor/advanced/partials/#example-streaming-partial-responses","title":"Example: streaming partial responses","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $update) {\n    // render updated person view\n    // for example:\n    $view-&gt;updateView($update); // render the updated person view\n}\n\n// now you can get final, fully processed person object\n$person = $stream-&gt;finalValue();\n// ...and for example save it to the database\n$db-&gt;savePerson($person);\n</code></pre>"},{"location":"packages/instructor/advanced/partials/#example-streaming-sequence-items","title":"Example: streaming sequence items","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)\n    -&gt;with(\n        messages: \"Jason is 28 years old, Amanda is 26 and John (CEO) is 40.\",\n        responseModel: Sequence::of(Participant::class),\n    )\n    -&gt;stream();\n\nforeach ($stream-&gt;sequence() as $update) {\n    // append last completed item from the sequence\n    // for example:\n    $view-&gt;appendParticipant($update-&gt;last());\n}\n\n// now you can get final, fully processed sequence of participants\n$participants = $stream-&gt;finalValue();\n// ...and for example save it to the database\n$db-&gt;saveParticipants($participants-&gt;toArray());\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/","title":"Prompt Templates","text":""},{"location":"packages/instructor/advanced/prompts/#overview","title":"Overview","text":"<p>As your applications grow in complexity, your prompts may become large and complex, with multiple variables and metadata. Managing these prompts can become a challenge, especially when you need to reuse them across different parts of your application. Large prompts are also hard to maintain if they are part of your codebase.</p> <p><code>Prompt</code> addon provides a powerful and flexible way to manage your prompts. It supports multiple template engines (Twig, Blade), prompt metadata, variable injection, and validation.</p>"},{"location":"packages/instructor/advanced/prompts/#what-are-prompts","title":"What are Prompts","text":"<p>Prompts in Instructor are based on structured text templates that can be rendered to text or series of chat messages. As many of your prompts will be dynamically generated based on input data, you can use syntax of one of the supported template engines (Twig, Blade) to define your prompts.</p> <p>This document will be using Twig syntax for prompt templates for simplicity and consistency, but you can use Blade syntax in your prompts if you prefer it.</p> <ul> <li>For more information on Twig syntax see Twig documentation.</li> <li>For more information on Blade syntax see Blade documentation.</li> </ul>"},{"location":"packages/instructor/advanced/prompts/#basic-prompt-template","title":"Basic Prompt Template","text":"<p>Example prompt template in Twig: <pre><code>Hello, world.\n</code></pre></p>"},{"location":"packages/instructor/advanced/prompts/#prompt-template-with-variables","title":"Prompt Template with Variables","text":"<p>You can define variables in your prompt templates and inject values when rendering the prompt.</p> <pre><code>Hello, {{ name }}!\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#chat-messages","title":"Chat Messages","text":"<p>You can define chat messages in your prompts, which can be used to generate a sequence of messages for LLM chat APIs.</p> <pre><code>&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;What is the capital of {{ country }}?&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#prompt-template-metadata","title":"Prompt Template Metadata","text":"<p>We recommend to preface each prompt with front matter - a block of metadata that describes the prompt and its variables. This metadata can be used for validation, documentation, and schema generation.</p> <pre><code>{#---\ndescription: Capital finder template\nvariables:\n    country:\n        description: Country name\n        type: string\n        default: France\nschema:\n    name: capital\n    properties:\n        name:\n            description: Capital city name\n            type: string\n    required: [name]\n---#}\n&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;What is the capital of {{ country }}?&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#template-libraries","title":"Template Libraries","text":"<p>Instructor allows you to define multiple <code>template libraries</code> in your app. Library is just a collection of prompt templates which is stored under a specific directory. Library can have a nested structure, which allows you to organize your prompts in a way that makes sense for your application.</p> <p>Library properties are specified in <code>config/prompt.php</code> configuration file.</p> <p>where you can define:  - <code>templateEngine</code> - template engine used for prompts in this library,  - <code>resourcePath</code> - path to prompt templates,  - <code>cachePath</code> - path to compiled templates,  - <code>extension</code> - file extension for prompt templates,  - <code>frontMatterTags</code> - start and end tags for front matter,  - <code>frontMatterFormat</code> - format of front matter (yaml, json, toml),  - <code>metadata</code> - engine-specific configuration.</p> <p>Instructor comes with 3 default prompt libraries:  - <code>system</code> - prompt templates used by Instructor itself,  - <code>demo-twig</code> - demo prompt templates using Twig template engine,  - <code>demo-blade</code> - demo prompt templates using Blade template engine.</p> <p>Instructor's does not specify how you should organize or manage your prompt templates, but it provides a flexible way to do it in a way that suits your application.</p>"},{"location":"packages/instructor/advanced/prompts/#using-prompt-templates","title":"Using Prompt Templates","text":""},{"location":"packages/instructor/advanced/prompts/#rendering-a-simple-prompt","title":"Rendering a Simple Prompt","text":"<p>To get started, you can create and render a simple prompt defined in the bundled library using the <code>Prompt::using</code> or <code>Prompt::make</code> methods. Here's how you can use them:</p> <p><pre><code>&lt;?php\nuse Cognesy\\Template\\Template;\n\n// Basic example using \"using-&gt;get-&gt;with\" syntax\n$prompt = Template::using('demo-twig')-&gt;get('hello')-&gt;with(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre> Or, using the shorthand <code>make()</code> syntax:</p> <pre><code>&lt;?php\n$prompt = Template::make('demo-twig:hello')-&gt;with(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#rendering-a-chat-template","title":"Rendering a Chat Template","text":"<p>The Prompt class can also render prompts directly as chat-style messages:</p> <pre><code>&lt;?php\n$messages = Template::messages('demo-twig:hello', ['name' =&gt; 'World']);\n\nprint_r($messages-&gt;toArray());\n// Outputs:\n// [\n//     ['role' =&gt; 'user', 'content' =&gt; 'Hello, World!']\n// ]\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#custom-configuration-and-template-content","title":"Custom Configuration and Template Content","text":"<p>If you need to customize the configuration or set the template content directly, you can do so with additional methods:</p> <pre><code>&lt;?php\nuse Cognesy\\Template\\Config\\TemplateEngineConfig;use Cognesy\\Template\\Enums\\TemplateEngineType;\n\n// Setting custom configuration\n$config = new TemplateEngineConfig(\n    templateEngine: TemplateEngineType::Twig,\n    resourcePath: '',\n    cachePath: '/tmp/cache',\n    extension: '.twig',\n);\n\n$prompt = new Template();\n$prompt-&gt;withConfig($config)\n       -&gt;withTemplateContent('Hello, {{ name }}!')\n       -&gt;withValues(['name' =&gt; 'World']);\n\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#in-memory-templates","title":"In Memory Templates","text":"<p>If you need to create an inline prompt (without saving it to a file), you can use following syntax:</p> <pre><code>&lt;?php\n$prompt = Template::twig() // or Template::blade() for Blade syntax\n    -&gt;withTemplateContent('Hello, {{ name }}!')\n    -&gt;withValues(['name' =&gt; 'World'])\n    -&gt;toText();\n?&gt;\n</code></pre> <p>There's shorter syntax for creating in-memory prompts:</p> <pre><code>&lt;?php\n$prompt = Template::twig() // or Template::blade() for Blade syntax\n    -&gt;from('Hello, {{ name }}!')\n    -&gt;with(['name' =&gt; 'World'])\n    -&gt;toText();\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#handling-template-variables","title":"Handling Template Variables","text":"<p>To check which variables are available in a prompt template:</p> <pre><code>&lt;?php\n$prompt = Template::using('demo-twig')\n    -&gt;withTemplateContent('Hello, {{ name }}!')\n    -&gt;withValues(['name' =&gt; 'World']);\n\n$variables = $prompt-&gt;variables();\n\nprint_r($variables); // Outputs: ['name']\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#loading-templates-by-name-and-using-dsns","title":"Loading Templates by Name and Using DSNs","text":"<p>For more flexible template loading, you can load templates by name or use a 'DSN-like' (Data Source Name) syntax:</p> <pre><code>&lt;?php\n// Load a template by name using specified library 'demo-blade'\n$prompt = Template::using('demo-blade')-&gt;withTemplate('hello');\necho $prompt-&gt;template();\n\n// Load a template from specified library using DSN syntax\n$prompt = Template::fromDsn('demo-blade:hello')-&gt;with(['name' =&gt; 'World']);\necho $prompt-&gt;toText(); // Outputs: \"Hello, World!\"\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/prompts/#converting-to-messages-with-markup","title":"Converting to Messages with Markup","text":"<p>The Prompt class also supports converting templates containing chat-specific markup into structured messages:</p> <p>Here is an example XML that can be used to generate a sequence of chat messages: <pre><code>&lt;chat&gt;\n    &lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;\n    &lt;message role=\"user\"&gt;Hello, {{ name }}&lt;/message&gt;\n&lt;/chat&gt;\n</code></pre></p> <p>And here is how you use <code>Prompt</code> class to convert XML template into a sequence of messages:</p> <pre><code>&lt;?php\n\n$prompt = Template::using('demo-blade')\n    -&gt;withTemplateContent('&lt;chat&gt;&lt;message role=\"system\"&gt;You are a helpful assistant.&lt;/message&gt;&lt;message role=\"user\"&gt;Hello, {{ $name }}&lt;/message&gt;&lt;/chat&gt;')\n    -&gt;withValues(['name' =&gt; 'assistant']);\n\n$messages = $prompt-&gt;toMessages();\n\necho $messages-&gt;toArray();\n// Outputs:\n// [\n//     ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant.'],\n//     ['role' =&gt; 'user', 'content' =&gt; 'Hello, assistant']\n// ]\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/sequences/","title":"Sequences","text":""},{"location":"packages/instructor/advanced/sequences/#extracting-sequences-of-objects","title":"Extracting Sequences of Objects","text":"<p>Sequence is a wrapper class that can be used to represent a list of objects to be extracted by Instructor from provided context.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <pre><code>&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new StructuredOutput)\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    )-&gt;get();\n</code></pre>"},{"location":"packages/instructor/advanced/sequences/#streaming-sequences","title":"Streaming Sequences","text":"<p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <p>NOTE This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive sequence updates provide a callback via Instructor's <code>onSequenceUpdate()</code> that will be called each  time a new item is received from LLM.</p> <p>The callback provided a full sequence that has been retrieved so far. You can get the last added object from the sequence via <code>$sequence-&gt;last()</code>.</p> <p>Remember that while the sequence is being updated, the data is not validated - only when the sequence is fully extracted, the objects are validated and a full sequence is returned (see example below).</p> <pre><code>&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\nfunction updateUI(Person $person) {\n    // add newly extracted person to the UI list\n    $this-&gt;ui-&gt;appendToList($person);\n    // remember those objects are not validated yet\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new StructuredOutput)\n    -&gt;onSequenceUpdate(\n        fn($sequence) =&gt; updateUI($sequence-&gt;last()) // get last added object\n    )\n    -&gt;withResponseClass(Sequence::of(Person::class))\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        options: ['stream' =&gt; true]\n    )\n    -&gt;get();\n\n// now the list is fully extracted and validated\nforeach ($list as $person) {\n    // do something with each person\n    $this-&gt;db-&gt;save($person);\n}\n</code></pre>"},{"location":"packages/instructor/advanced/sequences/#working-with-sequences","title":"Working with Sequences","text":"<p>Sequences offer array access (via ArrayAccess) and convenience methods to work with the list of extracted objects.</p> <pre><code>&lt;?php\n$sequence-&gt;count();   // returns the number of extracted items\n$sequence-&gt;first();   // returns the first extracted item\n$sequence-&gt;last();    // returns the last extracted item\n$sequence-&gt;get(1);    // returns the second extracted item\n$sequence-&gt;toArray(); // returns the list of extracted items as an array\n</code></pre>"},{"location":"packages/instructor/advanced/sequences/#streaming-sequence-updates","title":"Streaming sequence updates","text":"<p>See: Streaming and partial updates for more information on how to get partial updates and streaming of sequences.</p>"},{"location":"packages/instructor/advanced/structure-to-structure/","title":"Structure To Structure","text":""},{"location":"packages/instructor/advanced/structure-to-structure/#structured-to-structured-processing","title":"Structured-to-structured processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>create()</code> method can be an object, but also an array or just a string.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new StructuredOutput)-&gt;with(\n    input: $email,\n    responseModel: Email::class,\n    prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n)-&gt;get();\n\nassert($translation instanceof Email); // true\ndump($translation);\n// Email {\n//     address: \"joe@gmail\",\n//     subject: \"Actualizaci\u00f3n de estado\",\n//     body: \"Su cuenta ha sido actualizada.\"\n// }\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/structures/","title":"Structures","text":""},{"location":"packages/instructor/advanced/structures/#handling-dynamic-data-models","title":"Handling dynamic data models","text":"<p>If you want to define the shape of data during runtime, you can use <code>Structure</code> class.</p> <p>Structures allow you to define and modify arbitrary shape of data to be extracted by LLM. Classes may not be the best fit for this purpose, as declaring or changing them during execution is not possible.</p> <p>With structures, you can define custom data shapes dynamically, for example based on the user input or context of the processing, to specify the information you need LLM to infer from the provided text or chat messages.</p>"},{"location":"packages/instructor/advanced/structures/#defining-a-shape-of-data","title":"Defining a shape of data","text":"<p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns a <code>Structure</code> object with dynamic properties in the shape you defined.</p> <p>You can access properties directly or call <code>-&gt;toArray()</code> to convert to an array.</p> <p>Note: If you need raw arrays instead of objects, use <code>-&gt;intoArray()</code> when calling StructuredOutput. See: Output Formats</p> <p><code>Structure::define()</code> accepts array of <code>Field</code> objects.</p> <p>Let's first define the structure, which is a shape of the data we want to extract from the message.</p> <pre><code>&lt;?php\nuse Cognesy\\Dynamic\\Field;\nuse Cognesy\\Dynamic\\Structure;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::enum('role', Role::class),\n]);\n?&gt;\n</code></pre> <p>Following types of fields are currently supported:</p> <ul> <li><code>Field::bool()</code> - boolean value</li> <li><code>Field::int()</code> - int value</li> <li><code>Field::string()</code> - string value</li> <li><code>Field::float()</code> - float value</li> <li><code>Field::enum()</code> - enum value</li> <li><code>Field::structure()</code> - for nesting structures</li> </ul>"},{"location":"packages/instructor/advanced/structures/#optional-fields","title":"Optional fields","text":"<p>Fields can be marked as optional with <code>$field-&gt;optional()</code>.  By default, all defined fields are required.</p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    //...\n    Field::int('age')-&gt;optional(),\n    //...\n]);\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/structures/#descriptions-for-guiding-llm-inference","title":"Descriptions for guiding LLM inference","text":"<p>Instructor includes field descriptions in the content of instructions for LLM, so you can use them to provide explanations, detailed specifications or requirements for each field.</p> <p>You can also provide extra inference instructions for LLM at the structure level with <code>$structure-&gt;description(string $description)</code></p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name', 'Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;optional(),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/structures/#nesting-structures","title":"Nesting structures","text":"<p>You can use <code>Field::structure()</code> to nest structures in case you want to define more complex data shapes.</p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/structures/#validation-of-structure-data","title":"Validation of structure data","text":"<p>Instructor supports validation of structures.</p> <p>You can define field validator with:</p> <ul> <li><code>$field-&gt;validator(callable $validator)</code> - $validator has to return an instance of <code>ValidationResult</code></li> <li><code>$field-&gt;validIf(callable $condition, string $message)</code> - $condition has to return false if validation has not succeeded, $message with be provided to LLM as explanation for self-correction of the next extraction attempt</li> </ul> <p>Let's add a simple field validation to the example above: </p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    // ...\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    // ...\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/structures/#extracting-data","title":"Extracting data","text":"<p>Now, let's extract the data from the message.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield. She is 25 years old and works as a line worker. \n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345.\n    TEXT;\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;get();\n\n// $person is a Structure object, not an array\necho $person-&gt;name;  // \"Jane Doe\"\necho $person-&gt;age;   // 25\necho $person-&gt;role;  // \"line\"\n\n// Convert to array if needed\ndump($person-&gt;toArray());\n// array [\n//   \"name\" =&gt; \"Jane Doe\"\n//   \"age\" =&gt; 25\n//   \"address\" =&gt; array [\n//     \"city\" =&gt; \"Springfield\"\n//   ]\n//   \"role\" =&gt; \"line\"\n// ]\n\n// Or use intoArray() to get raw array directly\n$personArray = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: $structure,\n)-&gt;intoArray()-&gt;get();\n\ndump($personArray);\n// array [ \"name\" =&gt; \"Jane Doe\", \"age\" =&gt; 25, ... ]\n?&gt;\n</code></pre>"},{"location":"packages/instructor/advanced/structures/#working-with-structure-objects","title":"Working with <code>Structure</code> objects","text":"<p>Structure object properties can be accessed using <code>get()</code> and <code>set()</code> methods, but also directly as properties.</p> <pre><code>&lt;?php\n$person = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::structure('role', [\n        Field::string('name'),\n        Field::int('level'),\n    ])\n]);\n\n// Setting properties via set()\n$person-&gt;set('name', 'John Doe');\n$person-&gt;set('age', 30);\n$person-&gt;get('role')-&gt;set('name', 'Manager');\n$person-&gt;get('role')-&gt;set('level', 1);\n\n// Setting properties directly \n$person-&gt;name = 'John Doe';\n$person-&gt;age = 30;\n$person-&gt;role-&gt;name = 'Manager';\n$person-&gt;role-&gt;level = 1;\n\n// Getting properties via get()\n$name = $person-&gt;get('name');\n$age = $person-&gt;get('age');\n$role = $person-&gt;get('role')-&gt;get('name');\n$level = $person-&gt;get('role')-&gt;get('level');\n\n// Getting properties directly\n$name = $person-&gt;name;\n$age = $person-&gt;age;\n$role = $person-&gt;role-&gt;name;\n$level = $person-&gt;role-&gt;level;\n?&gt;\n</code></pre>"},{"location":"packages/instructor/concepts/overview/","title":"Overview","text":""},{"location":"packages/instructor/concepts/overview/#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to get structured, validated data from multiple types of inputs: text,  chat messages, or images. It is powered by Large Language Models (LLMs).</p> <p>The library is inspired by the Instructor for Python created by Jason Liu.</p>"},{"location":"packages/instructor/concepts/overview/#how-it-works","title":"How it works","text":"<p>Instructor uses Large Language Models (LLMs) to process data and return structured information you can easily use in your code.</p> <p></p>"},{"location":"packages/instructor/concepts/overview/#instructor-in-action","title":"Instructor in action","text":"<p>Here's a simple CLI demo app using Instructor to extract structured data from text:</p> <p></p>"},{"location":"packages/instructor/concepts/overview/#how-instructor-enhances-your-workflow","title":"How Instructor Enhances Your Workflow","text":"<p>Instructor introduces three key enhancements compared to direct API usage.</p>"},{"location":"packages/instructor/concepts/overview/#response-model","title":"Response Model","text":"<p>You just specify a PHP class to extract data into via the 'magic' of LLM chat completion. And that's it.</p> <p>Instructor reduces brittleness of the code extracting the information from textual data by leveraging structured LLM responses.</p> <p>Instructor helps you write simpler, easier to understand code - you no longer have to define lengthy function call definitions or write code for assigning returned JSON into target data objects.</p>"},{"location":"packages/instructor/concepts/overview/#validation","title":"Validation","text":"<p>Response model generated by LLM can be automatically validated, following set of rules. Currently, Instructor supports only Symfony validation.</p> <p>You can also provide a context object to use enhanced validator capabilities.</p>"},{"location":"packages/instructor/concepts/overview/#max-retries","title":"Max Retries","text":"<p>You can set the number of retry attempts for requests.</p> <p>Instructor will repeat requests in case of validation or deserialization error up to the specified number of times, trying to get a valid response from LLM.</p>"},{"location":"packages/instructor/concepts/why/","title":"Why use Instructor?","text":"<p>Our library introduces three key enhancements:</p> <ul> <li>Response Model: Specify a data model to be returned by LLM to simplify your code.</li> <li>Validation: Automatically validate response generated by LLM before you start using it.</li> <li>Max Retries: Automated retry attempts for invalid responses.</li> </ul>"},{"location":"packages/instructor/concepts/why/#a-glimpse-into-instructors-capabilities","title":"A Glimpse into Instructor's Capabilities","text":"<p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek.</p>"},{"location":"packages/instructor/concepts/why/#understanding-the-workflow","title":"Understanding the workflow","text":"<p>Let's see how we can leverage it to make use of instructor</p>"},{"location":"packages/instructor/concepts/why/#step-1-define-the-data-model","title":"Step 1: Define the data model","text":"<p>Create a data model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>&lt;?php\nclass UserDetail {\n    public string $name;\n    public int $age;\n}\n</code></pre>"},{"location":"packages/instructor/concepts/why/#step-2-extract","title":"Step 2: Extract","text":"<p>Use the <code>StructuredOutput::create()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies the model to use for extraction.</p> <pre><code>/** @var UserDetail */\n$user = (new StructuredOutput)-&gt;with(\n    messages: [[\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"]],\n    responseModel: UserDetail::class,\n    model: \"gpt-3.5-turbo\",\n)-&gt;get();\n\nassert($user-&gt;name == \"Jason\")\nassert($user-&gt;age == 25)\n</code></pre> <p>It's helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p>"},{"location":"packages/instructor/concepts/why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same data model. If the response triggers any validation rules Instructor will raise a validation error.</p>"},{"location":"packages/instructor/concepts/why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>LeadReport</code> model is passed as the <code>$responseModel</code>, and <code>$maxRetries</code> is set to 2. It means that if the extracted data does not match the model, Instructor will re-ask the model 2 times before giving up.</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via jason@gmailcom -- Jason\"]],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;email === \"jason@gmail.com\");\n</code></pre> <p>More about Validation</p> <p>Check out Jason's blog post Good LLM validation is just good validation</p>"},{"location":"packages/instructor/concepts/why/#custom-validators","title":"Custom Validators","text":"<p>Instructor uses Symfony validation component to validate extracted data. You can use #[Assert/Callback] annotation to build fully customized validation logic.</p> <p>See Symfony docs for more details on how to use Callback constraint.</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/","title":"Configuration Options","text":"<p>Instructor provides extensive configuration options through fluent API methods to customize its behavior, processing, and integration with various LLM providers.</p>"},{"location":"packages/instructor/essentials/configuration/#request-configuration","title":"Request Configuration","text":"<p>Configure how Instructor processes your input and builds requests:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMessages($messages)           // Set chat messages\n    -&gt;withInput($input)                 // Set input (converted to messages)\n    -&gt;withSystem($systemPrompt)         // Set system prompt\n    -&gt;withPrompt($prompt)               // Set additional prompt\n    -&gt;withExamples($examples)           // Set example data for context\n    -&gt;withModel($modelName)             // Set LLM model name\n    -&gt;withOptions($options)             // Set LLM-specific options\n    -&gt;withOption($key, $value)          // Set individual LLM option\n    -&gt;withStreaming(true)               // Enable streaming responses\n    -&gt;withCachedContext($messages, $system, $prompt, $examples) // Use cached context\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#response-configuration","title":"Response Configuration","text":"<p>Define how Instructor should process and validate responses:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMaxRetries(3)                 // Set retry count for failed validations\n    -&gt;withOutputMode(OutputMode::Tools) // Set output mode (Tools, Json, JsonSchema, MdJson)\n    -&gt;withRetryPrompt($prompt)          // Set custom retry prompt for validation failures\n    -&gt;withSchemaName($name)             // Set schema name for documentation\n    -&gt;withToolName($name)               // Set tool name for Tools mode\n    -&gt;withToolDescription($description) // Set tool description for Tools mode\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<p>Fine-tune Instructor's internal processing:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withConfig($configObject)         // Use custom StructuredOutputConfig instance\n    -&gt;withConfigPreset($presetName)     // Use predefined configuration preset\n    -&gt;withConfigProvider($provider)     // Use custom configuration provider\n    -&gt;withObjectReferences(true)        // Enable object reference handling\n    -&gt;withDefaultToStdClass(true)       // Default to stdClass for unknown types\n    -&gt;withDeserializationErrorPrompt($prompt) // Custom deserialization error prompt\n    -&gt;withThrowOnTransformationFailure(true)  // Throw on transformation failures\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<p>Configure connection and communication with LLM providers:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;using($preset)                    // Use LLM preset (e.g., 'openai', 'anthropic')\n    -&gt;withDsn($dsn)                     // Set connection DSN\n    -&gt;withLLMProvider($provider)        // Set custom LLM provider instance\n    -&gt;withLLMConfig($config)            // Set LLM configuration object\n    -&gt;withLLMConfigOverrides($overrides) // Override specific LLM config values\n    -&gt;withDriver($driver)               // Set custom inference driver\n    -&gt;withHttpClient($client)           // Set custom HTTP client\n    -&gt;withHttpClientPreset($preset)     // Use HTTP client preset\n    -&gt;withHttpDebugPreset($preset)      // Enable debug preset\n    -&gt;withClientInstance($driverName, $instance) // Set client instance for specific driver\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#processing-pipeline-overrides","title":"Processing Pipeline Overrides","text":"<p>Customize validation, transformation, and deserialization:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withValidators(...$validators)    // Override response validators\n    -&gt;withTransformers(...$transformers) // Override response transformers  \n    -&gt;withDeserializers(...$deserializers) // Override response deserializers\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#event-handling","title":"Event Handling","text":"<p>Configure real-time processing callbacks:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;onPartialUpdate($callback)        // Handle partial response updates during streaming\n    -&gt;onSequenceUpdate($callback)       // Handle sequence item completion during streaming\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"packages/instructor/essentials/configuration/#basic-openai-configuration","title":"Basic OpenAI Configuration","text":"<pre><code>$result = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4')\n    -&gt;withMaxRetries(3)\n    -&gt;withMessages(\"Extract person data from: John is 25 years old\")\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#streaming-with-callbacks","title":"Streaming with Callbacks","text":"<pre><code>$result = (new StructuredOutput)\n    -&gt;using('openai')\n    -&gt;withStreaming(true)\n    -&gt;onPartialUpdate(fn($partial) =&gt; updateUI($partial))\n    -&gt;withMessages(\"Generate a list of tasks\")\n    -&gt;withResponseClass(Sequence::of(Task::class))\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/configuration/#custom-configuration-object","title":"Custom Configuration Object","text":"<pre><code>$config = new StructuredOutputConfig(\n    maxRetries: 5,\n    outputMode: OutputMode::JsonSchema,\n    retryPrompt: \"Please fix the validation errors and try again.\"\n);\n\n$result = (new StructuredOutput)\n    -&gt;withConfig($config)\n    -&gt;withMessages($input)\n    -&gt;withResponseClass(Person::class)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/customize_prompts/","title":"Customize Prompts","text":""},{"location":"packages/instructor/essentials/customize_prompts/#customizing-prompts","title":"Customizing prompts","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>create()</code> method.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"packages/instructor/essentials/customize_prompts/#prompting-models-with-tool-calling-support","title":"Prompting models with tool calling support","text":"<p><code>OutputMode::Tools</code> is usually most reliable way to get structured outputs following provided response schema.</p> <p><code>OutputMode::Tools</code> can make use of <code>$toolName</code> and <code>$toolDescription</code> parameters to provide additional semantic context to the LLM, describing the tool to be used for processing the input. <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> ignore these parameters, as tools are not used in these modes.</p> <pre><code>&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        toolName: 'extract',\n        toolDescription: 'Extract information from provided content',\n        mode: OutputMode::Tools)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/customize_prompts/#prompting-models-supporting-json-output","title":"Prompting models supporting JSON output","text":"<p>Aside from tool calling Instructor supports two other modes for getting structured outputs from LLM: <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code>.</p> <p><code>OutputMode::Json</code> uses JSON mode offered by some models and API providers to get LLM respond in JSON format rather than plain text.</p> <p><pre><code>&lt;?php\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    prompt: \"\\nYour task is to respond correctly with JSON object.\",\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre> Note that various models and API providers have specific requirements on the input format, e.g. for OpenAI JSON mode you are required to include <code>JSON</code> string in the prompt.</p>"},{"location":"packages/instructor/essentials/customize_prompts/#including-json-schema-in-the-prompt","title":"Including JSON Schema in the prompt","text":"<p>Instructor takes care of automatically setting the <code>response_format</code> parameter, but this may not be sufficient for some models or providers - some of them require specifying JSON response format as part of the prompt, rather than just as <code>response_format</code> parameter in the request (e.g. OpenAI).</p> <p>For this reason, when using Instructor's <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> you should include the expected JSON Schema in the prompt. Otherwise, the response is unlikely to match your target model, making it impossible for Instructor to deserialize it correctly.</p> <pre><code>&lt;?php\n// NOTE: You don't have to create JSON Schema manually, you can use\n// schema automatically generated by Instructor for your response model.\n// See the next example.\n\n$jsonSchema = json_encode([\n    \"type\" =&gt; \"object\",\n    \"properties\" =&gt; [\n        \"name\" =&gt; [\"type\" =&gt; \"string\"],\n        \"age\" =&gt; [\"type\" =&gt; \"integer\"]\n    ],\n    \"required\" =&gt; [\"name\", \"age\"]\n]);\n\n$user = $structuredOutput\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema: $jsonSchema\\n\",\n        mode: OutputMode::Json)\n    -&gt;get();\n</code></pre> <p>The example above demonstrates how to manually create JSON Schema, but with Instructor you do not have to build the schema manually - you can use prompt template placeholder syntax to use Instructor-generated JSON Schema.</p>"},{"location":"packages/instructor/essentials/customize_prompts/#prompt-as-template","title":"Prompt as template","text":"<p>Instructor allows you to use a template string as a prompt. You can use <code>&lt;|variable|&gt;</code> placeholders in the template string, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>&lt;|json_schema|&gt;</code> - replaced with the JSON Schema for current response model</p> <p>Example below demonstrates how to use a template string as a prompt:</p> <pre><code>&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/customize_prompts/#prompting-the-models-with-no-support-for-tool-calling-or-json-output","title":"Prompting the models with no support for tool calling or JSON output","text":"<p><code>OutputMode::MdJson</code> is the most basic (and least reliable) way to get structured outputs from LLM. Still, you may want to use it with the models which do not support tool calling or JSON output.</p> <p><code>OutputMode::MdJson</code> relies on the prompting to get LLM response in JSON formatted data.</p> <p>Many models prompted in this mode will respond with a mixture of plain text and JSON data. Instructor will try to find JSON data fragment in the response and ignore the rest of the text.</p> <p>This approach is most prone to deserialization and validation errors and needs providing JSON Schema in the prompt to increase the probability that the response is correctly structured and contains the expected data.</p> <pre><code>&lt;?php\n$user = (new StructuredOutput)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: OutputMode::MdJson)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/data_model/","title":"Data Model","text":"<p>Instructor provides several ways the data model of LLM response.</p>"},{"location":"packages/instructor/essentials/data_model/#using-classes","title":"Using classes","text":"<p>The default way is to use PHP classes to define the data model. You can also use PHPDoc comments to specify the types of fields of the response. Additionally, you can use attributes to provide more context to the language model or to provide additional instructions to the model.</p>"},{"location":"packages/instructor/essentials/data_model/#type-hints","title":"Type Hints","text":"<p>Use PHP type hints to specify the type of extracted data.</p> <p>Use nullable types to indicate that given field is optional.</p> <pre><code>&lt;?php\n\nclass Person {\n    public string $name;\n    public ?int $age;\n    public Address $address;\n}\n</code></pre> <p>Instructor will only fill in the fields that are public. Private and protected fields are ignored and their values are not going to be extracted (they will be left empty, with default values set as defined in your class).</p>"},{"location":"packages/instructor/essentials/data_model/#private-vs-public-object-field","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM.</p> <p>Private and protected fields are left unchanged, unless the class has setter methods defined or there are parameters in the constructor that match the field names.</p> <p>Provide default values for the fields that are not set by Instructor, to avoid unexpected behavior when accessing those fields.</p> <p>See:  - <code>examples/A01_Basics/BasicPrivateVsPublicFields/run.php</code> to check the details on the behavior of extraction for classes with private and public fields,  - <code>examples/A01_Basics/BasicGetSet/run.php</code> to see how Instructor uses getter and setter methods,  - <code>examples/A01_Basics/BasicConstructor/run.php</code> to see how Instructor uses constructor parameters.</p>"},{"location":"packages/instructor/essentials/data_model/#docblock-type-hints","title":"DocBlock type hints","text":"<p>You can also use PHP DocBlock style comments to specify the type of extracted data. This is useful when you want to specify property types for LLM, but can't or don't want to enforce type at the code level.</p> <pre><code>&lt;?php\n\nclass Person {\n    /** @var string */\n    public $name;\n    /** @var int */\n    public $age;\n    /** @var Address $address person's address */\n    public $address;\n}\n</code></pre> <p>See PHPDoc documentation for more details on DocBlock: https://docs.phpdoc.org/3.0/guide/getting-started/what-is-a-docblock.html#what-is-a-docblock</p>"},{"location":"packages/instructor/essentials/data_model/#using-docblocks-as-additional-instructions-for-llm","title":"Using DocBlocks as Additional Instructions for LLM","text":"<p>You can use PHP DocBlocks (/** */) to provide additional instructions for LLM at class or field level, for example to clarify what you expect or how LLM should process your data.</p> <p>Instructor extracts PHP DocBlocks comments from class and property defined and includes them in specification of response model sent to LLM.</p> <p>Using PHP DocBlocks instructions is not required, but sometimes you may want to clarify your intentions to improve LLM's inference results.</p> <pre><code>    /**\n     * Represents a skill of a person and context in which it was mentioned. \n     */\n    class Skill {\n        public string $name;\n        /** @var SkillType $type type of the skill, derived from the description and context */\n        public SkillType $type;\n        /** Directly quoted, full sentence mentioning person's skill */\n        public string $context;\n    }\n</code></pre>"},{"location":"packages/instructor/essentials/data_model/#attributes-for-data-model-descriptions-and-instructions","title":"Attributes for data model descriptions and instructions","text":"<p>Instructor supports <code>#[Description]</code> and <code>#[Instructions]</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p> <p><code>#[Description]</code> attribute is used to describe a class or property in your data model. Instructor will use this text to provide more context to the language model.</p> <p><code>#[Instructions]</code> attribute is used to provide additional instructions to the language model, such as how to process the data.</p> <p>You can add multiple attributes to a class or property - Instructor will merge them into a single block of text.</p> <p>Instructor will still include any PHPDoc comments provided in the class, but using attributes might be more convenient and easier to read.</p> <pre><code>&lt;?php\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make it ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    public string $job;\n}\n</code></pre> <p>NOTE: Technically both <code>#[Description]</code> and <code>#[Instructions]</code> attributes do the same thing - they provide additional context to the language model. Yet, providing them in separate attributes allows you to better organize your code and make it more readable. In the future, we may extend the functionality of these attributes to provide more specific instructions to the language model, so it is a good idea to use them now.</p>"},{"location":"packages/instructor/essentials/data_model/#typed-collections-arrays","title":"Typed Collections / Arrays","text":"<p>PHP currently does not support generics or typehints to specify array element types.</p> <p>Use PHP DocBlock style comments to specify the type of array elements.</p> <pre><code>&lt;?php\nclass Person {\n    // ...\n}\n\nclass Event {\n    // ...\n    /** @var Person[] list of extracted event participants */\n    public array $participants;\n    // ...\n}\n</code></pre>"},{"location":"packages/instructor/essentials/data_model/#example-of-complex-data-extraction","title":"Example of complex data extraction","text":"<p>Instructor can retrieve complex data structures from text. Your response model can contain nested objects, arrays, and enums.</p> <pre><code>&lt;?php\nuse Cognesy/Instructor/Instructor;\n\n// define a data structures to extract data into\nclass Person {\n    public string $name;\n    public int $age;\n    public string $profession;\n    /** @var Skill[] */\n    public array $skills;\n}\n\nclass Skill {\n    public string $name;\n    public SkillType $type;\n}\n\nenum SkillType : string {\n    case Technical = 'technical';\n    case Other = 'other';\n}\n\n$text = \"Alex is 25 years old software engineer, who knows PHP, Python and can play the guitar.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n)-&gt;get(); // client is passed explicitly, can specify e.g. different base URL\n\n// data is extracted into an object of given class\nassert($person instanceof Person); // true\n\n// you can access object's extracted property values\necho $person-&gt;name; // Alex\necho $person-&gt;age; // 25\necho $person-&gt;profession; // software engineer\necho $person-&gt;skills[0]-&gt;name; // PHP\necho $person-&gt;skills[0]-&gt;type; // SkillType::Technical\n// ...\n\nvar_dump($person);\n// Person {\n//     name: \"Alex\",\n//     age: 25,\n//     profession: \"software engineer\",\n//     skills: [\n//         Skill {\n//              name: \"PHP\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"Python\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"guitar\",\n//              type: SkillType::Other\n//         },\n//     ]\n// }\n</code></pre>"},{"location":"packages/instructor/essentials/data_model/#dynamic-data-schemas-with-structure-class","title":"Dynamic data schemas with <code>Structure</code> class","text":"<p>In case you work with dynamic data schemas, you can use <code>Structure</code> class to define the data model.</p> <p>See Structures for more details on how to work with dynamic data schemas.</p>"},{"location":"packages/instructor/essentials/data_model/#optional-data-with-maybe-class","title":"Optional data with <code>Maybe</code> class","text":"<p>The <code>Maybe</code> class provides a way to handle optional data that may or may not be present in the input text. It wraps a value type and indicates whether the data was found or not, along with an error message when the data is missing.</p>"},{"location":"packages/instructor/essentials/data_model/#basic-usage","title":"Basic Usage","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\n\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n$maybe = Maybe::is(Person::class, 'person', 'Person data if found in the text');\n\n$result = (new StructuredOutput)\n    -&gt;with(\n        messages: \"The document mentions some information but no person details.\",\n        responseModel: $maybe,\n    )\n    -&gt;get();\n\nif ($result-&gt;hasValue()) {\n    $person = $result-&gt;get();\n    echo \"Found person: \" . $person-&gt;name;\n} else {\n    echo \"No person found. Error: \" . $result-&gt;error();\n}\n</code></pre>"},{"location":"packages/instructor/essentials/data_model/#maybe-methods","title":"Maybe Methods","text":"<ul> <li><code>Maybe::is(class, name?, description?)</code> - Static factory method to create a Maybe instance</li> <li><code>get()</code> - Get the value if present, or null if not found</li> <li><code>error()</code> - Get the error message explaining why the value wasn't found</li> <li><code>hasValue()</code> - Check if a value was successfully extracted</li> <li><code>toJsonSchema()</code> - Generate JSON schema for the Maybe wrapper</li> </ul>"},{"location":"packages/instructor/essentials/demonstrations/","title":"Demonstrations","text":""},{"location":"packages/instructor/essentials/demonstrations/#providing-examples-to-llm","title":"Providing examples to LLM","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>OutputMode::Json</code> and <code>OutputMode::MdJson</code> modes, where the output is expected to be a JSON object.</p> <p>Instructor's <code>request()</code> method accepts an array of examples as the <code>examples</code> parameter, where each example is an instance of the <code>Example</code> class.</p>"},{"location":"packages/instructor/essentials/demonstrations/#example-class","title":"<code>Example</code> class","text":"<p><code>Example</code> constructor have two main arguments: <code>input</code> and <code>output</code>.</p> <p>The <code>input</code> property is  a string which describes the input message, while he <code>output</code> property is an array which represents the expected output.</p> <p>Instructor will append the list of examples to the prompt sent to LLM, with output array data rendered as JSON text.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Example\\Example;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n        ),\n    ],\n    mode: OutputMode::Json\n)-&gt;get();\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/demonstrations/#modifying-the-example-template","title":"Modifying the example template","text":"<p>You can use a template string as an input for the Example class. The template string may contain placeholders for the input data, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>{input}</code> - replaced with the actual input message  - <code>{output}</code> - replaced with the actual output data</p> <p>In case input or output data is an array, Instructor will automatically convert it to a JSON string before replacing the placeholders.</p> <pre><code>$user = (new StructuredOutput)-&gt;with(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50],\n            template: \"EXAMPLE:\\n{input} =&gt; {output}\\n\",\n        ),\n    ],\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/demonstrations/#convenience-factory-methods","title":"Convenience factory methods","text":"<p>You can also create Example instances using the <code>fromText()</code>, <code>fromChat()</code>, <code>fromData()</code> helper static methods. All of them accept $output as an array of the expected output data and differ in the way the input data is provided.</p>"},{"location":"packages/instructor/essentials/demonstrations/#make-example-from-text","title":"Make example from text","text":"<p><code>Example::fromText()</code> method accepts a string as an input. It is equivalent to creating an instance of Example using the constructor.</p> <pre><code>$example = Example::fromText(\n    input: 'Ian is 27 yo',\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"packages/instructor/essentials/demonstrations/#make-example-from-chat","title":"Make example from chat","text":"<p><code>Example::fromChat()</code> method accepts an array of messages, which may be useful when you want to use a chat or chat fragment as a demonstration of the input.</p> <pre><code>$example = Example::fromChat(\n    input: [['role' =&gt; 'user', 'content' =&gt; 'Ian is 27 yo']],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"packages/instructor/essentials/demonstrations/#make-example-from-data","title":"Make example from data","text":"<p><code>Example::fromData()</code> method accepts any data type and uses the <code>Json::encode()</code> method to convert it to a string. It may be useful to provide a complex data structure as an example input.</p> <pre><code>$example = Example::fromData(\n    input: ['firstName' =&gt; 'Ian', 'lastName' =&gt; 'Brown', 'birthData' =&gt; '1994-01-01'],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"packages/instructor/essentials/modes/","title":"Modes","text":""},{"location":"packages/instructor/essentials/modes/#extraction-modes","title":"Extraction modes","text":"<p>Instructor supports several ways to extract data from the response.</p>"},{"location":"packages/instructor/essentials/modes/#output-modes","title":"Output Modes","text":"<p>Instructor supports multiple output modes to allow working with various models depending on their capabilities. - <code>OutputMode::Json</code> - generate structured output via LLM's native JSON generation - <code>OutputMode::JsonSchema</code> - use native JSON Schema enforcement (guaranteed only when the provider supports it) - <code>OutputMode::Tools</code> - use tool calling API to get LLM follow provided schema - <code>OutputMode::MdJson</code> - use prompting to generate structured output; fallback for the models that do not support JSON generation or tool calling</p> <p>Additionally, you can use <code>Text</code> and <code>Unrestricted</code> modes to get LLM to generate text output without any structured data extraction.</p> <p>Those modes are not useful for <code>StructuredOutput</code> class (as it is focused on structured output generation) but can be used with <code>Inference</code> class.</p> <ul> <li><code>OutputMode::Text</code> - generate text output</li> <li><code>OutputMode::Unrestricted</code> - generate unrestricted output based on inputs provided by the user (with no enforcement of specific output format)</li> </ul>"},{"location":"packages/instructor/essentials/modes/#example-of-using-modes","title":"Example of Using Modes","text":"<p>Mode can be set via parameter of <code>StructuredOutput::create()</code> method.</p> <p>The default mode is <code>OutputMode::Tools</code>, which leverages OpenAI-style tool calls.</p> <p><pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$structuredOutput = new StructuredOutput();\n\n$response = $structuredOutput-&gt;with(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: OutputMode::Json\n)-&gt;get();\n</code></pre> Mode, like other parameters can also be set via fluent API methods.</p> <pre><code>&lt;?php\n$response = $structuredOutput\n    -&gt;withMessages(\"...\")\n    -&gt;withResponseModel(...)\n    //...\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"packages/instructor/essentials/modes/#modes","title":"Modes","text":""},{"location":"packages/instructor/essentials/modes/#outputmodetools","title":"<code>OutputMode::Tools</code>","text":"<p>This mode is the default one. It uses OpenAI tools to extract data from the response.</p> <p>It is the most reliable mode, but not all models and API providers support it - check their documentation for more information.</p> <ul> <li>https://platform.openai.com/docs/guides/function-calling</li> <li>https://docs.anthropic.com/en/docs/build-with-claude/tool-use</li> <li>https://docs.mistral.ai/capabilities/function_calling/</li> </ul>"},{"location":"packages/instructor/essentials/modes/#outputmodejson","title":"<code>OutputMode::Json</code>","text":"<p>In this mode Instructor provides response format as JSONSchema and asks LLM to respond with JSON object following provided schema.</p> <p>It is supported by many open source models and API providers - check their documentation.</p> <p>See more about JSON mode in:</p> <ul> <li>https://platform.openai.com/docs/guides/text-generation/json-mode</li> <li>https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency</li> <li>https://docs.mistral.ai/capabilities/json_mode/</li> </ul>"},{"location":"packages/instructor/essentials/modes/#outputmodejsonschema","title":"<code>OutputMode::JsonSchema</code>","text":"<p>In contrast to <code>OutputMode::Json</code> which may not always manage to meet the schema requirements, <code>OutputMode::JsonSchema</code> is strict and guarantees the response to be a valid JSON object that matches the provided schema when the provider supports native JSON Schema mode. For providers without native support, results fall back to best-effort JSON output without a strict guarantee.</p> <p>Native JSON Schema support is currently limited to newer OpenAI models (check their docs for details). If you need broad provider compatibility, use <code>OutputMode::Json</code> or <code>OutputMode::MdJson</code>.</p> <p>NOTE: OpenAI JsonSchema mode does not support optional properties. If you need to have optional properties in your schema, use <code>OutputMode::Tools</code> or <code>OutputMode::Json</code>.</p> <p>See more about JSONSchema mode in:</p> <ul> <li>https://platform.openai.com/docs/guides/structured-outputs</li> </ul>"},{"location":"packages/instructor/essentials/modes/#outputmodemdjson","title":"<code>OutputMode::MdJson</code>","text":"<p>In this mode Instructor asks LLM to answer with JSON object following provided schema and return answer as Markdown codeblock.</p> <p>It may improve the results for LLMs that have not been finetuned to respond with JSON as they are likely to be already trained on large amounts of programming docs and have seen a lot of properly formatted JSON objects within MD codeblocks.</p>"},{"location":"packages/instructor/essentials/scalars/","title":"Scalars","text":""},{"location":"packages/instructor/essentials/scalars/#extracting-scalar-values","title":"Extracting Scalar Values","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::integer('age'),\n    )\n    -&gt;get();\n\nvar_dump($value);\n// int(28)\n</code></pre> <p>In this example, we're extracting a single integer value from the text. You can also use <code>Scalar::string()</code>, <code>Scalar::boolean()</code> and <code>Scalar::float()</code> to extract other types of values.</p> <p>Additionally, you can use Scalar adapter to extract enums via <code>Scalar::enum()</code>.</p>"},{"location":"packages/instructor/essentials/scalars/#examples","title":"Examples","text":""},{"location":"packages/instructor/essentials/scalars/#string-result","title":"String result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::string(name: 'firstName'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe(\"Jason\");\n</code></pre>"},{"location":"packages/instructor/essentials/scalars/#integer-result","title":"Integer result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::integer('age'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeInt();\n// expect($value)-&gt;toBe(28);\n</code></pre>"},{"location":"packages/instructor/essentials/scalars/#boolean-result","title":"Boolean result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Scalar::boolean(name: 'isAdult'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeBool();\n// expect($value)-&gt;toBe(true);\n</code></pre>"},{"location":"packages/instructor/essentials/scalars/#float-result","title":"Float result","text":"<pre><code>&lt;?php\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old and his 100m sprint record is 11.6 seconds.\",\n        responseModel: Scalar::float(name: 'recordTime'),\n    )\n    -&gt;get();\n// expect($value)-&gt;toBeFloat();\n// expect($value)-&gt;toBe(11.6);\n</code></pre>"},{"location":"packages/instructor/essentials/scalars/#enum-result-select-one-of-the-options","title":"Enum result / select one of the options","text":"<pre><code>&lt;?php\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'system', 'content' =&gt; $text],\n            ['role' =&gt; 'user', 'content' =&gt; 'What is Jason\\'s citizenship?'],\n        ],\n        responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n    )-&gt;get();\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe('other');\n</code></pre>"},{"location":"packages/instructor/essentials/usage/","title":"Usage","text":""},{"location":"packages/instructor/essentials/usage/#basic-usage","title":"Basic usage","text":"<p>This is a simple example demonstrating how Instructor retrieves structured information from provided text (or chat message sequence).</p> <p>Response model class is a plain PHP class with typehints specifying the types of fields of the object.</p> <p>NOTE: By default, Instructor looks for OPENAI_API_KEY environment variable to get your API key. You can also provide the API key explicitly when creating the Instructor instance.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Step 0: Create .env file in your project root:\n// OPENAI_API_KEY=your_api_key\n\n// Step 1: Define target data structure(s)\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n// Step 2: Provide content to process\n$text = \"His name is Jason and he is 28 years old.\";\n\n// Step 3: Use Instructor to run LLM inference\n$person = (new StructuredOutput)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        responseModel: Person::class,\n    )\n    -&gt;get();\n\n// Step 4: Work with structured response data\nassert($person instanceof Person); // true\nassert($person-&gt;name === 'Jason'); // true\nassert($person-&gt;age === 28); // true\n\necho $person-&gt;name; // Jason\necho $person-&gt;age; // 28\n\nvar_dump($person);\n// Person {\n//     name: \"Jason\",\n//     age: 28\n// }\n?&gt;\n</code></pre> <p>Note</p> <p>Instructor supports classes/objects as response models, as well as specialized helper classes like <code>Scalar</code> for simple values, <code>Maybe</code> for optional data, <code>Sequence</code> for arrays, and <code>Structure</code> for dynamically defined schemas.</p>"},{"location":"packages/instructor/essentials/usage/#fluent-api-methods","title":"Fluent API Methods","text":"<p>StructuredOutput provides a comprehensive fluent API for configuring requests:</p>"},{"location":"packages/instructor/essentials/usage/#request-configuration","title":"Request Configuration","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMessages($messages)           // Set chat messages\n    -&gt;withInput($input)                 // Set input (converted to messages)\n    -&gt;withSystem($systemPrompt)         // Set system prompt\n    -&gt;withPrompt($prompt)               // Set additional prompt\n    -&gt;withExamples($examples)           // Set example data\n    -&gt;withModel($modelName)             // Set LLM model\n    -&gt;withOptions($options)             // Set LLM options\n    -&gt;withStreaming(true)               // Enable streaming\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#response-model-configuration","title":"Response Model Configuration","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withResponseModel($model)         // Set response model (class/object/array)\n    -&gt;withResponseClass($className)     // Set response class specifically\n    -&gt;withResponseObject($object)       // Set response object instance\n    -&gt;withResponseJsonSchema($schema)   // Set JSON schema directly\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#output-formats","title":"Output Formats","text":"<p>Control how extracted data is returned while keeping the same schema:</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;intoArray()                       // Return as associative array\n    -&gt;intoInstanceOf($className)        // Return as different class\n    -&gt;intoObject($selfDeserializing)    // Return as self-deserializing object\n</code></pre> <p>Example - Get raw arrays: <pre><code>$userData = (new StructuredOutput)\n    -&gt;withResponseClass(User::class)    // Schema from User class\n    -&gt;intoArray()                        // Return as array (not object)\n    -&gt;with(messages: 'Extract: John Doe, 30 years old')\n    -&gt;get();\n\n// Result: ['name' =&gt; 'John Doe', 'age' =&gt; 30]\n</code></pre></p> <p>Example - Use different output class: <pre><code>$dto = (new StructuredOutput)\n    -&gt;withResponseClass(UserProfile::class)  // Rich schema (5 fields)\n    -&gt;intoInstanceOf(UserDTO::class)         // Simple output (2 fields)\n    -&gt;with(messages: 'Extract user data')\n    -&gt;get();\n\n// Schema defines 5 fields, output has 2 fields\n</code></pre></p> <p>See: Output Formats Guide for comprehensive documentation.</p>"},{"location":"packages/instructor/essentials/usage/#configuration-and-behavior","title":"Configuration and Behavior","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withMaxRetries(3)                 // Set retry count\n    -&gt;withOutputMode($mode)             // Set output mode\n    -&gt;withToolName($name)               // Set tool name for Tools mode\n    -&gt;withToolDescription($desc)        // Set tool description\n    -&gt;withRetryPrompt($prompt)          // Set retry prompt\n    -&gt;withConfig($config)               // Set configuration object\n    -&gt;withConfigPreset($preset)         // Use configuration preset\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;using($preset)                    // Use LLM preset (e.g., 'openai')\n    -&gt;withDsn($dsn)                     // Set connection DSN\n    -&gt;withLLMProvider($provider)        // Set custom LLM provider\n    -&gt;withLLMConfig($config)            // Set LLM configuration\n    -&gt;withDriver($driver)               // Set inference driver\n    -&gt;withHttpClient($client)           // Set HTTP client\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#processing-overrides","title":"Processing Overrides","text":"<pre><code>$structuredOutput = (new StructuredOutput)\n    -&gt;withValidators(...$validators)    // Override validators\n    -&gt;withTransformers(...$transformers) // Override transformers\n    -&gt;withDeserializers(...$deserializers) // Override deserializers\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#request-execution-methods","title":"Request Execution Methods","text":"<p>After configuring your <code>StructuredOutput</code> instance, you have several ways to execute the request and access different types of responses:</p>"},{"location":"packages/instructor/essentials/usage/#direct-execution-methods","title":"Direct Execution Methods","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$structuredOutput = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n\n// Get structured result directly\n$person = $structuredOutput-&gt;get();\n\n// Get raw LLM response\n$llmResponse = $structuredOutput-&gt;response();\n\n// Get streaming interface\n$stream = $structuredOutput-&gt;stream();\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#pending-execution-with-create","title":"Pending Execution with <code>create()</code>","text":"<p>The <code>create()</code> method returns a <code>PendingStructuredOutput</code> instance, which acts as an execution handler that provides the same access methods:</p> <pre><code>&lt;?php\n$pending = $structuredOutput-&gt;create();\n\n// Execute and get structured result\n$person = $pending-&gt;get();\n\n// Execute and get raw LLM response\n$llmResponse = $pending-&gt;response();\n\n// Execute and get streaming interface\n$stream = $pending-&gt;stream();\n\n// Additional utility methods\n$json = $pending-&gt;toJson();      // Convert result to JSON string\n$array = $pending-&gt;toArray();    // Convert result to array\n$jsonObj = $pending-&gt;toJsonObject(); // Convert result to Json object\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#runtime-first-execution-cancreatestructuredoutput","title":"Runtime-First Execution (<code>CanCreateStructuredOutput</code>)","text":"<p>When you need constructor-injected creators (for agents, addons, or DI containers), use <code>toRuntime()</code> and execute via request objects:</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Data\\StructuredOutputRequest;\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$creator = (new StructuredOutput())\n    -&gt;using('openai')\n    -&gt;toRuntime();\n\n$request = new StructuredOutputRequest(\n    messages: \"His name is Jason, he is 28 years old.\",\n    requestedSchema: Person::class,\n);\n\n$requestId = $request-&gt;id()-&gt;toString(); // StructuredOutputRequestId at boundary\n$person = $creator-&gt;create($request)-&gt;get();\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#response-types-explained","title":"Response Types Explained","text":"<ul> <li><code>get()</code>: Returns the parsed and validated structured result (e.g., <code>Person</code> object)</li> <li><code>response()</code>: Returns the raw LLM response object with metadata like tokens, model info, etc.</li> <li><code>stream()</code>: Returns <code>StructuredOutputStream</code> for real-time processing of streaming responses</li> </ul> <p>The <code>PendingStructuredOutput</code> class serves as a flexible execution interface that lets you choose how to process the LLM response based on your specific needs.</p>"},{"location":"packages/instructor/essentials/usage/#string-as-input","title":"String as Input","text":"<p>You can provide a string instead of an array of messages. This is useful when you want to extract data from a single block of text and want to keep your code simple.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$value = (new StructuredOutput)\n    -&gt;with(\n        messages: \"His name is Jason, he is 28 years old.\",\n        responseModel: Person::class,\n    )\n    -&gt;get();\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#structured-to-structured-data-processing","title":"Structured-to-structured data processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>with()</code> method can be an object, but also an array or just a string.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new StructuredOutput)-&gt;with(\n        input: $email,\n        responseModel: Email::class,\n        prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n    )\n    -&gt;get();\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#streaming-support","title":"Streaming support","text":"<p>Instructor supports streaming of partial results, allowing you to start processing the data as soon as it is available.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n$stream = (new StructuredOutput)-&gt;with(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n    options: ['stream' =&gt; true]\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $partialPerson) {\n    // process partial person data\n    echo \"Name: \" $partialPerson-&gt;name ?? '...';\n    echo \"Age: \" $partialPerson-&gt;age ?? '...';\n}\n\n// after streaming is done you can get the final, fully processed person object...\n$person = $stream-&gt;lastUpdate()\n// ...to, for example, save it to the database\n$db-&gt;save($person);\n?&gt;\n</code></pre>"},{"location":"packages/instructor/essentials/usage/#scalar-responses","title":"Scalar responses","text":"<p>See Scalar responses for more information on how to generate scalar responses with <code>Scalar</code> adapter class.</p>"},{"location":"packages/instructor/essentials/usage/#partial-responses-and-streaming","title":"Partial responses and streaming","text":"<p>See Streaming and partial updates for more information on how to work with partial updates and streaming.</p>"},{"location":"packages/instructor/essentials/usage/#extracting-arguments-for-function-call","title":"Extracting arguments for function call","text":"<p>See FunctionCall helper class for more information on how to extract arguments for callable objects.</p>"},{"location":"packages/instructor/essentials/usage/#execution-methods-summary","title":"Execution Methods Summary","text":"<p>Once configured, you can execute your request using different methods depending on your needs:</p> <pre><code>// Direct execution methods\n$result = $structuredOutput-&gt;get();       // Get structured result\n$response = $structuredOutput-&gt;response(); // Get raw LLM response  \n$stream = $structuredOutput-&gt;stream();     // Get streaming interface\n\n// Or use create() to get PendingStructuredOutput for flexible execution\n$pending = $structuredOutput-&gt;create();\n$result = $pending-&gt;get();                 // Same methods available\n$json = $pending-&gt;toJson();               // Plus utility methods\n</code></pre> <ul> <li><code>get()</code>: Returns the parsed and validated structured result</li> <li><code>response()</code>: Returns the raw LLM response with metadata</li> <li><code>stream()</code>: Returns <code>StructuredOutputStream</code> for real-time processing</li> <li><code>create()</code>: Returns <code>PendingStructuredOutput</code> for flexible execution control</li> </ul>"},{"location":"packages/instructor/essentials/validation/","title":"Validation","text":""},{"location":"packages/instructor/essentials/validation/#basic-validation","title":"Basic validation","text":"<p>Instructor validates results of LLM response against validation rules specified in your data model.</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is Jason, he is -28 years old.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: Person::class,\n)-&gt;get();\n\n// if the resulting object does not validate, Instructor throws an exception\n</code></pre> <p>NOTE: For further details on available validation rules, check Symfony Validation constraints.</p>"},{"location":"packages/instructor/essentials/validation/#max-retries","title":"Max Retries","text":"<p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    #[Assert\\Length(min: 3)]\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is JX, aka Jason, he is -28 years old.\";\n\n$person = (new StructuredOutput)-&gt;with(\n    messages: $text,\n    responseModel: Person::class,\n    maxRetries: 3,\n)-&gt;get();\n\n// if all LLM's attempts to self-correct the results fail, Instructor throws an exception\n</code></pre>"},{"location":"packages/instructor/essentials/validation/#custom-validation","title":"Custom Validation","text":"<p>You can easily add custom validation code to your response model by using <code>ValidationTrait</code> and defining validation logic in <code>validate()</code> method.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $age;\n\n    public function validate() : array {\n        if ($this-&gt;name === strtoupper($this-&gt;name)) {\n            return [];\n        }\n        return [[\n            'message' =&gt; \"Name must be in uppercase.\",\n            'path' =&gt; 'name',\n            'value' =&gt; $this-&gt;name\n        ]];\n    }\n}\n\n$user = (new StructuredOutput)-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n)-&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note that method <code>validate()</code> has to return:  * an empty array if the object is valid,  * or an array of validation violations.</p> <p>This information will be used by LLM to make subsequent attempts to correct the response.</p> <pre><code>$violations = [\n    [\n        'message' =&gt; \"Error message with violation details.\",\n        'path' =&gt; 'path.to.property',\n        'value' =&gt; '' // invalid value\n    ],\n    // ...other violations\n];\n</code></pre>"},{"location":"packages/instructor/essentials/validation/#custom-validation-via-symfony-assertcallback","title":"Custom Validation via Symfony <code>#[Assert/Callback]</code>","text":"<p>Instructor uses Symfony validation component to validate extracted data.</p> <p>You can use <code>#[Assert/Callback]</code> annotation to build fully customized validation logic.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new StructuredOutput)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n        responseModel: UserDetails::class,\n        maxRetries: 2\n    )\n    -&gt;get();\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>NOTE: See Symfony docs for more details on how to use Callback constraint.</p>"},{"location":"packages/instructor/internals/config_files/","title":"Config Files","text":""},{"location":"packages/instructor/internals/config_files/#configuration-groups","title":"Configuration Groups","text":"<p>Instructor's configuration is organized into groups. Each group contains a set of settings that are related to a specific aspect of Instructor's functionality.</p> <p>Instructor comes with the following default settings groups: - <code>debug</code>: Debugging settings - <code>embed</code>: Embedding provider connections - <code>http</code>: HTTP client configurations - <code>llm</code>: LLM provider connections - <code>prompt</code>: Prompt libraries and their settings - <code>web</code>: Web service providers (e.g. scraper API configurations)</p> <p>Each group is stored in a separate file in the configuration directory. The file name corresponds to the group name.</p>"},{"location":"packages/instructor/internals/configuration_path/","title":"Configuration Path","text":"<p>Instructor comes with a set of configuration files and prompt templates that you can publish to your project directory.</p> <p>There are 2 ways to set up the location of Instructor's configuration directory: - Using <code>Settings</code> class method <code>setPath()</code> - Using environment variable (recommended)</p> <p> To check how to publish configuration files to your project see Setup section. </p>"},{"location":"packages/instructor/internals/configuration_path/#setting-configuration-path-via-settings-class","title":"Setting Configuration Path via <code>Settings</code> Class","text":"<p>You can set Instructor configuration path using the <code>Settings::setPath()</code> method:</p> <pre><code>&lt;?php\nuse Cognesy\\Config\\Settings;\n\nSettings::setPath('/path/to/config');\n?&gt;\n</code></pre>"},{"location":"packages/instructor/internals/configuration_path/#setting-configuration-path-via-environment-variable","title":"Setting Configuration Path via Environment Variable","text":"<p>You can set the path to Instructor's configuration directory in your <code>.env</code> file:</p> <pre><code>INSTRUCTOR_CONFIG_PATHS='/path/to/config/,another/path'\n</code></pre>"},{"location":"packages/instructor/internals/configuration_path/#configuration-location-resolution","title":"Configuration Location Resolution","text":"<p>Instructor uses a configuration directory with a set of <code>.php</code> files to store its settings, e.g. LLM provider configurations.</p> <p>Instructor will look for its configuration location in the following order: - If static variable value <code>$path</code> in <code>Settings</code> class is set, it will use it, - If <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable is set, it will use its value, - Finally, it will default to the directory, which is bundled with Instructor package (under <code>/config</code>) and contains default set of configuration files.</p>"},{"location":"packages/instructor/internals/debugging/","title":"Debugging","text":"<p>Instructor offers several ways to debug it's internal state and execution flow.</p>"},{"location":"packages/instructor/internals/debugging/#events","title":"Events","text":"<p>Instructor emits events at various points in its lifecycle, which you can listen to and react to. You can use these events to debug execution flow and to inspect data at various stages of processing.</p> <p>For more details see the Events section.</p>"},{"location":"packages/instructor/internals/debugging/#http-debugging","title":"HTTP Debugging","text":"<p>The <code>StructuredOutput</code> class has a <code>withDebug()</code> method that can be used to debug the request and response.</p> <pre><code>$result = (new StructuredOutput)\n    -&gt;withHttpDebugPreset('on')\n    -&gt;with(\n        messages: \"Jason is 25 years old\",\n        responseModel: User:class,\n    )\n    -&gt;get();\n</code></pre> <p>It displays detailed information about the request being sent to LLM API and response received from it, including:</p> <ul> <li>request headers, URI, method and body,</li> <li>response status, headers, and body.</li> </ul> <p>This is useful for debugging the request and response when you are not getting the expected results.</p>"},{"location":"packages/instructor/internals/environment/","title":"Environment","text":""},{"location":"packages/instructor/internals/environment/#environment-configuration","title":"Environment Configuration","text":"<p>Instructor uses environment variables for configuration settings and API keys. The library comes with a <code>.env-dist</code> template file that lists all supported variables.</p> <p>You can copy it to <code>.env</code> (or merge with your <code>.env</code> file) and fill in your values.</p> <p>Alternatively, you can set the variables directly in your environment</p> <p>Check setup instructions for more details on how to set up your environment and how it can be done automatically with the Instructor's CLI tool.</p> <p>     Keep your .env file secure and never commit it to version control.     For production, consider using your environment's secrets management system. </p>"},{"location":"packages/instructor/internals/environment/#llm-provider-api-keys","title":"LLM Provider API Keys","text":"<p>Instructor supports multiple LLM providers.</p> <p>Configure the ones you plan to use:</p> <pre><code># OpenAI (default provider)\nOPENAI_API_KEY=''\n\n# Alternative providers\nANTHROPIC_API_KEY=''\nANYSCALE_API_KEY=''\nAZURE_OPENAI_API_KEY=''\nAZURE_OPENAI_EMBED_API_KEY=''\nCOHERE_API_KEY=''\nFIREWORKS_API_KEY=''\nGEMINI_API_KEY=''\nGROK_API_KEY=''\nGROQ_API_KEY=''\nMISTRAL_API_KEY=''\nOLLAMA_API_KEY=''\nOPENROUTER_API_KEY=''\nTOGETHER_API_KEY=''\nJINA_API_KEY=''\n</code></pre> <p>Only configure the services you plan to use; others can remain empty.</p>"},{"location":"packages/instructor/internals/environment/#other-api-keys-in-env-dist","title":"Other API Keys in <code>.env-dist</code>","text":"<p>Instructor comes with bundled add-on capabilities that use other APIs for various purposes. You can find them in the <code>.env-dist</code> file. They are not required for the core functionality of Instructor.</p>"},{"location":"packages/instructor/internals/environment/#instructor-configuration-directory-path","title":"Instructor Configuration Directory Path","text":"<p>Instructor uses a configuration directory to store its settings, e.g. LLM provider configurations.</p> <p>You can set the path to this directory in your <code>.env</code> file: <pre><code>INSTRUCTOR_CONFIG_PATHS='/../../config/,another/path'\n</code></pre></p> <p>This tells Instructor where to find its configuration files, if it has not been configured manually via <code>Settings</code> class. The path is relative to the vendor directory where Instructor is installed.</p> <p> <code>INSTRUCTOR_CONFIG_PATHS</code> is set automatically if you use the Instructor CLI tool to publish assets. </p>"},{"location":"packages/instructor/internals/events/","title":"Events","text":""},{"location":"packages/instructor/internals/events/#event-classes","title":"Event classes","text":"<p>Instructor dispatches multiple classes of events during its execution. All of them are descendants of <code>Event</code> class.</p> <p>You can listen to these events and react to them in your application, for example to log information or to monitor the execution process.</p> <p>Check the list of available event classes in the <code>Cognesy\\Instructor\\Events</code> namespace.</p>"},{"location":"packages/instructor/internals/events/#event-methods","title":"Event methods","text":"<p>Each Instructor event offers following methods, which make interacting with them more convenient:</p> <ul> <li><code>print()</code> - prints a string representation of the event to console output</li> <li><code>printDebug()</code> - prints a string representation of the event to console output, with additional debug information</li> <li><code>asConsole()</code> - returns the event in a format suitable for console output</li> <li><code>asLog()</code> - returns the event in a format suitable for logging</li> </ul>"},{"location":"packages/instructor/internals/events/#receiving-notification-on-internal-events","title":"Receiving notification on internal events","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <ul> <li><code>(new StructuredOutput)-&gt;onEvent(string $class, callable $callback)</code> method - receive callback when specified type of event is dispatched</li> <li><code>(new StructuredOutput)-&gt;wiretap(callable $callback)</code> method - receive any event dispatched by Instructor, may be useful for debugging or performance analysis</li> </ul> <p>Receiving events can help you to monitor the execution process and makes it easier for a developer to understand and resolve any processing issues.</p> <pre><code>$structuredOutput = (new StructuredOutput)\n    // see requests to LLM\n    -&gt;onEvent(HttpRequestSent::class, fn($e) =&gt; dump($e))\n    // see responses from LLM\n    -&gt;onEvent(HttpResponseReceived::class, fn($event) =&gt; dump($event))\n    // see all events in console-friendly format\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    // log all events in log-friendly format\n    -&gt;wiretap(fn($event) =&gt; YourLogger::log($event-&gt;asLog()))\n\n$structuredOutput-&gt;with(\n    messages: \"What is the population of Paris?\",\n    responseModel: Scalar::integer(),\n)-&gt;get();\n// check your console for the details on the Instructor execution\n</code></pre>"},{"location":"packages/instructor/internals/events/#convenience-methods-for-get-streamed-model-updates","title":"Convenience methods for get streamed model updates","text":"<p><code>StructuredOutput</code> class provides convenience methods allowing client code to receive model updates  when streaming is enabled:</p> <ul> <li><code>onPartialUpdate(callable $callback)</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate(callable $callback)</code> - to handle partial sequence updates of the response</li> </ul> <p>In both cases your callback will receive updated model, so you don't have to extract it from the event.</p>"},{"location":"packages/instructor/internals/instructor/","title":"Instructor","text":""},{"location":"packages/instructor/internals/instructor/#structuredoutput-class","title":"<code>StructuredOutput</code> class","text":"<p><code>StructuredOutput</code> class is the main entry point to the library. It is responsible for handling all interactions with the client code and internal Instructor components.</p>"},{"location":"packages/instructor/internals/instructor/#request-handlers","title":"Request handlers","text":"<p>One of the essential tasks of the <code>StructuredOutput</code> class is to read the configuration and use it to retrieve a component implementing <code>CanHandleRequest</code> interface (specified in the configuration) to process the request and return the response.</p>"},{"location":"packages/instructor/internals/instructor/#dispatched-events","title":"Dispatched events","text":"<p><code>StructuredOutput</code> class dispatches several high level events during initialization and processing of the request and response:</p> <ul> <li><code>StructuredOutputStarted</code> - dispatched when the structured output processes starts</li> <li><code>StructuredOutputRequestReceived</code> - dispatched when the request is received</li> <li><code>StructuredOutputResponseGenerated</code> - dispatched when the response is generated</li> <li><code>StructuredOutputResponseUpdated</code> - dispatched when the response update is streamed</li> </ul>"},{"location":"packages/instructor/internals/instructor/#event-listeners","title":"Event listeners","text":"<p><code>StructuredOutput</code> class provides several methods allowing client code to plug into Instructor event system, including:  - <code>onEvent()</code> - to receive a callback when specified type of event is dispatched  - <code>wiretap()</code> - to receive any event dispatched by Instructor</p>"},{"location":"packages/instructor/internals/instructor/#response-model-updates","title":"Response model updates","text":"<p>Additionally, <code>StructuredOutput</code> class provides convenience methods allowing client code to receive model updates when streaming is enabled:</p> <ul> <li><code>onPartialUpdate()</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate()</code> - to handle partial sequence updates of the response</li> </ul>"},{"location":"packages/instructor/internals/instructor/#error-handling","title":"Error handling","text":"<p><code>StructuredOutput</code> class contains top level try-catch block to let user handle any uncaught errors before throwing them back to the client code. It allows you to register a handler which will log the error or notify your monitoring system about a problem.</p>"},{"location":"packages/instructor/internals/lifecycle/","title":"Lifecycle","text":""},{"location":"packages/instructor/internals/lifecycle/#instructors-request-lifecycle","title":"Instructor's request lifecycle","text":"<p>As Instructor for PHP processes your request, it goes through several stages:</p> <ol> <li>Initialize and self-configure (with possible overrides defined by developer).</li> <li>Analyze classes and properties of the response data model specified by developer.</li> <li>Translate data model into a schema that can be provided to LLM.</li> <li>Execute request to LLM using specified messages (content) and response model metadata.</li> <li>Receive a response from LLM or multiple partial responses (if streaming is enabled).</li> <li>Deserialize response received from LLM into originally requested classes and their properties.</li> <li>In case response contained unserializable data - create feedback message for LLM and request regeneration of the response.</li> <li>Execute validations defined by developer on the deserialized data - if any of them fail, create feedback message for LLM and requests regeneration of the response.</li> <li>Repeat the steps 4-8, unless specified limit of retries has been reached or response passes validation.</li> </ol> <p>Internally, request, execution, and attempt identities are tracked as typed value objects (<code>StructuredOutputRequestId</code>, <code>StructuredOutputExecutionId</code>, <code>StructuredOutputAttemptId</code>) and serialized as strings at boundaries.</p>"},{"location":"packages/instructor/internals/response_models/","title":"Response Models","text":"<p>Instructor's request parameter <code>responseModel</code> allows you to specify shape of the response you expect from LLM .</p> <p>Instructor translates the <code>responseModel</code> parameter into actual schema based on the type and value of the parameter.</p>"},{"location":"packages/instructor/internals/response_models/#handling-string-responsemodel-value","title":"Handling string $responseModel value","text":"<p>If <code>string</code> value is provided, it is used as a name of the class of the response model.</p> <p>Instructor checks if the class exists and analyzes the class &amp; properties type information &amp; doc comments to generate a schema needed to specify LLM response constraints.</p> <p>The best way to provide the name of the response model class is to use <code>NameOfTheClass::class</code>, making it easy for IDE to check the type, handle refactorings, etc.</p>"},{"location":"packages/instructor/internals/response_models/#handling-object-responsemodel-value","title":"Handling object $responseModel value","text":"<p>If <code>object</code> value is provided, it is considered an instance of the response model. Instructor checks the class of the instance, then analyzes it and its property type data to specify LLM response constraints.</p>"},{"location":"packages/instructor/internals/response_models/#handling-array-responsemodel-value","title":"Handling array $responseModel value","text":"<p>If <code>array</code> value is provided, it is considered a raw JSON Schema, therefore allowing Instructor to use it directly in LLM requests (after wrapping in appropriate context - e.g. function call).</p> <p>Instructor requires information on the class of each nested object in your JSON Schema, so it can correctly deserialize the data into appropriate type.</p> <p>This information is available to Instructor when you are passing $responseModel as a class name or an instance, but it is missing from raw JSON Schema. Lack of the information on target class makes it impossible for Instructor to deserialize the data into appropriate, expected type.</p> <p>Current design uses JSON Schema <code>$comment</code> field (or <code>x-php-class</code> field) on property to overcome this information gap. Instructor expects developer to use this field to provide fully qualified name of the target class to be used to deserialize property data of object or enum type.</p>"},{"location":"packages/instructor/internals/response_models/#important-arrays-as-schema-input-vs-output","title":"Important: Arrays as Schema INPUT vs OUTPUT","text":"<p>Arrays as Schema INPUT \u2705</p> <p>You can provide an array as <code>responseModel</code> to specify the JSON Schema:</p> <pre><code>$output = StructuredOutput::create()\n    -&gt;with(\n        responseModel: [\n            'x-php-class' =&gt; User::class,  // \u2190 Required for deserialization\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'name' =&gt; ['type' =&gt; 'string'],\n                'age' =&gt; ['type' =&gt; 'integer'],\n            ],\n            'required' =&gt; ['name'],\n        ],\n        // ...\n    )\n    -&gt;get();\n</code></pre> <p>Important: The <code>x-php-class</code> field is REQUIRED for InstructorPHP to know which class to deserialize the data into.</p> <p>Arrays as OUTPUT \u274c</p> <p>InstructorPHP does NOT return raw arrays. The output is ALWAYS an object:</p> <pre><code>$user = StructuredOutput::create()\n    -&gt;with(responseModel: User::class, ...)\n    -&gt;get();\n\n// Result is User object (not array)\n$user-&gt;name;   // \u2705 Works\n$user['name']; // \u274c Error - not an array\n</code></pre> <p>Getting Raw Arrays (v1.3+) \u2705</p> <p>If you need raw arrays instead of objects, use the <code>intoArray()</code> output format:</p> <pre><code>$userArray = StructuredOutput::create()\n    -&gt;with(responseModel: User::class, ...)\n    -&gt;intoArray()  // Returns array instead of object\n    -&gt;get();\n\n// Result is array (not object)\n$userArray['name'];  // \u2705 Works\n$userArray-&gt;name;    // \u274c Error - not an object\n\n// $userArray = ['name' =&gt; 'John', 'age' =&gt; 30]\n</code></pre> <p>You can also use different output classes or self-deserializing objects:</p> <pre><code>// Use different class for output than schema\n$dto = StructuredOutput::create()\n    -&gt;with(responseModel: UserProfile::class, ...)  // Rich schema (5 fields)\n    -&gt;intoInstanceOf(UserDTO::class)                // Simple output (2 fields)\n    -&gt;get();\n\n// Self-deserializing objects\n$scalar = StructuredOutput::create()\n    -&gt;with(responseModel: Rating::class, ...)\n    -&gt;intoObject(new Scalar('rating', 'integer'))\n    -&gt;get();\n</code></pre> <p>See: Output Formats for comprehensive guide.</p>"},{"location":"packages/instructor/internals/response_models/#custom-response-handling-strategy","title":"Custom response handling strategy","text":"<p>Instructor allows you to customize processing of <code>$responseModel</code> value also by looking at the interfaces the class or instance implements:</p> <ul> <li><code>CanProvideJsonSchema</code> - implement to be able to provide raw JSON Schema (as an array) of the response model, overriding the default approach of Instructor, which is analyzing $responseModel value class information,</li> <li><code>CanProvideSchema</code> - implement to be able to provide <code>Schema</code> object of the response model, overriding class analysis stage; can be useful in building object wrappers (see: <code>Sequence</code> class),</li> <li><code>CanDeserializeSelf</code> - implement to customize the way the response from LLM is deserialized from JSON into PHP object,</li> <li><code>CanValidateSelf</code> - implement to customize the way the deserialized object is validated - it fully replaces the default validation process for given response model,</li> <li><code>CanTransformSelf</code> - implement to transform the validated object into any target value that will be then passed back to the caller (e.g. unwrap simple type from a class to scalar value)</li> </ul> <p>Methods implemented by those interfaces are executed as following:</p> <ul> <li><code>CanProvideJsonSchema</code> - executed during the schema generation phase,</li> <li><code>CanDeserializeSelf</code> - executed during the deserialization phase,</li> <li><code>CanValidateSelf</code> - executed during the validation phase,</li> <li><code>CanTransformSelf</code> - executed during the transformation phase.</li> </ul> <p>When implementing custom response handling strategy, avoid doing all transformations in a single block of code. Split the logic between relevant methods implemented by your class for clarity and easier code maintenance.</p>"},{"location":"packages/instructor/internals/response_models/#example-implementations","title":"Example implementations","text":"<p>For a practical example of using those contracts to customize Instructor processing flow see:</p> <ul> <li>src/Extras/Scalar/</li> <li>src/Extras/Sequence/</li> </ul> <p>Examples contain an implementation of custom response model handling strategies, e.g. providing scalar value support via a wrapper class implementing:  - custom schema provider,  - deserialization,  - validation  - and transformation</p> <p>into requested value type.</p>"},{"location":"packages/instructor/internals/settings_class/","title":"Settings Class","text":""},{"location":"packages/instructor/internals/settings_class/#settings-class","title":"<code>Settings</code> Class","text":"<p><code>Settings</code> class is the main entry point for telling Instructor where to look for its configuration. It allows you to set up path of Instructor's configuration directory and access Instructor settings.</p> <p><code>Settings</code> class provides the following methods: - <code>setPath(string $path)</code>: Set the path to Instructor's configuration directory - <code>getPath(): string</code>: Get current path to Instructor's configuration directory - <code>has($group, $key): bool</code>: Check if a specific setting exists in Instructor's configuration - <code>get($group, $key, $default): mixed</code>: Get a specific setting from Instructor's configuration - <code>set($group, $key, $value)</code>: Set a specific setting in Instructor's configuration</p> <p>You won't usually need to use these methods directly, but they are used internally by Instructor to access configuration settings.</p>"},{"location":"packages/instructor/misc/contributing/","title":"Contributing","text":""},{"location":"packages/instructor/misc/contributing/#contributing","title":"Contributing","text":"<p>If you want to help, check out some of the issues. They could be anything from code improvements, a guest blog post, or a new cookbook.</p>"},{"location":"packages/instructor/misc/help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p> <p>          The concepts section explains the core concepts of Instructor and how to prompt with models.               The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.      <pre><code>&lt;Card\n    title=\"GitHub Discussions\"\n    icon=\"github\"\n    href=\"https://github.com/cognesy/instructor-php/discussions\"\n&gt;\n    [GitHub discussions](https://github.com/cognesy/instructor-php/discussions) are useful for asking questions, your question and the answer will help everyone.\n&lt;/Card&gt;\n&lt;Card\n    title=\"GitHub Issues\"\n    icon=\"bug\"\n    href=\"https://github.com/cognesy/instructor-php/issues\"\n&gt;\n    [GitHub issues](https://github.com/cognesy/instructor-php/issues) are useful for reporting bugs or requesting new features.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Discord\"\n    icon=\"discord\"\n    href=\"https://discord.gg/CV8sPM5k5Y\"\n&gt;\n    The [Discord](https://discord.gg/CV8sPM5k5Y) is a great place to ask questions and get help from the community.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Blog\"\n    icon=\"newspaper\"\n    href=\"https://cognesy.com/blog\"\n&gt;\n    The [blog](blog/index.md) contains articles that explain how to use Instructor in different scenarios.\n&lt;/Card&gt;\n&lt;Card\n    title=\"Twitter\"\n    icon=\"twitter\"\n    href=\"https://twitter.com/ddebowczyk\"\n&gt;\n    You can also reach out to me [@ddebowczyk](https://twitter.com/ddebowczyk) if you have any questions or ideas.\n&lt;/Card&gt;\n</code></pre> <p></p>"},{"location":"packages/instructor/misc/llm_providers/","title":"LLM API Providers","text":"<p>Following table lists the supported providers:</p> <ul> <li>A21</li> <li>Anthropic</li> <li>Azure</li> <li>Cerebras</li> <li>Cohere</li> <li>Cohere (OpenAI compatible API)</li> <li>DeepSeek</li> <li>Fireworks</li> <li>Gemini</li> <li>Gemini (OpenAI compatible API)</li> <li>Groq</li> <li>MiniMaxi</li> <li>Mistral</li> <li>Moonshot</li> <li>Ollama</li> <li>OpenAI</li> <li>OpenRouter</li> <li>Perplexity</li> <li>SambaNova</li> <li>Together</li> <li>XAI</li> </ul> <p>Check the cookbook for examples of how to use them.</p>"},{"location":"packages/instructor/misc/philosophy/","title":"Philosophy","text":""},{"location":"packages/instructor/misc/philosophy/#philosophy-of-instructor","title":"Philosophy of Instructor","text":"<p>NOTE: Philosophy behind Instructor was formulated by Jason Liu, the creator of original version of Instructor in Python and adapted for the PHP port.</p> <p>Instructor values simplicity and flexibility in leveraging language models. It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions.</p> <p>\u201cSimplicity is a great virtue, but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"packages/instructor/misc/philosophy/#simplicity","title":"Simplicity","text":"<ol> <li>Most users will only need to learn <code>responseModel</code> and <code>StructuredOutput::create()</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"packages/instructor/misc/philosophy/#transparency","title":"Transparency","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We give you config over the prompts we do write ('reasking' and in the future - JSON_MODE prompts).</li> </ol>"},{"location":"packages/instructor/misc/philosophy/#flexibility","title":"Flexibility","text":"<ol> <li>If you build a system with OpenAI directly, it is easy to incrementally adopt Instructor by just adding <code>StructuredOutput::create()</code> with data schemas fed in via <code>responseModel</code>.</li> <li>Use any class to define your data schema (no need to inherit from some base class).</li> </ol>"},{"location":"packages/instructor/misc/philosophy/#the-zen-of-instructor","title":"The zen of Instructor","text":"<p>Maintain the flexibility and power of PHP classes, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define data schema <code>class SomeStructuredData { ... }</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>function extract($input) : SomeStructuredData</code></li> <li>Define typed computations against your data with <code>function compute(SomeStructuredData $data):</code> or call methods on your schema <code>$data-&gt;compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"packages/instructor/misc/philosophy/#our-goals","title":"Our Goals","text":"<p>The goal for the library and documentation is to help you be a better programmer and, as a result, a better AI engineer.</p> <ul> <li>The library is a result of our desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>We won't try to write prompts for you,</li> <li>We don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit us up @jnxlco or @ddebowczyk</p> <p>Cheers!</p>"},{"location":"packages/instructor/techniques/classification/","title":"Classification","text":""},{"location":"packages/instructor/techniques/classification/#text-classification-using-llm","title":"Text Classification using LLM","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using LLM (via OpenAI API), PHP's <code>enums</code> and classes.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using language models in combination with PHP data structures.</p>"},{"location":"packages/instructor/techniques/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"packages/instructor/techniques/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>&lt;?php\n// Enumeration for single-label text classification. \nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction. \nclass SinglePrediction {\n    public Label $classLabel;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n/**\n * Perform single-label classification on the input text. \n */\nfunction classify(string $data) : SinglePrediction {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n        model: \"gpt-3.5-turbo-0613\",\n    )-&gt;get();\n}\n</code></pre>"},{"location":"packages/instructor/techniques/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\nassert($prediction-&gt;classLabel == Label::SPAM);\n</code></pre>"},{"location":"packages/instructor/techniques/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"packages/instructor/techniques/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass Ticket {\n    /** @var Label[] */\n    public array $ticketLabels = [];\n}\n</code></pre>"},{"location":"packages/instructor/techniques/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : Ticket {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify following support ticket: {$data}\",\n        ]],\n        responseModel: Ticket::class,\n        model: \"gpt-3.5-turbo-0613\",\n    )-&gt;get();\n}\n</code></pre>"},{"location":"packages/instructor/techniques/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;classLabels));\nassert(in_array(Label::BILLING, $prediction-&gt;classLabels));\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/","title":"Prompting","text":""},{"location":"packages/instructor/techniques/prompting/#general-tips-for-prompt-engineering","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor for function calling is to make the models self-descriptive, modular, and flexible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use PHPDoc comments or #[Description('')] attribute for clear field descriptions.</li> <li>Optionality: Use PHP's nullable types (e.g. ?int) for optional fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"packages/instructor/techniques/prompting/#utilize-nullable-attribute","title":"Utilize Nullable Attribute","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?Role $role = null; \n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?string $role = null;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $result = null;\n    public ?string $errorMessage = '';\n    public bool $error = false;\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;error ? null : $this-&gt;result;\n    }\n}\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in 'errorMessage'.</p> <p>Original Instructor implementation in Python provides utility class Maybe making this pattern even easier. Such mechanism is not yet available in PHP version of Instructor.</p>"},{"location":"packages/instructor/techniques/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>&lt;?php\nenum Role : string {\n    case Principal = 'principal'\n    case Teacher = 'teacher'\n    case Student = 'student'\n    case Other = 'other'\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /**  Correctly assign one of the predefined roles to the user. */\n    public Role $role;\n}\n</code></pre> <p>If you'd like to improve LLM inference performance, try reiterating the requirements in the field descriptions (in the docstrings).</p>"},{"location":"packages/instructor/techniques/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>&lt;?php\n/** Extract the role based on the following rules: &lt;your rules go here&gt; */\nclass Role\n{\n    /** Restate the instructions and rules to correctly determine the title. */\n    public string $instructions;\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>&lt;?php\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>&lt;?php\nclass Property\n{\n    /**  Monotonically increasing ID */\n    public string $index; \n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age\n    public string $name;\n    /** @var Property[] Numbered list of arbitrary extracted properties, should be less than 3 */\n    public array $properties;\n}\n</code></pre> <p>To be 100% certain the list does not exceed the limit add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"packages/instructor/techniques/prompting/#consistent-arbitrary-properties","title":"Consistent Arbitrary Properties","text":"<p>For multiple records containing arbitrary properties, instruct LLM to use consistent key names when extracting properties.</p> <pre><code>&lt;?php\nclass Property {\n    public int $id;\n    public string $key;\n    public string $name;\n}\n\nclass UserDetails\n{\n    /** @var UserDetail[] Extract information for multiple users. Use consistent key names for properties across users. */\n    public array $users;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> field:</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /** @var int[] Correct and complete list of coworker IDs, representing collaboration between users. */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /** @var UserDetail[] Collection of users, correctly capturing the relationships among them. */\n    public array $users;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>&lt;?php\nclass Role\n{\n    /** Think step by step to determine the correct title. */\n    public string $chainOfThought = '';\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>&lt;?php\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public int $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/prompting/#adding-context-to-components","title":"Adding Context to Components","text":"<p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>&lt;?php\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n</code></pre>"},{"location":"packages/instructor/techniques/search/","title":"Search","text":""},{"location":"packages/instructor/techniques/search/#expanding-search-queries","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p>"},{"location":"packages/instructor/techniques/search/#motivation","title":"Motivation","text":"<p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"packages/instructor/techniques/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>&lt;?php\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n</code></pre>"},{"location":"packages/instructor/techniques/search/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>StructuredOutput::create()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\StructuredOutput;\n\nfunction segment(string $data) : Search {\n    return (new StructuredOutput())-&gt;with(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    )-&gt;get();\n}\n\nforeach (segment(\"Search for a picture of a cat and a video of a dog\")-&gt;queries as $query) {\n    $query-&gt;execute();\n    // dump($query);\n}\n</code></pre>"},{"location":"packages/laravel/","title":"Instructor for Laravel","text":"<p>Laravel integration for Instructor PHP - the structured output library for LLMs.</p>"},{"location":"packages/laravel/#overview","title":"Overview","text":"<p>This package provides seamless integration between Instructor PHP and Laravel, giving you:</p> <ul> <li>Laravel Facades - Use <code>StructuredOutput::</code>, <code>Inference::</code>, <code>Embeddings::</code>, and <code>AgentCtrl::</code> facades</li> <li>Dependency Injection - Inject services directly into your classes</li> <li>Testing Fakes - Mock LLM responses with <code>StructuredOutput::fake()</code> and assertions</li> <li>Laravel HTTP Client - Uses <code>Http::</code> internally, enabling <code>Http::fake()</code> in tests</li> <li>Event Bridge - Instructor events dispatched to Laravel's event system</li> <li>Artisan Commands - Generate response models and test your configuration</li> <li>Configuration Publishing - Laravel-style config with environment variables</li> </ul>"},{"location":"packages/laravel/#quick-start","title":"Quick Start","text":""},{"location":"packages/laravel/#1-install-the-package","title":"1. Install the Package","text":"<pre><code>composer require cognesy/instructor-laravel\n</code></pre>"},{"location":"packages/laravel/#2-configure-api-key","title":"2. Configure API Key","text":"<p>Add to your <code>.env</code>:</p> <pre><code>OPENAI_API_KEY=your-openai-api-key\n</code></pre>"},{"location":"packages/laravel/#3-extract-structured-data","title":"3. Extract Structured Data","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\n// Define a response model\nfinal class PersonData\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly int $age,\n    ) {}\n}\n\n// Extract structured data from text\n$person = StructuredOutput::with(\n    messages: 'John Smith is 30 years old',\n    responseModel: PersonData::class,\n)-&gt;get();\n\necho $person-&gt;name; // \"John Smith\"\necho $person-&gt;age;  // 30\n</code></pre>"},{"location":"packages/laravel/#documentation","title":"Documentation","text":"Guide Description Installation Detailed installation and setup instructions Configuration Complete configuration reference Facades Using StructuredOutput, Inference, and Embeddings facades Response Models Creating and using response models Code Agents Using AgentCtrl for Claude Code, Codex, and OpenCode Testing Testing with fakes and assertions Events Event handling and Laravel integration Commands Artisan command reference Advanced Streaming, validation, and advanced patterns Troubleshooting Common issues and solutions"},{"location":"packages/laravel/#example-complete-workflow","title":"Example: Complete Workflow","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\nuse App\\ResponseModels\\InvoiceData;\n\nclass InvoiceProcessor\n{\n    public function extractFromEmail(string $emailBody): InvoiceData\n    {\n        return StructuredOutput::with(\n            messages: $emailBody,\n            responseModel: InvoiceData::class,\n            system: 'Extract invoice details from the email.',\n        )-&gt;get();\n    }\n}\n\n// In your test\npublic function test_extracts_invoice_data(): void\n{\n    $fake = StructuredOutput::fake([\n        InvoiceData::class =&gt; new InvoiceData(\n            invoiceNumber: 'INV-001',\n            amount: 150.00,\n            dueDate: '2024-12-31',\n        ),\n    ]);\n\n    $processor = new InvoiceProcessor();\n    $invoice = $processor-&gt;extractFromEmail('Invoice #INV-001...');\n\n    $this-&gt;assertEquals('INV-001', $invoice-&gt;invoiceNumber);\n    $fake-&gt;assertExtracted(InvoiceData::class);\n}\n</code></pre>"},{"location":"packages/laravel/#requirements","title":"Requirements","text":"<ul> <li>PHP 8.2+</li> <li>Laravel 10.x, 11.x, or 12.x</li> </ul>"},{"location":"packages/laravel/#support","title":"Support","text":"<ul> <li>GitHub Issues</li> <li>Documentation</li> </ul>"},{"location":"packages/laravel/advanced/","title":"Advanced Usage","text":"<p>This guide covers advanced patterns and features for power users.</p>"},{"location":"packages/laravel/advanced/#streaming","title":"Streaming","text":"<p>Stream responses for real-time updates:</p> <pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\n$stream = StructuredOutput::with(\n    messages: 'Extract detailed company information from this long document...',\n    responseModel: CompanyData::class,\n)-&gt;withStreaming()-&gt;stream();\n\n// Handle partial updates\nforeach ($stream-&gt;partials() as $partial) {\n    echo \"Company: \" . ($partial-&gt;name ?? 'Loading...') . \"\\n\";\n    echo \"Industry: \" . ($partial-&gt;industry ?? 'Loading...') . \"\\n\";\n    echo \"---\\n\";\n}\n\n// Get final complete result\n$company = $stream-&gt;final();\n</code></pre>"},{"location":"packages/laravel/advanced/#streaming-with-callbacks","title":"Streaming with Callbacks","text":"<pre><code>$result = StructuredOutput::with(\n    messages: 'Extract data...',\n    responseModel: MyModel::class,\n)\n-&gt;withStreaming()\n-&gt;onPartialUpdate(function ($partial) {\n    // Called on each partial update\n    broadcast(new PartialUpdateEvent($partial));\n})\n-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#streaming-sequences","title":"Streaming Sequences","text":"<p>For extracting multiple items:</p> <pre><code>$stream = StructuredOutput::with(\n    messages: 'Extract all products from this catalog...',\n    responseModel: [\n        'type' =&gt; 'array',\n        'items' =&gt; ProductData::class,\n    ],\n)\n-&gt;withStreaming()\n-&gt;onSequenceUpdate(function ($items) {\n    // Called as each item is completed\n    foreach ($items as $item) {\n        echo \"Found: {$item-&gt;name}\\n\";\n    }\n})\n-&gt;stream();\n</code></pre>"},{"location":"packages/laravel/advanced/#validation-and-retries","title":"Validation and Retries","text":""},{"location":"packages/laravel/advanced/#automatic-validation","title":"Automatic Validation","text":"<p>Response models are automatically validated. Invalid responses trigger retries:</p> <pre><code>use Symfony\\Component\\Validator\\Constraints as Assert;\n\nfinal class UserData\n{\n    public function __construct(\n        #[Assert\\NotBlank]\n        #[Assert\\Length(min: 2, max: 100)]\n        public readonly string $name,\n\n        #[Assert\\Email]\n        public readonly string $email,\n\n        #[Assert\\Range(min: 18, max: 120)]\n        public readonly int $age,\n    ) {}\n}\n\n// Extraction will retry if validation fails\n$user = StructuredOutput::with(\n    messages: 'Extract user from: john doe, email: invalid, age: 5',\n    responseModel: UserData::class,\n    maxRetries: 3,\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#custom-validators","title":"Custom Validators","text":"<pre><code>use Cognesy\\Instructor\\Validation\\Contracts\\CanValidateObject;\n\nclass BusinessRulesValidator implements CanValidateObject\n{\n    public function validate(object $object): array\n    {\n        $errors = [];\n\n        if ($object instanceof OrderData) {\n            if ($object-&gt;total &lt; $object-&gt;minimumOrderValue) {\n                $errors[] = \"Order total must be at least {$object-&gt;minimumOrderValue}\";\n            }\n        }\n\n        return $errors;\n    }\n}\n\n$order = StructuredOutput::with(\n    messages: 'Extract order...',\n    responseModel: OrderData::class,\n)\n-&gt;withValidators(BusinessRulesValidator::class)\n-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#custom-retry-prompt","title":"Custom Retry Prompt","text":"<pre><code>$result = StructuredOutput::with(\n    messages: 'Extract data...',\n    responseModel: MyModel::class,\n    maxRetries: 3,\n    retryPrompt: 'The extraction failed validation. Errors: {errors}. Please correct and try again.',\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#data-transformation","title":"Data Transformation","text":"<p>Transform data after extraction:</p> <pre><code>use Cognesy\\Instructor\\Transformation\\Contracts\\CanTransformData;\n\nclass NormalizePhoneNumbers implements CanTransformData\n{\n    public function transform(mixed $data): mixed\n    {\n        if ($data instanceof ContactData) {\n            $data-&gt;phone = $this-&gt;normalize($data-&gt;phone);\n        }\n        return $data;\n    }\n\n    private function normalize(string $phone): string\n    {\n        return preg_replace('/[^0-9+]/', '', $phone);\n    }\n}\n\n$contact = StructuredOutput::with(\n    messages: 'Contact: John, phone: (555) 123-4567',\n    responseModel: ContactData::class,\n)\n-&gt;withTransformers(NormalizePhoneNumbers::class)\n-&gt;get();\n\n// $contact-&gt;phone === '+15551234567'\n</code></pre>"},{"location":"packages/laravel/advanced/#output-modes","title":"Output Modes","text":"<p>Different LLMs support different output modes:</p> <pre><code>use Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// JSON Schema mode (recommended for OpenAI)\n$result = StructuredOutput::with(...)\n    -&gt;withOutputMode(OutputMode::JsonSchema)\n    -&gt;get();\n\n// Tool/Function calling mode\n$result = StructuredOutput::with(...)\n    -&gt;withOutputMode(OutputMode::Tools)\n    -&gt;get();\n\n// Simple JSON mode\n$result = StructuredOutput::with(...)\n    -&gt;withOutputMode(OutputMode::Json)\n    -&gt;get();\n\n// Markdown JSON (for Gemini)\n$result = StructuredOutput::with(...)\n    -&gt;withOutputMode(OutputMode::MdJson)\n    -&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#few-shot-learning","title":"Few-Shot Learning","text":"<p>Provide examples to improve extraction quality:</p> <pre><code>$person = StructuredOutput::with(\n    messages: 'Extract: Jane Doe, 25 years old, jane@example.com',\n    responseModel: PersonData::class,\n    examples: [\n        [\n            'input' =&gt; 'John Smith is 30 years old and works at john@company.com',\n            'output' =&gt; new PersonData(\n                name: 'John Smith',\n                age: 30,\n                email: 'john@company.com',\n            ),\n        ],\n        [\n            'input' =&gt; 'Mary Johnson, age 45',\n            'output' =&gt; new PersonData(\n                name: 'Mary Johnson',\n                age: 45,\n                email: null,\n            ),\n        ],\n    ],\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#system-prompts","title":"System Prompts","text":"<p>Customize the system prompt for specific domains:</p> <pre><code>$medical = StructuredOutput::with(\n    messages: $patientNotes,\n    responseModel: MedicalRecord::class,\n    system: &lt;&lt;&lt;'PROMPT'\n        You are a medical records extraction specialist.\n        Extract structured data from clinical notes.\n        Use standard medical terminology.\n        If information is unclear, mark as null rather than guessing.\n        PROMPT,\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#tool-descriptions","title":"Tool Descriptions","text":"<p>Customize how the response model is described to the LLM:</p> <pre><code>$result = StructuredOutput::with(\n    messages: 'Extract invoice details...',\n    responseModel: InvoiceData::class,\n    toolName: 'extract_invoice',\n    toolDescription: 'Extracts structured invoice data including line items, totals, and payment terms.',\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#multiple-providers","title":"Multiple Providers","text":"<p>Switch between providers based on task:</p> <pre><code>class AIService\n{\n    // Fast, cheap extraction for simple tasks\n    public function quickExtract(string $text, string $model): mixed\n    {\n        return StructuredOutput::using('groq')\n            -&gt;with(messages: $text, responseModel: $model)\n            -&gt;get();\n    }\n\n    // High-quality extraction for complex tasks\n    public function precisionExtract(string $text, string $model): mixed\n    {\n        return StructuredOutput::using('anthropic')\n            -&gt;withModel('claude-3-opus-20240229')\n            -&gt;with(messages: $text, responseModel: $model)\n            -&gt;get();\n    }\n\n    // Local extraction for sensitive data\n    public function privateExtract(string $text, string $model): mixed\n    {\n        return StructuredOutput::using('ollama')\n            -&gt;with(messages: $text, responseModel: $model)\n            -&gt;get();\n    }\n}\n</code></pre>"},{"location":"packages/laravel/advanced/#caching-strategies","title":"Caching Strategies","text":""},{"location":"packages/laravel/advanced/#response-caching","title":"Response Caching","text":"<pre><code>use Illuminate\\Support\\Facades\\Cache;\n\nclass CachedExtractor\n{\n    public function extract(string $text, string $responseModel): mixed\n    {\n        $cacheKey = 'extract:' . md5($text . $responseModel);\n\n        return Cache::remember($cacheKey, 3600, function () use ($text, $responseModel) {\n            return StructuredOutput::with(\n                messages: $text,\n                responseModel: $responseModel,\n            )-&gt;get();\n        });\n    }\n}\n</code></pre>"},{"location":"packages/laravel/advanced/#semantic-caching","title":"Semantic Caching","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\Embeddings;\n\nclass SemanticCache\n{\n    public function extractWithCache(string $text, string $responseModel): mixed\n    {\n        // Generate embedding for input\n        $embedding = Embeddings::withInputs($text)-&gt;first();\n\n        // Check for similar cached results\n        $cached = $this-&gt;findSimilar($embedding);\n        if ($cached) {\n            return $cached;\n        }\n\n        // Extract and cache\n        $result = StructuredOutput::with(\n            messages: $text,\n            responseModel: $responseModel,\n        )-&gt;get();\n\n        $this-&gt;store($embedding, $result);\n\n        return $result;\n    }\n}\n</code></pre>"},{"location":"packages/laravel/advanced/#batch-processing","title":"Batch Processing","text":"<p>Process multiple items efficiently:</p> <pre><code>use Illuminate\\Support\\Collection;\nuse Illuminate\\Support\\Facades\\Bus;\n\nclass BatchExtractor\n{\n    public function extractBatch(Collection $documents): Collection\n    {\n        return $documents-&gt;map(function ($document) {\n            return StructuredOutput::with(\n                messages: $document-&gt;content,\n                responseModel: DocumentData::class,\n            )-&gt;get();\n        });\n    }\n\n    // Or with queued jobs for large batches\n    public function extractBatchAsync(Collection $documents): void\n    {\n        $jobs = $documents-&gt;map(fn ($doc) =&gt; new ExtractDocumentJob($doc));\n\n        Bus::batch($jobs)\n            -&gt;name('Document Extraction')\n            -&gt;dispatch();\n    }\n}\n</code></pre>"},{"location":"packages/laravel/advanced/#error-handling","title":"Error Handling","text":""},{"location":"packages/laravel/advanced/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\nclass ResilientExtractor\n{\n    public function extract(string $text): ?PersonData\n    {\n        try {\n            return StructuredOutput::with(\n                messages: $text,\n                responseModel: PersonData::class,\n            )-&gt;get();\n        } catch (\\Throwable $e) {\n            Log::warning('Extraction failed', [\n                'error' =&gt; $e-&gt;getMessage(),\n                'text' =&gt; substr($text, 0, 100),\n            ]);\n\n            return null;\n        }\n    }\n}\n</code></pre>"},{"location":"packages/laravel/advanced/#fallback-providers","title":"Fallback Providers","text":"<pre><code>class FallbackExtractor\n{\n    private array $providers = ['openai', 'anthropic', 'groq'];\n\n    public function extract(string $text, string $model): mixed\n    {\n        foreach ($this-&gt;providers as $provider) {\n            try {\n                return StructuredOutput::using($provider)\n                    -&gt;with(messages: $text, responseModel: $model)\n                    -&gt;get();\n            } catch (\\Throwable $e) {\n                Log::warning(\"Provider {$provider} failed\", [\n                    'error' =&gt; $e-&gt;getMessage(),\n                ]);\n                continue;\n            }\n        }\n\n        throw new RuntimeException('All providers failed');\n    }\n}\n</code></pre>"},{"location":"packages/laravel/advanced/#performance-optimization","title":"Performance Optimization","text":""},{"location":"packages/laravel/advanced/#reduce-token-usage","title":"Reduce Token Usage","text":"<pre><code>// Be concise in system prompts\n$result = StructuredOutput::with(\n    messages: $text,\n    responseModel: MyModel::class,\n    system: 'Extract data. Be concise.', // Short system prompt\n)-&gt;get();\n\n// Use smaller models for simple extractions\n$result = StructuredOutput::withModel('gpt-4o-mini')\n    -&gt;with(messages: $text, responseModel: SimpleModel::class)\n    -&gt;get();\n</code></pre>"},{"location":"packages/laravel/advanced/#parallel-extraction","title":"Parallel Extraction","text":"<pre><code>use Illuminate\\Support\\Facades\\Concurrency;\n\n$results = Concurrency::run([\n    fn () =&gt; StructuredOutput::with(messages: $text1, responseModel: Model::class)-&gt;get(),\n    fn () =&gt; StructuredOutput::with(messages: $text2, responseModel: Model::class)-&gt;get(),\n    fn () =&gt; StructuredOutput::with(messages: $text3, responseModel: Model::class)-&gt;get(),\n]);\n</code></pre>"},{"location":"packages/laravel/agents/","title":"Code Agents","text":"<p>The <code>AgentCtrl</code> facade provides a unified interface for invoking CLI-based code agents that can execute code, modify files, and perform complex tasks.</p>"},{"location":"packages/laravel/agents/#supported-agents","title":"Supported Agents","text":"Agent Description Use Case Claude Code Anthropic's Claude agent with code execution General coding tasks, file modifications Codex OpenAI's Codex agent Code generation and completion OpenCode Multi-model code agent Research and coding with model flexibility"},{"location":"packages/laravel/agents/#quick-start","title":"Quick Start","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\n// Execute a task with Claude Code\n$response = AgentCtrl::claudeCode()\n    -&gt;execute('Generate a Laravel migration for a users table with name, email, and password fields');\n\n// Check if successful\nif ($response-&gt;isSuccess()) {\n    echo $response-&gt;text();\n}\n</code></pre>"},{"location":"packages/laravel/agents/#agent-selection","title":"Agent Selection","text":""},{"location":"packages/laravel/agents/#claude-code","title":"Claude Code","text":"<p>Best for general coding tasks with Anthropic's Claude models:</p> <pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\n$response = AgentCtrl::claudeCode()\n    -&gt;withModel('claude-opus-4-5')\n    -&gt;inDirectory(base_path())\n    -&gt;withTimeout(300)\n    -&gt;execute('Refactor the User model to use DTOs');\n\necho $response-&gt;text();\necho \"Session ID: \" . $response-&gt;sessionId;\n</code></pre>"},{"location":"packages/laravel/agents/#codex","title":"Codex","text":"<p>Best for OpenAI-powered code generation:</p> <pre><code>$response = AgentCtrl::codex()\n    -&gt;withModel('codex')\n    -&gt;execute('Write unit tests for the UserService class');\n</code></pre>"},{"location":"packages/laravel/agents/#opencode","title":"OpenCode","text":"<p>Best for multi-model flexibility:</p> <pre><code>$response = AgentCtrl::openCode()\n    -&gt;withModel('anthropic/claude-sonnet-4-5')\n    -&gt;execute('Analyze the codebase architecture');\n</code></pre>"},{"location":"packages/laravel/agents/#dynamic-selection","title":"Dynamic Selection","text":"<p>Select agent type at runtime:</p> <pre><code>use Cognesy\\Auxiliary\\Agents\\Enum\\AgentType;\n\n$agentType = AgentType::from(config('app.default_agent'));\n\n$response = AgentCtrl::make($agentType)\n    -&gt;execute('Generate API documentation');\n</code></pre>"},{"location":"packages/laravel/agents/#configuration","title":"Configuration","text":""},{"location":"packages/laravel/agents/#builder-methods","title":"Builder Methods","text":"<p>All agents support these configuration methods:</p> <pre><code>AgentCtrl::claudeCode()\n    -&gt;withModel('claude-opus-4-5')           // AI model to use\n    -&gt;withTimeout(300)                        // Execution timeout in seconds\n    -&gt;inDirectory('/path/to/project')         // Working directory\n    -&gt;withSandboxDriver(SandboxDriver::Host)  // Sandbox isolation\n    -&gt;withMaxRetries(3)                       // Retry on failure\n    -&gt;execute('Your prompt');\n</code></pre>"},{"location":"packages/laravel/agents/#laravel-configuration","title":"Laravel Configuration","text":"<p>Configure defaults in <code>config/instructor.php</code>:</p> <pre><code>'agents' =&gt; [\n    // Default timeout for all agents\n    'timeout' =&gt; env('INSTRUCTOR_AGENT_TIMEOUT', 300),\n\n    // Default working directory\n    'directory' =&gt; env('INSTRUCTOR_AGENT_DIRECTORY'),\n\n    // Default sandbox driver: host, docker, podman, firejail, bubblewrap\n    'sandbox' =&gt; env('INSTRUCTOR_AGENT_SANDBOX', 'host'),\n\n    // Claude Code specific\n    'claude_code' =&gt; [\n        'model' =&gt; env('CLAUDE_CODE_MODEL', 'claude-sonnet-4-20250514'),\n        'timeout' =&gt; env('CLAUDE_CODE_TIMEOUT'),\n        'directory' =&gt; env('CLAUDE_CODE_DIRECTORY'),\n        'sandbox' =&gt; env('CLAUDE_CODE_SANDBOX'),\n    ],\n\n    // Codex specific\n    'codex' =&gt; [\n        'model' =&gt; env('CODEX_MODEL', 'codex'),\n        'timeout' =&gt; env('CODEX_TIMEOUT'),\n    ],\n\n    // OpenCode specific\n    'opencode' =&gt; [\n        'model' =&gt; env('OPENCODE_MODEL', 'anthropic/claude-sonnet-4-5'),\n        'timeout' =&gt; env('OPENCODE_TIMEOUT'),\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/agents/#environment-variables","title":"Environment Variables","text":"<pre><code># Default agent settings\nINSTRUCTOR_AGENT_TIMEOUT=300\nINSTRUCTOR_AGENT_DIRECTORY=/path/to/project\nINSTRUCTOR_AGENT_SANDBOX=host\n\n# Claude Code\nCLAUDE_CODE_MODEL=claude-opus-4-5\n\n# Codex\nCODEX_MODEL=codex\n\n# OpenCode\nOPENCODE_MODEL=anthropic/claude-sonnet-4-5\n</code></pre>"},{"location":"packages/laravel/agents/#streaming","title":"Streaming","text":"<p>Process output in real-time with streaming callbacks:</p> <pre><code>$response = AgentCtrl::claudeCode()\n    -&gt;onText(function (string $text) {\n        // Called as text is generated\n        echo $text;\n    })\n    -&gt;onToolUse(function (string $tool, array $input, ?string $output) {\n        // Called when agent uses a tool\n        echo \"Tool: $tool\\n\";\n        echo \"Input: \" . json_encode($input) . \"\\n\";\n    })\n    -&gt;onComplete(function (AgentResponse $response) {\n        // Called when execution completes\n        echo \"\\nDone! Exit code: \" . $response-&gt;exitCode;\n    })\n    -&gt;executeStreaming('Generate a REST API for products');\n</code></pre>"},{"location":"packages/laravel/agents/#response-object","title":"Response Object","text":"<p>The <code>AgentResponse</code> object contains:</p> <pre><code>$response = AgentCtrl::claudeCode()-&gt;execute('...');\n\n// Main content\n$response-&gt;text();           // string - Generated text output\n$response-&gt;isSuccess();      // bool - True if exitCode is 0\n\n// Metadata\n$response-&gt;exitCode;         // int - Process exit code\n$response-&gt;sessionId;        // ?string - Session ID for resuming\n$response-&gt;agentType;        // AgentType - Which agent was used\n\n// Usage (when available)\n$response-&gt;usage;            // ?TokenUsage - Token statistics\n$response-&gt;usage-&gt;input;     // int - Input tokens\n$response-&gt;usage-&gt;output;    // int - Output tokens\n$response-&gt;usage-&gt;total();   // int - Total tokens\n\n// Cost (when available)\n$response-&gt;cost;             // ?float - Cost in USD\n\n// Tool calls\n$response-&gt;toolCalls;        // array&lt;ToolCall&gt; - Tools used\nforeach ($response-&gt;toolCalls as $call) {\n    $call-&gt;tool;             // string - Tool name\n    $call-&gt;input;            // array - Tool input\n    $call-&gt;output;           // ?string - Tool output\n    $call-&gt;isError;          // bool - If tool call failed\n}\n</code></pre>"},{"location":"packages/laravel/agents/#session-management","title":"Session Management","text":"<p>Resume previous sessions for continued work:</p> <pre><code>// First execution\n$response = AgentCtrl::claudeCode()\n    -&gt;execute('Start refactoring the User model');\n\n$sessionId = $response-&gt;sessionId;\n\n// Later: Resume the session\n$response = AgentCtrl::claudeCode()\n    -&gt;resumeSession($sessionId)\n    -&gt;execute('Continue with the Address model');\n</code></pre>"},{"location":"packages/laravel/agents/#error-handling","title":"Error Handling","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\ntry {\n    $response = AgentCtrl::claudeCode()\n        -&gt;withTimeout(60)\n        -&gt;execute($prompt);\n\n    if (!$response-&gt;isSuccess()) {\n        // Non-zero exit code\n        Log::error('Agent failed', [\n            'exit_code' =&gt; $response-&gt;exitCode,\n            'output' =&gt; $response-&gt;text(),\n        ]);\n        return null;\n    }\n\n    // Check for tool errors\n    foreach ($response-&gt;toolCalls as $call) {\n        if ($call-&gt;isError) {\n            Log::warning('Tool error', [\n                'tool' =&gt; $call-&gt;tool,\n                'error' =&gt; $call-&gt;output,\n            ]);\n        }\n    }\n\n    return $response-&gt;text();\n\n} catch (\\Throwable $e) {\n    // Timeout, sandbox error, etc.\n    Log::error('Agent exception', ['error' =&gt; $e-&gt;getMessage()]);\n    return null;\n}\n</code></pre>"},{"location":"packages/laravel/agents/#testing","title":"Testing","text":"<p>Use <code>AgentCtrl::fake()</code> for testing without actual agent execution:</p> <pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\ntest('generates migration code', function () {\n    // Setup fake with expected responses\n    $fake = AgentCtrl::fake([\n        'Generated migration file: 2024_01_01_create_users_table.php',\n    ]);\n\n    // Execute code under test\n    $result = app(MigrationGenerator::class)-&gt;generate('users');\n\n    // Assertions\n    $fake-&gt;assertExecuted();\n    $fake-&gt;assertExecutedTimes(1);\n    $fake-&gt;assertUsedClaudeCode();\n    $fake-&gt;assertExecutedWith('migration');\n\n    expect($result)-&gt;toContain('users_table');\n});\n\ntest('handles multiple agent calls', function () {\n    $fake = AgentCtrl::fake([\n        'First response',\n        'Second response',\n        'Third response',\n    ]);\n\n    // Multiple calls return responses in sequence\n    $first = AgentCtrl::claudeCode()-&gt;execute('First');\n    $second = AgentCtrl::claudeCode()-&gt;execute('Second');\n\n    expect($first-&gt;text())-&gt;toBe('First response');\n    expect($second-&gt;text())-&gt;toBe('Second response');\n\n    $fake-&gt;assertExecutedTimes(2);\n});\n\ntest('can create custom fake responses', function () {\n    use Cognesy\\Auxiliary\\Agents\\Enum\\AgentType;\n    use Cognesy\\Instructor\\Laravel\\Testing\\AgentCtrlFake;\n\n    $customResponse = AgentCtrlFake::response(\n        text: 'Custom output',\n        exitCode: 0,\n        agentType: AgentType::ClaudeCode,\n        cost: 0.05,\n    );\n\n    $fake = AgentCtrl::fake([$customResponse]);\n\n    $response = AgentCtrl::claudeCode()-&gt;execute('Test');\n\n    expect($response-&gt;cost)-&gt;toBe(0.05);\n});\n</code></pre>"},{"location":"packages/laravel/agents/#available-assertions","title":"Available Assertions","text":"Method Description <code>assertExecuted()</code> Agent was executed at least once <code>assertNotExecuted()</code> Agent was never executed <code>assertExecutedTimes(n)</code> Agent was executed exactly n times <code>assertExecutedWith(prompt)</code> Prompt contains specific text <code>assertAgentType(type)</code> Specific agent type was used <code>assertUsedClaudeCode()</code> Claude Code agent was used <code>assertUsedCodex()</code> Codex agent was used <code>assertUsedOpenCode()</code> OpenCode agent was used <code>assertStreaming()</code> Streaming execution was used <code>getExecutions()</code> Get all recorded executions <code>reset()</code> Reset fake state"},{"location":"packages/laravel/agents/#real-world-examples","title":"Real-World Examples","text":""},{"location":"packages/laravel/agents/#code-generation-service","title":"Code Generation Service","text":"<pre><code>namespace App\\Services;\n\nuse Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\nclass CodeGenerationService\n{\n    public function generateMigration(array $schema): string\n    {\n        $prompt = $this-&gt;buildMigrationPrompt($schema);\n\n        $response = AgentCtrl::claudeCode()\n            -&gt;inDirectory(database_path('migrations'))\n            -&gt;execute($prompt);\n\n        if (!$response-&gt;isSuccess()) {\n            throw new \\RuntimeException('Failed to generate migration');\n        }\n\n        return $response-&gt;text();\n    }\n\n    public function generateTest(string $className): string\n    {\n        $response = AgentCtrl::claudeCode()\n            -&gt;inDirectory(base_path('tests'))\n            -&gt;execute(\"Generate comprehensive tests for: $className\");\n\n        return $response-&gt;text();\n    }\n\n    private function buildMigrationPrompt(array $schema): string\n    {\n        return \"Generate a Laravel migration for:\\n\" . json_encode($schema, JSON_PRETTY_PRINT);\n    }\n}\n</code></pre>"},{"location":"packages/laravel/agents/#queued-code-generation","title":"Queued Code Generation","text":"<pre><code>namespace App\\Jobs;\n\nuse Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\nuse Illuminate\\Bus\\Queueable;\nuse Illuminate\\Contracts\\Queue\\ShouldQueue;\n\nclass GenerateCodeJob implements ShouldQueue\n{\n    use Queueable;\n\n    public function __construct(\n        public string $prompt,\n        public string $outputPath,\n    ) {}\n\n    public function handle(): void\n    {\n        $response = AgentCtrl::claudeCode()\n            -&gt;withTimeout(600) // 10 minutes for complex tasks\n            -&gt;inDirectory(dirname($this-&gt;outputPath))\n            -&gt;execute($this-&gt;prompt);\n\n        if ($response-&gt;isSuccess()) {\n            file_put_contents($this-&gt;outputPath, $response-&gt;text());\n        }\n    }\n}\n\n// Usage\nGenerateCodeJob::dispatch(\n    'Generate a complete CRUD controller for Products',\n    app_path('Http/Controllers/ProductController.php')\n);\n</code></pre>"},{"location":"packages/laravel/agents/#interactive-code-review","title":"Interactive Code Review","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\nuse Illuminate\\Support\\Facades\\Log;\n\nclass CodeReviewer\n{\n    public function review(string $filePath, callable $onProgress = null): array\n    {\n        $builder = AgentCtrl::claudeCode()\n            -&gt;inDirectory(dirname($filePath));\n\n        if ($onProgress) {\n            $builder-&gt;onText($onProgress);\n        }\n\n        $response = $builder-&gt;execute(\n            \"Review this file for bugs, security issues, and improvements: $filePath\"\n        );\n\n        return [\n            'review' =&gt; $response-&gt;text(),\n            'success' =&gt; $response-&gt;isSuccess(),\n            'session' =&gt; $response-&gt;sessionId,\n        ];\n    }\n}\n</code></pre>"},{"location":"packages/laravel/agents/#sandbox-drivers","title":"Sandbox Drivers","text":"<p>Control agent execution isolation:</p> Driver Description Use Case <code>host</code> Direct execution (no isolation) Development, trusted environments <code>docker</code> Docker container isolation Production, untrusted code <code>podman</code> Podman container isolation Rootless containers <code>firejail</code> Linux sandbox Lightweight isolation <code>bubblewrap</code> Minimal sandbox CI/CD environments <pre><code>use Cognesy\\Auxiliary\\Agents\\Common\\Enum\\SandboxDriver;\n\n// Development (direct execution)\nAgentCtrl::claudeCode()\n    -&gt;withSandboxDriver(SandboxDriver::Host)\n    -&gt;execute('...');\n\n// Production (Docker isolation)\nAgentCtrl::claudeCode()\n    -&gt;withSandboxDriver(SandboxDriver::Docker)\n    -&gt;execute('...');\n</code></pre>"},{"location":"packages/laravel/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Set Timeouts: Always set appropriate timeouts for your use case</li> <li>Use Sandbox: In production, use Docker or other sandbox drivers</li> <li>Handle Errors: Check <code>isSuccess()</code> and handle failures gracefully</li> <li>Log Sessions: Store session IDs for debugging and continuation</li> <li>Test with Fakes: Use <code>AgentCtrl::fake()</code> in tests to avoid API calls</li> <li>Queue Long Tasks: Use Laravel queues for time-consuming operations</li> </ol>"},{"location":"packages/laravel/commands/","title":"Artisan Commands","text":"<p>The package provides several Artisan commands to help with development and testing.</p>"},{"location":"packages/laravel/commands/#instructorinstall","title":"instructor:install","text":"<p>Installs and configures the Instructor package.</p> <pre><code>php artisan instructor:install\n</code></pre>"},{"location":"packages/laravel/commands/#what-it-does","title":"What It Does","text":"<ol> <li>Publishes the configuration file (<code>config/instructor.php</code>)</li> <li>Checks for API key configuration in <code>.env</code></li> <li>Shows next steps for getting started</li> </ol>"},{"location":"packages/laravel/commands/#options","title":"Options","text":"Option Description <code>--force</code> Overwrite existing configuration files"},{"location":"packages/laravel/commands/#example-output","title":"Example Output","text":"<pre><code>Installing Instructor for Laravel...\n\nPublishing configuration... done\nChecking API key configuration... \u2713\n\nNext steps:\n\n  1. Configure your API keys in .env:\n     OPENAI_API_KEY=your-key-here\n\n  2. Create a response model:\n     php artisan make:response-model PersonData\n\n  3. Extract structured data:\n     $person = StructuredOutput::with(\n         messages: \"John is 30 years old\",\n         responseModel: PersonData::class,\n     )-&gt;get();\n\n  4. Test your installation:\n     php artisan instructor:test\n\nInstructor installed successfully!\n</code></pre>"},{"location":"packages/laravel/commands/#instructortest","title":"instructor:test","text":"<p>Tests your Instructor installation and API configuration.</p> <pre><code>php artisan instructor:test\n</code></pre>"},{"location":"packages/laravel/commands/#what-it-does_1","title":"What It Does","text":"<ol> <li>Displays current configuration</li> <li>Makes a test API call</li> <li>Verifies the response</li> </ol>"},{"location":"packages/laravel/commands/#options_1","title":"Options","text":"Option Description <code>--preset=</code> Test a specific connection preset <code>--inference</code> Test raw inference instead of structured output"},{"location":"packages/laravel/commands/#examples","title":"Examples","text":"<pre><code># Test default connection\nphp artisan instructor:test\n\n# Test specific connection\nphp artisan instructor:test --preset=anthropic\n\n# Test raw inference\nphp artisan instructor:test --inference\n</code></pre>"},{"location":"packages/laravel/commands/#example-output_1","title":"Example Output","text":"<pre><code>Testing Instructor installation...\n\nPreset ............................................. openai\nDriver ............................................. openai\nModel .............................................. gpt-4o-mini\nAPI Key ............................................ sk-...abc \u2713\n\nTesting structured output extraction... \u2713\n\nStructured output test completed!\n</code></pre>"},{"location":"packages/laravel/commands/#makeresponse-model","title":"make:response-model","text":"<p>Generates a new response model class.</p> <pre><code>php artisan make:response-model {name}\n</code></pre>"},{"location":"packages/laravel/commands/#arguments","title":"Arguments","text":"Argument Description <code>name</code> The name of the response model class"},{"location":"packages/laravel/commands/#options_2","title":"Options","text":"Option Description <code>--collection</code>, <code>-c</code> Create a collection response model <code>--nested</code>, <code>-n</code> Create a nested objects response model <code>--description=</code>, <code>-d</code> Set the class description <code>--force</code>, <code>-f</code> Overwrite existing file"},{"location":"packages/laravel/commands/#examples_1","title":"Examples","text":""},{"location":"packages/laravel/commands/#basic-response-model","title":"Basic Response Model","text":"<pre><code>php artisan make:response-model PersonData\n</code></pre> <p>Creates <code>app/ResponseModels/PersonData.php</code>:</p> <pre><code>&lt;?php\n\ndeclare(strict_types=1);\n\nnamespace App\\ResponseModels;\n\n/**\n * TODO: Add description for this response model\n */\nfinal class PersonData\n{\n    public function __construct(\n        /** The name of the person */\n        public readonly string $name,\n\n        /** The age of the person in years */\n        public readonly int $age,\n\n        /** Optional email address */\n        public readonly ?string $email = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/commands/#collection-response-model","title":"Collection Response Model","text":"<pre><code>php artisan make:response-model ProductList --collection\n</code></pre> <p>Creates a model with an array of items:</p> <pre><code>final class ProductList\n{\n    public function __construct(\n        /** List of extracted items */\n        public readonly array $items,\n    ) {}\n}\n\nfinal class ProductListItem\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly ?string $description = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/commands/#nested-objects-response-model","title":"Nested Objects Response Model","text":"<pre><code>php artisan make:response-model CompanyProfile --nested\n</code></pre> <p>Creates a model with nested objects:</p> <pre><code>final class CompanyProfile\n{\n    public function __construct(\n        public readonly string $title,\n        public readonly CompanyProfileContact $contact,\n        public readonly ?CompanyProfileAddress $address = null,\n    ) {}\n}\n\nfinal class CompanyProfileContact\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly string $email,\n        public readonly ?string $phone = null,\n    ) {}\n}\n\nfinal class CompanyProfileAddress\n{\n    public function __construct(\n        public readonly string $street,\n        public readonly string $city,\n        public readonly string $country,\n        public readonly ?string $postalCode = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/commands/#with-description","title":"With Description","text":"<pre><code>php artisan make:response-model Invoice --description=\"Represents an invoice extracted from PDF documents\"\n</code></pre> <p>Creates a model with the specified description in the docblock.</p>"},{"location":"packages/laravel/commands/#customizing-stubs","title":"Customizing Stubs","text":"<p>Publish the stubs to customize them:</p> <pre><code>php artisan vendor:publish --tag=instructor-stubs\n</code></pre> <p>This copies stubs to <code>stubs/instructor/</code>:</p> <pre><code>stubs/instructor/\n\u251c\u2500\u2500 response-model.stub\n\u251c\u2500\u2500 response-model.collection.stub\n\u2514\u2500\u2500 response-model.nested.stub\n</code></pre> <p>Edit these files to customize generated response models.</p>"},{"location":"packages/laravel/commands/#stub-placeholders","title":"Stub Placeholders","text":"Placeholder Description <code>{{ namespace }}</code> The class namespace <code>{{ class }}</code> The class name <code>{{ description }}</code> The class description"},{"location":"packages/laravel/commands/#creating-custom-commands","title":"Creating Custom Commands","text":"<p>Extend the package commands for your specific needs:</p> <pre><code>// app/Console/Commands/ExtractInvoiceCommand.php\nnamespace App\\Console\\Commands;\n\nuse App\\ResponseModels\\InvoiceData;\nuse Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\nuse Illuminate\\Console\\Command;\n\nclass ExtractInvoiceCommand extends Command\n{\n    protected $signature = 'invoice:extract {file}';\n    protected $description = 'Extract invoice data from a file';\n\n    public function handle(): int\n    {\n        $content = file_get_contents($this-&gt;argument('file'));\n\n        $invoice = StructuredOutput::with(\n            messages: $content,\n            responseModel: InvoiceData::class,\n        )-&gt;get();\n\n        $this-&gt;info(\"Invoice Number: {$invoice-&gt;number}\");\n        $this-&gt;info(\"Amount: \\${$invoice-&gt;amount}\");\n        $this-&gt;info(\"Due Date: {$invoice-&gt;dueDate}\");\n\n        return self::SUCCESS;\n    }\n}\n</code></pre>"},{"location":"packages/laravel/commands/#command-reference","title":"Command Reference","text":"Command Description <code>instructor:install</code> Install and configure the package <code>instructor:test</code> Test API configuration <code>make:response-model</code> Generate a response model class"},{"location":"packages/laravel/configuration/","title":"Configuration","text":"<p>After publishing the configuration file, you'll find it at <code>config/instructor.php</code>.</p>"},{"location":"packages/laravel/configuration/#default-connection","title":"Default Connection","text":"<pre><code>'default' =&gt; env('INSTRUCTOR_CONNECTION', 'openai'),\n</code></pre> <p>Set the default LLM connection. Can be overridden at runtime with <code>-&gt;using('connection')</code>.</p>"},{"location":"packages/laravel/configuration/#connections","title":"Connections","text":"<p>Configure multiple LLM provider connections:</p> <pre><code>'connections' =&gt; [\n    'openai' =&gt; [\n        'driver' =&gt; 'openai',\n        'api_url' =&gt; env('OPENAI_API_URL', 'https://api.openai.com/v1'),\n        'api_key' =&gt; env('OPENAI_API_KEY'),\n        'organization' =&gt; env('OPENAI_ORGANIZATION'),\n        'model' =&gt; env('OPENAI_MODEL', 'gpt-4o-mini'),\n        'max_tokens' =&gt; env('OPENAI_MAX_TOKENS', 4096),\n    ],\n\n    'anthropic' =&gt; [\n        'driver' =&gt; 'anthropic',\n        'api_url' =&gt; env('ANTHROPIC_API_URL', 'https://api.anthropic.com/v1'),\n        'api_key' =&gt; env('ANTHROPIC_API_KEY'),\n        'model' =&gt; env('ANTHROPIC_MODEL', 'claude-sonnet-4-20250514'),\n        'max_tokens' =&gt; env('ANTHROPIC_MAX_TOKENS', 4096),\n    ],\n\n    'azure' =&gt; [\n        'driver' =&gt; 'azure',\n        'api_key' =&gt; env('AZURE_OPENAI_API_KEY'),\n        'resource_name' =&gt; env('AZURE_OPENAI_RESOURCE'),\n        'deployment_id' =&gt; env('AZURE_OPENAI_DEPLOYMENT'),\n        'api_version' =&gt; env('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n        'model' =&gt; env('AZURE_OPENAI_MODEL', 'gpt-4o-mini'),\n        'max_tokens' =&gt; env('AZURE_OPENAI_MAX_TOKENS', 4096),\n    ],\n\n    'gemini' =&gt; [\n        'driver' =&gt; 'gemini',\n        'api_url' =&gt; env('GEMINI_API_URL', 'https://generativelanguage.googleapis.com/v1beta'),\n        'api_key' =&gt; env('GEMINI_API_KEY'),\n        'model' =&gt; env('GEMINI_MODEL', 'gemini-2.0-flash'),\n        'max_tokens' =&gt; env('GEMINI_MAX_TOKENS', 4096),\n    ],\n\n    'ollama' =&gt; [\n        'driver' =&gt; 'ollama',\n        'api_url' =&gt; env('OLLAMA_API_URL', 'http://localhost:11434/v1'),\n        'api_key' =&gt; env('OLLAMA_API_KEY', 'ollama'),\n        'model' =&gt; env('OLLAMA_MODEL', 'llama3.2'),\n        'max_tokens' =&gt; env('OLLAMA_MAX_TOKENS', 4096),\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#supported-drivers","title":"Supported Drivers","text":"Driver Provider Description <code>openai</code> OpenAI GPT-4, GPT-4o, GPT-4o-mini <code>anthropic</code> Anthropic Claude 3, Claude 3.5, Claude 4 <code>azure</code> Azure OpenAI Azure-hosted OpenAI models <code>gemini</code> Google Gemini 1.5, Gemini 2.0 <code>mistral</code> Mistral AI Mistral, Mixtral models <code>groq</code> Groq Fast inference with Llama, Mixtral <code>cohere</code> Cohere Command models <code>deepseek</code> DeepSeek DeepSeek models <code>ollama</code> Ollama Local open-source models <code>perplexity</code> Perplexity Perplexity models"},{"location":"packages/laravel/configuration/#adding-a-custom-connection","title":"Adding a Custom Connection","text":"<pre><code>'connections' =&gt; [\n    // ... existing connections\n\n    'my-custom' =&gt; [\n        'driver' =&gt; 'openai', // Use OpenAI-compatible API\n        'api_url' =&gt; 'https://my-custom-api.com/v1',\n        'api_key' =&gt; env('MY_CUSTOM_API_KEY'),\n        'model' =&gt; 'custom-model',\n        'max_tokens' =&gt; 4096,\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#embeddings-connections","title":"Embeddings Connections","text":"<p>Configure embedding model connections:</p> <pre><code>'embeddings' =&gt; [\n    'default' =&gt; env('INSTRUCTOR_EMBEDDINGS_CONNECTION', 'openai'),\n\n    'connections' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'api_url' =&gt; env('OPENAI_API_URL', 'https://api.openai.com/v1'),\n            'api_key' =&gt; env('OPENAI_API_KEY'),\n            'model' =&gt; env('OPENAI_EMBEDDINGS_MODEL', 'text-embedding-3-small'),\n            'dimensions' =&gt; env('OPENAI_EMBEDDINGS_DIMENSIONS', 1536),\n        ],\n\n        'ollama' =&gt; [\n            'driver' =&gt; 'ollama',\n            'api_url' =&gt; env('OLLAMA_API_URL', 'http://localhost:11434/v1'),\n            'api_key' =&gt; env('OLLAMA_API_KEY', 'ollama'),\n            'model' =&gt; env('OLLAMA_EMBEDDINGS_MODEL', 'nomic-embed-text'),\n            'dimensions' =&gt; env('OLLAMA_EMBEDDINGS_DIMENSIONS', 768),\n        ],\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#extraction-settings","title":"Extraction Settings","text":"<p>Configure structured output extraction defaults:</p> <pre><code>'extraction' =&gt; [\n    // Output mode: json_schema, json, tools, md_json\n    'output_mode' =&gt; env('INSTRUCTOR_OUTPUT_MODE', 'json_schema'),\n\n    // Maximum retry attempts when validation fails\n    'max_retries' =&gt; env('INSTRUCTOR_MAX_RETRIES', 2),\n\n    // Prompt template for retry attempts\n    'retry_prompt' =&gt; 'The response did not pass validation. Please fix the following errors and try again: {errors}',\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#output-modes","title":"Output Modes","text":"Mode Description Best For <code>json_schema</code> Uses JSON Schema for structured output Most reliable, OpenAI recommended <code>json</code> Simple JSON mode Fallback for unsupported models <code>tools</code> Uses tool/function calling Alternative structured output <code>md_json</code> Markdown-wrapped JSON Gemini and other models"},{"location":"packages/laravel/configuration/#http-client-settings","title":"HTTP Client Settings","text":"<p>Configure the HTTP client:</p> <pre><code>'http' =&gt; [\n    // Driver: 'laravel' uses Laravel's HTTP client (enables Http::fake())\n    'driver' =&gt; env('INSTRUCTOR_HTTP_DRIVER', 'laravel'),\n\n    // Request timeout in seconds\n    'timeout' =&gt; env('INSTRUCTOR_HTTP_TIMEOUT', 120),\n\n    // Connection timeout in seconds\n    'connect_timeout' =&gt; env('INSTRUCTOR_HTTP_CONNECT_TIMEOUT', 30),\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#logging-settings","title":"Logging Settings","text":"<p>Configure logging:</p> <pre><code>'logging' =&gt; [\n    // Enable/disable logging\n    'enabled' =&gt; env('INSTRUCTOR_LOGGING_ENABLED', true),\n\n    // Log channel\n    'channel' =&gt; env('INSTRUCTOR_LOG_CHANNEL', 'stack'),\n\n    // Minimum log level\n    'level' =&gt; env('INSTRUCTOR_LOG_LEVEL', 'debug'),\n\n    // Logging preset: default, production, or custom\n    'preset' =&gt; env('INSTRUCTOR_LOGGING_PRESET', 'default'),\n\n    // Events to exclude from logging\n    'exclude_events' =&gt; [\n        // Cognesy\\Http\\Events\\DebugRequestBodyUsed::class,\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#logging-presets","title":"Logging Presets","text":"Preset Description <code>default</code> Full logging with request/response details <code>production</code> Minimal logging, no sensitive data <code>custom</code> Define your own pipeline"},{"location":"packages/laravel/configuration/#events-settings","title":"Events Settings","text":"<p>Configure event dispatching:</p> <pre><code>'events' =&gt; [\n    // Bridge Instructor events to Laravel's event dispatcher\n    'dispatch_to_laravel' =&gt; env('INSTRUCTOR_DISPATCH_EVENTS', true),\n\n    // Specific events to bridge (empty = all events)\n    'bridge_events' =&gt; [\n        // \\Cognesy\\Instructor\\Events\\ExtractionComplete::class,\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#cache-settings","title":"Cache Settings","text":"<p>Configure response caching:</p> <pre><code>'cache' =&gt; [\n    // Enable response caching\n    'enabled' =&gt; env('INSTRUCTOR_CACHE_ENABLED', false),\n\n    // Cache store to use\n    'store' =&gt; env('INSTRUCTOR_CACHE_STORE'),\n\n    // Default TTL in seconds\n    'ttl' =&gt; env('INSTRUCTOR_CACHE_TTL', 3600),\n\n    // Cache key prefix\n    'prefix' =&gt; 'instructor',\n],\n</code></pre>"},{"location":"packages/laravel/configuration/#environment-variables-reference","title":"Environment Variables Reference","text":"Variable Default Description <code>INSTRUCTOR_CONNECTION</code> <code>openai</code> Default LLM connection <code>INSTRUCTOR_OUTPUT_MODE</code> <code>json_schema</code> Output mode for extraction <code>INSTRUCTOR_MAX_RETRIES</code> <code>2</code> Max validation retry attempts <code>INSTRUCTOR_HTTP_DRIVER</code> <code>laravel</code> HTTP client driver <code>INSTRUCTOR_HTTP_TIMEOUT</code> <code>120</code> Request timeout (seconds) <code>INSTRUCTOR_LOGGING_ENABLED</code> <code>true</code> Enable logging <code>INSTRUCTOR_LOG_CHANNEL</code> <code>stack</code> Laravel log channel <code>INSTRUCTOR_DISPATCH_EVENTS</code> <code>true</code> Bridge events to Laravel <code>INSTRUCTOR_CACHE_ENABLED</code> <code>false</code> Enable response caching <code>OPENAI_API_KEY</code> - OpenAI API key <code>ANTHROPIC_API_KEY</code> - Anthropic API key"},{"location":"packages/laravel/configuration/#runtime-configuration","title":"Runtime Configuration","text":"<p>Override configuration at runtime:</p> <pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\n$result = StructuredOutput::using('anthropic')  // Switch connection\n    -&gt;withModel('claude-3-opus-20240229')        // Override model\n    -&gt;withMaxRetries(5)                          // Override retries\n    -&gt;with(\n        messages: 'Extract data...',\n        responseModel: MyModel::class,\n    )\n    -&gt;get();\n</code></pre>"},{"location":"packages/laravel/events/","title":"Events","text":"<p>Instructor dispatches events throughout the extraction lifecycle. These events are bridged to Laravel's event system, allowing you to listen and respond using standard Laravel patterns.</p>"},{"location":"packages/laravel/events/#event-bridge-configuration","title":"Event Bridge Configuration","text":"<p>Configure event bridging in <code>config/instructor.php</code>:</p> <pre><code>'events' =&gt; [\n    // Enable bridging to Laravel's event dispatcher\n    'dispatch_to_laravel' =&gt; env('INSTRUCTOR_DISPATCH_EVENTS', true),\n\n    // Specify which events to bridge (empty = all events)\n    'bridge_events' =&gt; [\n        // Only bridge specific events\n        \\Cognesy\\Instructor\\Events\\ExtractionComplete::class,\n        \\Cognesy\\Instructor\\Events\\ExtractionFailed::class,\n    ],\n],\n</code></pre>"},{"location":"packages/laravel/events/#available-events","title":"Available Events","text":""},{"location":"packages/laravel/events/#extraction-events","title":"Extraction Events","text":"Event Description <code>ExtractionStarted</code> Extraction process has begun <code>ExtractionComplete</code> Extraction completed successfully <code>ExtractionFailed</code> Extraction failed with an error <code>ValidationFailed</code> Response failed validation <code>RetryAttempted</code> A retry attempt is being made"},{"location":"packages/laravel/events/#inference-events","title":"Inference Events","text":"Event Description <code>InferenceRequested</code> LLM API call initiated <code>InferenceComplete</code> LLM API call completed <code>InferenceFailed</code> LLM API call failed"},{"location":"packages/laravel/events/#streaming-events","title":"Streaming Events","text":"Event Description <code>StreamStarted</code> Streaming response started <code>StreamChunkReceived</code> Received a chunk of streaming data <code>StreamComplete</code> Streaming completed"},{"location":"packages/laravel/events/#listening-to-events","title":"Listening to Events","text":""},{"location":"packages/laravel/events/#using-event-listeners","title":"Using Event Listeners","text":"<p>Create a listener:</p> <pre><code>// app/Listeners/LogExtractionComplete.php\nnamespace App\\Listeners;\n\nuse Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Illuminate\\Support\\Facades\\Log;\n\nclass LogExtractionComplete\n{\n    public function handle(ExtractionComplete $event): void\n    {\n        Log::info('Extraction completed', [\n            'response_model' =&gt; $event-&gt;responseModel,\n            'duration_ms' =&gt; $event-&gt;durationMs,\n        ]);\n    }\n}\n</code></pre> <p>Register in <code>EventServiceProvider</code>:</p> <pre><code>// app/Providers/EventServiceProvider.php\nnamespace App\\Providers;\n\nuse App\\Listeners\\LogExtractionComplete;\nuse Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Illuminate\\Foundation\\Support\\Providers\\EventServiceProvider as ServiceProvider;\n\nclass EventServiceProvider extends ServiceProvider\n{\n    protected $listen = [\n        ExtractionComplete::class =&gt; [\n            LogExtractionComplete::class,\n        ],\n    ];\n}\n</code></pre>"},{"location":"packages/laravel/events/#using-closures","title":"Using Closures","text":"<p>Register listeners in a service provider:</p> <pre><code>// app/Providers/AppServiceProvider.php\nuse Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Cognesy\\Instructor\\Events\\ExtractionFailed;\nuse Illuminate\\Support\\Facades\\Event;\n\npublic function boot(): void\n{\n    Event::listen(ExtractionComplete::class, function ($event) {\n        // Handle successful extraction\n    });\n\n    Event::listen(ExtractionFailed::class, function ($event) {\n        // Handle failed extraction\n    });\n}\n</code></pre>"},{"location":"packages/laravel/events/#using-event-subscribers","title":"Using Event Subscribers","text":"<pre><code>// app/Listeners/InstructorEventSubscriber.php\nnamespace App\\Listeners;\n\nuse Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Cognesy\\Instructor\\Events\\ExtractionFailed;\nuse Cognesy\\Instructor\\Events\\ExtractionStarted;\nuse Illuminate\\Events\\Dispatcher;\n\nclass InstructorEventSubscriber\n{\n    public function handleStart(ExtractionStarted $event): void\n    {\n        // Log start\n    }\n\n    public function handleComplete(ExtractionComplete $event): void\n    {\n        // Log completion\n    }\n\n    public function handleFailed(ExtractionFailed $event): void\n    {\n        // Alert on failure\n    }\n\n    public function subscribe(Dispatcher $events): array\n    {\n        return [\n            ExtractionStarted::class =&gt; 'handleStart',\n            ExtractionComplete::class =&gt; 'handleComplete',\n            ExtractionFailed::class =&gt; 'handleFailed',\n        ];\n    }\n}\n\n// Register in EventServiceProvider\nprotected $subscribe = [\n    InstructorEventSubscriber::class,\n];\n</code></pre>"},{"location":"packages/laravel/events/#common-use-cases","title":"Common Use Cases","text":""},{"location":"packages/laravel/events/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code>use Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Cognesy\\Instructor\\Events\\ExtractionFailed;\nuse Illuminate\\Support\\Facades\\Log;\n\nEvent::listen(ExtractionComplete::class, function ($event) {\n    Log::channel('llm')-&gt;info('Extraction successful', [\n        'model' =&gt; $event-&gt;model,\n        'tokens_used' =&gt; $event-&gt;tokensUsed,\n        'duration_ms' =&gt; $event-&gt;durationMs,\n    ]);\n});\n\nEvent::listen(ExtractionFailed::class, function ($event) {\n    Log::channel('llm')-&gt;error('Extraction failed', [\n        'error' =&gt; $event-&gt;error-&gt;getMessage(),\n        'model' =&gt; $event-&gt;model,\n    ]);\n});\n</code></pre>"},{"location":"packages/laravel/events/#metrics-and-analytics","title":"Metrics and Analytics","text":"<pre><code>use Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse App\\Services\\MetricsService;\n\nEvent::listen(ExtractionComplete::class, function ($event) {\n    app(MetricsService::class)-&gt;recordExtraction([\n        'model' =&gt; $event-&gt;model,\n        'tokens' =&gt; $event-&gt;tokensUsed,\n        'latency' =&gt; $event-&gt;durationMs,\n        'success' =&gt; true,\n    ]);\n});\n</code></pre>"},{"location":"packages/laravel/events/#alerting-on-failures","title":"Alerting on Failures","text":"<pre><code>use Cognesy\\Instructor\\Events\\ExtractionFailed;\nuse Illuminate\\Support\\Facades\\Notification;\nuse App\\Notifications\\ExtractionFailedNotification;\n\nEvent::listen(ExtractionFailed::class, function ($event) {\n    if ($event-&gt;isCritical) {\n        Notification::route('slack', config('services.slack.webhook'))\n            -&gt;notify(new ExtractionFailedNotification($event));\n    }\n});\n</code></pre>"},{"location":"packages/laravel/events/#caching-responses","title":"Caching Responses","text":"<pre><code>use Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Illuminate\\Support\\Facades\\Cache;\n\nEvent::listen(ExtractionComplete::class, function ($event) {\n    $cacheKey = 'extraction:' . md5($event-&gt;inputHash);\n\n    Cache::put($cacheKey, $event-&gt;result, now()-&gt;addHours(24));\n});\n</code></pre>"},{"location":"packages/laravel/events/#queued-event-listeners","title":"Queued Event Listeners","text":"<p>For heavy processing, use queued listeners:</p> <pre><code>// app/Listeners/ProcessExtractionAnalytics.php\nnamespace App\\Listeners;\n\nuse Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Illuminate\\Contracts\\Queue\\ShouldQueue;\n\nclass ProcessExtractionAnalytics implements ShouldQueue\n{\n    public $queue = 'analytics';\n\n    public function handle(ExtractionComplete $event): void\n    {\n        // Heavy analytics processing\n    }\n}\n</code></pre>"},{"location":"packages/laravel/events/#wiretap-direct-event-handling","title":"Wiretap (Direct Event Handling)","text":"<p>For direct access to all events without Laravel's dispatcher:</p> <pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\n$person = StructuredOutput::with(\n    messages: 'Extract person data...',\n    responseModel: PersonData::class,\n)\n-&gt;wiretap(function ($event) {\n    // Called for every event\n    logger()-&gt;debug('Event: ' . get_class($event));\n})\n-&gt;get();\n</code></pre>"},{"location":"packages/laravel/events/#disabling-event-bridge","title":"Disabling Event Bridge","text":"<p>To disable event bridging (e.g., for performance):</p> <pre><code>// config/instructor.php\n'events' =&gt; [\n    'dispatch_to_laravel' =&gt; false,\n],\n</code></pre> <p>Or via environment variable:</p> <pre><code>INSTRUCTOR_DISPATCH_EVENTS=false\n</code></pre>"},{"location":"packages/laravel/events/#testing-events","title":"Testing Events","text":"<p>Assert events were dispatched:</p> <pre><code>use Cognesy\\Instructor\\Events\\ExtractionComplete;\nuse Illuminate\\Support\\Facades\\Event;\n\npublic function test_dispatches_extraction_event(): void\n{\n    Event::fake([ExtractionComplete::class]);\n\n    StructuredOutput::with(\n        messages: 'John is 30',\n        responseModel: PersonData::class,\n    )-&gt;get();\n\n    Event::assertDispatched(ExtractionComplete::class);\n}\n</code></pre> <p>Assert event properties:</p> <pre><code>Event::assertDispatched(ExtractionComplete::class, function ($event) {\n    return $event-&gt;responseModel === PersonData::class\n        &amp;&amp; $event-&gt;tokensUsed &gt; 0;\n});\n</code></pre>"},{"location":"packages/laravel/facades/","title":"Facades","text":"<p>The package provides four main facades for interacting with LLMs and code agents.</p>"},{"location":"packages/laravel/facades/#structuredoutput","title":"StructuredOutput","text":"<p>The primary facade for extracting structured data from text.</p>"},{"location":"packages/laravel/facades/#basic-usage","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\n$person = StructuredOutput::with(\n    messages: 'John Smith is 30 years old',\n    responseModel: PersonData::class,\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#with-system-prompt","title":"With System Prompt","text":"<pre><code>$person = StructuredOutput::with(\n    messages: 'Process this text: John, age 30',\n    responseModel: PersonData::class,\n    system: 'You are a data extraction assistant.',\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#with-examples-few-shot-learning","title":"With Examples (Few-Shot Learning)","text":"<pre><code>$person = StructuredOutput::with(\n    messages: 'Extract: Jane Doe, 25 years',\n    responseModel: PersonData::class,\n    examples: [\n        ['input' =&gt; 'Bob is 40', 'output' =&gt; new PersonData(name: 'Bob', age: 40)],\n    ],\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#switching-connections","title":"Switching Connections","text":"<pre><code>$person = StructuredOutput::using('anthropic')-&gt;with(\n    messages: 'Extract person data...',\n    responseModel: PersonData::class,\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#fluent-api","title":"Fluent API","text":"<pre><code>$person = StructuredOutput::withMessages('John is 30')\n    -&gt;withResponseModel(PersonData::class)\n    -&gt;withModel('gpt-4o')\n    -&gt;withMaxRetries(3)\n    -&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#return-types","title":"Return Types","text":"<pre><code>// Get as typed object (default)\n$person = StructuredOutput::with(...)-&gt;get();\n\n// Get as string\n$name = StructuredOutput::with(...)-&gt;getString();\n\n// Get as integer\n$count = StructuredOutput::with(...)-&gt;getInt();\n\n// Get as float\n$price = StructuredOutput::with(...)-&gt;getFloat();\n\n// Get as boolean\n$valid = StructuredOutput::with(...)-&gt;getBoolean();\n\n// Get as array\n$items = StructuredOutput::with(...)-&gt;getArray();\n</code></pre>"},{"location":"packages/laravel/facades/#available-methods","title":"Available Methods","text":"Method Description <code>using(string $preset)</code> Switch to a different connection <code>with(...)</code> Configure extraction with all parameters <code>withMessages(...)</code> Set input messages <code>withResponseModel(...)</code> Set the response model class <code>withSystem(string)</code> Set system prompt <code>withPrompt(string)</code> Set user prompt template <code>withExamples(array)</code> Set few-shot examples <code>withModel(string)</code> Override the model <code>withMaxRetries(int)</code> Set max retry attempts <code>withOptions(array)</code> Set additional options <code>withOutputMode(OutputMode)</code> Set output mode <code>withStreaming(bool)</code> Enable streaming <code>withValidators(...)</code> Add custom validators <code>withTransformers(...)</code> Add data transformers <code>get()</code> Execute and return result <code>stream()</code> Execute and return stream"},{"location":"packages/laravel/facades/#inference","title":"Inference","text":"<p>For raw LLM inference without structured output.</p>"},{"location":"packages/laravel/facades/#basic-usage_1","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\Inference;\n\n$response = Inference::with(\n    messages: 'What is the capital of France?',\n)-&gt;get();\n\necho $response; // \"The capital of France is Paris.\"\n</code></pre>"},{"location":"packages/laravel/facades/#with-system-message","title":"With System Message","text":"<pre><code>$response = Inference::with(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'Hello!'],\n    ],\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#json-response","title":"JSON Response","text":"<pre><code>$data = Inference::with(\n    messages: 'List 3 colors as JSON',\n    responseFormat: ['type' =&gt; 'json_object'],\n)-&gt;asJsonData();\n\n// ['colors' =&gt; ['red', 'green', 'blue']]\n</code></pre>"},{"location":"packages/laravel/facades/#switching-connections_1","title":"Switching Connections","text":"<pre><code>$response = Inference::using('groq')-&gt;with(\n    messages: 'Explain quantum computing',\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/facades/#available-methods_1","title":"Available Methods","text":"Method Description <code>using(string $preset)</code> Switch connection <code>with(...)</code> Configure with all parameters <code>withMessages(...)</code> Set messages <code>withModel(string)</code> Override model <code>withTools(array)</code> Add tool definitions <code>withToolChoice(...)</code> Set tool choice <code>withResponseFormat(array)</code> Set response format <code>withOptions(array)</code> Set options <code>withStreaming(bool)</code> Enable streaming <code>get()</code> Execute and return text <code>asJson()</code> Return as JSON string <code>asJsonData()</code> Return as array <code>response()</code> Return full response object <code>stream()</code> Return stream iterator"},{"location":"packages/laravel/facades/#embeddings","title":"Embeddings","text":"<p>For generating text embeddings (vector representations).</p>"},{"location":"packages/laravel/facades/#basic-usage_2","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\Embeddings;\n\n// Get single embedding\n$embedding = Embeddings::withInputs('Hello world')-&gt;first();\n// [0.123, -0.456, 0.789, ...]\n\n// Get multiple embeddings\n$embeddings = Embeddings::withInputs([\n    'First text',\n    'Second text',\n])-&gt;all();\n</code></pre>"},{"location":"packages/laravel/facades/#switching-connections_2","title":"Switching Connections","text":"<pre><code>$embedding = Embeddings::using('ollama')\n    -&gt;withInputs('Local embedding test')\n    -&gt;first();\n</code></pre>"},{"location":"packages/laravel/facades/#with-custom-model","title":"With Custom Model","text":"<pre><code>$embedding = Embeddings::withInputs('Test')\n    -&gt;withModel('text-embedding-3-large')\n    -&gt;first();\n</code></pre>"},{"location":"packages/laravel/facades/#full-response","title":"Full Response","text":"<pre><code>$response = Embeddings::withInputs('Test')-&gt;get();\n\n$vectors = $response-&gt;embeddings;\n$usage = $response-&gt;usage;\n</code></pre>"},{"location":"packages/laravel/facades/#available-methods_2","title":"Available Methods","text":"Method Description <code>using(string $preset)</code> Switch connection <code>withInputs(string\\|array)</code> Set input text(s) <code>withModel(string)</code> Override model <code>withOptions(array)</code> Set options <code>first()</code> Get first embedding vector <code>all()</code> Get all embedding vectors <code>get()</code> Get full response object"},{"location":"packages/laravel/facades/#agentctrl","title":"AgentCtrl","text":"<p>For invoking CLI-based code agents (Claude Code, Codex, OpenCode) that can execute code, modify files, and perform complex tasks.</p>"},{"location":"packages/laravel/facades/#basic-usage_3","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\n// Execute a task with Claude Code\n$response = AgentCtrl::claudeCode()\n    -&gt;execute('Generate a Laravel migration for a users table');\n\nif ($response-&gt;isSuccess()) {\n    echo $response-&gt;text();\n}\n</code></pre>"},{"location":"packages/laravel/facades/#agent-selection","title":"Agent Selection","text":"<pre><code>// Claude Code (Anthropic)\n$response = AgentCtrl::claudeCode()\n    -&gt;withModel('claude-opus-4-5')\n    -&gt;execute('Refactor the User model');\n\n// Codex (OpenAI)\n$response = AgentCtrl::codex()\n    -&gt;execute('Write unit tests for UserService');\n\n// OpenCode (Multi-model)\n$response = AgentCtrl::openCode()\n    -&gt;withModel('anthropic/claude-sonnet-4-5')\n    -&gt;execute('Analyze codebase architecture');\n\n// Dynamic selection\nuse Cognesy\\Auxiliary\\Agents\\Enum\\AgentType;\n\n$response = AgentCtrl::make(AgentType::ClaudeCode)\n    -&gt;execute('Generate API documentation');\n</code></pre>"},{"location":"packages/laravel/facades/#configuration","title":"Configuration","text":"<pre><code>$response = AgentCtrl::claudeCode()\n    -&gt;withModel('claude-opus-4-5')           // AI model\n    -&gt;withTimeout(300)                        // Timeout in seconds\n    -&gt;inDirectory(base_path())                // Working directory\n    -&gt;withSandboxDriver(SandboxDriver::Host)  // Sandbox isolation\n    -&gt;withMaxRetries(3)                       // Retry on failure\n    -&gt;execute('Your prompt');\n</code></pre>"},{"location":"packages/laravel/facades/#streaming","title":"Streaming","text":"<pre><code>$response = AgentCtrl::claudeCode()\n    -&gt;onText(function (string $text) {\n        echo $text;\n    })\n    -&gt;onToolUse(function (string $tool, array $input, ?string $output) {\n        echo \"Tool: $tool\\n\";\n    })\n    -&gt;onComplete(function (AgentResponse $response) {\n        echo \"Done! Exit code: \" . $response-&gt;exitCode;\n    })\n    -&gt;executeStreaming('Generate a REST API');\n</code></pre>"},{"location":"packages/laravel/facades/#response-object","title":"Response Object","text":"<pre><code>$response = AgentCtrl::claudeCode()-&gt;execute('...');\n\n// Main content\n$response-&gt;text();           // Generated text output\n$response-&gt;isSuccess();      // True if exitCode is 0\n\n// Metadata\n$response-&gt;exitCode;         // Process exit code\n$response-&gt;sessionId;        // Session ID for resuming\n$response-&gt;agentType;        // Which agent was used\n\n// Usage &amp; cost\n$response-&gt;usage-&gt;input;     // Input tokens\n$response-&gt;usage-&gt;output;    // Output tokens\n$response-&gt;cost;             // Cost in USD\n\n// Tool calls\nforeach ($response-&gt;toolCalls as $call) {\n    $call-&gt;tool;             // Tool name\n    $call-&gt;input;            // Tool input\n    $call-&gt;output;           // Tool output\n    $call-&gt;isError;          // If tool failed\n}\n</code></pre>"},{"location":"packages/laravel/facades/#session-management","title":"Session Management","text":"<pre><code>// First execution\n$response = AgentCtrl::claudeCode()\n    -&gt;execute('Start refactoring the User model');\n\n$sessionId = $response-&gt;sessionId;\n\n// Resume later\n$response = AgentCtrl::claudeCode()\n    -&gt;resumeSession($sessionId)\n    -&gt;execute('Continue with the Address model');\n</code></pre>"},{"location":"packages/laravel/facades/#available-methods_3","title":"Available Methods","text":"Method Description <code>claudeCode()</code> Get Claude Code agent builder <code>codex()</code> Get Codex agent builder <code>openCode()</code> Get OpenCode agent builder <code>make(AgentType)</code> Get agent builder by type <code>fake(array $responses)</code> Create testing fake <code>withModel(string)</code> Set AI model <code>withTimeout(int)</code> Set execution timeout <code>inDirectory(string)</code> Set working directory <code>withSandboxDriver(SandboxDriver)</code> Set sandbox isolation <code>withMaxRetries(int)</code> Set retry count <code>onText(callable)</code> Stream text callback <code>onToolUse(callable)</code> Tool use callback <code>onComplete(callable)</code> Completion callback <code>resumeSession(string)</code> Resume previous session <code>execute(string)</code> Execute and return response <code>executeStreaming(string)</code> Execute with streaming"},{"location":"packages/laravel/facades/#dependency-injection","title":"Dependency Injection","text":"<p>Instead of facades, you can inject services directly:</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass MyService\n{\n    public function __construct(\n        private StructuredOutput $structuredOutput,\n        private Inference $inference,\n        private Embeddings $embeddings,\n    ) {}\n\n    public function process(string $text): PersonData\n    {\n        return $this-&gt;structuredOutput\n            -&gt;with(messages: $text, responseModel: PersonData::class)\n            -&gt;get();\n    }\n}\n</code></pre> <p>This is useful for: - Better testability (easier mocking) - Explicit dependencies - IDE autocompletion</p>"},{"location":"packages/laravel/facades/#facade-real-time-methods","title":"Facade Real-Time Methods","text":"<p>All facades proxy to the underlying service classes. The facades resolve fresh instances from the container, so you can chain methods:</p> <pre><code>// Each call gets a fresh instance\nStructuredOutput::using('openai')-&gt;with(...)-&gt;get();\nStructuredOutput::using('anthropic')-&gt;with(...)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/installation/","title":"Installation","text":""},{"location":"packages/laravel/installation/#requirements","title":"Requirements","text":"<ul> <li>PHP 8.2 or higher</li> <li>Laravel 10.x, 11.x, or 12.x</li> <li>A valid API key from a supported LLM provider</li> </ul>"},{"location":"packages/laravel/installation/#install-via-composer","title":"Install via Composer","text":"<pre><code>composer require cognesy/instructor-laravel\n</code></pre> <p>The package uses Laravel's auto-discovery, so the service provider and facades are registered automatically.</p>"},{"location":"packages/laravel/installation/#publish-configuration","title":"Publish Configuration","text":"<p>Publish the configuration file to customize settings:</p> <pre><code>php artisan vendor:publish --tag=instructor-config\n</code></pre> <p>This creates <code>config/instructor.php</code> with all available options.</p>"},{"location":"packages/laravel/installation/#configure-api-keys","title":"Configure API Keys","text":"<p>Add your LLM provider API key to <code>.env</code>:</p> <pre><code># OpenAI (default)\nOPENAI_API_KEY=sk-...\n\n# Or Anthropic\nANTHROPIC_API_KEY=sk-ant-...\n\n# Or other providers\nGEMINI_API_KEY=...\nGROQ_API_KEY=...\nMISTRAL_API_KEY=...\n</code></pre>"},{"location":"packages/laravel/installation/#verify-installation","title":"Verify Installation","text":"<p>Run the installation command to verify everything is configured correctly:</p> <pre><code>php artisan instructor:install\n</code></pre> <p>This will: 1. Publish the configuration if not already published 2. Check for API key configuration 3. Show next steps</p>"},{"location":"packages/laravel/installation/#test-your-connection","title":"Test Your Connection","text":"<p>Test that your API configuration is working:</p> <pre><code>php artisan instructor:test\n</code></pre> <p>Or test a specific connection:</p> <pre><code>php artisan instructor:test --preset=anthropic\n</code></pre>"},{"location":"packages/laravel/installation/#optional-publish-stubs","title":"Optional: Publish Stubs","text":"<p>If you want to customize the response model stubs:</p> <pre><code>php artisan vendor:publish --tag=instructor-stubs\n</code></pre> <p>This publishes stubs to <code>stubs/instructor/</code> in your application.</p>"},{"location":"packages/laravel/installation/#manual-registration-optional","title":"Manual Registration (Optional)","text":"<p>If you've disabled auto-discovery, manually register the service provider in <code>config/app.php</code>:</p> <pre><code>'providers' =&gt; [\n    // ...\n    Cognesy\\Instructor\\Laravel\\InstructorServiceProvider::class,\n],\n\n'aliases' =&gt; [\n    // ...\n    'StructuredOutput' =&gt; Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput::class,\n    'Inference' =&gt; Cognesy\\Instructor\\Laravel\\Facades\\Inference::class,\n    'Embeddings' =&gt; Cognesy\\Instructor\\Laravel\\Facades\\Embeddings::class,\n],\n</code></pre>"},{"location":"packages/laravel/installation/#upgrading","title":"Upgrading","text":"<p>When upgrading to a new version, republish the configuration if there are new options:</p> <pre><code>php artisan vendor:publish --tag=instructor-config --force\n</code></pre> <p>Review the changelog for breaking changes.</p>"},{"location":"packages/laravel/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Configure connections and settings</li> <li>Facades - Learn how to use the facades</li> <li>Response Models - Create your first response model</li> </ul>"},{"location":"packages/laravel/response-models/","title":"Response Models","text":"<p>Response models define the structure of data you want to extract from text. They are PHP classes with typed properties that guide the LLM in generating structured output.</p>"},{"location":"packages/laravel/response-models/#creating-response-models","title":"Creating Response Models","text":""},{"location":"packages/laravel/response-models/#using-artisan-command","title":"Using Artisan Command","text":"<pre><code># Basic response model\nphp artisan make:response-model PersonData\n\n# Collection response model\nphp artisan make:response-model ProductList --collection\n\n# Nested objects response model\nphp artisan make:response-model CompanyProfile --nested\n\n# With description\nphp artisan make:response-model Invoice --description=\"Invoice extracted from PDF\"\n</code></pre>"},{"location":"packages/laravel/response-models/#manual-creation","title":"Manual Creation","text":"<p>Create a class in <code>app/ResponseModels/</code>:</p> <pre><code>&lt;?php\n\nnamespace App\\ResponseModels;\n\nfinal class PersonData\n{\n    public function __construct(\n        /** The person's full name */\n        public readonly string $name,\n\n        /** The person's age in years */\n        public readonly int $age,\n\n        /** The person's email address */\n        public readonly ?string $email = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#property-types","title":"Property Types","text":""},{"location":"packages/laravel/response-models/#basic-types","title":"Basic Types","text":"<pre><code>final class BasicTypes\n{\n    public function __construct(\n        public readonly string $text,\n        public readonly int $count,\n        public readonly float $price,\n        public readonly bool $isActive,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#nullable-properties","title":"Nullable Properties","text":"<pre><code>final class WithOptional\n{\n    public function __construct(\n        public readonly string $required,\n        public readonly ?string $optional = null,\n        public readonly ?int $maybeNumber = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#arrays","title":"Arrays","text":"<pre><code>final class WithArrays\n{\n    public function __construct(\n        /** @var string[] List of tags */\n        public readonly array $tags,\n\n        /** @var int[] List of scores */\n        public readonly array $scores,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#enums","title":"Enums","text":"<pre><code>enum Priority: string\n{\n    case Low = 'low';\n    case Medium = 'medium';\n    case High = 'high';\n}\n\nfinal class Task\n{\n    public function __construct(\n        public readonly string $title,\n        public readonly Priority $priority,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#nested-objects","title":"Nested Objects","text":"<pre><code>final class Address\n{\n    public function __construct(\n        public readonly string $street,\n        public readonly string $city,\n        public readonly string $country,\n    ) {}\n}\n\nfinal class Person\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly Address $address,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#collections","title":"Collections","text":"<pre><code>final class OrderItem\n{\n    public function __construct(\n        public readonly string $product,\n        public readonly int $quantity,\n        public readonly float $price,\n    ) {}\n}\n\nfinal class Order\n{\n    public function __construct(\n        public readonly string $orderId,\n\n        /** @var OrderItem[] */\n        public readonly array $items,\n\n        public readonly float $total,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#property-descriptions","title":"Property Descriptions","text":"<p>Property descriptions in docblocks guide the LLM:</p> <pre><code>final class ProductReview\n{\n    public function __construct(\n        /** The overall sentiment: positive, negative, or neutral */\n        public readonly string $sentiment,\n\n        /** A score from 1-5 indicating review quality */\n        public readonly int $rating,\n\n        /** Key points mentioned in the review */\n        public readonly array $highlights,\n\n        /** Any concerns or complaints raised */\n        public readonly ?array $concerns = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#using-response-models","title":"Using Response Models","text":""},{"location":"packages/laravel/response-models/#basic-extraction","title":"Basic Extraction","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\nuse App\\ResponseModels\\PersonData;\n\n$person = StructuredOutput::with(\n    messages: 'John Smith is a 30-year-old developer at john@example.com',\n    responseModel: PersonData::class,\n)-&gt;get();\n\necho $person-&gt;name;  // \"John Smith\"\necho $person-&gt;age;   // 30\necho $person-&gt;email; // \"john@example.com\"\n</code></pre>"},{"location":"packages/laravel/response-models/#with-array-schema","title":"With Array Schema","text":"<p>For simple cases, you can use an array schema instead of a class:</p> <pre><code>$person = StructuredOutput::with(\n    messages: 'John is 30 years old',\n    responseModel: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'name' =&gt; ['type' =&gt; 'string', 'description' =&gt; 'Person name'],\n            'age' =&gt; ['type' =&gt; 'integer', 'description' =&gt; 'Person age'],\n        ],\n        'required' =&gt; ['name', 'age'],\n    ],\n)-&gt;get();\n\necho $person['name']; // \"John\"\necho $person['age'];  // 30\n</code></pre>"},{"location":"packages/laravel/response-models/#extracting-collections","title":"Extracting Collections","text":"<pre><code>final class Product\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly float $price,\n    ) {}\n}\n\n// Wrap in array for multiple items\n$products = StructuredOutput::with(\n    messages: 'Products: iPhone $999, MacBook $1299, AirPods $199',\n    responseModel: [\n        'type' =&gt; 'array',\n        'items' =&gt; Product::class,\n    ],\n)-&gt;get();\n\nforeach ($products as $product) {\n    echo \"{$product-&gt;name}: \\${$product-&gt;price}\\n\";\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#validation","title":"Validation","text":""},{"location":"packages/laravel/response-models/#using-symfony-validator","title":"Using Symfony Validator","text":"<pre><code>use Symfony\\Component\\Validator\\Constraints as Assert;\n\nfinal class UserRegistration\n{\n    public function __construct(\n        #[Assert\\NotBlank]\n        #[Assert\\Length(min: 2, max: 100)]\n        public readonly string $name,\n\n        #[Assert\\NotBlank]\n        #[Assert\\Email]\n        public readonly string $email,\n\n        #[Assert\\Range(min: 18, max: 120)]\n        public readonly int $age,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#custom-validation","title":"Custom Validation","text":"<pre><code>use Cognesy\\Instructor\\Validation\\Contracts\\CanValidateObject;\n\nclass AgeValidator implements CanValidateObject\n{\n    public function validate(object $object): array\n    {\n        $errors = [];\n\n        if ($object-&gt;age &lt; 0) {\n            $errors[] = 'Age cannot be negative';\n        }\n\n        return $errors;\n    }\n}\n\n// Use the validator\n$user = StructuredOutput::with(\n    messages: 'User: John, age -5',\n    responseModel: UserData::class,\n)\n-&gt;withValidators(AgeValidator::class)\n-&gt;get();\n</code></pre>"},{"location":"packages/laravel/response-models/#best-practices","title":"Best Practices","text":""},{"location":"packages/laravel/response-models/#1-use-descriptive-property-names","title":"1. Use Descriptive Property Names","text":"<pre><code>// Good\npublic readonly string $customerEmailAddress;\n\n// Less clear\npublic readonly string $email;\n</code></pre>"},{"location":"packages/laravel/response-models/#2-add-detailed-descriptions","title":"2. Add Detailed Descriptions","text":"<pre><code>public function __construct(\n    /**\n     * The product SKU in format XXX-YYYY-ZZZ\n     * Example: ABC-1234-XYZ\n     */\n    public readonly string $sku,\n) {}\n</code></pre>"},{"location":"packages/laravel/response-models/#3-use-appropriate-types","title":"3. Use Appropriate Types","text":"<pre><code>// Use int for counts\npublic readonly int $quantity;\n\n// Use float for prices\npublic readonly float $price;\n\n// Use enums for fixed options\npublic readonly Status $status;\n</code></pre>"},{"location":"packages/laravel/response-models/#4-make-optional-properties-nullable","title":"4. Make Optional Properties Nullable","text":"<pre><code>// Required\npublic readonly string $name,\n\n// Optional\npublic readonly ?string $nickname = null,\n</code></pre>"},{"location":"packages/laravel/response-models/#5-use-readonly-properties","title":"5. Use Readonly Properties","text":"<pre><code>// Immutable - recommended\npublic readonly string $name;\n\n// Mutable - avoid unless necessary\npublic string $name;\n</code></pre>"},{"location":"packages/laravel/response-models/#generated-stubs","title":"Generated Stubs","text":"<p>The <code>make:response-model</code> command generates these stub types:</p>"},{"location":"packages/laravel/response-models/#basic-stub","title":"Basic Stub","text":"<pre><code>final class {{ class }}\n{\n    public function __construct(\n        /** The name of the person */\n        public readonly string $name,\n\n        /** The age of the person in years */\n        public readonly int $age,\n\n        /** Optional email address */\n        public readonly ?string $email = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#collection-stub-collection","title":"Collection Stub (<code>--collection</code>)","text":"<pre><code>final class {{ class }}\n{\n    public function __construct(\n        /** List of extracted items */\n        public readonly array $items,\n    ) {}\n}\n\nfinal class {{ class }}Item\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly ?string $description = null,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/response-models/#nested-stub-nested","title":"Nested Stub (<code>--nested</code>)","text":"<pre><code>final class {{ class }}\n{\n    public function __construct(\n        public readonly string $title,\n        public readonly {{ class }}Contact $contact,\n        public readonly ?{{ class }}Address $address = null,\n    ) {}\n}\n\nfinal class {{ class }}Contact\n{\n    public function __construct(\n        public readonly string $name,\n        public readonly string $email,\n    ) {}\n}\n\nfinal class {{ class }}Address\n{\n    public function __construct(\n        public readonly string $street,\n        public readonly string $city,\n        public readonly string $country,\n    ) {}\n}\n</code></pre>"},{"location":"packages/laravel/testing/","title":"Testing","text":"<p>The package provides testing fakes that allow you to mock LLM responses and make assertions about how your code interacts with the facades.</p>"},{"location":"packages/laravel/testing/#structuredoutputfake","title":"StructuredOutput::fake()","text":""},{"location":"packages/laravel/testing/#basic-usage","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\nuse App\\ResponseModels\\PersonData;\nuse Tests\\TestCase;\n\nclass PersonExtractionTest extends TestCase\n{\n    public function test_extracts_person_data(): void\n    {\n        // Arrange - Setup the fake with expected response\n        $fake = StructuredOutput::fake([\n            PersonData::class =&gt; new PersonData(\n                name: 'John Smith',\n                age: 30,\n                email: 'john@example.com',\n            ),\n        ]);\n\n        // Act - Your code calls StructuredOutput\n        $person = StructuredOutput::with(\n            messages: 'John Smith is 30 years old',\n            responseModel: PersonData::class,\n        )-&gt;get();\n\n        // Assert - Verify the result\n        $this-&gt;assertEquals('John Smith', $person-&gt;name);\n        $this-&gt;assertEquals(30, $person-&gt;age);\n\n        // Assert that extraction was performed\n        $fake-&gt;assertExtracted(PersonData::class);\n    }\n}\n</code></pre>"},{"location":"packages/laravel/testing/#response-mapping","title":"Response Mapping","text":"<p>Map response model classes to their fake responses:</p> <pre><code>$fake = StructuredOutput::fake([\n    PersonData::class =&gt; new PersonData(name: 'John', age: 30),\n    AddressData::class =&gt; new AddressData(city: 'New York'),\n    OrderData::class =&gt; new OrderData(total: 99.99),\n]);\n\n// Each class returns its mapped response\n$person = StructuredOutput::with(..., responseModel: PersonData::class)-&gt;get();\n$address = StructuredOutput::with(..., responseModel: AddressData::class)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/testing/#response-sequences","title":"Response Sequences","text":"<p>Return different responses for sequential calls:</p> <pre><code>$fake = StructuredOutput::fake();\n\n$fake-&gt;respondWithSequence(PersonData::class, [\n    new PersonData(name: 'First Person', age: 25),\n    new PersonData(name: 'Second Person', age: 30),\n    new PersonData(name: 'Third Person', age: 35),\n]);\n\n// First call\n$first = StructuredOutput::with(...)-&gt;get();  // First Person\n\n// Second call\n$second = StructuredOutput::with(...)-&gt;get(); // Second Person\n\n// Third call\n$third = StructuredOutput::with(...)-&gt;get();  // Third Person\n</code></pre>"},{"location":"packages/laravel/testing/#available-assertions","title":"Available Assertions","text":"<pre><code>$fake = StructuredOutput::fake([...]);\n\n// Run your code...\n\n// Assert extraction was called for a class\n$fake-&gt;assertExtracted(PersonData::class);\n\n// Assert extraction count\n$fake-&gt;assertExtractedTimes(PersonData::class, 1);\n$fake-&gt;assertExtractedTimes(PersonData::class, 3);\n\n// Assert no extractions were performed\n$fake-&gt;assertNothingExtracted();\n\n// Assert messages contained specific text\n$fake-&gt;assertExtractedWith(PersonData::class, 'John Smith');\n\n// Assert preset was used\n$fake-&gt;assertUsedPreset('anthropic');\n\n// Assert model was used\n$fake-&gt;assertUsedModel('gpt-4o');\n</code></pre>"},{"location":"packages/laravel/testing/#accessing-recorded-calls","title":"Accessing Recorded Calls","text":"<pre><code>$fake = StructuredOutput::fake([...]);\n\n// Run your code...\n\n// Get all recorded extractions\n$recorded = $fake-&gt;recorded();\n\nforeach ($recorded as $extraction) {\n    echo \"Class: \" . $extraction['class'];\n    echo \"Messages: \" . json_encode($extraction['messages']);\n    echo \"Model: \" . $extraction['model'];\n    echo \"Preset: \" . $extraction['preset'];\n}\n</code></pre>"},{"location":"packages/laravel/testing/#inferencefake","title":"Inference::fake()","text":""},{"location":"packages/laravel/testing/#basic-usage_1","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\Inference;\n\npublic function test_calls_inference(): void\n{\n    // Arrange\n    $fake = Inference::fake([\n        'What is 2+2?' =&gt; 'The answer is 4.',\n        'default' =&gt; 'I don\\'t know.',\n    ]);\n\n    // Act\n    $response = Inference::with(\n        messages: 'What is 2+2?',\n    )-&gt;get();\n\n    // Assert\n    $this-&gt;assertEquals('The answer is 4.', $response);\n    $fake-&gt;assertCalled();\n    $fake-&gt;assertCalledWith('What is 2+2?');\n}\n</code></pre>"},{"location":"packages/laravel/testing/#pattern-matching","title":"Pattern Matching","text":"<p>Responses are matched by pattern:</p> <pre><code>$fake = Inference::fake([\n    'capital' =&gt; 'Paris is the capital of France.',\n    'weather' =&gt; 'The weather is sunny.',\n    'default' =&gt; 'I don\\'t understand.',\n]);\n\n// Matches 'capital'\n$response1 = Inference::with(messages: 'What is the capital of France?')-&gt;get();\n\n// Matches 'weather'\n$response2 = Inference::with(messages: 'How is the weather today?')-&gt;get();\n\n// No match, uses 'default'\n$response3 = Inference::with(messages: 'Random question')-&gt;get();\n</code></pre>"},{"location":"packages/laravel/testing/#response-sequences_1","title":"Response Sequences","text":"<pre><code>$fake = Inference::fake();\n\n$fake-&gt;respondWithSequence([\n    'First response',\n    'Second response',\n    'Third response',\n]);\n\n// Returns responses in order\n$first = Inference::with(...)-&gt;get();  // \"First response\"\n$second = Inference::with(...)-&gt;get(); // \"Second response\"\n</code></pre>"},{"location":"packages/laravel/testing/#available-assertions_1","title":"Available Assertions","text":"<pre><code>$fake = Inference::fake([...]);\n\n// Assert inference was called\n$fake-&gt;assertCalled();\n\n// Assert call count\n$fake-&gt;assertCalledTimes(3);\n\n// Assert never called\n$fake-&gt;assertNotCalled();\n\n// Assert called with specific message\n$fake-&gt;assertCalledWith('What is the capital');\n\n// Assert preset was used\n$fake-&gt;assertUsedPreset('groq');\n\n// Assert model was used\n$fake-&gt;assertUsedModel('llama-3.3-70b');\n\n// Assert called with specific tools\n$fake-&gt;assertCalledWithTools(['search', 'calculate']);\n</code></pre>"},{"location":"packages/laravel/testing/#embeddingsfake","title":"Embeddings::fake()","text":""},{"location":"packages/laravel/testing/#basic-usage_2","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\Embeddings;\n\npublic function test_generates_embeddings(): void\n{\n    // Arrange\n    $fake = Embeddings::fake([\n        'hello' =&gt; [0.1, 0.2, 0.3, 0.4, 0.5],\n    ]);\n\n    // Act\n    $embedding = Embeddings::withInputs('hello world')-&gt;first();\n\n    // Assert\n    $this-&gt;assertIsArray($embedding);\n    $fake-&gt;assertCalled();\n    $fake-&gt;assertCalledWith('hello world');\n}\n</code></pre>"},{"location":"packages/laravel/testing/#default-embeddings","title":"Default Embeddings","text":"<p>If no pattern matches, a random normalized embedding is generated:</p> <pre><code>$fake = Embeddings::fake();\n\n// Returns random 1536-dimensional embedding\n$embedding = Embeddings::withInputs('anything')-&gt;first();\n\n$this-&gt;assertCount(1536, $embedding);\n</code></pre>"},{"location":"packages/laravel/testing/#custom-dimensions","title":"Custom Dimensions","text":"<pre><code>$fake = Embeddings::fake()\n    -&gt;withDimensions(768); // Use 768 dimensions\n\n$embedding = Embeddings::withInputs('test')-&gt;first();\n$this-&gt;assertCount(768, $embedding);\n</code></pre>"},{"location":"packages/laravel/testing/#available-assertions_2","title":"Available Assertions","text":"<pre><code>$fake = Embeddings::fake([...]);\n\n// Assert embeddings were called\n$fake-&gt;assertCalled();\n\n// Assert call count\n$fake-&gt;assertCalledTimes(2);\n\n// Assert never called\n$fake-&gt;assertNotCalled();\n\n// Assert called with specific input\n$fake-&gt;assertCalledWith('hello world');\n\n// Assert preset was used\n$fake-&gt;assertUsedPreset('openai');\n\n// Assert model was used\n$fake-&gt;assertUsedModel('text-embedding-3-large');\n</code></pre>"},{"location":"packages/laravel/testing/#agentctrlfake","title":"AgentCtrl::fake()","text":""},{"location":"packages/laravel/testing/#basic-usage_3","title":"Basic Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\npublic function test_generates_code(): void\n{\n    // Arrange - Setup fake with expected responses\n    $fake = AgentCtrl::fake([\n        'Generated migration file: 2024_01_01_create_users_table.php',\n    ]);\n\n    // Act - Your code calls AgentCtrl\n    $result = AgentCtrl::claudeCode()\n        -&gt;execute('Generate a users table migration');\n\n    // Assert\n    $this-&gt;assertEquals(0, $result-&gt;exitCode);\n    $this-&gt;assertStringContains('migration', $result-&gt;text());\n\n    $fake-&gt;assertExecuted();\n    $fake-&gt;assertExecutedWith('migration');\n}\n</code></pre>"},{"location":"packages/laravel/testing/#response-sequences_2","title":"Response Sequences","text":"<p>Return different responses for sequential calls:</p> <pre><code>$fake = AgentCtrl::fake([\n    'First response',\n    'Second response',\n    'Third response',\n]);\n\n$first = AgentCtrl::claudeCode()-&gt;execute('First');   // \"First response\"\n$second = AgentCtrl::claudeCode()-&gt;execute('Second'); // \"Second response\"\n$third = AgentCtrl::claudeCode()-&gt;execute('Third');   // \"Third response\"\n\n$fake-&gt;assertExecutedTimes(3);\n</code></pre>"},{"location":"packages/laravel/testing/#custom-responses","title":"Custom Responses","text":"<p>Create detailed fake responses with metadata:</p> <pre><code>use Cognesy\\Auxiliary\\Agents\\Enum\\AgentType;\nuse Cognesy\\Instructor\\Laravel\\Testing\\AgentCtrlFake;\n\n$customResponse = AgentCtrlFake::response(\n    text: 'Generated code output',\n    exitCode: 0,\n    agentType: AgentType::ClaudeCode,\n    cost: 0.05,\n);\n\n$fake = AgentCtrl::fake([$customResponse]);\n\n$response = AgentCtrl::claudeCode()-&gt;execute('Test');\n\nexpect($response-&gt;cost)-&gt;toBe(0.05);\nexpect($response-&gt;agentType)-&gt;toBe(AgentType::ClaudeCode);\n</code></pre>"},{"location":"packages/laravel/testing/#fake-tool-calls","title":"Fake Tool Calls","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Testing\\AgentCtrlFake;\n\n$responseWithTools = AgentCtrlFake::response(\n    text: 'Created file',\n    toolCalls: [\n        AgentCtrlFake::toolCall(\n            tool: 'write_file',\n            input: ['path' =&gt; 'app/Models/User.php'],\n            output: 'File created successfully',\n        ),\n        AgentCtrlFake::toolCall(\n            tool: 'run_tests',\n            input: ['path' =&gt; 'tests/'],\n            output: 'All tests passed',\n        ),\n    ],\n);\n\n$fake = AgentCtrl::fake([$responseWithTools]);\n\n$response = AgentCtrl::claudeCode()-&gt;execute('...');\n\nexpect($response-&gt;toolCalls)-&gt;toHaveCount(2);\nexpect($response-&gt;toolCalls[0]-&gt;tool)-&gt;toBe('write_file');\n</code></pre>"},{"location":"packages/laravel/testing/#available-assertions_3","title":"Available Assertions","text":"<pre><code>$fake = AgentCtrl::fake([...]);\n\n// Run your code...\n\n// Assert execution occurred\n$fake-&gt;assertExecuted();\n$fake-&gt;assertNotExecuted();\n$fake-&gt;assertExecutedTimes(3);\n\n// Assert prompt content\n$fake-&gt;assertExecutedWith('Generate a migration');\n\n// Assert agent type\n$fake-&gt;assertAgentType(AgentType::ClaudeCode);\n$fake-&gt;assertUsedClaudeCode();\n$fake-&gt;assertUsedCodex();\n$fake-&gt;assertUsedOpenCode();\n\n// Assert streaming was used\n$fake-&gt;assertStreaming();\n\n// Access recorded executions\n$executions = $fake-&gt;getExecutions();\nforeach ($executions as $exec) {\n    echo $exec['prompt'];\n    echo $exec['agentType']-&gt;name;\n    echo $exec['model'];\n    echo $exec['timeout'];\n    echo $exec['directory'];\n    echo $exec['streaming'] ? 'yes' : 'no';\n}\n\n// Reset fake state\n$fake-&gt;reset();\n</code></pre>"},{"location":"packages/laravel/testing/#testing-agent-services","title":"Testing Agent Services","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\nclass CodeGeneratorService\n{\n    public function generateMigration(array $schema): string\n    {\n        $response = AgentCtrl::claudeCode()\n            -&gt;inDirectory(database_path('migrations'))\n            -&gt;execute(\"Generate migration for: \" . json_encode($schema));\n\n        if (!$response-&gt;isSuccess()) {\n            throw new \\RuntimeException('Code generation failed');\n        }\n\n        return $response-&gt;text();\n    }\n}\n\n// Test\npublic function test_generates_migration(): void\n{\n    $fake = AgentCtrl::fake([\n        'Migration created successfully',\n    ]);\n\n    $service = app(CodeGeneratorService::class);\n    $result = $service-&gt;generateMigration(['table' =&gt; 'users']);\n\n    $this-&gt;assertStringContains('Migration', $result);\n    $fake-&gt;assertUsedClaudeCode();\n    $fake-&gt;assertExecutedWith('users');\n}\n</code></pre>"},{"location":"packages/laravel/testing/#http-client-faking","title":"HTTP Client Faking","text":"<p>Since the package uses Laravel's HTTP client, you can also use <code>Http::fake()</code>:</p> <pre><code>use Illuminate\\Support\\Facades\\Http;\n\npublic function test_with_http_fake(): void\n{\n    Http::fake([\n        'api.openai.com/*' =&gt; Http::response([\n            'choices' =&gt; [\n                [\n                    'message' =&gt; [\n                        'content' =&gt; '{\"name\":\"John\",\"age\":30}',\n                    ],\n                ],\n            ],\n        ]),\n    ]);\n\n    // Your StructuredOutput calls will use the fake HTTP response\n    $person = StructuredOutput::with(...)-&gt;get();\n\n    Http::assertSent(function ($request) {\n        return $request-&gt;url() === 'https://api.openai.com/v1/chat/completions';\n    });\n}\n</code></pre>"},{"location":"packages/laravel/testing/#testing-services","title":"Testing Services","text":"<p>When testing services that use Instructor, prefer dependency injection:</p> <pre><code>use Cognesy\\Instructor\\StructuredOutput;\n\nclass PersonExtractor\n{\n    public function __construct(\n        private StructuredOutput $structuredOutput,\n    ) {}\n\n    public function extract(string $text): PersonData\n    {\n        return $this-&gt;structuredOutput\n            -&gt;with(messages: $text, responseModel: PersonData::class)\n            -&gt;get();\n    }\n}\n\n// In your test\npublic function test_extracts_person(): void\n{\n    $fake = StructuredOutput::fake([\n        PersonData::class =&gt; new PersonData(name: 'John', age: 30),\n    ]);\n\n    // The container will resolve the fake\n    $extractor = app(PersonExtractor::class);\n    $person = $extractor-&gt;extract('Some text');\n\n    $this-&gt;assertEquals('John', $person-&gt;name);\n}\n</code></pre>"},{"location":"packages/laravel/testing/#best-practices","title":"Best Practices","text":""},{"location":"packages/laravel/testing/#1-always-setup-fakes-first","title":"1. Always Setup Fakes First","text":"<pre><code>public function test_example(): void\n{\n    // FIRST: Setup fake\n    $fake = StructuredOutput::fake([...]);\n\n    // THEN: Run your code\n    $result = $this-&gt;service-&gt;process();\n\n    // FINALLY: Assert\n    $fake-&gt;assertExtracted(...);\n}\n</code></pre>"},{"location":"packages/laravel/testing/#2-use-realistic-test-data","title":"2. Use Realistic Test Data","text":"<pre><code>// Good - realistic data\n$fake = StructuredOutput::fake([\n    InvoiceData::class =&gt; new InvoiceData(\n        invoiceNumber: 'INV-2024-001',\n        amount: 1234.56,\n        dueDate: '2024-12-31',\n    ),\n]);\n\n// Avoid - placeholder data\n$fake = StructuredOutput::fake([\n    InvoiceData::class =&gt; new InvoiceData(\n        invoiceNumber: 'test',\n        amount: 0,\n        dueDate: '',\n    ),\n]);\n</code></pre>"},{"location":"packages/laravel/testing/#3-test-edge-cases","title":"3. Test Edge Cases","text":"<pre><code>public function test_handles_empty_response(): void\n{\n    $fake = StructuredOutput::fake([\n        ItemList::class =&gt; new ItemList(items: []),\n    ]);\n\n    $result = $this-&gt;service-&gt;getItems();\n\n    $this-&gt;assertEmpty($result-&gt;items);\n}\n\npublic function test_handles_null_optional_fields(): void\n{\n    $fake = StructuredOutput::fake([\n        PersonData::class =&gt; new PersonData(\n            name: 'John',\n            age: 30,\n            email: null, // Optional field\n        ),\n    ]);\n\n    $person = $this-&gt;service-&gt;getPerson();\n\n    $this-&gt;assertNull($person-&gt;email);\n}\n</code></pre>"},{"location":"packages/laravel/testing/#4-verify-assertions","title":"4. Verify Assertions","text":"<pre><code>public function test_uses_correct_model(): void\n{\n    $fake = StructuredOutput::fake([...]);\n\n    $this-&gt;service-&gt;processWithClaude();\n\n    $fake-&gt;assertUsedPreset('anthropic');\n    $fake-&gt;assertUsedModel('claude-3-5-sonnet-20241022');\n}\n</code></pre>"},{"location":"packages/laravel/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and their solutions.</p>"},{"location":"packages/laravel/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"packages/laravel/troubleshooting/#package-not-found","title":"Package Not Found","text":"<p>Error: <pre><code>Package cognesy/instructor-laravel not found\n</code></pre></p> <p>Solution: Ensure you have the correct package name and your Composer is up to date:</p> <pre><code>composer clear-cache\ncomposer require cognesy/instructor-laravel\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#service-provider-not-registered","title":"Service Provider Not Registered","text":"<p>Error: <pre><code>Class 'Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput' not found\n</code></pre></p> <p>Solution: If auto-discovery is disabled, manually register the provider:</p> <pre><code>// config/app.php\n'providers' =&gt; [\n    Cognesy\\Instructor\\Laravel\\InstructorServiceProvider::class,\n],\n</code></pre> <p>Or clear the cached configuration:</p> <pre><code>php artisan config:clear\nphp artisan cache:clear\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#api-key-issues","title":"API Key Issues","text":""},{"location":"packages/laravel/troubleshooting/#api-key-not-configured","title":"API Key Not Configured","text":"<p>Error: <pre><code>No API key configured for connection 'openai'\n</code></pre></p> <p>Solution: Add your API key to <code>.env</code>:</p> <pre><code>OPENAI_API_KEY=sk-your-key-here\n</code></pre> <p>Then clear config cache:</p> <pre><code>php artisan config:clear\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#invalid-api-key","title":"Invalid API Key","text":"<p>Error: <pre><code>401 Unauthorized: Invalid API key\n</code></pre></p> <p>Solution: 1. Verify your API key is correct 2. Check the key hasn't expired 3. Ensure the key has the required permissions 4. Verify there are no extra spaces in <code>.env</code></p>"},{"location":"packages/laravel/troubleshooting/#rate-limiting","title":"Rate Limiting","text":"<p>Error: <pre><code>429 Too Many Requests\n</code></pre></p> <p>Solution: 1. Implement retry logic with exponential backoff 2. Upgrade your API plan for higher limits 3. Add caching to reduce API calls</p> <pre><code>use Illuminate\\Support\\Facades\\RateLimiter;\n\nif (RateLimiter::tooManyAttempts('llm-calls', 60)) {\n    throw new TooManyRequestsException();\n}\n\nRateLimiter::hit('llm-calls');\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#extraction-issues","title":"Extraction Issues","text":""},{"location":"packages/laravel/troubleshooting/#response-does-not-match-model","title":"Response Does Not Match Model","text":"<p>Error: <pre><code>Failed to deserialize response to PersonData\n</code></pre></p> <p>Solution: 1. Add more descriptive property comments 2. Provide examples 3. Increase max retries</p> <pre><code>final class PersonData\n{\n    public function __construct(\n        /** The person's full legal name (first and last) */\n        public readonly string $name,\n\n        /** The person's age as a whole number */\n        public readonly int $age,\n    ) {}\n}\n\n$result = StructuredOutput::with(\n    messages: $text,\n    responseModel: PersonData::class,\n    maxRetries: 5,  // Increase retries\n    examples: [...], // Add examples\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#validation-failures","title":"Validation Failures","text":"<p>Error: <pre><code>Validation failed after 3 retries\n</code></pre></p> <p>Solution: 1. Check your validation constraints aren't too strict 2. Review the error messages in logs 3. Adjust the retry prompt</p> <pre><code>$result = StructuredOutput::with(\n    messages: $text,\n    responseModel: MyModel::class,\n    maxRetries: 5,\n    retryPrompt: 'Previous response failed: {errors}. Please fix these specific issues.',\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#null-values-for-required-fields","title":"Null Values for Required Fields","text":"<p>Problem: LLM returns null for fields you expected to have values.</p> <p>Solution: 1. Make the input text clearer 2. Add better property descriptions 3. Use system prompts</p> <pre><code>$result = StructuredOutput::with(\n    messages: $text,\n    responseModel: MyModel::class,\n    system: 'Extract all available information. If a field is not found in the text, make a reasonable inference based on context.',\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#timeout-issues","title":"Timeout Issues","text":""},{"location":"packages/laravel/troubleshooting/#request-timeout","title":"Request Timeout","text":"<p>Error: <pre><code>cURL error 28: Operation timed out\n</code></pre></p> <p>Solution: Increase timeout in configuration:</p> <pre><code>// config/instructor.php\n'http' =&gt; [\n    'timeout' =&gt; 300, // 5 minutes\n    'connect_timeout' =&gt; 60,\n],\n</code></pre> <p>Or per-request:</p> <pre><code>$result = StructuredOutput::withOptions([\n    'timeout' =&gt; 300,\n])-&gt;with(...)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#streaming-timeout","title":"Streaming Timeout","text":"<p>Problem: Streaming requests timeout before completion.</p> <p>Solution: <pre><code>set_time_limit(0); // Disable PHP timeout\n\n$stream = StructuredOutput::with(...)\n    -&gt;withStreaming()\n    -&gt;stream();\n</code></pre></p>"},{"location":"packages/laravel/troubleshooting/#testing-issues","title":"Testing Issues","text":""},{"location":"packages/laravel/troubleshooting/#fake-not-working","title":"Fake Not Working","text":"<p>Problem: Real API calls are made despite using <code>fake()</code>.</p> <p>Solution: Ensure you call <code>fake()</code> BEFORE any extraction:</p> <pre><code>// CORRECT\n$fake = StructuredOutput::fake([...]);\n$result = $myService-&gt;extract(); // Uses fake\n\n// WRONG\n$result = $myService-&gt;extract(); // Real API call\n$fake = StructuredOutput::fake([...]); // Too late!\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#httpfake-not-mocking","title":"Http::fake() Not Mocking","text":"<p>Problem: <code>Http::fake()</code> doesn't affect Instructor calls.</p> <p>Solution: Ensure the HTTP driver is set to 'laravel':</p> <pre><code>// config/instructor.php\n'http' =&gt; [\n    'driver' =&gt; 'laravel',\n],\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"packages/laravel/troubleshooting/#slow-responses","title":"Slow Responses","text":"<p>Solutions: 1. Use a faster model (e.g., <code>gpt-4o-mini</code> instead of <code>gpt-4o</code>) 2. Use Groq for faster inference 3. Enable response caching 4. Reduce input size</p> <pre><code>// Use faster provider\n$result = StructuredOutput::using('groq')\n    -&gt;with(...)-&gt;get();\n\n// Cache responses\n$result = Cache::remember($cacheKey, 3600, fn () =&gt;\n    StructuredOutput::with(...)-&gt;get()\n);\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#high-token-usage","title":"High Token Usage","text":"<p>Solutions: 1. Use concise system prompts 2. Truncate long inputs 3. Use smaller context windows</p> <pre><code>// Truncate long text\n$text = Str::limit($longText, 8000);\n\n$result = StructuredOutput::with(\n    messages: $text,\n    responseModel: MyModel::class,\n    system: 'Extract data. Be concise.', // Short prompt\n)-&gt;get();\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#memory-issues","title":"Memory Issues","text":""},{"location":"packages/laravel/troubleshooting/#out-of-memory","title":"Out of Memory","text":"<p>Error: <pre><code>Allowed memory size exhausted\n</code></pre></p> <p>Solution: 1. Process in batches 2. Use streaming for large responses 3. Increase PHP memory limit</p> <pre><code>// Process in chunks\n$documents-&gt;chunk(10)-&gt;each(function ($chunk) {\n    foreach ($chunk as $doc) {\n        $result = StructuredOutput::with(...)\n            -&gt;get();\n        // Process and free memory\n    }\n    gc_collect_cycles();\n});\n</code></pre>"},{"location":"packages/laravel/troubleshooting/#common-error-messages","title":"Common Error Messages","text":"Error Cause Solution <code>Connection refused</code> API unreachable Check network/firewall <code>Invalid JSON</code> Malformed response Increase retries, simplify model <code>Model not found</code> Wrong model name Check model name spelling <code>Quota exceeded</code> API limit reached Upgrade plan or wait <code>Context length exceeded</code> Input too long Truncate input <code>Invalid request</code> Malformed request Check request parameters"},{"location":"packages/laravel/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li> <p>Check the logs: <pre><code>tail -f storage/logs/laravel.log\n</code></pre></p> </li> <li> <p>Enable debug logging: <pre><code>// config/instructor.php\n'logging' =&gt; [\n    'enabled' =&gt; true,\n    'level' =&gt; 'debug',\n],\n</code></pre></p> </li> <li> <p>Test the API directly: <pre><code>php artisan instructor:test --preset=openai\n</code></pre></p> </li> <li> <p>Check GitHub issues: https://github.com/cognesy/instructor-php/issues</p> </li> <li> <p>Ask for help:    Open a new issue with:</p> </li> <li>PHP version</li> <li>Laravel version</li> <li>Package version</li> <li>Error message</li> <li>Minimal reproduction code</li> </ol>"},{"location":"packages/polyglot/overview/","title":"Overview","text":"<p>Polyglot is a PHP library that provides a unified API for interacting with various Large Language Model (LLM) providers. It allows developers to build applications that use LLMs without being locked into a specific provider or having to rewrite code when switching between providers.</p> <p>The core philosophy behind Polyglot is to create a consistent, provider-agnostic interface that abstracts away the differences between LLM APIs, while still allowing access to provider-specific features when needed. This enables developers to:</p> <ul> <li>Write code once and use it with any supported LLM provider</li> <li>Easily switch between providers without changing application code</li> <li>Use different providers in different environments (development, testing, production)</li> <li>Fall back to alternative providers if one becomes unavailable</li> </ul> <p>Polyglot was developed as part of the Instructor for PHP library, which focuses on structured outputs from LLMs, but can also be used as a standalone library for general LLM interactions.</p>"},{"location":"packages/polyglot/overview/#key-features","title":"Key Features","text":""},{"location":"packages/polyglot/overview/#unified-llm-api","title":"Unified LLM API","text":"<p>Polyglot's primary feature is its unified API that works across multiple LLM providers:</p> <ul> <li>Consistent interface for making inference or embedding requests</li> <li>Common message format across all providers</li> <li>Standardized response handling</li> <li>Unified error handling</li> </ul>"},{"location":"packages/polyglot/overview/#framework-agnostic","title":"Framework-Agnostic","text":"<p>Polyglot is designed to work with any PHP framework or even in plain PHP applications. It does not depend on any specific framework, making it easy to integrate into existing projects.</p> <ul> <li>Compatible with Laravel, Symfony, CodeIgniter, and others</li> <li>Can be used in CLI scripts or web applications</li> <li>Lightweight and easy to install</li> </ul>"},{"location":"packages/polyglot/overview/#comprehensive-provider-support","title":"Comprehensive Provider Support","text":"<p>Polyglot supports a wide range of LLM providers, including:</p> <ul> <li>OpenAI (GPT models)</li> <li>Anthropic (Claude models)</li> <li>Google Gemini (native and OpenAI compatible)</li> <li>Mistral AI</li> <li>Azure OpenAI</li> <li>Cohere</li> <li>And many others (see full list below)</li> </ul>"},{"location":"packages/polyglot/overview/#multiple-interaction-modes","title":"Multiple Interaction Modes","text":"<p>Polyglot supports various modes of interaction with LLMs:</p> <ul> <li>Text mode: Simple text completion/chat</li> <li>JSON mode: Structured JSON responses</li> <li>JSON Schema mode: Responses validated against a schema when native support is available</li> <li>Tools mode: Function/tool calling for task execution</li> </ul>"},{"location":"packages/polyglot/overview/#streaming-support","title":"Streaming Support","text":"<p>Real-time streaming of responses is supported across compatible providers:</p> <ul> <li>Token-by-token streaming</li> <li>Progress handling</li> <li>Partial response accumulation</li> </ul>"},{"location":"packages/polyglot/overview/#embeddings-generation","title":"Embeddings Generation","text":"<p>Beyond text generation, Polyglot includes support for vector embeddings:</p> <ul> <li>Generate embeddings from text</li> <li>Support for multiple embedding providers</li> <li>Utilities for finding similar documents</li> </ul>"},{"location":"packages/polyglot/overview/#configuration-flexibility","title":"Configuration Flexibility","text":"<p>Polyglot offers a flexible configuration system:</p> <ul> <li>Configure multiple providers simultaneously</li> <li>Environment-based configuration</li> <li>Runtime provider switching</li> <li>Per-request customization</li> </ul>"},{"location":"packages/polyglot/overview/#middleware-and-extensibility","title":"Middleware and Extensibility","text":"<p>The library is built with extensibility in mind:</p> <ul> <li>HTTP client middleware for customization</li> <li>Event system for request/response monitoring</li> <li>Ability to add custom providers</li> </ul>"},{"location":"packages/polyglot/overview/#use-cases","title":"Use Cases","text":"<p>Polyglot is a good choice for a variety of use cases:</p> <ul> <li>Applications requiring LLM provider flexibility: Switch between providers based on cost, performance, or feature needs</li> <li>Multi-environment deployments: Use different LLM providers in development, staging, and production</li> <li>Redundancy and fallback: Implement fallback strategies when a provider is unavailable</li> <li>Hybrid approaches: Combine different providers for different tasks based on their strengths</li> <li>Local + cloud development: Use local models (via Ollama) for development and cloud providers for production</li> </ul>"},{"location":"packages/polyglot/overview/#supported-providers","title":"Supported Providers","text":""},{"location":"packages/polyglot/overview/#inference-providers","title":"Inference Providers","text":"<p>Polyglot currently supports the following LLM providers for chat completion:</p> <ul> <li>A21: API access to Jamba models</li> <li>Anthropic: Claude family of models</li> <li>Microsoft Azure: Azure-hosted OpenAI models</li> <li>Cerebras: Cerebras LLMs</li> <li>Cohere: Command models (both native and OpenAI compatible interfaces)</li> <li>Deepseek: Deepseek models including reasoning capabilities</li> <li>Google Gemini: Google's Gemini models (both native and OpenAI compatible)</li> <li>Groq: High-performance inference platform</li> <li>Hugging Face: Hugging Face hosted models</li> <li>Meta: Jina AI models</li> <li>Minimaxi: MiniMax models</li> <li>Mistral: Mistral AI models</li> <li>Moonshot: Kimi models</li> <li>Ollama: Self-hosted open source models</li> <li>OpenAI: GPT models family</li> <li>OpenRouter: Multi-provider routing service</li> <li>Perplexity: Perplexity models</li> <li>SambaNova: SambaNova hosted models</li> <li>Together: Together AI hosted models</li> <li>xAI: xAI's Grok models</li> </ul>"},{"location":"packages/polyglot/overview/#embeddings-providers","title":"Embeddings Providers","text":"<p>For embeddings generation, Polyglot supports:</p> <ul> <li>Microsoft Azure: Azure-hosted OpenAI embeddings</li> <li>Cohere: Cohere embeddings models</li> <li>Google Gemini: Google's embedding models</li> <li>Jina: Jina embeddings</li> <li>Mistral: Mistral embedding models</li> <li>Ollama: Self-hosted embedding models</li> <li>OpenAI: OpenAI embeddings</li> </ul>"},{"location":"packages/polyglot/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with Polyglot in your PHP project in under 5 minutes.</p> <p>For detailed setup instructions, see Setup.</p>"},{"location":"packages/polyglot/quickstart/#install-polyglot-with-composer","title":"Install Polyglot with Composer","text":"<p>To install Polyglot in your project, run following command in your terminal:</p> <pre><code>composer require cognesy/instructor-polyglot\n</code></pre> <p>NOTE: Polyglot is already included in Instructor for PHP package, so if you have it installed, you don't need to install Polyglot separately.</p>"},{"location":"packages/polyglot/quickstart/#create-and-run-example","title":"Create and Run Example","text":""},{"location":"packages/polyglot/quickstart/#step-1-prepare-your-openai-api-key","title":"Step 1: Prepare your OpenAI API Key","text":"<p>In this example, we'll use OpenAI as the LLM provider. You can get it from the OpenAI dashboard.</p>"},{"location":"packages/polyglot/quickstart/#step-2-create-a-new-php-file","title":"Step 2: Create a New PHP File","text":"<p>In your project directory, create a new PHP file <code>test-polyglot.php</code>:</p> <pre><code>&lt;?php\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Set up OpenAI API key\n$apiKey = 'your-openai-api-key';\nputenv(\"OPENAI_API_KEY=\" . $apiKey);\n// WARNING: In real project you should set up API key in .env file.\n\n$answer = (new Inference)\n    -&gt;withMessages('What is capital of Germany')\n    -&gt;get();\n\necho \"USER: What is capital of Germany\\n\";\necho \"ASSISTANT: $answer\\n\";\n</code></pre> <p>     You should never put your API keys directly in your real project code to avoid getting them compromised. Set them up in your .env file. </p>"},{"location":"packages/polyglot/quickstart/#step-3-run-the-example","title":"Step 3: Run the Example","text":"<p>Now, you can run the example:</p> <pre><code>php test-polyglot.php\n\n# Output:\n# USER: What is capital of Germany\n# ASSISTANT: Berlin\n</code></pre>"},{"location":"packages/polyglot/quickstart/#next-steps","title":"Next Steps","text":"<p>You can start using Polyglot in your project right away after installation.</p> <p>But it's recommended to publish configuration files and prompt templates to your project directory, so you can customize the library's behavior and use your own prompt templates.</p> <p>You should also set up LLM provider API keys in your <code>.env</code> file instead of putting them directly in your code.</p> <p>See setup instructions for more details.</p>"},{"location":"packages/polyglot/setup/","title":"Setup","text":"<p>This chapter will guide you through the initial steps of setting up and using Polyglot in your PHP project. We'll cover installation and configuration to get you up and running quickly.</p>"},{"location":"packages/polyglot/setup/#installation","title":"Installation","text":"<p>You can install it using Composer:</p> <pre><code>composer require cognesy/instructor-polyglot\n</code></pre> <p>This will install Polyglot along with its dependencies.</p> <p>NOTE: Polyglot is distributed as part of the Instructor PHP package, so if you have it installed, you don't need to install Polyglot separately.</p>"},{"location":"packages/polyglot/setup/#requirements","title":"Requirements","text":"<ul> <li>PHP 8.2 or higher</li> <li>Composer</li> <li>Valid API keys for at least one supported LLM provider</li> </ul>"},{"location":"packages/polyglot/setup/#configuration","title":"Configuration","text":""},{"location":"packages/polyglot/setup/#setting-up-api-keys","title":"Setting Up API Keys","text":"<p>Polyglot requires API keys to authenticate with LLM providers. The recommended approach is to use environment variables:</p> <ol> <li>Create a <code>.env</code> file in your project root (or use your existing one)</li> <li>Add your API keys:</li> </ol> <pre><code># OpenAI\nOPENAI_API_KEY=sk-your-openai-key\n\n# Anthropic\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\n\n# Other providers as needed\nMISTRAL_API_KEY=your-mistral-key\nGEMINI_API_KEY=your-gemini-key\n# etc.\n</code></pre>"},{"location":"packages/polyglot/setup/#configuration-files","title":"Configuration Files","text":"<p>Polyglot loads its configuration from PHP files.</p> <p>The default configuration files are located in the Instructor package, but you can publish and customize them:</p> <ol> <li>Create a <code>config</code> directory in your project if it doesn't exist</li> <li>Copy the configuration files from the Instructor package:</li> </ol> <pre><code># Create config directory if it doesn't exist\nmkdir -p config\n\n# Copy configuration files\ncp vendor/cognesy/instructor-polyglot/config/* config/\n</code></pre> <ol> <li>Customize the configuration files as needed</li> </ol>"},{"location":"packages/polyglot/setup/#llm-configuration","title":"LLM Configuration","text":"<p>The <code>llm.php</code> configuration file contains settings for LLM providers:</p> <pre><code>&lt;?php\n// Example of a simplified config/llm.php\n\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',  // Default connection to use\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        'anthropic' =&gt; [\n            'driver' =&gt; 'anthropic',\n            'apiUrl' =&gt; 'https://api.anthropic.com/v1',\n            'apiKey' =&gt; Env::get('ANTHROPIC_API_KEY', ''),\n            'endpoint' =&gt; '/messages',\n            'metadata' =&gt; [\n                'apiVersion' =&gt; '2023-06-01',\n            ],\n            'model' =&gt; 'claude-3-haiku-20240307',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"packages/polyglot/setup/#embeddings-configuration","title":"Embeddings Configuration","text":"<p>The <code>embed.php</code> configuration file contains settings for embeddings providers:</p> <pre><code>&lt;?php\n// Example of a simplified config/embed.php\n\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/embeddings',\n            'model' =&gt; 'text-embedding-3-small',\n            'dimensions' =&gt; 1536,\n            'maxInputs' =&gt; 16,\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"packages/polyglot/setup/#custom-configuration-location","title":"Custom Configuration Location","text":"<p>By default, Polyglot looks for custom configuration files in the <code>config</code> directory relative to your project root. You can specify a different location by setting the <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable:</p> <pre><code>INSTRUCTOR_CONFIG_PATHS='/path/to/your/config,alternative/path'\n</code></pre>"},{"location":"packages/polyglot/setup/#overriding-configuration-location","title":"Overriding Configuration Location","text":"<p>You can use <code>Settings</code> class static <code>setPath()</code> method to override the value of config path set in environment variable with your own value.</p> <pre><code>use Cognesy\\Config\\Settings;\n\nSettings::setPath('/your/path/to/config');\n</code></pre>"},{"location":"packages/polyglot/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"packages/polyglot/setup/#common-installation-issues","title":"Common Installation Issues","text":"<ul> <li>Composer Dependencies: Make sure you have PHP 8.2+ installed and Composer correctly configured.</li> <li>API Keys: Verify that your API keys are correctly set in your environment variables.</li> <li>Configuration Files: Check that your configuration files are properly formatted and accessible.</li> </ul>"},{"location":"packages/polyglot/setup/#testing-your-installation","title":"Testing Your Installation","text":"<p>A simple way to test if everything is working correctly is to run a small script:</p> <pre><code>&lt;?php\nrequire 'vendor/autoload.php';\n\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$result = (new Inference)\n    -&gt;withMessages('Say hello.')\n    -&gt;get();\n</code></pre> <p>If you see a friendly greeting, your installation is working correctly!</p>"},{"location":"packages/polyglot/advanced/connection-mgmt/","title":"Preset Management","text":"<p>One of Polyglot's strengths is the ability to easily switch between different LLM providers, which is made easy by using connection presets.</p> <p>More complex applications may need to manage multiple LLM provider connections and switch between them dynamically to implement fallback strategies or leverage the strengths of different models and providers for various tasks.</p>"},{"location":"packages/polyglot/advanced/connection-mgmt/#switching-providers","title":"Switching Providers","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n// Use OpenAI\n$openaiResponse = $inference\n    -&gt;using('openai')\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\necho \"OpenAI response: $openaiResponse\\n\";\n\n// Switch to Anthropic\n$anthropicResponse = $inference\n    -&gt;using('anthropic')\n    -&gt;withMessages('What is the capital of Germany?')\n    -&gt;get();\n\necho \"Anthropic response: $anthropicResponse\\n\";\n</code></pre>"},{"location":"packages/polyglot/advanced/connection-mgmt/#implementing-fallbacks","title":"Implementing Fallbacks","text":"<p>You can implement a fallback mechanism to try alternative providers if one fails:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withFallback(array $providers, callable $requestFn) {\n    $lastException = null;\n\n    foreach ($providers as $provider) {\n        try {\n            $inference = (new Inference)-&gt;using($provider);\n            return $requestFn($inference);\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            echo \"Provider '$provider' failed: {$e-&gt;getMessage()}. Trying next provider...\\n\";\n        }\n    }\n\n    throw new \\Exception(\"All providers failed. Last error: \" .\n        ($lastException ? $lastException-&gt;getMessage() : \"Unknown error\"));\n}\n\n// Usage\ntry {\n    $providers = ['openai', 'anthropic', 'gemini'];\n\n    $response = withFallback($providers, function($inference) {\n        return $inference-&gt;with(\n            messages: 'What is the capital of France?'\n        )-&gt;toText();\n    });\n\n    echo \"Response: $response\\n\";\n} catch (\\Exception $e) {\n    echo \"Error: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/advanced/connection-mgmt/#cost-aware-provider-selection","title":"Cost-Aware Provider Selection","text":"<p>You might want to select providers based on cost considerations:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nclass CostAwareLLM {\n    private $inference;\n    private $providers = [\n        'low' =&gt; [\n            'preset' =&gt; 'ollama',\n            'model' =&gt; 'llama2',\n        ],\n        'medium' =&gt; [\n            'preset' =&gt; 'mistral',\n            'model' =&gt; 'mistral-small-latest',\n        ],\n        'high' =&gt; [\n            'preset' =&gt; 'openai',\n            'model' =&gt; 'gpt-4o',\n        ],\n    ];\n\n    public function __construct() {\n        $this-&gt;inference = new Inference();\n    }\n\n    public function ask(string $question, string $tier = 'medium'): string {\n        $provider = $this-&gt;providers[$tier] ?? $this-&gt;providers['medium'];\n\n        return $this-&gt;inference-&gt;using($provider['preset'])\n            -&gt;with(\n                messages: $question,\n                model: $provider['model']\n            )\n            -&gt;get();\n    }\n}\n\n// Usage\n$costAwareLLM = new CostAwareLLM();\n\n// Simple question - use low-cost tier\n$simpleQuestion = \"What is the capital of France?\";\necho \"Simple question (low cost): $simpleQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($simpleQuestion, 'low') . \"\\n\\n\";\n\n// More complex question - use medium-cost tier\n$mediumQuestion = \"Explain the concept of deep learning in simple terms.\";\necho \"Medium question (medium cost): $mediumQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($mediumQuestion, 'medium') . \"\\n\\n\";\n\n// Critical question - use high-cost tier\n$complexQuestion = \"Analyze the ethical implications of AI in healthcare.\";\necho \"Complex question (high cost): $complexQuestion\\n\";\necho \"Response: \" . $costAwareLLM-&gt;ask($complexQuestion, 'high') . \"\\n\\n\";\n</code></pre>"},{"location":"packages/polyglot/advanced/connection-mgmt/#provider-selection-strategy","title":"Provider Selection Strategy","text":"<p>You can implement a strategy to select the most appropriate provider for each request:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\nclass GroupOfExperts {\n    private $inference;\n    private $providerStrategies = [\n        'creative' =&gt; 'anthropic',\n        'factual' =&gt; 'openai',\n        'code' =&gt; 'gemini',\n        'default' =&gt; 'openai',\n    ];\n\n    public function __construct() {\n        $this-&gt;inference = new Inference();\n    }\n\n    public function ask(string $question, string $taskType = 'default'): string {\n        // Select the appropriate provider based on the task type\n        $preset = $this-&gt;providerStrategies[$taskType] ?? $this-&gt;providerStrategies['default'];\n\n        // Use the selected provider\n        return $this-&gt;inference-&gt;using($preset)\n            -&gt;with(messages: $question)\n            -&gt;get();\n    }\n}\n\n// Usage\n$experts = new GroupOfExperts();\n\n$tasks = [\n    [\"Write a short poem about the ocean.\", 'creative'],\n    [\"Create a brief story about a robot discovering emotions.\", 'creative'],\n    [\"What is the capital of France?\", 'factual'],\n    [\"Who wrote 'Pride and Prejudice'?\", 'factual'],\n    [\"Write a PHP function to check if a string is a palindrome.\", 'code'],\n    [\"Create a simple JavaScript function to sort an array of objects by a property.\", 'code'],\n];\n\nforeach ($tasks as $task) {\n    echo \"Task: $task\\n\";\n    echo \"Response: \" . $experts-&gt;ask($task[0], $task[1]) . \"\\n\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/advanced/context-caching/","title":"Context Caching","text":"<p>Context caching improves performance by reusing parts of a conversation, reducing token usage and API costs. This is particularly useful for multi-turn conversations or when processing large documents.</p>"},{"location":"packages/polyglot/advanced/context-caching/#using-cached-context","title":"Using Cached Context","text":"<p>Polyglot supports context caching through the <code>withCachedContext()</code> method:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference()-&gt;using('anthropic');\n\n// Set up a conversation with cached context\n$inference-&gt;withCachedContext(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant who provides concise answers.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'I want to discuss machine learning concepts.'],\n        ['role' =&gt; 'assistant', 'content' =&gt; 'Great! I\\'d be happy to discuss machine learning concepts with you. What specific aspect would you like to explore?'],\n    ]\n);\n\n// First query using the cached context\n$response1 = $inference-&gt;with(\n    messages: 'What is supervised learning?'\n)-&gt;response();\n\necho \"Response 1: \" . $response1-&gt;content() . \"\\n\";\necho \"Tokens from cache: \" . $response1-&gt;usage()-&gt;cacheReadTokens . \"\\n\\n\";\n\n// Second query, still using the same cached context\n$response2 = $inference-&gt;with(\n    messages: 'And what about unsupervised learning?'\n)-&gt;response();\n\necho \"Response 2: \" . $response2-&gt;content() . \"\\n\";\necho \"Tokens from cache: \" . $response2-&gt;usage()-&gt;cacheReadTokens . \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/advanced/context-caching/#provider-support-for-context-caching","title":"Provider Support for Context Caching","text":"<p>Different providers have varying levels of support for context caching:</p> <ul> <li>Anthropic: Supports native context caching with explicit cache markers</li> <li>OpenAI: Provides automatic caching for optimization, but not as explicit as Anthropic</li> <li>Other providers: May not support native caching, but Polyglot still helps manage conversation state</li> </ul>"},{"location":"packages/polyglot/advanced/context-caching/#processing-large-documents-with-cached-context","title":"Processing Large Documents with Cached Context","text":"<p>Context caching is particularly valuable when working with large documents:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Load a large document\n$documentContent = file_get_contents('large_document.txt');\n\n// Set up cached context with the document\n$inference = new Inference()-&gt;using('anthropic');\n$inference-&gt;withCachedContext(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; 'You will help analyze and summarize documents.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'Here is the document to analyze:'],\n        ['role' =&gt; 'user', 'content' =&gt; $documentContent],\n    ]\n);\n\n// Ask multiple questions about the document without resending it each time\n$questions = [\n    'Summarize the key points of this document in 3 bullets.',\n    'What are the main arguments presented?',\n    'Are there any contradictions or inconsistencies in the text?',\n    'What conclusions can be drawn from this document?',\n];\n\nforeach ($questions as $index =&gt; $question) {\n    $response = $inference-&gt;with(messages: $question)-&gt;response();\n\n    echo \"Question \" . ($index + 1) . \": $question\\n\";\n    echo \"Answer: \" . $response-&gt;content() . \"\\n\";\n    echo \"Tokens from cache: \" . $response-&gt;usage()-&gt;cacheReadTokens . \"\\n\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/","title":"Configuration Deep Dive","text":"<p>One of Polyglot's core strengths is its ability to work with multiple LLM providers through a unified API. This chapter covers how to configure, manage, and switch between different providers and models to get the most out of the library.</p>"},{"location":"packages/polyglot/advanced/custom-config/#understanding-provider-configuration","title":"Understanding Provider Configuration","text":"<p>Polyglot organizes provider settings through connection presets - named configurations that include the details needed to communicate with a specific LLM provider. These connection presets are defined in the configuration files and can be selected at runtime.</p>"},{"location":"packages/polyglot/advanced/custom-config/#the-configuration-files","title":"The Configuration Files","text":"<p>The primary configuration files for Polyglot are:</p> <ol> <li><code>config/llm.php</code>: Contains configurations for LLM providers (chat/completion)</li> <li><code>config/embed.php</code>: Contains configurations for embedding providers</li> </ol> <p>Let's focus on the structure of these configuration files.</p>"},{"location":"packages/polyglot/advanced/custom-config/#llm-configuration-structure","title":"LLM Configuration Structure","text":"<p>The <code>llm.php</code> configuration file has the following structure:</p> <pre><code>&lt;?php\nuse Cognesy\\Config\\Env;\n\nreturn [\n    // Default connection to use when none is specified\n    'defaultPreset' =&gt; 'openai',\n\n    // Connection preset definitions\n    'presets' =&gt; [\n        // OpenAI connection\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'metadata' =&gt; [\n                'organization' =&gt; '',\n                'project' =&gt; '',\n            ],\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 128_000,\n            'maxOutputLength' =&gt; 16384,\n        ],\n\n        // Anthropic connection\n        'anthropic' =&gt; [\n            'providerType' =&gt; 'anthropic',\n            'apiUrl' =&gt; 'https://api.anthropic.com/v1',\n            'apiKey' =&gt; Env::get('ANTHROPIC_API_KEY', ''),\n            'endpoint' =&gt; '/messages',\n            'metadata' =&gt; [\n                'apiVersion' =&gt; '2023-06-01',\n                'beta' =&gt; 'prompt-caching-2024-07-31',\n            ],\n            'model' =&gt; 'claude-3-haiku-20240307',\n            'maxTokens' =&gt; 1024,\n            'contextLength' =&gt; 200_000,\n            'maxOutputLength' =&gt; 8192,\n        ],\n\n        // Additional connections...\n    ],\n];\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#embedding-configuration-structure","title":"Embedding Configuration Structure","text":"<p>The <code>embed.php</code> configuration file follows a similar pattern:</p> <pre><code>&lt;?php\nuse Cognesy\\Config\\Env;\n\nreturn [\n    'defaultPreset' =&gt; 'openai',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/embeddings',\n            'metadata' =&gt; [\n                'organization' =&gt; ''\n            ],\n            'model' =&gt; 'text-embedding-3-small',\n            'defaultDimensions' =&gt; 1536,\n            'maxInputs' =&gt; 2048,\n        ],\n\n        // Additional embedding connections...\n    ],\n];\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#connection-parameters","title":"Connection Parameters","text":"<p>Each connection includes several parameters:</p> <ul> <li><code>providerType</code>: The type of provider (OpenAI, Anthropic, etc.)</li> <li><code>apiUrl</code>: The base URL for the provider's API</li> <li><code>apiKey</code>: The API key for authentication</li> <li><code>endpoint</code>: The specific API endpoint for chat completions or embeddings</li> <li><code>metadata</code>: Additional provider-specific settings</li> <li><code>model</code>: The default model to use</li> <li><code>maxTokens</code>: Default maximum tokens for responses</li> <li><code>contextLength</code>: Maximum context length supported by the model</li> <li><code>maxOutputLength</code>: Maximum output length supported by the model</li> <li><code>httpClient</code>: (Optional) Custom HTTP client to use</li> </ul> <p>For embedding connections, the parameters are:</p> <ul> <li><code>providerType</code>: The type of provider (OpenAI, Anthropic, etc.)</li> <li><code>apiUrl</code>: The base URL for the provider's API</li> <li><code>apiKey</code>: The API key for authentication</li> <li><code>endpoint</code>: The specific API endpoint for chat completions or embeddings</li> <li><code>metadata</code>: Additional provider-specific settings</li> <li><code>model</code>: The default model to use</li> <li><code>defaultDimensions</code>: The default dimensions of embedding vectors</li> <li><code>maxInputs</code>: Maximum number of inputs that can be processed in a single request</li> </ul>"},{"location":"packages/polyglot/advanced/custom-config/#connection-preset-name-vs-provider-type","title":"Connection preset name vs provider type","text":"<p>Configuration file <code>llm.php</code> contains a list of connection presets with the default names that might resemble provider type names, but those are separate entities.</p> <p>Provider type name refers to one of the supported LLM API providers and its underlying driver implementation, either specific to this provider or a generic one - for example compatible with OpenAI ('openai-compatible').</p> <p>Connection preset name refers to LLM API provider endpoint configuration with specific provider type, but also URL, credentials, default model name, and default model parameter values.</p>"},{"location":"packages/polyglot/advanced/custom-config/#managing-api-keys","title":"Managing API Keys","text":"<p>API keys should be stored securely and never committed to your codebase. Polyglot uses environment variables for API keys.</p>"},{"location":"packages/polyglot/advanced/custom-config/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># OpenAI\nOPENAI_API_KEY=sk-your-key-here\n\n# Anthropic\nANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# Other providers\nGEMINI_API_KEY=your-key-here\nMISTRAL_API_KEY=your-key-here\nCOHERE_API_KEY=your-key-here\n# etc.\n</code></pre> <p>Then load these environment variables using a package like <code>vlucas/phpdotenv</code>:</p> <pre><code>require_once __DIR__ . '/vendor/autoload.php';\n\n$dotenv = Dotenv\\Dotenv::createImmutable(__DIR__);\n$dotenv-&gt;load();\n</code></pre> <p>Or in frameworks like Laravel, environment variables are automatically loaded.</p>"},{"location":"packages/polyglot/advanced/custom-config/#rotating-api-keys","title":"Rotating API Keys","text":"<p>For better security, consider rotating your API keys regularly. You can update the environment variables without changing your code.</p>"},{"location":"packages/polyglot/advanced/custom-config/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>Different providers may support unique parameters and features. You can pass these as options to the <code>create()</code> method.</p>"},{"location":"packages/polyglot/advanced/custom-config/#openai-specific-parameters","title":"OpenAI-Specific Parameters","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = (new Inference())-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'Generate a creative story.',\n    options: [\n        'temperature' =&gt; 0.8,         // Controls randomness (0.0 to 1.0)\n        'top_p' =&gt; 0.95,              // Nucleus sampling parameter\n        'frequency_penalty' =&gt; 0.5,   // Penalize repeated tokens\n        'presence_penalty' =&gt; 0.5,    // Penalize repeated topics\n        'stop' =&gt; [\"\\n\\n\", \"THE END\"],// Stop sequences\n        'logit_bias' =&gt; [             // Adjust token probabilities\n            // Token ID =&gt; bias value (-100 to +100)\n            15043 =&gt; -100,  // Discourage a specific token\n        ],\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#anthropic-specific-parameters","title":"Anthropic-Specific Parameters","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = (new Inference())-&gt;using('anthropic');\n\n$response = $inference-&gt;with(\n    messages: 'Generate a creative story.',\n    options: [\n        'temperature' =&gt; 0.7,\n        'top_p' =&gt; 0.9,\n        'top_k' =&gt; 40,               // Consider only the top 40 tokens\n        'max_tokens' =&gt; 1000,\n        'stop_sequences' =&gt; [\"\\n\\nHuman:\"],\n        'system' =&gt; 'You are a creative storyteller who specializes in magical realism.',\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#creating-custom-provider-configurations","title":"Creating Custom Provider Configurations","text":"<p>You can create custom configurations for providers that aren't included in the default settings or to modify existing ones.</p>"},{"location":"packages/polyglot/advanced/custom-config/#modifying-configuration-files","title":"Modifying Configuration Files","text":"<p>You can edit the <code>config/llm.php</code> and <code>config/embed.php</code> files directly:</p> <pre><code>// In config/llm.php\nreturn [\n    'defaultPreset' =&gt; 'custom_openai',\n\n    'presets' =&gt; [\n        'custom_openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://custom.openai-proxy.com/v1',\n            'apiKey' =&gt; Env::get('CUSTOM_OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4-turbo',\n            'maxTokens' =&gt; 2048,\n            'contextLength' =&gt; 128_000,\n            'maxOutputLength' =&gt; 16384,\n            // HTTP client configuration is defined via HttpClientBuilder or facade-level methods\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#runtime-configuration","title":"Runtime Configuration","text":"<p>You can also create custom configurations at runtime using the <code>LLMConfig</code> class:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom configuration\n$customConfig = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'gpt-4-turbo',\n    maxTokens: 2048,\n    contextLength: 128000,\n    driver: 'openai'\n);\n\n// Use the custom configuration\n$inference = (new Inference)-&gt;withLLMConfig($customConfig);\n\n$response = $inference-&gt;with(\n    messages: 'What are the benefits of using custom configurations?'\n)-&gt;get();\n\necho $response;\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>You might want to use different providers in different environments:</p> <pre><code>&lt;?php\n// config/llm.php\n\nuse Cognesy\\Config\\Env;\n\n$environment = Env::get('APP_ENV', 'production');\n\nreturn [\n    'defaultPreset' =&gt; $environment === 'production' ? 'openai' : 'ollama',\n\n    'presets' =&gt; [\n        'openai' =&gt; [\n            'providerType' =&gt; 'openai',\n            'apiUrl' =&gt; 'https://api.openai.com/v1',\n            'apiKey' =&gt; Env::get('OPENAI_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'gpt-4o-mini',\n            'maxTokens' =&gt; 1024,\n        ],\n\n        'ollama' =&gt; [\n            'providerType' =&gt; 'ollama',\n            'apiUrl' =&gt; 'http://localhost:11434/v1',\n            'apiKey' =&gt; '',\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'llama2',\n            'maxTokens' =&gt; 1024,\n            // Select appropriate HTTP client preset via HttpClientBuilder if needed\n        ],\n\n        // Other connections...\n    ],\n];\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-config/#creating-custom-inference-drivers","title":"Creating Custom Inference Drivers","text":"<p>In this example we will use an existing driver bundled with Polyglot (OpenAIDriver) as a base class for our custom driver.</p> <p>The driver can be any class that implements <code>CanHandleInference</code> interface.</p> <pre><code>// we register new provider type - 'custom-driver'\nLLM::registerDriver(\n    'custom-driver',\n    fn($config, $httpClient) =&gt; new class($config, $httpClient) extends OpenAIDriver {\n        public function handle(InferenceRequest $request): HttpResponse {\n            // some extra functionality to demonstrate our driver is being used\n            echo \"&gt;&gt;&gt; Handling request...\\n\";\n            return parent::handle($request);\n        }\n    }\n);\n\n// in configuration we use newly defined provider type - 'custom-driver'\n$config = new LLMConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: Env::get('OPENAI_API_KEY'),\n    endpoint: '/chat/completions',\n    model: 'gpt-4o-mini',\n    maxTokens: 128,\n    httpClient: 'guzzle',\n    providerType: 'custom-driver',\n);\n\n// now we're calling inference using our configuration\n$answer = (new Inference)\n    -&gt;withLLMConfig($config)\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;toText();\n</code></pre> <p>An alternative way of providing driver definition is via class-string:</p> <pre><code>LLM::registerDriver('another-driver', AnotherDriver::class);\n</code></pre>"},{"location":"packages/polyglot/advanced/custom-http-client/","title":"Customizing HTTP Client","text":"<p>Polyglot allows you to use custom HTTP clients for specific connection requirements:</p> <pre><code>&lt;?php\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;use Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a custom HTTP client configuration\n$httpConfig = new HttpClientConfig(\n    connectTimeout: 5,      // 5 seconds connection timeout\n    requestTimeout: 60,     // 60 seconds request timeout\n    idleTimeout: 120,       // 120 seconds idle timeout for streaming\n    maxConcurrent: 10,      // Maximum 10 concurrent requests\n    failOnError: true,      // Throw exceptions on HTTP errors\n);\n\n// Create a custom HTTP client\n$httpClient = new HttpClient('guzzle', $httpConfig);\n\n// Use the custom HTTP client with Inference\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Make a request with the custom HTTP client\n$response = $inference-&gt;with(\n    messages: 'This request uses a custom HTTP client.'\n)-&gt;get();\n\necho $response;\n</code></pre>"},{"location":"packages/polyglot/advanced/extending/","title":"Extending Polyglot","text":"<p>Understanding Polyglot's architecture makes it easier to extend the library to support new providers or add new functionality.</p>"},{"location":"packages/polyglot/advanced/extending/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<p>To add support for a new LLM provider, you need to implement several components:</p> <ol> <li>Message Format Adapter: Implements <code>CanMapMessages</code> to convert Polyglot's message format to the provider's format</li> <li>Body Format Adapter: Implements <code>CanMapRequestBody</code> to structure the request body according to the provider's API</li> <li>Request Adapter: Implements <code>ProviderRequestAdapter</code> to build HTTP requests for the provider</li> <li>Response Adapter: Implements <code>ProviderResponseAdapter</code> to parse responses from the provider</li> <li>Usage Format Adapter: Implements <code>CanMapUsage</code> to extract token usage information</li> </ol> <p>Then, you need to modify the <code>InferenceDriverFactory</code> to create the appropriate driver for your provider:</p> <pre><code>// In InferenceDriverFactory\npublic function newProvider(LLMConfig $config, CanHandleHttpRequest $httpClient, EventDispatcher $events): CanHandleInference {\n    return new ModularLLMDriver(\n        $config,\n        new NewProviderRequestAdapter(\n            $config,\n            new NewProviderBodyFormat($config, new NewProviderMessageFormat())\n        ),\n        new NewProviderResponseAdapter(new NewProviderUsageFormat()),\n        $httpClient,\n        $events\n    );\n}\n</code></pre> <p>Finally, add your provider to the <code>make</code> method's match statement.</p>"},{"location":"packages/polyglot/advanced/extending/#adding-a-new-embeddings-provider","title":"Adding a New Embeddings Provider","text":"<p>Similarly, to add a new embeddings provider, implement the <code>CanVectorize</code> interface:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Drivers;\n\nclass NewEmbeddingsDriver implements CanVectorize {\n    public function __construct(\n        protected EmbeddingsConfig $config,\n        protected ?CanHandleHttpRequest $httpClient = null,\n        protected ?EventDispatcher $events = null\n    ) { ... }\n\n    public function vectorize(array $input, array $options = []): EmbeddingsResponse { ... }\n\n    protected function getEndpointUrl(): string { ... }\n    protected function getRequestHeaders(): array { ... }\n    protected function getRequestBody(array $input, array $options): array { ... }\n    protected function toResponse(array $response): EmbeddingsResponse { ... }\n    protected function makeUsage(array $response): Usage { ... }\n}\n</code></pre> <p>Then, modify the <code>Embeddings</code> class to create your driver:</p> <pre><code>// In Embeddings::getDriver\nprotected function getDriver(EmbeddingsConfig $config, CanHandleHttpRequest $httpClient): CanVectorize {\n    return match ($config-&gt;providerType) {\n        // Existing providers...\n        'new-provider' =&gt; new NewEmbeddingsDriver($config, $httpClient, $this-&gt;events),\n        default =&gt; throw new InvalidArgumentException(\"Unknown client: {$config-&gt;providerType}\"),\n    };\n}\n</code></pre>"},{"location":"packages/polyglot/advanced/extending/#adding-custom-middleware","title":"Adding Custom Middleware","text":"<p>You can extend Polyglot's HTTP layer by creating custom middleware:</p> <pre><code>namespace YourNamespace\\Http\\Middleware;\n\nuse Cognesy\\Http\\Contracts\\CanAdaptHttpResponse;use Cognesy\\Http\\Data\\HttpRequest;use Cognesy\\Http\\Middleware\\Base\\BaseMiddleware;\n\nclass YourCustomMiddleware extends BaseMiddleware {\n    protected function beforeRequest(HttpRequest $request): void {\n        // Modify the request before it's sent\n    }\n\n    protected function afterRequest(\n        HttpRequest $request,\n        CanAdaptHttpResponse $response\n    ): CanAdaptHttpResponse {\n        // Modify the response after it's received\n        return $response;\n    }\n}\n</code></pre> <p>Then, add your middleware to the HTTP client:</p> <pre><code>$httpClient = new HttpClient();\n$httpClient-&gt;withMiddleware(new YourCustomMiddleware());\n\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/","title":"Structured outputs with JsonSchema class","text":"<p>JsonSchema is a powerful utility in the Polyglot library that enables developers to define structured data schemas for LLM interactions. This guide explains how to use JsonSchema to shape your LLM outputs and ensure consistent, typed responses from language models.</p> <p>Native JSON Schema enforcement depends on provider support. If your provider does not support <code>OutputMode::JsonSchema</code> natively, use <code>OutputMode::Json</code> or <code>OutputMode::MdJson</code> for best-effort output.</p>"},{"location":"packages/polyglot/advanced/json-schema/#quick-start","title":"Quick Start","text":"<p>Here's a simple example of how to use JsonSchema with Polyglot's Inference API:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define your schema\n$schema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', description: 'City name'),\n        JsonSchema::integer('population', description: 'City population'),\n        JsonSchema::integer('founded', description: 'Founding year'),\n    ],\n    requiredProperties: ['name', 'population', 'founded'],\n);\n\n// Use the schema with Inference\n$data = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is capital of France? Respond with JSON data.']\n        ],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'City data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'city_data',\n                'schema' =&gt; $schema-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        options: ['max_tokens' =&gt; 64],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#why-use-jsonschema","title":"Why Use JsonSchema?","text":"<p>JsonSchema provides several benefits when working with LLMs:</p> <ol> <li>Type Safety: Ensure LLM outputs conform to your expected data structure</li> <li>Data Validation: Specify required fields and data types</li> <li>Structured Responses: Get consistent, well-formatted data instead of raw text</li> <li>Complex Nesting: Define deeply nested structures for sophisticated applications</li> <li>Better LLM Guidance: Help the LLM understand exactly what format you need</li> </ol>"},{"location":"packages/polyglot/advanced/json-schema/#available-types","title":"Available Types","text":""},{"location":"packages/polyglot/advanced/json-schema/#string","title":"String","text":"<p>For text values of any length:</p> <pre><code>use Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n$nameSchema = JsonSchema::string(\n    name: 'full_name',\n    description: 'The user\\'s full name'\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#number-integer","title":"Number &amp; Integer","text":"<p>For numeric values:</p> <pre><code>$ageSchema = JsonSchema::integer(\n    name: 'age',\n    description: 'The user\\'s age in years'\n);\n\n$priceSchema = JsonSchema::number(\n    name: 'price',\n    description: 'Product price'\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#boolean","title":"Boolean","text":"<p>For true/false values:</p> <pre><code>$activeSchema = JsonSchema::boolean(\n    name: 'is_active',\n    description: 'Whether the user account is active'\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#array","title":"Array","text":"<p>For lists of items:</p> <pre><code>$tagsSchema = JsonSchema::array(\n    name: 'tags',\n    description: 'List of tags associated with the post',\n    itemSchema: JsonSchema::string()\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#enum","title":"Enum","text":"<p>For values from a specific set of options:</p> <pre><code>$statusSchema = JsonSchema::enum(\n    name: 'status',\n    description: 'The current status of the post',\n    enumValues: ['draft', 'published', 'archived']\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#object","title":"Object","text":"<p>For complex, nested data structures:</p> <pre><code>$profileSchema = JsonSchema::object(\n    name: 'profile',\n    description: 'A user\\'s public profile information',\n    properties: [\n        JsonSchema::string('username', 'The unique username'),\n        JsonSchema::string('bio', 'A short biography'),\n        JsonSchema::integer('joined_year', 'Year the user joined'),\n    ],\n    requiredProperties: ['username']\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#working-with-required-and-nullable-fields","title":"Working with Required and Nullable Fields","text":""},{"location":"packages/polyglot/advanced/json-schema/#required-fields","title":"Required Fields","text":"<p>Required fields are specified at the object level using the <code>requiredProperties</code> parameter:</p> <pre><code>$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('email', 'Primary email address'),\n        JsonSchema::string('name', 'User\\'s full name'),\n        JsonSchema::string('bio', 'User biography'),\n    ],\n    requiredProperties: ['email', 'name'] // email and name must be present\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#nullable-fields","title":"Nullable Fields","text":"<p>Nullable fields are specified at the individual field level:</p> <pre><code>$bioSchema = JsonSchema::string(\n    name: 'bio',\n    description: 'Optional user biography',\n    nullable: true\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#understanding-required-vs-nullable","title":"Understanding Required vs. Nullable","text":"<ul> <li>Required: The field must be present in the data structure</li> <li>Nullable: The field can contain a null value</li> <li>A field can be both required and nullable (must be present, can be null)</li> <li>A field can be non-required and non-nullable (when present, cannot be null)</li> </ul>"},{"location":"packages/polyglot/advanced/json-schema/#common-patterns","title":"Common Patterns","text":"<pre><code>// Required and Non-nullable (most strict)\nJsonSchema::string('email', 'Primary email', nullable: false);\n// requiredProperties: ['email']\n\n// Required but Nullable (must be present, can be null)\nJsonSchema::string('bio', 'User bio', nullable: true);\n// requiredProperties: ['bio']\n\n// Optional and Non-nullable (can be omitted, but if present cannot be null)\nJsonSchema::string('phone', 'Phone number', nullable: false);\n// requiredProperties: [] (doesn't include 'phone')\n\n// Optional and Nullable (most permissive)\nJsonSchema::string('website', 'Personal website', nullable: true);\n// requiredProperties: [] (doesn't include 'website')\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#working-with-openai-and-other-providers","title":"Working with OpenAI and Other Providers","text":"<p>When working with OpenAI in strict mode, follow these guidelines:</p> <pre><code>// For OpenAI strict mode: \n// - All fields should be required\n// - Use nullable: true for optional fields\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('email', 'Required email address'),\n        JsonSchema::string('bio', 'Optional biography', nullable: true),\n    ],\n    requiredProperties: ['email', 'bio'] // Note: bio is required but nullable\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#building-complex-schemas","title":"Building Complex Schemas","text":"<p>For more complex data structures, you can nest schemas:</p> <pre><code>// Define child schemas first\n$addressSchema = JsonSchema::object(\n    name: 'address',\n    properties: [\n        JsonSchema::string('street', 'Street address'),\n        JsonSchema::string('city', 'City name'),\n        JsonSchema::string('country', 'Country name'),\n    ],\n    requiredProperties: ['street', 'city', 'country']\n);\n\n// Use them in parent schemas\n$userSchema = JsonSchema::object(\n    name: 'user',\n    properties: [\n        JsonSchema::string('name', 'User name'),\n        $addressSchema, // embed the address schema\n    ],\n    requiredProperties: ['name', 'address']\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#fluent-api-for-schema-creation","title":"Fluent API for Schema Creation","text":"<p>JsonSchema supports method chaining for a more fluent API:</p> <pre><code>$schema = JsonSchema::array('tags')\n    -&gt;withItemSchema(JsonSchema::string())\n    -&gt;withDescription('A list of tags')\n    -&gt;withNullable(true);\n</code></pre> <p>Available methods include: - <code>withName(string $name)</code> - <code>withDescription(string $description)</code> - <code>withTitle(string $title)</code> - <code>withNullable(bool $nullable = true)</code> - <code>withMeta(array $meta = [])</code> - <code>withEnumValues(?array $enum = null)</code> - <code>withProperties(?array $properties = null)</code> - <code>withItemSchema(JsonSchema $itemSchema = null)</code> - <code>withRequiredProperties(?array $required = null)</code> - <code>withAdditionalProperties(bool $additionalProperties = false)</code></p>"},{"location":"packages/polyglot/advanced/json-schema/#accessing-schema-properties","title":"Accessing Schema Properties","text":"<p>JsonSchema provides various methods to access schema properties:</p> <pre><code>$schema-&gt;type();                // Get schema type (e.g., 'object')\n$schema-&gt;name();                // Get schema name\n$schema-&gt;isNullable();          // Check if schema is nullable\n$schema-&gt;requiredProperties();  // Get array of required properties\n$schema-&gt;properties();          // Get array of all properties\n$schema-&gt;property('name');      // Get specific property\n$schema-&gt;itemSchema();          // Get item schema for array schemas\n$schema-&gt;enumValues();          // Get enum values\n$schema-&gt;hasAdditionalProperties(); // Check if additional properties are allowed\n$schema-&gt;description();         // Get schema description\n$schema-&gt;title();               // Get schema title\n$schema-&gt;meta();                // Get all meta fields\n$schema-&gt;meta('key');           // Get specific meta field\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#converting-schemas-to-arrays-and-function-calls","title":"Converting Schemas to Arrays and Function Calls","text":"<p>JsonSchema can be converted to arrays and function calls:</p> <pre><code>// Convert to array\n$schemaArray = $schema-&gt;toArray();\n\n// Convert to JSON schema\n$jsonSchema = $schema-&gt;toJsonSchema();\n\n// Convert to function call (for tools/functions)\n$functionCall = $schema-&gt;toFunctionCall(\n    functionName: 'getUserProfile',\n    functionDescription: 'Gets the user profile information',\n    strict: true\n);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#meta-fields","title":"Meta Fields","text":"<p>You can add custom meta fields to your schemas:</p> <pre><code>$schema = JsonSchema::string(\n    name: 'username',\n    description: 'The username',\n    meta: [\n        'min_length' =&gt; 3,\n        'max_length' =&gt; 50,\n        'pattern' =&gt; '^[a-zA-Z0-9_]+$',\n    ]\n);\n</code></pre> <p>Meta fields will be transformed to include the <code>x-</code> prefix when converted to arrays (e.g., <code>x-min_length</code>).</p>"},{"location":"packages/polyglot/advanced/json-schema/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Descriptions: Write clear, concise descriptions for each field.</li> </ol> <pre><code>// \u274c Not helpful\nJsonSchema::string('name', 'the name');\n\n// \u2705 Much better\nJsonSchema::string('name', 'The user\\'s display name (2-50 characters)');\n</code></pre> <ol> <li> <p>Only Mark Required Fields: Only mark fields as required if they're truly necessary.</p> </li> <li> <p>Organize Nested Schemas: Keep your schemas organized when dealing with complex structures.</p> </li> </ol> <pre><code>// Define child schemas first for clarity\n$addressSchema = JsonSchema::object(/*...*/);\n$contactSchema = JsonSchema::object(/*...*/);\n\n// Then use them in your parent schema\n$userSchema = JsonSchema::object(\n    properties: [$addressSchema, $contactSchema]\n);\n</code></pre> <ol> <li>Be Explicit About Requirements: Specify both the nullable status and required fields for clarity.</li> </ol>"},{"location":"packages/polyglot/advanced/json-schema/#full-example-creating-user-profile-schema","title":"Full Example: Creating User Profile Schema","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define address schema\n$addressSchema = JsonSchema::object(\n    name: 'address',\n    properties: [\n        JsonSchema::string('street', 'Street address'),\n        JsonSchema::string('city', 'City name'),\n        JsonSchema::string('postal_code', 'Postal/ZIP code'),\n        JsonSchema::string('country', 'Country name'),\n    ],\n    requiredProperties: ['city', 'country'],\n);\n\n// Define contact schema\n$contactSchema = JsonSchema::object(\n    name: 'contact',\n    properties: [\n        JsonSchema::string('email', 'Email address'),\n        JsonSchema::string('phone', 'Phone number', nullable: true),\n    ],\n    requiredProperties: ['email', 'phone'],\n);\n\n// Define hobbies schema\n$hobbiesSchema = JsonSchema::array(\n    name: 'hobbies',\n    description: 'List of user hobbies',\n    itemSchema: JsonSchema::object(\n        properties: [\n            JsonSchema::string('name', 'Hobby name'),\n            JsonSchema::string('description', 'Hobby description', nullable: true),\n            JsonSchema::integer('years_experience', 'Years of experience', nullable: true),\n        ],\n        requiredProperties: ['name'],\n    ),\n);\n\n// Define main user schema\n$userSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('name', 'User\\'s full name'),\n        JsonSchema::integer('age', 'User\\'s age'),\n        $addressSchema,\n        $contactSchema,\n        $hobbiesSchema,\n        JsonSchema::enum(\n            'status',\n            'Account status',\n            enumValues: ['active', 'inactive', 'pending'],\n        ),\n    ],\n    requiredProperties: ['name', 'age', 'address', 'contact', 'status'],\n);\n\n// Use the schema with Inference\n$userData = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'Generate a profile for John Doe who lives in New York.']\n        ],\n        responseFormat: [\n            'type' =&gt; 'json_schema',\n            'description' =&gt; 'User profile data',\n            'json_schema' =&gt; [\n                'name' =&gt; 'user_profile',\n                'schema' =&gt; $userSchema-&gt;toJsonSchema(),\n                'strict' =&gt; true,\n            ],\n        ],\n        mode: OutputMode::JsonSchema,\n    )\n    -&gt;asJsonData();\n\nprint_r($userData);\n</code></pre>"},{"location":"packages/polyglot/advanced/json-schema/#advanced-creating-function-calls","title":"Advanced: Creating Function Calls","text":"<p>JsonSchema can be used to define function/tool parameters for LLMs:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Utils\\JsonSchema\\JsonSchema;\n\n// Define the schema for the function parameters\n$weatherParamsSchema = JsonSchema::object(\n    properties: [\n        JsonSchema::string('location', 'City and country name'),\n        JsonSchema::enum(\n            'unit', \n            'Temperature unit', \n            enumValues: ['celsius', 'fahrenheit'],\n            nullable: true\n        ),\n    ],\n    requiredProperties: ['location'],\n);\n\n// Convert schema to function call format\n$functionDefinition = $weatherParamsSchema-&gt;toFunctionCall(\n    functionName: 'getWeather',\n    functionDescription: 'Get the current weather for a location',\n    strict: true\n);\n\n// Use with Polyglot's Inference API\n$result = (new Inference)\n    -&gt;using('openai')\n    -&gt;with(\n        messages: [\n            ['role' =&gt; 'user', 'content' =&gt; 'What\\'s the weather like in Tokyo?']\n        ],\n        tools: [$functionDefinition],\n        // Additional configuration...\n    )\n    -&gt;create();\n</code></pre>"},{"location":"packages/polyglot/embeddings/optimization/","title":"Optimization","text":""},{"location":"packages/polyglot/embeddings/optimization/#optimization","title":"Optimization","text":""},{"location":"packages/polyglot/embeddings/optimization/#batch-processing-for-efficiency","title":"Batch Processing for Efficiency","text":"<p>When processing many documents, it's more efficient to batch them:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$allDocuments = [/* large array of documents */];\n\n// Process in batches of 25 (check provider-specific limits)\n$batchSize = 25;\n$vectors = [];\n\nfor ($i = 0; $i &lt; count($allDocuments); $i += $batchSize) {\n    $batch = array_slice($allDocuments, $i, $batchSize);\n\n    try {\n        $response = $embeddings-&gt;with($batch)-&gt;get();\n        $batchVectors = $response-&gt;toValuesArray();\n\n        // Add to our vectors array\n        $vectors = array_merge($vectors, $batchVectors);\n\n        echo \"Processed batch \" . (floor($i / $batchSize) + 1) . \" of \" . ceil(count($allDocuments) / $batchSize) . \"\\n\";\n    } catch (\\Exception $e) {\n        echo \"Error processing batch: \" . $e-&gt;getMessage() . \"\\n\";\n    }\n\n    // Optional: Add a small delay to avoid hitting rate limits\n    usleep(100000); // 100ms\n}\n\necho \"Processed \" . count($vectors) . \" embeddings in total.\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/optimization/#retry-policy-options","title":"Retry Policy Options","text":"<p>You can enable automatic retries with exponential backoff and jitter via options:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n\n$response = $embeddings-&gt;with(\n    input: ['doc one', 'doc two'],\n    options: [\n        'retryPolicy' =&gt; [\n            'maxAttempts' =&gt; 3,\n            'baseDelayMs' =&gt; 200,\n            'maxDelayMs' =&gt; 4000,\n            'jitter' =&gt; 'full',\n            'retryOnStatus' =&gt; [429, 500, 502, 503, 504],\n        ],\n    ],\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/embeddings/optimization/#caching-embeddings","title":"Caching Embeddings","text":"<p>For better performance, you can cache embeddings to avoid regenerating them:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\nclass CachedEmbeddings {\n    private $embeddings;\n    private $cache = [];\n\n    public function __construct(?Embeddings $embeddings = null) {\n        $this-&gt;embeddings = $embeddings ?? new Embeddings();\n    }\n\n    public function create($input, array $options = []): array {\n        if (is_string($input)) {\n            // Single string input\n            $cacheKey = $this-&gt;getCacheKey($input, $options);\n\n            if (isset($this-&gt;cache[$cacheKey])) {\n                return $this-&gt;cache[$cacheKey];\n            }\n\n            $response = $this-&gt;embeddings-&gt;with($input, $options)-&gt;get();\n            $vector = $response-&gt;first()-&gt;values();\n\n            $this-&gt;cache[$cacheKey] = $vector;\n            return $vector;\n        } else {\n            // Array of strings\n            $results = [];\n            $uncachedInputs = [];\n            $uncachedIndices = [];\n\n            // Check cache for each input\n            foreach ($input as $i =&gt; $text) {\n                $cacheKey = $this-&gt;getCacheKey($text, $options);\n\n                if (isset($this-&gt;cache[$cacheKey])) {\n                    $results[$i] = $this-&gt;cache[$cacheKey];\n                } else {\n                    $uncachedInputs[] = $text;\n                    $uncachedIndices[] = $i;\n                }\n            }\n\n            // Generate embeddings for uncached inputs\n            if (!empty($uncachedInputs)) {\n                $response = $this-&gt;embeddings-&gt;with($uncachedInputs, $options)-&gt;get();\n                $vectors = $response-&gt;toValuesArray();\n\n                foreach ($vectors as $j =&gt; $vector) {\n                    $i = $uncachedIndices[$j];\n                    $results[$i] = $vector;\n\n                    // Update cache\n                    $cacheKey = $this-&gt;getCacheKey($input[$i], $options);\n                    $this-&gt;cache[$cacheKey] = $vector;\n                }\n            }\n\n            // Sort by original indices\n            ksort($results);\n            return $results;\n        }\n    }\n\n    private function getCacheKey(string $input, array $options): string {\n        $model = $options['model'] ?? '';\n        return md5($input . serialize($options) . $model);\n    }\n}\n\n// Usage\n$cachedEmbeddings = new CachedEmbeddings((new Embeddings())-&gt;using('openai'));\n\n// First call will generate embeddings\n$vector1 = $cachedEmbeddings-&gt;create(\"This is a test\");\necho \"First call completed, generated vector with \" . count($vector1) . \" dimensions.\\n\";\n\n// Second call will use the cache\n$vector2 = $cachedEmbeddings-&gt;create(\"This is a test\");\necho \"Second call completed (from cache).\\n\";\n\n// Compare vectors to verify they're the same\n$equal = (serialize($vector1) === serialize($vector2));\necho \"Vectors are \" . ($equal ? \"identical\" : \"different\") . \".\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/","title":"Overview of Embeddings","text":"<p>Embeddings are a key component of many LLM-based solutions and are used to represent text (or multimodal data) with numbers capturing their meaning and relationships.</p> <p>Embeddings are numerical representations of text or other data that capture semantic meaning in a way that computers can process efficiently. They enable powerful applications like semantic search, document clustering, recommendation systems, and more. This chapter explores how to use Polyglot's Embeddings API to work with vector embeddings across multiple providers.</p>"},{"location":"packages/polyglot/embeddings/overview/#understanding-embeddings","title":"Understanding Embeddings","text":"<p>Before diving into code, it's helpful to understand what embeddings are and how they work:</p> <ul> <li>Embeddings represent words, phrases, or documents as vectors of floating-point numbers in a high-dimensional space</li> <li>Similar items (semantically related) have vectors that are closer together in this space</li> <li>The \"distance\" between vectors can be measured using metrics like cosine similarity or Euclidean distance</li> <li>Modern embedding models are trained on massive corpora of text to capture nuanced relationships</li> </ul> <p>Common use cases for embeddings include:</p> <ul> <li>Semantic search: Finding documents similar to a query based on meaning, not just keywords</li> <li>Clustering: Grouping similar documents together</li> <li>Classification: Assigning categories to documents based on their content</li> <li>Recommendations: Suggesting related items</li> <li>Information retrieval: Finding relevant information in large datasets</li> </ul>"},{"location":"packages/polyglot/embeddings/overview/#embeddings-class","title":"<code>Embeddings</code> class","text":"<p>The <code>Embeddings</code> class is a facade that provides access to embeddings APIs across multiple providers. It combines functionality through traits for provider configuration, request building, and result handling.</p>"},{"location":"packages/polyglot/embeddings/overview/#architecture-overview","title":"Architecture Overview","text":"<p>The <code>Embeddings</code> class combines functionality through traits: - HandlesInitMethods: Provider configuration and setup - HandlesFluentMethods: Request parameter configuration - HandlesInvocation: Request execution and PendingEmbeddings creation - HandlesShortcuts: Convenient methods for common result formats</p>"},{"location":"packages/polyglot/embeddings/overview/#supported-providers","title":"Supported providers","text":"<p><code>Embeddings</code> class supports the following embeddings providers: - Azure OpenAI: Azure-hosted OpenAI embedding models - Cohere: Cohere's embedding models - Gemini: Google's Gemini embedding models - Jina: Jina AI's embedding models - OpenAI: OpenAI's embedding models (text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large)</p> <p>Provider configurations are managed through the configuration system.</p>"},{"location":"packages/polyglot/embeddings/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Simple embedding generation\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('The quick brown fox jumps over the lazy dog.')-&gt;get();\n\n// Get the vector values from the first result\n$vector = $result-&gt;first()-&gt;values();\necho \"Generated a vector with \" . count($vector) . \" dimensions.\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/#provider-configuration-methods","title":"Provider Configuration Methods","text":"<p>Configure the underlying embeddings provider:</p> <pre><code>// Provider selection and configuration\n$embeddings-&gt;using('openai');                          // Use preset configuration\n$embeddings-&gt;withDsn('openai://model=text-embedding-3-large'); // Configure via DSN\n$embeddings-&gt;withConfig($customConfig);                // Explicit configuration\n$embeddings-&gt;withConfigProvider($configProvider);     // Custom config provider\n\n// HTTP and debugging\n$embeddings-&gt;withHttpClient($customHttpClient);       // Custom HTTP client\n$embeddings-&gt;withHttpDebugPreset('verbose');          // Debug configuration\n\n// Driver management\n$embeddings-&gt;withDriver($customDriver);               // Custom vectorization driver\n$embeddings-&gt;withProvider($customProvider);           // Custom provider instance\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/#request-configuration-methods","title":"Request Configuration Methods","text":"<p>Configure the embedding request:</p> <pre><code>// Input configuration\n$embeddings-&gt;withInputs('Single text input');         // Single string\n$embeddings-&gt;withInputs(['Text 1', 'Text 2']);       // Multiple strings\n$embeddings-&gt;with('Input text');                      // Shorthand input method\n\n// Model and options\n$embeddings-&gt;withModel('text-embedding-3-large');    // Specific model\n$embeddings-&gt;withOptions(['dimensions' =&gt; 1536]);     // Provider-specific options\n\n// Complete configuration\n$embeddings-&gt;with(\n    input: ['Text 1', 'Text 2'],\n    options: ['dimensions' =&gt; 1536],\n    model: 'text-embedding-3-large'\n);\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/#response-methods","title":"Response Methods","text":"<p>Get embeddings in different formats:</p> <pre><code>// Full response object\n$response = $embeddings-&gt;get();                       // EmbeddingsResponse object\n\n// Vector extraction\n$vectors = $embeddings-&gt;vectors();                    // Array of Vector objects\n$firstVector = $embeddings-&gt;first();                 // First Vector object\n$values = $embeddings-&gt;first()-&gt;values();           // Array of floats\n\n// Advanced response handling\n$pending = $embeddings-&gt;create();                    // PendingEmbeddings for custom handling\n$response = $pending-&gt;get();                         // Execute and get response\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/#working-with-multiple-providers","title":"Working with Multiple Providers","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// OpenAI embeddings\n$openaiVectors = (new Embeddings())\n    -&gt;using('openai')\n    -&gt;withModel('text-embedding-3-large')\n    -&gt;with(['Document 1', 'Document 2'])\n    -&gt;vectors();\n\n// Cohere embeddings  \n$cohereVectors = (new Embeddings())\n    -&gt;using('cohere')\n    -&gt;withModel('embed-english-v3.0')\n    -&gt;with(['Document 1', 'Document 2'])\n    -&gt;vectors();\n\necho \"OpenAI dimensions: \" . count($openaiVectors[0]-&gt;values()) . \"\\n\";\necho \"Cohere dimensions: \" . count($cohereVectors[0]-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/#custom-configuration","title":"Custom Configuration","text":"<p>Create custom configurations for specific use cases:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create custom configuration\n$config = new EmbeddingsConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/embeddings',\n    model: 'text-embedding-3-large',\n    dimensions: 3072,\n    maxInputs: 100,\n    driver: 'openai'\n);\n\n// Use custom configuration\n$embeddings = (new Embeddings())\n    -&gt;withConfig($config)\n    -&gt;with('Custom configuration example');\n\n$vector = $embeddings-&gt;first()-&gt;values();\necho \"Generated embedding with \" . count($vector) . \" dimensions\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/overview/#driver-registration","title":"Driver Registration","text":"<p>Register custom drivers for new providers:</p> <pre><code>// Register with class name\nEmbeddings::registerDriver('custom-provider', CustomEmbeddingsDriver::class);\n\n// Register with factory callable\nEmbeddings::registerDriver('custom-provider', function($config, $httpClient) {\n    return new CustomEmbeddingsDriver($config, $httpClient);\n});\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/","title":"Working with Embeddings","text":""},{"location":"packages/polyglot/embeddings/work-with-embeddings/#the-embeddings-class","title":"The Embeddings Class","text":"<p>Polyglot provides the <code>Embeddings</code> class as the primary interface for generating and working with vector embeddings.</p>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#creating-an-embeddings-instance","title":"Creating an Embeddings Instance","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create a basic embeddings instance with default settings\n$embeddings = new Embeddings();\n\n// Create an embeddings instance with a specific connection\n$embeddings = (new Embeddings())-&gt;using('openai');\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#key-methods","title":"Key Methods","text":"<p>The <code>Embeddings</code> class provides several important methods:</p> <ul> <li><code>create()</code>: Generates embeddings for input text</li> <li><code>using()</code>: Specifies which connection preset to use</li> <li><code>withConfig()</code>: Sets a custom configuration</li> <li><code>withHttpClient()</code>: Specifies a custom HTTP client</li> <li><code>withModel()</code>: Overrides the default model</li> <li><code>toRuntime()</code>: Exposes a reusable runtime creator for DI/composition</li> </ul> <p>For similarity search helpers, use <code>Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils::findSimilar(...)</code>.</p>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#generating-embeddings","title":"Generating Embeddings","text":"<p>The core functionality of the <code>Embeddings</code> class is to transform text into vector representations.</p>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#basic-embedding-generation","title":"Basic Embedding Generation","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('The quick brown fox jumps over the lazy dog.')-&gt;get();\n\n// Get the vector values from the first (and only) result\n$vector = $result-&gt;first()?-&gt;values();\n\necho \"Generated a vector with \" . count($vector) . \" dimensions.\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#embedding-multiple-texts","title":"Embedding Multiple Texts","text":"<p>You can generate embeddings for multiple texts in a single request, which is more efficient than making separate requests:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n\n$documents = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning models can process text into vector representations.\",\n    \"Embeddings capture semantic relationships between words and documents.\"\n];\n\n$result = $embeddings-&gt;with($documents)-&gt;get();\n\n// Get all vectors\n$vectors = $result-&gt;vectors();\n\nforeach ($vectors as $index =&gt; $vector) {\n    echo \"Document \" . ($index + 1) . \" has a vector with \" . count($vector-&gt;values()) . \" dimensions.\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#accessing-embedding-results","title":"Accessing Embedding Results","text":"<p>The <code>create()</code> method returns an <code>EmbeddingsResponse</code> object with several useful methods:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('Sample text for embedding')-&gt;get();\n\n// Get the first vector\n$firstVector = $result-&gt;first();\n\n// Get the last vector (useful when processing multiple inputs)\n$lastVector = $result-&gt;last();\n\n// Get all vectors\n$allVectors = $result-&gt;vectors();\n\n// Get all vector values as a simple array of arrays\n$valuesArray = $result-&gt;toValuesArray();\n\n// Get usage information\n$usage = $result-&gt;usage();\necho \"Input tokens: \" . $usage-&gt;input() . \"\\n\";\necho \"Output tokens: \" . $usage-&gt;output() . \"\\n\";\necho \"Total tokens: \" . $usage-&gt;total() . \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#working-with-vector-objects","title":"Working with Vector Objects","text":"<p>Each vector in the response is represented by a <code>Vector</code> object with its own methods:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n$embeddings = new Embeddings();\n$result = $embeddings-&gt;with('Sample text for embedding')-&gt;get();\n$vector = $result-&gt;first();\n\n// Get vector values\n$values = $vector-&gt;values();\n\n// Get vector ID (index)\n$id = $vector-&gt;id();\n\n// Compare with another vector\n$otherVector = $embeddings-&gt;with('Another text for comparison')-&gt;get()-&gt;first();\n$similarity = $vector-&gt;compareTo($otherVector, 'cosine');\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#working-with-different-providers","title":"Working with Different Providers","text":"<p>Polyglot supports multiple embedding providers, each with their own strengths and characteristics.</p>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#switching-between-providers","title":"Switching Between Providers","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Compare embeddings from different providers\n$text = \"Artificial intelligence is transforming industries worldwide.\";\n\n// OpenAI embeddings\n$openaiEmbeddings = (new Embeddings())-&gt;using('openai');\n$openaiResult = $openaiEmbeddings-&gt;with($text)-&gt;get();\necho \"OpenAI embedding dimensions: \" . count($openaiResult-&gt;first()?-&gt;values()) . \"\\n\";\n\n// Cohere embeddings\n$cohereEmbeddings = (new Embeddings())-&gt;using('cohere');\n$cohereResult = $cohereEmbeddings-&gt;with($text)-&gt;get();\necho \"Cohere embedding dimensions: \" . count($cohereResult-&gt;first()?-&gt;values()) . \"\\n\";\n\n// Mistral embeddings\n$mistralEmbeddings = (new Embeddings())-&gt;using('mistral');\n$mistralResult = $mistralEmbeddings-&gt;with($text)-&gt;get();\necho \"Mistral embedding dimensions: \" . count($mistralResult-&gt;first()?-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#provider-specific-options","title":"Provider-Specific Options","text":"<p>Different providers may support additional options for embedding generation:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Example with OpenAI-specific options\n$openaiEmbeddings = (new Embeddings())-&gt;using('openai');\n$response = $openaiEmbeddings-&gt;with(\n    input: [\"Sample text for embedding\"],\n    options: [\n        'encoding_format' =&gt; 'float',  // Get float values instead of base64\n        'dimensions' =&gt; 512,           // Request a specific vector size (if supported)\n    ]\n)-&gt;get();\n\n// Example with Cohere-specific options\n$cohereEmbeddings = (new Embeddings())-&gt;using('cohere');\n$response = $cohereEmbeddings-&gt;with(\n    input: [\"Sample text for embedding\"],\n    options: [\n        'input_type' =&gt; 'classification',  // Cohere-specific option\n        'truncate' =&gt; 'END',               // How to handle texts that exceed the token limit\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/embeddings/work-with-embeddings/#models-and-dimensions","title":"Models and Dimensions","text":"<p>Different embedding models produce vectors of different dimensions:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;use Cognesy\\Polyglot\\Embeddings\\Embeddings;\n\n// Create custom configuration with a specific model\n$config = new EmbeddingsConfig(\n    apiUrl: 'https://api.openai.com/v1',\n    apiKey: getenv('OPENAI_API_KEY'),\n    endpoint: '/embeddings',\n    model: 'text-embedding-3-large',  // Use the larger model\n    dimensions: 3072,                 // Specify expected dimensions\n);\n\n$embeddings = new Embeddings();\n$embeddings = $embeddings-&gt;withConfig($config);\n\n$response = $embeddings-&gt;with(\"Test text for large embedding model\")-&gt;get();\necho \"Vector dimensions: \" . count($response-&gt;first()?-&gt;values()) . \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/essentials/creating-requests/","title":"Creating Requests","text":"<p>This section covers how to create requests to LLM providers using the Polyglot library. It includes examples of basic requests, handling multiple messages, and using different message formats.</p>"},{"location":"packages/polyglot/essentials/creating-requests/#creating-requests","title":"Creating Requests","text":"<p>The <code>with()</code> method is the main way to set the parameters of the requests to LLM providers.</p> <p>It accepts several parameters:</p> <pre><code>public function with(\n    string|array $messages = [],    // The messages to send to the LLM\n    string $model = '',             // The model to use (overrides default)\n    array $tools = [],              // Tools/functions for the model to use\n    string|array $toolChoice = [],  // Tool selection preference\n    array $responseFormat = [],     // Response format specification\n    array $options = [],            // Additional request options\n    Mode $mode = OutputMode::Text   // Output mode (Text, JSON, etc.)\n) : self\n</code></pre>"},{"location":"packages/polyglot/essentials/creating-requests/#basic-request-example","title":"Basic Request Example","text":"<p>Here's a simple example of creating a request:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;with(\n        messages: 'What is the capital of France?'\n    )\n    -&gt;create() // create pending inference\n    -&gt;get();   // get the data - here it executes the request\n\necho \"Response: $response\";\n</code></pre>"},{"location":"packages/polyglot/essentials/creating-requests/#request-with-multiple-messages","title":"Request with Multiple Messages","text":"<p>For chat-based interactions, you can pass an array of messages:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages([\n        ['role' =&gt; 'system', 'content' =&gt; 'You are a helpful assistant who provides concise answers.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?'],\n        ['role' =&gt; 'assistant', 'content' =&gt; 'Paris.'],\n        ['role' =&gt; 'user', 'content' =&gt; 'And what about Germany?']\n    ])\n    -&gt;get();\n\necho \"Response: $response\";\n</code></pre>"},{"location":"packages/polyglot/essentials/creating-requests/#using-messages-class","title":"Using <code>Messages</code> Class","text":"<p>You can also use the <code>Messages</code> class to create message sequences more conveniently:</p> <pre><code>&lt;?php\nuse Cognesy\\Messages\\Messages;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$messages = (new Messages)\n    -&gt;asSystem('You are a senior PHP8 backend developer.')\n    -&gt;asDeveloper('Be concise and use modern PHP8.2+ features.') // OpenAI developer role is supported and normalized for other providers\n    -&gt;asUser([\n        'What is the best way to handle errors in PHP8?',\n        'Provide a code example.',\n    ]); // you can pass array of strings to create multiple content parts\n\n$response = (new Inference)\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4o')\n    -&gt;withMessages($messages)\n    -&gt;get();\n</code></pre>"},{"location":"packages/polyglot/essentials/creating-requests/#message-formats","title":"Message Formats","text":"<p>Polyglot supports different message formats depending on the provider:</p> <ul> <li>String: A simple string will be converted to a user message</li> <li>Array of messages: Each message should have a <code>role</code> and <code>content</code> field</li> <li>Multimodal content: Some providers support images in messages</li> </ul> <p>Example with image (for providers that support it):</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$imageData = base64_encode(file_get_contents('image.jpg'));\n$messages = [\n    [\n        'role' =&gt; 'user',\n        'content' =&gt; [\n            [\n                'type' =&gt; 'text',\n                'text' =&gt; 'What\\'s in this image?'\n            ],\n            [\n                'type' =&gt; 'image_url',\n                'image_url' =&gt; [\n                    'url' =&gt; \"data:image/jpeg;base64,$imageData\"\n                ]\n            ]\n        ]\n    ]\n];\n\n$response = (new Inference())\n    -&gt;using('openai')\n    -&gt;withModel('gpt-4o') // use multimodal model\n    -&gt;with(messages: $messages)\n    -&gt;get();\n</code></pre> <p>Instructor library offers <code>Cognesy\\Messages\\Utils\\Image</code> class for easier conversion of image files to the message format.</p>"},{"location":"packages/polyglot/essentials/inference-class/","title":"Inference Class","text":"<p>The <code>Inference</code> class is the primary facade for making requests to LLM providers in Polyglot. It provides a unified interface for configuring providers, building requests, and executing inference operations.</p>"},{"location":"packages/polyglot/essentials/inference-class/#architecture-overview","title":"Architecture Overview","text":"<p>The <code>Inference</code> class combines functionality through traits: - HandlesLLMProvider: Provider configuration and driver management - HandlesRequestBuilder: Request construction and configuration - HandlesInvocation: Request execution and PendingInference creation - HandlesShortcuts: Convenient methods for common response formats</p>"},{"location":"packages/polyglot/essentials/inference-class/#basic-usage","title":"Basic Usage","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Simple text completion\n$response = (new Inference())\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;get();\n\n// Using a specific provider\n$response = (new Inference())\n    -&gt;using('openai')\n    -&gt;withMessages('Explain quantum physics')\n    -&gt;get();\n</code></pre>"},{"location":"packages/polyglot/essentials/inference-class/#llm-provider-configuration-methods","title":"LLM Provider Configuration Methods","text":"<p>Configure the underlying LLM provider:</p> <pre><code>// Provider selection and configuration\n$inference-&gt;using('openai');                           // Use preset configuration\n$inference-&gt;withDsn('openai://model=gpt-4');          // Configure via DSN\n$inference-&gt;withLLMConfig($customConfig);              // Explicit configuration\n$inference-&gt;withConfigProvider($configProvider);      // Custom config provider\n$inference-&gt;withLLMConfigOverrides(['temperature' =&gt; 0.7]);\n\n// HTTP client configuration\n$inference-&gt;withHttpClient($customHttpClient);        // Custom HTTP client\n$inference-&gt;withHttpClientPreset('debug');           // HTTP client preset\n$inference-&gt;withHttpDebugPreset('verbose');          // Debug configuration\n\n// Driver management\n$inference-&gt;withDriver($customDriver);               // Custom inference driver\n</code></pre>"},{"location":"packages/polyglot/essentials/inference-class/#request-building-methods","title":"Request Building Methods","text":"<p>Configure the inference request:</p> <pre><code>// Message configuration\n$inference-&gt;withMessages('Hello, world!');           // String message\n$inference-&gt;withMessages(['user' =&gt; 'Hello']);       // Array format\n$inference-&gt;withMessages($messageObject);            // Message object\n\n// Model and generation parameters\n$inference-&gt;withModel('gpt-4');                      // Specific model\n$inference-&gt;withMaxTokens(100);                      // Token limit\n$inference-&gt;withOutputMode($outputMode);             // Response format mode\n\n// Tool usage\n$inference-&gt;withTools($toolDefinitions);             // Available tools\n$inference-&gt;withToolChoice('auto');                  // Tool selection strategy\n\n// Response formatting\n$inference-&gt;withResponseFormat(['type' =&gt; 'json']); // Response format\n$inference-&gt;withOptions(['temperature' =&gt; 0.5]);    // Additional options\n\n// Advanced features\n$inference-&gt;withStreaming(true);                     // Enable streaming\n$inference-&gt;withCachedContext($messages, $tools);   // Context caching\n</code></pre>"},{"location":"packages/polyglot/essentials/inference-class/#invocation-methods","title":"Invocation Methods","text":"<p>Execute inference requests:</p> <pre><code>// Flexible configuration method\n$inference-&gt;with(\n    messages: 'Hello',\n    model: 'gpt-4',\n    tools: [],\n    toolChoice: 'auto',\n    responseFormat: ['type' =&gt; 'text'],\n    options: ['temperature' =&gt; 0.7],\n    mode: OutputMode::Text\n);\n\n// Create pending inference for advanced handling\n$pending = $inference-&gt;create();\n\n// Direct request execution\n$inference-&gt;withRequest($existingRequest);\n</code></pre>"},{"location":"packages/polyglot/essentials/inference-class/#runtime-first-usage-cancreateinference","title":"Runtime-First Usage (<code>CanCreateInference</code>)","text":"<p>For constructor-injected creators, convert the facade to runtime and call <code>create()</code> with an explicit <code>InferenceRequest</code>:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$creator = (new Inference())\n    -&gt;using('openai')\n    -&gt;toRuntime();\n\n$request = new InferenceRequest(\n    messages: 'What is the capital of France?',\n    model: 'gpt-4o-mini',\n);\n\n$response = $creator-&gt;create($request)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/essentials/inference-class/#response-shortcuts","title":"Response Shortcuts","text":"<p>Get responses in different formats:</p> <pre><code>// Text responses\n$text = $inference-&gt;get();                           // Plain text\n$response = $inference-&gt;response();                  // Full InferenceResponse object\n\n// JSON responses  \n$json = $inference-&gt;asJson();                        // JSON string\n$data = $inference-&gt;asJsonData();                    // Parsed array\n\n// Streaming\n$stream = $inference-&gt;stream();                      // InferenceStream object\n</code></pre>"},{"location":"packages/polyglot/essentials/inference-class/#driver-registration","title":"Driver Registration","text":"<p>Register custom drivers for new providers:</p> <pre><code>// Register with class name\nInference::registerDriver('custom-provider', CustomDriver::class);\n\n// Register with factory callable\nInference::registerDriver('custom-provider', function($config, $httpClient) {\n    return new CustomDriver($config, $httpClient);\n});\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/","title":"Overview of Inference","text":"<p><code>Inference</code> class offers access to LLM APIs and convenient methods to execute model inference, incl. chat completions, tool calling or JSON output generation.</p> <p>LLM providers access details can be found and modified via <code>/config/llm.php</code>.</p>"},{"location":"packages/polyglot/essentials/overview/#simple-text-generation","title":"Simple Text Generation","text":"<p>The simplest way to use Polyglot is to generate text using static <code>Inference::text()</code> method. Simplified inference API uses the default connection for convenient ad-hoc calls.</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Generate text using the default connection\n$answer = (new Inference)-&gt;with(messages: 'What is the capital of France?')-&gt;get();\n\necho \"Answer: $answer\";\n\n// Output: Answer: The capital of France is Paris.\n</code></pre> <p>This static method uses the default connection specified in your configuration. Default LLM connection can be configured via config/llm.php.</p>"},{"location":"packages/polyglot/essentials/overview/#creating-an-inference-object","title":"Creating an Inference Object","text":"<p>For more control, you can create an instance of the <code>Inference</code> class:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference();\n\n// Generate text using the default connection\n$answer = $inference-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']]\n)-&gt;get();\n\necho \"Answer: $answer\";\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#specifying-a-connection","title":"Specifying a Connection","text":"<p>You can specify which connection preset to use:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object with a specific connection\n$inference = new Inference();\n$answer = $inference-&gt;using('anthropic')\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France?']]\n    )-&gt;get();\n\necho \"Answer (using Anthropic): $answer\";\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#creating-chat-conversations","title":"Creating Chat Conversations","text":"<p>For multi-turn conversations, provide an array of messages:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a chat conversation\n$messages = [\n    ['role' =&gt; 'user', 'content' =&gt; 'Hello, can you help me with a math problem?'],\n    ['role' =&gt; 'assistant', 'content' =&gt; 'Of course! I\\'d be happy to help with a math problem. What would you like to solve?'],\n    ['role' =&gt; 'user', 'content' =&gt; 'What is the square root of 144?'],\n];\n\n$inference = new Inference();\n$answer = $inference-&gt;with(\n    messages: $messages\n)-&gt;get();\n\necho \"Answer: $answer\";\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#customizing-request-parameters","title":"Customizing Request Parameters","text":"<p>You can customize various parameters for your requests:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference with custom options\n$inference = new Inference();\n$answer = $inference-&gt;with(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'Write a short poem about coding.']],\n    model: 'gpt-4', // Override the default model\n    options: [\n        'temperature' =&gt; 0.7,\n        'max_tokens' =&gt; 100,\n    ]\n)-&gt;get();\n\necho \"Poem: $answer\";\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#fluent-api","title":"Fluent API","text":"<p>Regular inference API allows you to customize inference options, letting you set values specific for a given LLM provider.</p> <p>Most of the provider options are compatible with OpenAI API.</p> <p>This example shows how to create an inference object, specify a connection and generate text using the <code>create()</code> method. The <code>toText()</code> method returns text completion from the LLM response.</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$answer = (new Inference)\n    -&gt;using('openai') // optional, default is set in /config/llm.php\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'What is capital of France']])\n    -&gt;withOptions(['max_tokens' =&gt; 64])\n    -&gt;get();\n\necho \"USER: What is capital of France\\n\";\necho \"ASSISTANT: $answer\\n\";\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#streaming-inference-results","title":"Streaming inference results","text":"<p>Inference API allows streaming responses, which is useful for building more responsive UX as you can display partial responses from LLM as soon as they arrive, without waiting until the whole response is ready.</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$stream = (new Inference)\n    -&gt;withMessages([['role' =&gt; 'user', 'content' =&gt; 'Describe capital of Brasil']])\n    -&gt;withOptions(['max_tokens' =&gt; 512])\n    -&gt;withStreaming()\n    -&gt;stream()\n    -&gt;responses();\n\necho \"USER: Describe capital of Brasil\\n\";\necho \"ASSISTANT: \";\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n}\necho \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#connecting-to-a-specific-llm-api-provider","title":"Connecting to a specific LLM API provider","text":"<p>Instructor allows you to define multiple API connections in <code>llm.php</code> file. This is useful when you want to use different LLMs or API providers in your application.</p> <p>Default configuration is located in <code>/config/llm.php</code> in the root directory of Instructor codebase. It contains a set of predefined connections to all LLM APIs supported out-of-the-box by Instructor.</p> <p>Config file defines connections to LLM APIs and their parameters. It also specifies the default connection to be used when calling Instructor without specifying the client connection.</p> <p><pre><code>    // This is fragment of /config/llm.php file\n    'defaultPreset' =&gt; 'openai',\n    //...\n    'presets' =&gt; [\n        'anthropic' =&gt; [ ... ],\n        'azure' =&gt; [ ... ],\n        'cohere' =&gt; [ ... ],\n        'fireworks' =&gt; [ ... ],\n        'gemini' =&gt; [ ... ],\n        'xai' =&gt; [ ... ],\n        'groq' =&gt; [ ... ],\n        'mistral' =&gt; [ ... ],\n        'ollama' =&gt; [\n            'driver' =&gt; 'ollama',\n            'apiUrl' =&gt; 'http://localhost:11434/v1',\n            'apiKey' =&gt; Env::get('OLLAMA_API_KEY', ''),\n            'endpoint' =&gt; '/chat/completions',\n            'model' =&gt; 'qwen2.5:0.5b',\n            'maxTokens' =&gt; 1024,\n            // select HTTP behavior via HttpClientBuilder or facade-level methods\n        ],\n        'openai' =&gt; [ ... ],\n        'openrouter' =&gt; [ ... ],\n        'together' =&gt; [ ... ],\n    // ...\n</code></pre> To customize the available connections you can either modify existing entries or add your own.</p> <p>Connecting to LLM API via predefined connection is as simple as calling <code>withPreset</code> method with the connection preset name.</p> <pre><code>&lt;?php\n// ...\n$answer = (new Inference)\n    -&gt;using('ollama') // see /config/llm.php\n    -&gt;with(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'What is the capital of France']],\n        options: ['max_tokens' =&gt; 64]\n    )\n    -&gt;get();\n// ...\n</code></pre> <p>You can change the location of the configuration files for Instructor to use via <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable. You can use copies of the default configuration files as a starting point.</p>"},{"location":"packages/polyglot/essentials/overview/#switching-between-providers","title":"Switching Between Providers","text":"<p>Polyglot makes it easy to switch between different LLM providers at runtime.</p>"},{"location":"packages/polyglot/essentials/overview/#using-different-providers-for-llm-requests","title":"Using Different Providers for LLM Requests","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an inference object\n$inference = new Inference();\n\n// Use the default provider (set in config)\n$defaultResponse = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n\necho \"Default provider response: $defaultResponse\\n\\n\";\n\n// Switch to Anthropic\n$anthropicResponse = $inference-&gt;using('anthropic')\n    -&gt;with(\n        messages: 'What is the capital of Germany?'\n    )-&gt;get();\n\necho \"Anthropic response: $anthropicResponse\\n\\n\";\n\n// Switch to Mistral\n$mistralResponse = $inference-&gt;using('mistral')\n    -&gt;with(\n        messages: 'What is the capital of Italy?'\n    )-&gt;get();\n\necho \"Mistral response: $mistralResponse\\n\\n\";\n\n// You can create a new instance for each provider\n$openAI = (new Inference())-&gt;using('openai');\n$anthropic = (new Inference())-&gt;using('anthropic');\n$mistral = (new Inference())-&gt;using('mistral');\n\n// And use them independently\n$responses = [\n    'openai' =&gt; $openAI-&gt;with(messages: 'What is the capital of Spain?')-&gt;get(),\n    'anthropic' =&gt; $anthropic-&gt;with(messages: 'What is the capital of Portugal?')-&gt;get(),\n    'mistral' =&gt; $mistral-&gt;with(messages: 'What is the capital of Greece?')-&gt;get(),\n];\n\nforeach ($responses as $provider =&gt; $response) {\n    echo \"$provider response: $response\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/essentials/overview/#selecting-different-models","title":"Selecting Different Models","text":"<p>Each provider offers multiple models with different capabilities, context lengths, and pricing. Polyglot lets you override the default model for each request.</p>"},{"location":"packages/polyglot/essentials/overview/#specifying-models-for-llm-requests","title":"Specifying Models for LLM Requests","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = (new Inference())-&gt;using('openai');\n\n// Use the default model (set in config)\n$defaultModelResponse = $inference-&gt;with(\n    messages: 'What is machine learning?'\n)-&gt;get();\n\n// Use a specific model\n$specificModelResponse = $inference-&gt;with(\n    messages: 'What is machine learning?',\n    model: 'gpt-4o'  // Override the default model\n)-&gt;get();\n\n// You can also set the model and other options\n$customResponse = $inference-&gt;with(\n    messages: 'What is machine learning?',\n    model: 'gpt-4-turbo',\n    options: [\n        'temperature' =&gt; 0.7,\n        'max_tokens' =&gt; 500,\n    ]\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/essentials/request-options/","title":"Request Options","text":"<p>The <code>options</code> parameter allows you to customize various aspects of the request.</p> <p>NOTE: Except for <code>max_tokens</code>, all option parameters are provider-specific and may not be available or compatible with all providers. Check the provider's API documentation for details.</p>"},{"location":"packages/polyglot/essentials/request-options/#common-options","title":"Common Options","text":"<pre><code>$options = [\n    // Generation parameters\n    'temperature' =&gt; 0.7,         // Controls randomness (0.0 to 1.0)\n    'max_tokens' =&gt; 1000,         // Maximum tokens to generate\n    'top_p' =&gt; 0.95,              // Nucleus sampling parameter\n    'frequency_penalty' =&gt; 0.0,   // Penalize repeated tokens\n    'presence_penalty' =&gt; 0.0,    // Penalize repeated topics\n    'stream' =&gt; false,            // Enable streaming responses\n    'stop' =&gt; [\"\\n\\n\", \"User:\"],  // Stop sequences\n\n    // Provider-specific options\n    'top_k' =&gt; 40,                // For some providers\n    'response_format' =&gt; [        // OpenAI-specific format control\n        'type' =&gt; 'json_object'\n    ],\n    // Additional provider-specific options...\n];\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short poem about programming.',\n    options: $options\n)-&gt;toText();\n</code></pre>"},{"location":"packages/polyglot/essentials/request-options/#provider-specific-options","title":"Provider-Specific Options","text":"<p>Different providers may support additional options. Consult the provider's documentation for details:</p> <pre><code>// Anthropic-specific options\n$anthropicOptions = [\n    'temperature' =&gt; 0.7,\n    'max_tokens' =&gt; 1000,\n    'top_p' =&gt; 0.9,\n    'stop_sequences' =&gt; [\"\\n\\nHuman:\"],\n    'stream' =&gt; true,\n];\n\n$inference = new Inference()-&gt;using('anthropic');\n$response = $inference-&gt;with(\n    messages: 'Write a short poem about programming.',\n    options: $anthropicOptions\n)-&gt;toText();\n</code></pre>"},{"location":"packages/polyglot/essentials/response-handling/","title":"Response Handling","text":"<p>Polyglot's <code>PendingInference</code> class represents pending inference execution. It provides methods to access the response in different formats, but also provides access to streaming responses. It does not execute the request to underlying LLM until you actually access the response data.</p> <p>It is returned by the <code>Inference</code> class when you call the <code>create()</code> method.</p>"},{"location":"packages/polyglot/essentials/response-handling/#basic-response-handling","title":"Basic Response Handling","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages('What is the capital of France?')\n    -&gt;create();\n\n// Get the response as plain text\n$text = $response-&gt;get();\necho \"Text response: $text\\n\";\n\n// Get the response as a JSON object (for JSON responses)\n$json = $response-&gt;asJsonData();\necho \"JSON response: \" . json_encode($json) . \"\\n\";\n\n// Get the full response object\n$fullResponse = $response-&gt;response();\n\n// Access specific information\necho \"Content: \" . $fullResponse-&gt;content() . \"\\n\";\necho \"Finish reason: \" . $fullResponse-&gt;finishReason() . \"\\n\";\necho \"Usage - Total tokens: \" . $fullResponse-&gt;usage()-&gt;total() . \"\\n\";\necho \"Usage - Input tokens: \" . $fullResponse-&gt;usage()-&gt;input() . \"\\n\";\necho \"Usage - Output tokens: \" . $fullResponse-&gt;usage()-&gt;output() . \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/essentials/response-handling/#working-with-streaming-responses","title":"Working with Streaming Responses","text":"<p>For streaming responses, use the <code>stream()</code> method:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference\n    -&gt;withMessages('Write a short story about a robot.')\n    -&gt;withStreaming()\n    -&gt;create();\n\n// Get a generator that yields partial responses\n$stream = $response-&gt;stream()-&gt;responses();\n\necho \"Story: \";\nforeach ($stream as $partialResponse) {\n    // Output each chunk as it arrives\n    echo $partialResponse-&gt;contentDelta;\n\n    // Flush the output buffer to show progress in real-time\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n        flush();\n    }\n}\n\necho \"\\n\\nComplete response: \" . $response-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/essentials/response-handling/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>For models that support function calling or tools:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$tools = [\n    [\n        'type' =&gt; 'function',\n        'function' =&gt; [\n            'name' =&gt; 'get_weather',\n            'description' =&gt; 'Get the current weather in a location',\n            'parameters' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'location' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'The city and state, e.g. San Francisco, CA',\n                    ],\n                    'unit' =&gt; [\n                        'type' =&gt; 'string',\n                        'enum' =&gt; ['celsius', 'fahrenheit'],\n                        'description' =&gt; 'The temperature unit to use',\n                    ],\n                ],\n                'required' =&gt; ['location'],\n            ],\n        ],\n    ],\n];\n\n$inference = new Inference()-&gt;using('openai');\n$response = $inference-&gt;with(\n    messages: 'What is the weather in Paris?',\n    tools: $tools,\n    toolChoice: 'auto',  // Let the model decide when to use tools\n    mode: OutputMode::Tools    // Enable tools mode\n)-&gt;response();\n\n// Check if there are tool calls\nif ($response-&gt;hasToolCalls()) {\n    $toolCalls = $response-&gt;toolCalls();\n    foreach ($toolCalls-&gt;all() as $call) {\n        echo \"Tool called: \" . $call-&gt;name() . \"\\n\";\n        echo \"Arguments: \" . $call-&gt;argsAsJson() . \"\\n\";\n\n        // In a real application, you would call the actual function here\n        // and then send the result back to the model\n        $result = ['temperature' =&gt; 22, 'unit' =&gt; 'celsius', 'condition' =&gt; 'sunny'];\n\n        // Continue the conversation with the tool result\n        $newMessages = [\n            ['role' =&gt; 'user', 'content' =&gt; 'What is the weather in Paris?'],\n            [\n                'role' =&gt; 'assistant',\n                'content' =&gt; '',\n                '_metadata' =&gt; [\n                    'tool_calls' =&gt; [\n                        [\n                            'id' =&gt; $call-&gt;id(),\n                            'function' =&gt; [\n                                'name' =&gt; $call-&gt;name(),\n                                'arguments' =&gt; $call-&gt;argsAsJson(),\n                            ],\n                        ],\n                    ],\n                ],\n            ],\n            [\n                'role' =&gt; 'tool',\n                'content' =&gt; json_encode($result),\n                '_metadata' =&gt; [\n                    'tool_call_id' =&gt; $call-&gt;id(),\n                    'tool_name' =&gt; $call-&gt;name(),\n                ],\n            ],\n        ];\n\n        $finalResponse = $inference-&gt;with(\n            messages: $newMessages\n        )-&gt;get();\n\n        echo \"Final response: $finalResponse\\n\";\n    }\n} else {\n    echo \"Response: \" . $response-&gt;content() . \"\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/internals/adapters/","title":"Adapters","text":"<p>Each provider has a set of adapters that handle its specific format requirements:</p>"},{"location":"packages/polyglot/internals/adapters/#request-adapters","title":"Request Adapters","text":"<p>Request adapters convert Polyglot's unified request format to provider-specific HTTP requests:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIRequestAdapter implements CanTranslateInferenceRequest {\n    public function __construct(\n        protected LLMConfig $config,\n        protected CanMapRequestBody $bodyFormat,\n    ) { ... }\n\n    public function toHttpRequest(InferenceRequest $request): HttpRequest { ... }\n\n    protected function toHeaders(InferenceRequest $request): array { ... }\n    protected function toUrl(InferenceRequest $request): string { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/adapters/#message-formatters","title":"Message Formatters","text":"<p>Message formatters handle the conversion of messages to provider-specific formats:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIMessageFormat implements CanMapMessages {\n    public function map(array $messages): array { ... }\n\n    protected function mapMessage(array $message): array { ... }\n    protected function toNativeToolCall(array $message): array { ... }\n    protected function toNativeToolResult(array $message): array { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/adapters/#body-formatters","title":"Body Formatters","text":"<p>Body formatters handle the conversion of request bodies to provider-specific formats:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIBodyFormat implements CanMapRequestBody {\n    public function __construct(\n        protected LLMConfig $config,\n        protected CanMapMessages $messageFormat,\n    ) { ... }\n\n    public function toRequestBody(InferenceRequest $request): array { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/adapters/#response-adapters","title":"Response Adapters","text":"<p>Response adapters convert provider-specific responses to Polyglot's unified format:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIResponseAdapter implements CanTranslateInferenceResponse {\n    public function __construct(\n        protected CanMapUsage $usageFormat,\n    ) { ... }\n\n    public function fromResponse(HttpResponse $response): ?InferenceResponse { ... }\n    /** @return iterable&lt;PartialInferenceResponse&gt; */\n    public function fromStreamResponses(iterable $eventBodies, ?HttpResponse $responseData = null): iterable { ... }\n    public function toEventBody(string $data): string|bool { ... }\n\n    protected function makeToolCalls(array $data): ToolCalls { ... }\n    protected function makeContent(array $data): string { ... }\n    protected function makeContentDelta(array $data): string { ... }\n    protected function makeToolId(array $data): string { ... }\n    protected function makeToolNameDelta(array $data): string { ... }\n    protected function makeToolArgsDelta(array $data): string { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/adapters/#usage-formatters","title":"Usage Formatters","text":"<p>Usage formatters extract token usage information from provider responses:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers\\OpenAI;\n\nclass OpenAIUsageFormat implements CanMapUsage {\n    public function fromData(array $data): Usage { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/configuration/","title":"Configuration Layer","text":"<p>Polyglot's configuration layer manages settings for different providers.</p>"},{"location":"packages/polyglot/internals/configuration/#llm-configuration","title":"LLM Configuration","text":"<pre><code>namespace Cognesy\\Polyglot\\Inference\\Data;\n\nclass LLMConfig {\n    public function __construct(\n        public string $apiUrl = '',\n        public string $apiKey = '',\n        public string $endpoint = '',\n        public array $queryParams = [],\n        public array $metadata = [],\n        public string $model = '',\n        public int $maxTokens = 1024,\n        public int $contextLength = 8000,\n        public int $maxOutputLength = 4096,\n        public string $httpClient = '',\n        public string $providerType = 'openai-compatible'\n    ) { ... }\n\n    public static function load(string $connection): \\Cognesy\\Polyglot\\Inference\\Config\\LLMConfig { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/configuration/#embeddings-configuration","title":"Embeddings Configuration","text":"<pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Data;\n\nclass EmbeddingsConfig {\n    public function __construct(\n        public string $apiUrl = '',\n        public string $apiKey = '',\n        public string $endpoint = '',\n        public string $model = '',\n        public int $dimensions = 0,\n        public int $maxInputs = 0,\n        public array $metadata = [],\n        public string $httpClient = '',\n        public string $providerType = 'openai'\n    ) { ... }\n\n    public static function load(string $connection): \\Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/events/","title":"Events System","text":"<p>Polyglot uses an event system to generate internal notifications at the various stages of the execution process.</p> <p>It has been built primarily to ensure observability of the internal components of the library.</p> <pre><code>namespace Cognesy\\Utils\\Events;\n\nuse Cognesy\\Events\\Event;\n\nclass EventDispatcher {\n    public function dispatch(Event $event): void { ... }\n    public function wiretap(callable $listener): self { ... }\n    public function addListener(string $eventClass, callable $listener): self { ... }\n}\n\nnamespace Cognesy\\Polyglot\\Inference\\Events;\n\nclass InferenceResponseReceived extends Event {}\n\nclass InferenceRequested extends Event {}\n\nclass PartialInferenceResponseReceived extends Event {}\n</code></pre>"},{"location":"packages/polyglot/internals/http-client/","title":"HTTP Client Layer","text":"<p>At the lowest level, Polyglot uses an HTTP client layer to communicate with provider APIs. This layer includes:</p> <ol> <li>A unified <code>HttpClient</code> interface</li> <li>Implementations for different HTTP libraries (Guzzle, Symfony, Laravel)</li> <li>A middleware system for extending functionality</li> </ol>"},{"location":"packages/polyglot/internals/http-client/#httpclient","title":"HttpClient","text":"<p>The <code>HttpClient</code> class provides a unified interface for HTTP requests:</p> <pre><code>namespace Cognesy\\Http;\n\nuse Cognesy\\Http\\Middleware\\MiddlewareStack;class HttpClient implements CanHandleHttpRequest {\n    public function __construct(\n        string $client = '',\n        ?HttpClientConfig $config = null,\n        ?EventDispatcher $events = null\n    ) { ... }\n\n    public static function make(\n        string $client = '',\n        ?HttpClientConfig $config = null,\n        ?EventDispatcher $events = null\n    ): self { ... }\n\n    public function withClient(string $client): self { ... }\n    public function withConfig(HttpClientConfig $config): self { ... }\n    public function withMiddleware(...$middleware): self { ... }\n    public function withDebugPreset(?string $preset): self { ... }\n\n    public function handle(HttpClientRequest $request): HttpResponse { ... }\n    public function middleware(): MiddlewareStack { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/http-client/#httprequest-and-httpresponse","title":"HttpRequest and HttpResponse","text":"<p>These classes represent HTTP requests and responses:</p> <pre><code>namespace Cognesy\\Http\\Data;\n\nclass HttpRequest {\n    public function __construct(\n        private string $url,\n        private string $method,\n        private array $headers,\n        private mixed $body,\n        private array $options\n    ) { ... }\n\n    public function url(): string { ... }\n    public function method(): string { ... }\n    public function headers(): array { ... }\n    public function body(): HttpRequestBody { ... }\n    public function options(): array { ... }\n    public function isStreamed(): bool { ... }\n\n    public function withStreaming(bool $streaming): self { ... }\n}\n\ninterface HttpResponse {\n    public function statusCode(): int;\n    public function headers(): array;\n    public function body(): string;\n    public function stream(int $chunkSize = 1): Generator;\n    public function original(): mixed;\n}\n</code></pre>"},{"location":"packages/polyglot/internals/http-client/#middleware-system","title":"Middleware System","text":"<p>The HTTP client layer includes a middleware system that allows extending functionality:</p> <pre><code>namespace Cognesy\\Http;\n\ninterface HttpMiddleware {\n    public function handle(\n        HttpRequest $request,\n        CanHandleHttpRequest $next\n    ): HttpResponse;\n}\n\nabstract class BaseMiddleware implements HttpMiddleware {\n    public function handle(\n        HttpRequest $request,\n        CanHandleHttpRequest $next\n    ): HttpResponse { ... }\n\n    protected function beforeRequest(HttpClientRequest $request): void {}\n\n    protected function afterRequest(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return $response;\n    }\n\n    protected function shouldDecorateResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): bool {\n        return false;\n    }\n\n    protected function toResponse(\n        HttpRequest $request,\n        HttpResponse $response\n    ): HttpResponse {\n        return $response;\n    }\n}\n\nclass MiddlewareStack {\n    public function append(HttpMiddleware $middleware, string $name = ''): self { ... }\n    public function prepend(HttpMiddleware $middleware, string $name = ''): self { ... }\n    public function remove(string $name): self { ... }\n    public function replace(string $name, HttpMiddleware $middleware): self { ... }\n    public function clear(): self { ... }\n    public function has(string $name): bool { ... }\n    public function get(string|int $nameOrIndex): ?HttpMiddleware { ... }\n    public function all(): array { ... }\n    public function process(\n        HttpRequest $request,\n        CanHandleHttpRequest $handler\n    ): HttpResponse { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/lifecycle/","title":"Request/Response Lifecycle","text":"<p>Let's follow the complete flow of a request through Polyglot:</p>"},{"location":"packages/polyglot/internals/lifecycle/#request-processing","title":"Request Processing","text":"<ol> <li>Application creates an <code>Inference</code> object</li> <li>Application calls <code>create()</code> with parameters</li> <li><code>Inference</code> creates an <code>InferenceRequest</code>.</li> <li><code>Inference</code> creates a <code>PendingInference</code> object with the instances of request, driver and event dispatcher.</li> <li><code>Inference</code> returns a <code>PendingInference</code> object to the application.</li> </ol>"},{"location":"packages/polyglot/internals/lifecycle/#response-processing","title":"Response Processing","text":"<ol> <li>Application accesses the <code>PendingInference</code> object content, e.g. via <code>response()</code> method.</li> <li><code>PendingInference</code> checks if HTTP request has been already executed.</li> <li>If already sent, it returns the cached response.</li> <li><code>PendingInference</code> dispatches the <code>InferenceRequested</code> event</li> <li><code>PendingInference</code> passes the request to the driver.</li> <li>Driver uses request adapter to create HTTP request</li> <li>Request adapter uses request body formatter and message formatter.</li> <li>Driver sends the HTTP request and returns it to <code>PendingInference</code>.</li> <li><code>PendingInference</code> calls the driver to read and parse the response.</li> <li>Driver uses a response adapter to extract content into appropriate fields of <code>InferenceResponse</code> object</li> <li><code>PendingInference</code> dispatches the <code>InferenceResponseReceived</code> event</li> <li>Result <code>InferenceResponse</code> object is returned to the application</li> </ol>"},{"location":"packages/polyglot/internals/overview/","title":"Overview of Architecture","text":"<p>This section provides a detailed look at Polyglot's internal architecture.</p> <p>Understanding the core components, interfaces, and design patterns will help you extend the library, contribute to its development, or build your own integrations with new LLM providers.</p>"},{"location":"packages/polyglot/internals/overview/#core-architecture","title":"Core Architecture","text":"<p>Polyglot is built on a modular, layered architecture that separates concerns and promotes extensibility. The high-level architecture consists of:</p> <ol> <li>Public API Layer: Classes like <code>Inference</code> and <code>Embeddings</code> that provide a unified interface for applications</li> <li>Provider Abstraction Layer: Adapters, drivers, and formatters that translate between the unified API and provider-specific formats</li> <li>HTTP Client Layer: A flexible HTTP client with middleware support for communicating with LLM APIs</li> <li>Configuration Layer: Configuration management for different providers and models</li> </ol> <pre><code>+---------------------+    +---------------------+\n|      Inference      |    |     Embeddings      |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|  InferenceRequest   |    | EmbeddingsRequest   |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|   PendingInference  |    |  PendingEmbeddings  |\n+---------------------+    +---------------------+\n            |                        |\n+---------------------+    +---------------------+\n|  InferenceDrivers   |    |  EmbeddingsDrivers  |\n+---------------------+    +---------------------+\n            |                        |\n+------------------------------------------------+\n|               HTTP Client Layer                |\n+------------------------------------------------+\n                         |\n+------------------------------------------------+\n|           Provider-specific API Calls          |\n+------------------------------------------------+\n</code></pre>"},{"location":"packages/polyglot/internals/providers/","title":"Provider Abstraction Layer","text":"<p>The provider abstraction layer is where Polyglot handles the differences between LLM and embedding providers. This layer includes:</p> <ol> <li>Provider Classes: <code>LLMProvider</code> and <code>EmbeddingsProvider</code> - Builder classes for configuring and resolving driver configurations</li> <li>Drivers: Classes that implement provider-specific logic for inference and embeddings</li> <li>Adapters: Classes that convert between unified and provider-specific formats</li> <li>Factories: Classes that create appropriate drivers based on configuration</li> </ol>"},{"location":"packages/polyglot/internals/providers/#provider-builder-classes","title":"Provider Builder Classes","text":""},{"location":"packages/polyglot/internals/providers/#llmprovider","title":"LLMProvider","text":"<p>The <code>LLMProvider</code> class is a builder that configures inference settings and resolves configuration from various sources. It provides a fluent interface for setting up LLM configurations:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\n\n// Create with preset\n$provider = LLMProvider::using('openai');\n\n// Create with DSN\n$provider = LLMProvider::dsn('openai://model=gpt-4&amp;temperature=0.7');\n\n// Fluent configuration\n$provider = LLMProvider::new()\n    -&gt;withLLMPreset('openai')\n    -&gt;withLLMConfig($customConfig);\n\n// Create the final driver using a factory and injected HTTP client\n$httpClient = (new \\Cognesy\\Http\\Creation\\HttpClientBuilder())-&gt;create();\n$config = $provider-&gt;resolveConfig();\n$driver = (new \\Cognesy\\Polyglot\\Inference\\Creation\\InferenceDriverFactory($events))\n    -&gt;makeDriver($config, $httpClient);\n</code></pre> <p>Key methods: - <code>withLLMPreset(string $preset)</code>: Set configuration preset - <code>withLLMConfig(LLMConfig $config)</code>: Set explicit configuration - <code>withConfigOverrides(array $overrides)</code>: Override specific config values - <code>withDsn(string $dsn)</code>: Configure via DSN string - <code>withDriver(CanHandleInference $driver)</code>: Set explicit driver - Use <code>resolveConfig()</code> + <code>InferenceDriverFactory::makeDriver()</code> to create drivers</p>"},{"location":"packages/polyglot/internals/providers/#embeddingsprovider","title":"EmbeddingsProvider","text":"<p>The <code>EmbeddingsProvider</code> class builds and configures embeddings settings, resolving configuration from various sources:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsProvider;\n\n// Create with preset\n$provider = EmbeddingsProvider::using('openai');\n\n// Create with DSN\n$provider = EmbeddingsProvider::dsn('openai://model=text-embedding-3-large');\n\n// Fluent configuration\n$provider = EmbeddingsProvider::new()\n    -&gt;withPreset('openai')\n    -&gt;withConfig($customConfig);\n\n// Create the final driver using a factory and injected HTTP client\n$httpClient = (new \\Cognesy\\Http\\Creation\\HttpClientBuilder())-&gt;create();\n$config = $provider-&gt;resolveConfig();\n$driver = (new \\Cognesy\\Polyglot\\Embeddings\\Drivers\\EmbeddingsDriverFactory($events))\n    -&gt;makeDriver($config, $httpClient);\n</code></pre> <p>Key methods: - <code>withPreset(string $preset)</code>: Set configuration preset - <code>withConfig(EmbeddingsConfig $config)</code>: Set explicit configuration - <code>withDsn(string $dsn)</code>: Configure via DSN string - <code>withDriver(CanHandleVectorization $driver)</code>: Set explicit driver - Use <code>resolveConfig()</code> + <code>EmbeddingsDriverFactory::makeDriver()</code> to create drivers</p>"},{"location":"packages/polyglot/internals/providers/#key-interfaces-for-llm","title":"Key Interfaces for LLM","text":"<p>Several interfaces define the contract for LLM drivers and adapters:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Contracts;\n\ninterface CanHandleInference {\n    public function makeResponseFor(InferenceRequest $request): InferenceResponse;\n    /** @return iterable&lt;PartialInferenceResponse&gt; */\n    public function makeStreamResponsesFor(InferenceRequest $request): iterable;\n    public function capabilities(?string $model = null): DriverCapabilities;\n}\n\ninterface CanTranslateInferenceRequest {\n    public function toHttpRequest(InferenceRequest $request): HttpRequest;\n}\n\ninterface CanTranslateInferenceResponse {\n    public function fromResponse(HttpResponse $response): ?InferenceResponse;\n    /** @return iterable&lt;PartialInferenceResponse&gt; */\n    public function fromStreamResponses(iterable $eventBodies, ?HttpResponse $responseData = null): iterable;\n    public function toEventBody(string $data): string|bool;\n}\n\ninterface CanMapMessages {\n    public function map(array $messages): array;\n}\n\ninterface CanMapRequestBody {\n    public function toRequestBody(InferenceRequest $request): array;\n}\n\ninterface CanMapUsage {\n    public function fromData(array $data): Usage;\n}\n</code></pre>"},{"location":"packages/polyglot/internals/providers/#key-interfaces-for-embeddings","title":"Key Interfaces for Embeddings","text":"<p>The embeddings functionality uses these key interfaces:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Contracts;\n\n// Main driver interface\ninterface CanHandleVectorization {\n    public function vectorize(EmbeddingsRequest $request): EmbeddingsResponse;\n}\n\n// Request and response mapping interfaces\ninterface CanMapRequestBody {\n    public function map(EmbeddingsRequest $request): array;\n}\n\ninterface EmbedRequestAdapter {\n    public function toHttpRequest(EmbeddingsRequest $request): HttpRequest;\n}\n\ninterface EmbedResponseAdapter {\n    public function fromHttpResponse(HttpResponse $response): EmbeddingsResponse;\n}\n\ninterface CanMapUsage {\n    public function fromData(array $data): Usage;\n}\n</code></pre>"},{"location":"packages/polyglot/internals/providers/#baseinferencedriver","title":"BaseInferenceDriver","text":"<p>The <code>BaseInferenceDriver</code> is the abstract base class that implements <code>CanHandleInference</code> using request/response translators:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers;\n\nabstract class BaseInferenceDriver implements CanHandleInference {\n    protected LLMConfig $config;\n    protected HttpClient $httpClient;\n    protected EventDispatcherInterface $events;\n    protected CanTranslateInferenceRequest $requestTranslator;\n    protected CanTranslateInferenceResponse $responseTranslator;\n\n    public function makeResponseFor(InferenceRequest $request): InferenceResponse { ... }\n    /** @return iterable&lt;PartialInferenceResponse&gt; */\n    public function makeStreamResponsesFor(InferenceRequest $request): iterable { ... }\n    public function capabilities(?string $model = null): DriverCapabilities { ... }\n}\n</code></pre> <p>Provider-specific drivers (e.g. <code>OpenAIDriver</code>, <code>AnthropicDriver</code>) extend <code>BaseInferenceDriver</code> and wire up their own request/response translators in the constructor.</p>"},{"location":"packages/polyglot/internals/providers/#driver-factories","title":"Driver Factories","text":""},{"location":"packages/polyglot/internals/providers/#inferencedriverfactory","title":"InferenceDriverFactory","text":"<p>The <code>InferenceDriverFactory</code> creates the appropriate driver for each LLM provider:</p> <pre><code>namespace Cognesy\\Polyglot\\Inference\\Drivers;\n\nclass InferenceDriverFactory {\n    public function makeDriver(\n        LLMConfig $config,\n        HttpClient $httpClient\n    ): CanHandleInference { ... }\n\n    // Provider-specific factory methods\n    public function openAI(...): CanHandleInference { ... }\n    public function anthropic(...): CanHandleInference { ... }\n    public function mistral(...): CanHandleInference { ... }\n    // Other providers...\n\n    // Driver registration\n    public static function registerDriver(string $name, string|callable $driver): void { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/providers/#embeddingsdriverfactory","title":"EmbeddingsDriverFactory","text":"<p>The <code>EmbeddingsDriverFactory</code> creates embeddings drivers:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings\\Drivers;\n\nclass EmbeddingsDriverFactory {\n    public function makeDriver(\n        EmbeddingsConfig $config,\n        HttpClient $httpClient\n    ): CanHandleVectorization { ... }\n\n    // Provider-specific factory methods  \n    public function openAI(...): CanHandleVectorization { ... }\n    public function cohere(...): CanHandleVectorization { ... }\n    public function gemini(...): CanHandleVectorization { ... }\n    // Other providers...\n\n    // Driver registration\n    public static function registerDriver(string $name, string|callable $driver): void { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/public-api/","title":"Public API Layer","text":""},{"location":"packages/polyglot/internals/public-api/#the-inference-class","title":"The Inference Class","text":"<p>The <code>Inference</code> class is the main entry point for LLM interactions. It encapsulates the complexities of different providers behind a unified interface.</p> <pre><code>namespace Cognesy\\Polyglot\\Inference;\n\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Events\\Contracts\\CanHandleEvents;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Inference\\Config\\LLMConfig;\nuse Cognesy\\Polyglot\\Inference\\Contracts\\CanProcessInferenceRequest;\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;\nuse Cognesy\\Polyglot\\Inference\\InferenceRuntime;\nuse Psr\\EventDispatcher\\EventDispatcherInterface;\n\nclass Inference {\n    public function __construct(\n        null|CanHandleEvents|EventDispatcherInterface $events = null,\n        ?CanProvideConfig $configProvider = null\n    ) { ... }\n\n    // Infrastructure/provider configuration\n    public function using(string $preset): self { ... }\n    public function withDsn(string $dsn): self { ... }\n    public function withLLMConfig(LLMConfig $config): self { ... }\n    public function withLLMConfigOverrides(array $overrides): self { ... }\n    public function withConfigProvider(CanProvideConfig $configProvider): self { ... }\n    public function withHttpClient(HttpClient $httpClient): self { ... }\n    public function withDriver(CanProcessInferenceRequest $driver): self { ... }\n    public function withHttpClientPreset(string $preset): self { ... }\n    public function withHttpDebugPreset(?string $preset): self { ... }\n    public function withEventHandler(CanHandleEvents|EventDispatcherInterface $events): self { ... }\n\n    // Request building and execution\n    public function with(...): self { ... }\n    public function withRequest(InferenceRequest $request): self { ... }\n    public function create(?InferenceRequest $request = null): PendingInference { ... }\n    public function toRuntime(): InferenceRuntime { ... }\n\n    // Convenience response accessors\n    public function get(): string { ... }\n    public function response(): InferenceResponse { ... }\n    public function stream(): InferenceStream { ... }\n}\n</code></pre> <p>The <code>Inference</code> class follows a fluent interface pattern, allowing method chaining for configuration.</p>"},{"location":"packages/polyglot/internals/public-api/#the-embeddings-class","title":"The Embeddings Class","text":"<p>Similarly, the <code>Embeddings</code> class provides a unified interface for generating embeddings:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings;\n\nuse Cognesy\\Config\\Contracts\\CanProvideConfig;\nuse Cognesy\\Events\\Contracts\\CanHandleEvents;\nuse Cognesy\\Http\\HttpClient;\nuse Cognesy\\Polyglot\\Embeddings\\Config\\EmbeddingsConfig;\nuse Cognesy\\Polyglot\\Embeddings\\Contracts\\CanHandleVectorization;\nuse Cognesy\\Polyglot\\Embeddings\\Data\\EmbeddingsRequest;\nuse Cognesy\\Polyglot\\Embeddings\\EmbeddingsRuntime;\nuse Psr\\EventDispatcher\\EventDispatcherInterface;\n\nclass Embeddings {\n    public function __construct(\n        null|CanHandleEvents|EventDispatcherInterface $events = null,\n        ?CanProvideConfig $configProvider = null\n    ) { ... }\n\n    // Infrastructure/provider configuration\n    public function using(string $preset): self { ... }\n    public function withDsn(string $dsn): self { ... }\n    public function withConfig(EmbeddingsConfig $config): self { ... }\n    public function withConfigProvider(CanProvideConfig $configProvider): self { ... }\n    public function withHttpClient(HttpClient $httpClient): self { ... }\n    public function withDriver(CanHandleVectorization $driver): self { ... }\n    public function withHttpDebugPreset(?string $preset): self { ... }\n\n    // Request building and execution\n    public function with(...): self { ... }\n    public function withRequest(EmbeddingsRequest $request): self { ... }\n    public function create(?EmbeddingsRequest $request = null): PendingEmbeddings { ... }\n    public function toRuntime(): EmbeddingsRuntime { ... }\n\n    // Convenience response accessors\n    public function get(): EmbeddingsResponse { ... }\n    public function first(): ?Vector { ... }\n    public function vectors(): array { ... }\n}\n</code></pre> <p>Similarity-search helper methods are provided by <code>EmbedUtils</code>, not by the <code>Embeddings</code> facade:</p> <pre><code>use Cognesy\\Polyglot\\Embeddings\\Utils\\EmbedUtils;\n\n$matches = EmbedUtils::findSimilar(\n    embeddings: (new Embeddings())-&gt;using('openai')-&gt;toRuntime(),\n    query: $query,\n    documents: $documents,\n    topK: 5,\n);\n</code></pre>"},{"location":"packages/polyglot/internals/request-response/","title":"Request and Response Objects","text":""},{"location":"packages/polyglot/internals/request-response/#inferencerequest","title":"InferenceRequest","text":"<p>The <code>InferenceRequest</code> class encapsulates all the parameters needed for an LLM request:</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nclass InferenceRequest {\n    public array $messages = [];\n    public string $model = '';\n    public array $tools = [];\n    public string|array $toolChoice = [];\n    public array $responseFormat = [];\n    public array $options = [];\n    public Mode $mode = OutputMode::Text;\n    public ?CachedContext $cachedContext;\n\n    public function __construct(...) { ... }\n\n    // Getters\n    public function id(): InferenceRequestId { ... }\n    public function messages(): array { ... }\n    public function model(): string { ... }\n    public function isStreamed(): bool { ... }\n    public function tools(): array { ... }\n    public function toolChoice(): string|array { ... }\n    public function responseFormat(): array { ... }\n    public function options(): array { ... }\n    public function mode(): Mode { ... }\n    public function cachedContext(): ?CachedContext { ... }\n\n    // Fluent setters\n    public function withMessages(string|array $messages): self { ... }\n    public function withModel(string $model): self { ... }\n    public function withStreaming(bool $streaming): self { ... }\n    public function withTools(array $tools): self { ... }\n    public function withToolChoice(string|array $toolChoice): self { ... }\n    public function withResponseFormat(array $responseFormat): self { ... }\n    public function withOptions(array $options): self { ... }\n    public function withMode(Mode $mode): self { ... }\n    public function withCachedContext(?CachedContext $cachedContext): self { ... }\n\n    // Utility methods\n    public function toArray(): array { ... }\n    public function withCacheApplied(): self { ... }\n}\n</code></pre> <p>Internally, request identity is represented by <code>InferenceRequestId</code> and can be converted to string at boundaries via <code>-&gt;id()-&gt;toString()</code>. Attempt identity in execution state is represented by <code>InferenceAttemptId</code> and is serialized as string in event payloads and arrays. Execution identity is represented by <code>InferenceExecutionId</code> and converted to string when emitted in events and serialized output. Response identity is represented by <code>InferenceResponseId</code> and serialized as string at boundaries. Partial streaming chunk identity is represented by <code>PartialInferenceResponseId</code> and serialized as string at boundaries. External tool-call identity is represented by <code>ToolCallId</code>; OpenResponses stream item identity uses <code>OpenResponseItemId</code> internally and converts to string at stream/transport boundaries.</p>"},{"location":"packages/polyglot/internals/request-response/#pendinginference","title":"PendingInference","text":"<p>The <code>PendingInference</code> class handles the response from an LLM API:</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nuse Cognesy\\Polyglot\\Inference\\Data\\InferenceRequest;class PendingInference {\n    public function __construct(\n        InferenceRequest $request,\n        CanHandleInference $driver,\n        EventDispatcherInterface $events,\n    ) { ... }\n\n    // Access methods\n    public function isStreamed(): bool { ... }\n    public function toText(): string { ... }\n    public function toArray(): array { ... }\n    public function stream(): InferenceStream { ... }\n    public function response(): InferenceResponse { ... }\n}\n</code></pre> <p>For streaming responses, the <code>InferenceStream</code> class provides methods to process the stream:</p> <pre><code>namespace Cognesy\\Polyglot\\LLM;\n\nclass InferenceStream {\n    public function __construct(\n        HttpResponse        $httpResponse,\n        CanHandleInference        $driver,\n        EventDispatcherInterface  $events,\n    ) { ... }\n\n    // Stream processing methods\n    public function responses(): Generator { ... }\n    public function all(): array { ... }\n    public function final(): ?InferenceResponse { ... }\n    public function onPartialResponse(callable $callback): self { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/internals/request-response/#embeddingsresponse","title":"EmbeddingsResponse","text":"<p>The <code>EmbeddingsResponse</code> class encapsulates the result of an embeddings request:</p> <pre><code>namespace Cognesy\\Polyglot\\Embeddings;\n\nclass EmbeddingsResponse {\n    public function __construct(\n        public array $vectors,\n        public ?Usage $usage\n    ) { ... }\n\n    // Access methods\n    public function first(): Vector { ... }\n    public function last(): Vector { ... }\n    public function all(): array { ... }\n    public function usage(): Usage { ... }\n    public function toValuesArray(): array { ... }\n    public function totalTokens(): int { ... }\n    public function split(int $index): array { ... }\n}\n</code></pre>"},{"location":"packages/polyglot/modes/json-schema/","title":"JSON Schema Mode","text":"<p>JSON Schema mode takes JSON generation a step further by validating the response against a predefined schema. This guarantees the response has the expected structure when the provider supports native JSON Schema validation.</p>"},{"location":"packages/polyglot/modes/json-schema/#defining-and-using-a-json-schema","title":"Defining and Using a JSON Schema","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');  // Currently best supported by OpenAI\n\n// Define a schema for a weather report\n$schema = [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'location' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'The city and country'\n        ],\n        'current_temperature' =&gt; [\n            'type' =&gt; 'number',\n            'description' =&gt; 'Current temperature in Celsius'\n        ],\n        'conditions' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'Current weather conditions (e.g., sunny, rainy)'\n        ],\n        'forecast' =&gt; [\n            'type' =&gt; 'array',\n            'items' =&gt; [\n                'type' =&gt; 'object',\n                'properties' =&gt; [\n                    'day' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'Day of the week'\n                    ],\n                    'temperature_high' =&gt; [\n                        'type' =&gt; 'number',\n                        'description' =&gt; 'Expected high temperature in Celsius'\n                    ],\n                    'temperature_low' =&gt; [\n                        'type' =&gt; 'number',\n                        'description' =&gt; 'Expected low temperature in Celsius'\n                    ],\n                    'conditions' =&gt; [\n                        'type' =&gt; 'string',\n                        'description' =&gt; 'Expected weather conditions'\n                    ]\n                ],\n                'required' =&gt; ['day', 'temperature_high', 'temperature_low', 'conditions']\n            ],\n            'description' =&gt; 'Three-day weather forecast'\n        ]\n    ],\n    'required' =&gt; ['location', 'current_temperature', 'conditions', 'forecast']\n];\n\n// Request a weather report\n$response = $inference-&gt;with(\n    messages: 'Provide a weather report for Paris, France.',\n    responseFormat: [\n        'type' =&gt; 'json_schema',\n        'json_schema' =&gt; [\n            'name' =&gt; 'weather_report',\n            'schema' =&gt; $schema,\n            'strict' =&gt; true,\n        ],\n    ],\n    mode: OutputMode::JsonSchema\n)-&gt;asJsonData();\n\n// The response will match the schema's structure exactly\necho \"Weather in {$response['location']}:\\n\";\necho \"Currently {$response['conditions']} and {$response['current_temperature']}\u00b0C\\n\\n\";\n\necho \"Forecast:\\n\";\nforeach ($response['forecast'] as $day) {\n    echo \"{$day['day']}: {$day['conditions']}, {$day['temperature_low']}\u00b0C to {$day['temperature_high']}\u00b0C\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/modes/json-schema/#schema-validation","title":"Schema Validation","text":"<p>With JSON Schema mode, Polyglot enforces the schema when native support is available:</p> <ol> <li>The schema is sent to the model as part of the request</li> <li>The model structures its response to match the schema</li> <li>For providers with native schema support, validation happens at the API level</li> <li>For other providers, results are best-effort and should be treated as JSON output without strict guarantees</li> </ol>"},{"location":"packages/polyglot/modes/json-schema/#provider-support-for-json-schema","title":"Provider Support for JSON Schema","text":"<p>Provider support for JSON Schema varies:</p> <ul> <li>OpenAI (GPT-4 and newer): Native support with <code>json_schema</code> response format</li> <li>Other providers: No native guarantee; use <code>OutputMode::Json</code> or <code>OutputMode::MdJson</code> for best-effort output</li> </ul> <p>For guaranteed schema validation, use a provider with native JSON Schema support.</p>"},{"location":"packages/polyglot/modes/json-schema/#when-to-use-json-schema-mode","title":"When to Use JSON Schema Mode","text":"<p>JSON Schema mode is ideal for: - Applications requiring strictly typed data - Integration with databases or APIs that expect specific structures - Data extraction with complex nested structures - Ensuring consistent response formats across multiple requests</p>"},{"location":"packages/polyglot/modes/json/","title":"JSON Mode","text":"<p>JSON mode instructs the model to return responses formatted as valid JSON objects. This is useful when you need structured data that can be easily processed by your application.</p>"},{"location":"packages/polyglot/modes/json/#basic-json-generation","title":"Basic JSON Generation","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n$response = $inference-&gt;with(\n    messages: 'List the top 3 most populous cities in the world with their populations.',\n    mode: OutputMode::Json\n)-&gt;asJsonData();\n\n// $response is now a PHP array parsed from the JSON\necho \"Top cities:\\n\";\nforeach ($response['cities'] as $city) {\n    echo \"- {$city['name']}: {$city['population']} million\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/modes/json/#structuring-json-responses-with-instructions","title":"Structuring JSON Responses with Instructions","text":"<p>For best results, include clear instructions about the expected JSON structure:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// Include the expected structure in the prompt\n$prompt = &lt;&lt;&lt;EOT\nList the top 3 most populous cities in the world.\nReturn your answer as a JSON object with the following structure:\n{\n  \"cities\": [\n    {\n      \"name\": \"City name\",\n      \"country\": \"Country name\",\n      \"population\": population in millions (number)\n    },\n    ...\n  ]\n}\nEOT;\n\n$response = $inference-&gt;with(\n    messages: $prompt,\n    mode: OutputMode::Json\n)-&gt;asJsonData();\n\n// Process the response\necho \"Top cities by population:\\n\";\nforeach ($response['cities'] as $index =&gt; $city) {\n    echo ($index + 1) . \". {$city['name']}, {$city['country']}: {$city['population']} million\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/modes/json/#provider-specific-json-options","title":"Provider-Specific JSON Options","text":"<p>Some providers offer additional options for JSON mode:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// OpenAI example\n$inference = new Inference()-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'List the top 3 most populous cities in the world.',\n    mode: OutputMode::Json,\n    options: [\n        'response_format' =&gt; ['type' =&gt; 'json_object'],\n        // Other OpenAI-specific options...\n    ]\n)-&gt;asJsonData();\n\n// The response will be a JSON object\n</code></pre>"},{"location":"packages/polyglot/modes/json/#when-to-use-json-mode","title":"When to Use JSON Mode","text":"<p>JSON mode is ideal for: - Extracting structured data (lists, records, etc.) - API responses that need to be machine-readable - Generating data for web applications - Creating datasets</p>"},{"location":"packages/polyglot/modes/md-json/","title":"MdJSON Mode","text":"<p>Markdown JSON mode is a special mode that requests the model to format its response as JSON within a Markdown code block. This is particularly useful for models or providers that don't have native JSON output support.</p>"},{"location":"packages/polyglot/modes/md-json/#using-markdown-json-mode","title":"Using Markdown JSON Mode","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// This works with virtually any provider\n$response = $inference-&gt;with(\n    messages: 'List three programming languages and their key features.',\n    mode: OutputMode::MdJson\n)-&gt;asJsonData();\n\n// The model will return JSON wrapped in Markdown, which Polyglot processes for you\nforeach ($response['languages'] as $language) {\n    echo \"{$language['name']} - {$language['paradigm']}\\n\";\n    echo \"Key features: \" . implode(', ', $language['key_features']) . \"\\n\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/modes/md-json/#how-mdjson-mode-works","title":"How MdJson Mode Works","text":"<ol> <li>Polyglot instructs the model to respond with a JSON object wrapped in a Markdown code block</li> <li>The model formats its response accordingly (<code>json {...}</code>)</li> <li>Polyglot extracts the JSON content from the Markdown code block</li> <li>The JSON is parsed and returned to your application</li> </ol>"},{"location":"packages/polyglot/modes/md-json/#providing-guidance-for-mdjson","title":"Providing Guidance for MdJson","text":"<p>While MdJson is more flexible across providers, you still need to provide clear instructions:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// Include expected format in the prompt\n$prompt = &lt;&lt;&lt;EOT\nList three programming languages with their key features.\nRespond with a JSON object following this structure:\n{\n  \"languages\": [\n    {\n      \"name\": \"Language name\",\n      \"paradigm\": \"Programming paradigm\",\n      \"year_created\": year as number,\n      \"key_features\": [\"feature1\", \"feature2\", \"feature3\"]\n    },\n  ]\n}\nEOT;\n\n$response = $inference-&gt;with(\nmessages: $prompt,\nmode: OutputMode::MdJson\n)-&gt;toJson();\n\n// Process as normal JSON\n</code></pre>"},{"location":"packages/polyglot/modes/md-json/#when-to-use-mdjson-mode","title":"When to Use MdJson Mode","text":"<p>MdJson mode is ideal for: - Working with providers that don't have native JSON output - Ensuring portability across different providers - Getting structured responses from older model versions - Fallback option when JSON Schema mode isn't available</p>"},{"location":"packages/polyglot/modes/overview/","title":"Overview of Output Modes","text":"<p>One of Polyglot's key strengths is its ability to support various output formats from LLM providers. This flexibility allows you to structure responses in the format that best suits your application, whether you need plain text, structured JSON data, or function/tool calls. This chapter explores the different output modes supported by Polyglot and how to implement them effectively.</p> <p>Polyglot's support for different output formats gives you the flexibility to work with LLM responses in the way that best suits your application's needs. Whether you need simple text, structured JSON, or interactive tool calls, you can configure the output format to match your requirements.</p>"},{"location":"packages/polyglot/modes/overview/#understanding-output-modes","title":"Understanding Output Modes","text":"<p>Polyglot supports multiple output modes through the <code>Mode</code> enum:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n// Available modes\n// OutputMode::Text       - Plain text output (default)\n// OutputMode::Json       - JSON output\n// OutputMode::JsonSchema - JSON output validated against a schema (native support required)\n// OutputMode::MdJson     - JSON wrapped in Markdown code blocks\n// OutputMode::Tools      - Function/tool calling\n</code></pre> <p>Each mode influences: 1. How the request is formatted and sent to the provider 2. How the provider's response is processed 3. What extraction or validation is applied to the response</p>"},{"location":"packages/polyglot/modes/overview/#output-modes-overview","title":"Output Modes Overview","text":"Mode Description Best For <code>OutputMode::Text</code> Default mode, returns unstructured text Simple text generation <code>OutputMode::Json</code> Returns structured JSON data Structured data processing <code>OutputMode::JsonSchema</code> Returns JSON data validated against a schema (native support required) Strictly typed data <code>OutputMode::MdJson</code> Returns JSON wrapped in Markdown code blocks Compatibility across providers <code>OutputMode::Tools</code> Returns function/tool calls Function calling/external actions <p>JSON Schema guarantees apply only when the provider supports native schema validation; otherwise use <code>OutputMode::Json</code> or <code>OutputMode::MdJson</code> for best-effort structured output.</p>"},{"location":"packages/polyglot/modes/overview/#choosing-the-right-format","title":"Choosing the Right Format","text":"<p>Consider these factors when selecting an output format:</p> <ol> <li>Complexity of the data: More complex data structures benefit from JSON Schema</li> <li>Provider support: Check which formats are natively supported by your provider</li> <li>Consistency requirements: Stricter format requirements favor JSON Schema or Tools</li> <li>Application needs: Consider how the data will be used in your application</li> </ol>"},{"location":"packages/polyglot/modes/overview/#improving-format-reliability","title":"Improving Format Reliability","text":"<p>For better results:</p> <ol> <li>Be explicit in prompts: Clearly describe the expected format</li> <li>Provide examples: Show what good responses look like</li> <li>Use constraints: Specify limits and requirements</li> <li>Test across providers: Verify formats work with all providers you use</li> <li>Implement fallbacks: Have backup strategies for format failures</li> </ol>"},{"location":"packages/polyglot/modes/text/","title":"Text Mode","text":"<p>Text mode is the default and simplest output format, returning unstructured text from the model.</p>"},{"location":"packages/polyglot/modes/text/#basic-text-generation","title":"Basic Text Generation","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n\n// OutputMode::Text is the default, so you don't need to specify it\n$response = $inference\n    -&gt;with(\n        messages: 'What is the capital of France?',\n        mode: OutputMode::Text  // Optional, this is the default\n    )\n    -&gt;get();\n\necho \"Response: $response\\n\";\n// Output: Response: The capital of France is Paris.\n</code></pre>"},{"location":"packages/polyglot/modes/text/#when-to-use-text-mode","title":"When to Use Text Mode","text":"<p>Text mode is ideal for: - Simple question answering - Creative content generation - Conversational interactions - Summaries and paraphrasing - Any use case where structured data is not required</p>"},{"location":"packages/polyglot/modes/text/#text-mode-across-providers","title":"Text Mode Across Providers","text":"<p>Text mode works consistently across all providers, making it the most portable option:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n// Using OpenAI\n$openAIResponse = $inference\n    -&gt;using('openai')\n    -&gt;withMessages('Write a short poem about the ocean.')\n    -&gt;get();\n\necho \"OpenAI response:\\n$openAIResponse\\n\\n\";\n\n// Using Anthropic\n$anthropicResponse = $inference\n    -&gt;using('anthropic')\n    -&gt;with('Write a short poem about the ocean.')\n    -&gt;get();\n\necho \"Anthropic response:\\n$anthropicResponse\\n\\n\";\n\n// Using any other provider\n// ...\n</code></pre>"},{"location":"packages/polyglot/modes/tools/","title":"Tools Mode","text":"<p>Tools mode enables function calling, allowing the model to request specific actions to be performed by your application. This is powerful for creating LLM-powered applications that can interact with external systems or perform specific tasks.</p>"},{"location":"packages/polyglot/modes/tools/#setting-up-tools","title":"Setting Up Tools","text":"<pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');  // Tools are best supported by OpenAI\n\n// Define a tool for weather information\n$weatherTool = [\n    'type' =&gt; 'function',\n    'function' =&gt; [\n        'name' =&gt; 'get_weather',\n        'description' =&gt; 'Get the current weather for a location',\n        'parameters' =&gt; [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'location' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The city and state or country (e.g., \"San Francisco, CA\")',\n                ],\n                'unit' =&gt; [\n                    'type' =&gt; 'string',\n                    'enum' =&gt; ['celsius', 'fahrenheit'],\n                    'description' =&gt; 'The temperature unit to use',\n                ],\n            ],\n            'required' =&gt; ['location'],\n        ],\n    ],\n];\n\n// Define a tool for flight information\n$flightTool = [\n    'type' =&gt; 'function',\n    'function' =&gt; [\n        'name' =&gt; 'get_flight_info',\n        'description' =&gt; 'Get information about a flight',\n        'parameters' =&gt; [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'flight_number' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The flight number (e.g., \"AA123\")',\n                ],\n                'date' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'The date of the flight in YYYY-MM-DD format',\n                ],\n            ],\n            'required' =&gt; ['flight_number'],\n        ],\n    ],\n];\n\n// Create an array of tools\n$tools = [$weatherTool, $flightTool];\n\n// Make a request that might require tools\n$response = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris today?',\n    tools: $tools,\n    toolChoice: 'auto',  // Let the model decide which tool to use\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Check if the model called a tool\nif ($response-&gt;hasToolCalls()) {\n    $toolCalls = $response-&gt;toolCalls();\n\n    foreach ($toolCalls-&gt;all() as $call) {\n        $toolName = $call-&gt;name();\n        $args = $call-&gt;args();\n\n        echo \"Tool called: $toolName\\n\";\n        echo \"Arguments: \" . json_encode($args, JSON_PRETTY_PRINT) . \"\\n\";\n\n        // Handle the tool call\n        if ($toolName === 'get_weather') {\n            // In a real application, you would call a weather API here\n            $weatherData = simulateWeatherApi($args['location'], $args['unit'] ?? 'celsius');\n\n            // Send the tool result back to the model\n            $withToolResult = $inference-&gt;with(\n                messages: [\n                    ['role' =&gt; 'user', 'content' =&gt; 'What\\'s the weather like in Paris today?'],\n                    [\n                        'role' =&gt; 'assistant',\n                        'content' =&gt; '',\n                        '_metadata' =&gt; [\n                            'tool_calls' =&gt; [\n                                [\n                                    'id' =&gt; $call-&gt;id(),\n                                    'function' =&gt; [\n                                        'name' =&gt; $call-&gt;name(),\n                                        'arguments' =&gt; $call-&gt;argsAsJson(),\n                                    ],\n                                ],\n                            ],\n                        ],\n                    ],\n                    [\n                        'role' =&gt; 'tool',\n                        'content' =&gt; json_encode($weatherData),\n                        '_metadata' =&gt; [\n                            'tool_call_id' =&gt; $call-&gt;id(),\n                            'tool_name' =&gt; $call-&gt;name(),\n                        ],\n                    ],\n                ]\n            )-&gt;get();\n\n            echo \"Final response: $withToolResult\\n\";\n        }\n    }\n} else {\n    // Model responded directly\n    echo \"Response: \" . $response-&gt;content() . \"\\n\";\n}\n\n// Simulate a weather API call\nfunction simulateWeatherApi(string $location, string $unit): array {\n    return [\n        'location' =&gt; $location,\n        'temperature' =&gt; 22,\n        'unit' =&gt; $unit,\n        'conditions' =&gt; 'Partly cloudy',\n        'humidity' =&gt; 65,\n    ];\n}\n</code></pre>"},{"location":"packages/polyglot/modes/tools/#controlling-tool-usage","title":"Controlling Tool Usage","text":"<p>You can control how tools are used with the <code>toolChoice</code> parameter:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');\n\n// Let the model decide whether to use tools\n$autoResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: 'auto',\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Always require the model to use a specific tool\n$requiredToolResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: [\n        'type' =&gt; 'function',\n        'function' =&gt; [\n            'name' =&gt; 'get_weather'\n        ]\n    ],\n    mode: OutputMode::Tools\n)-&gt;response();\n\n// Prevent tool usage\n$noToolResponse = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris?',\n    tools: $tools,\n    toolChoice: 'none',\n    mode: OutputMode::Tools\n)-&gt;response();\n</code></pre>"},{"location":"packages/polyglot/modes/tools/#streaming-tool-calls","title":"Streaming Tool Calls","text":"<p>You can also stream tool calls to provide real-time feedback:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference()-&gt;using('openai');\n\n$response = $inference-&gt;with(\n    messages: 'What\\'s the weather like in Paris today?',\n    tools: $tools,\n    toolChoice: 'auto',\n    mode: OutputMode::Tools,\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n\n$toolName = '';\n$toolId = '';\n$toolArgs = '';\n\nforeach ($stream as $partialResponse) {\n    // Tool name is being generated\n    if (!empty($partialResponse-&gt;toolName)) {\n        if (empty($toolName)) {\n            $toolName = $partialResponse-&gt;toolName;\n            echo \"Tool being called: $toolName\\n\";\n        }\n    }\n\n    // Tool ID is received\n    if (!empty($partialResponse-&gt;toolId) &amp;&amp; empty($toolId)) {\n        $toolId = $partialResponse-&gt;toolId;\n    }\n\n    // Tool arguments are being generated\n    if (!empty($partialResponse-&gt;toolArgs)) {\n        $toolArgs .= $partialResponse-&gt;toolArgs;\n        echo \"Receiving tool arguments...\\n\";\n    }\n\n    // Regular content is being generated\n    if (!empty($partialResponse-&gt;contentDelta)) {\n        echo $partialResponse-&gt;contentDelta;\n        flush();\n    }\n\n    // Check for finish reason\n    if (!empty($partialResponse-&gt;finishReason())) {\n        echo \"\\nFinished with reason: {$partialResponse-&gt;finishReason()}\\n\";\n    }\n}\n\n// Process the complete tool call\nif (!empty($toolName) &amp;&amp; !empty($toolArgs)) {\n    try {\n        $args = json_decode($toolArgs, true, 512, JSON_THROW_ON_ERROR);\n        echo \"\\nFinal tool arguments: \" . json_encode($args, JSON_PRETTY_PRINT) . \"\\n\";\n\n        // Process the tool call as in the previous example\n    } catch (\\JsonException $e) {\n        echo \"Error parsing tool arguments: \" . $e-&gt;getMessage() . \"\\n\";\n    }\n}\n</code></pre>"},{"location":"packages/polyglot/modes/tools/#provider-support-for-tools","title":"Provider Support for Tools","text":"<p>Tool support varies across providers:</p> <ul> <li>OpenAI: Comprehensive support with <code>function_call</code>/<code>tool_call</code> features</li> <li>Anthropic: Growing support with <code>tool_use</code> feature in Claude 3 models</li> <li>Other providers: Implementation varies; check provider documentation</li> </ul>"},{"location":"packages/polyglot/modes/tools/#when-to-use-tools-mode","title":"When to Use Tools Mode","text":"<p>Tools mode is ideal for: - Creating agents that can interact with external systems - Building assistants that need to retrieve real-time information - Implementing complex workflows that require multiple steps - Giving the model access to specific capabilities (calculations, API calls, etc.)</p>"},{"location":"packages/polyglot/streaming/misc/","title":"Advanced Stream Processing","text":""},{"location":"packages/polyglot/streaming/misc/#using-callbacks","title":"Using Callbacks","text":"<p>You can use the <code>onPartialResponse</code> method to register a callback that is called for each partial response:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n// Set up a callback for processing partial responses\n$stream = $response-&gt;stream()-&gt;onPartialResponse(function($partialResponse) {\n    echo $partialResponse-&gt;contentDelta;\n    flush();\n});\n\n// Process all responses\nforeach ($stream-&gt;responses() as $_) {\n    // The callback is called for each partial response\n    // We don't need to do anything here\n}\n</code></pre>"},{"location":"packages/polyglot/streaming/misc/#transforming-stream-content","title":"Transforming Stream Content","text":"<p>You can process and transform the content as it streams:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Generate a list of 10 book titles.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$titleCount = 0;\n$currentTitle = '';\n\nforeach ($stream as $partialResponse) {\n    $content = $partialResponse-&gt;contentDelta;\n\n    // Check for new titles (assuming numbered list format)\n    if (preg_match('/(\\d+)\\.\\s+(.+?)(?=\\n\\d+\\.|\\Z)/s', $content, $matches)) {\n        $titleCount++;\n        $title = trim($matches[2]);\n        echo \"Title #{$matches[1]}: $title\\n\";\n    } elseif (!empty(trim($content))) {\n        echo $content;\n    }\n}\n</code></pre>"},{"location":"packages/polyglot/streaming/misc/#processing-json-streams","title":"Processing JSON Streams","text":"<p>For streaming JSON responses, you need to accumulate content until you have valid JSON:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\Enums\\OutputMode;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'List 5 countries and their capitals in JSON format.',\n    mode: OutputMode::Json,  // Request JSON response\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$jsonBuffer = '';\n\nforeach ($stream as $partialResponse) {\n    $jsonBuffer .= $partialResponse-&gt;contentDelta;\n\n    // Try to parse the accumulated JSON\n    $tempJson = $jsonBuffer;\n\n    // Attempt to complete any incomplete JSON\n    if (substr(trim($tempJson), -1) !== '}') {\n        $tempJson .= '}';\n    }\n\n    // Replace any trailing commas which would make the JSON invalid\n    $tempJson = preg_replace('/,\\s*}$/', '}', $tempJson);\n\n    try {\n        $data = json_decode($tempJson, true, 512, JSON_THROW_ON_ERROR);\n        echo \"Valid JSON received so far: \" . json_encode($data, JSON_PRETTY_PRINT) . \"\\n\";\n    } catch (\\JsonException $e) {\n        // Not a complete valid JSON yet\n        echo \"Accumulated content: $jsonBuffer\\n\";\n    }\n}\n\n// Process the final, complete JSON\ntry {\n    $finalData = json_decode($jsonBuffer, true, 512, JSON_THROW_ON_ERROR);\n    echo \"Final JSON: \" . json_encode($finalData, JSON_PRETTY_PRINT) . \"\\n\";\n} catch (\\JsonException $e) {\n    echo \"Error parsing final JSON: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/streaming/misc/#cancellation","title":"Cancellation","text":"<p>In some cases, you may want to stop the generation early:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a long story about space exploration.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$wordCount = 0;\n$maxWords = 100;  // Limit to 100 words\n\nforeach ($stream as $partialResponse) {\n    echo $partialResponse-&gt;contentDelta;\n    flush();\n\n    // Count words in the accumulated content\n    $words = str_word_count($partialResponse-&gt;content());\n\n    // Stop after reaching the word limit\n    if ($words &gt;= $maxWords) {\n        echo \"\\n\\n[Generation stopped after $maxWords words]\\n\";\n        break;  // Exit the loop early\n    }\n}\n</code></pre> <p>Note that when you break out of the loop, the request to the provider continues in the background, but your application stops processing the response.</p>"},{"location":"packages/polyglot/streaming/misc/#performance-considerations","title":"Performance Considerations","text":"<p>When working with streaming responses, keep these performance considerations in mind:</p> <ol> <li>Memory Usage: Be careful with how you accumulate content, especially for very long responses</li> <li>Buffer Flushing: In web applications, make sure output buffers are properly flushed</li> <li>Connection Stability: Streaming connections can be more sensitive to network issues</li> <li>Timeouts: Adjust timeout settings for long-running streams</li> </ol> <p>Here's an example of memory-efficient processing for very long responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Generate a very long story.',\n    options: [\n        'stream' =&gt; true,\n        'max_tokens' =&gt; 10000  // Request a long response\n    ]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\n$outputFile = fopen('generated_story.txt', 'w');\n\nforeach ($stream as $partialResponse) {\n    // Write chunks directly to file instead of keeping them in memory\n    fwrite($outputFile, $partialResponse-&gt;contentDelta);\n\n    // Optional: Show a progress indicator\n    echo \".\";\n    flush();\n}\n\nfclose($outputFile);\necho \"\\nGeneration complete. Story saved to generated_story.txt\\n\";\n</code></pre>"},{"location":"packages/polyglot/streaming/overview/","title":"Overview of Streaming","text":"<p>Streaming LLM responses may be preferred for user experience and system performance. Polyglot makes it easy to implement streaming with a consistent API across different providers.</p> <p>Streaming responses are a powerful feature of modern LLM APIs that allow you to receive and process model outputs incrementally as they're being generated, rather than waiting for the complete response. This chapter covers how to work with streaming responses in Polyglot, from basic setup to advanced processing techniques.</p>"},{"location":"packages/polyglot/streaming/overview/#benefits-of-streaming","title":"Benefits of Streaming","text":"<p>Streaming responses offer several advantages:</p> <ol> <li>Improved User Experience: Display content to users as it's generated, creating a more responsive interface</li> <li>Reduced Latency Perception: Users see the beginning of a response almost immediately</li> <li>Progressive Processing: Begin processing early parts of the response while later parts are still being generated</li> <li>Handling Long Outputs: Efficiently process responses that may be very long without hitting timeout limits</li> <li>Early Termination: Stop generation early if needed, saving resources</li> </ol>"},{"location":"packages/polyglot/streaming/overview/#enabling-streaming","title":"Enabling Streaming","text":"<p>Enabling streaming in Polyglot is straightforward - you need to set the <code>stream</code> option to <code>true</code> in your request:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]  // Enable streaming\n);\n</code></pre> <p>Once you have a streaming-enabled response, you can access the stream using the <code>stream()</code> method:</p> <pre><code>// Get the stream of partial responses\n$stream = $response-&gt;stream();\n</code></pre>"},{"location":"packages/polyglot/streaming/overview/#basic-stream-processing","title":"Basic Stream Processing","text":"<p>The most common way to process a stream is to iterate through the partial responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n$response = $inference-&gt;with(\n    messages: 'Write a short story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n// Get a generator that yields partial responses\n$stream = $response-&gt;stream()-&gt;responses();\n\necho \"Story: \";\nforeach ($stream as $partialResponse) {\n    // Output each chunk as it arrives\n    echo $partialResponse-&gt;contentDelta;\n\n    // Flush the output buffer to show progress in real-time (for CLI or streaming HTTP responses)\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n        flush();\n    }\n}\necho \"\\n\";\n</code></pre>"},{"location":"packages/polyglot/streaming/overview/#understanding-partial-responses","title":"Understanding Partial Responses","text":"<p>Each iteration of the stream yields a <code>PartialInferenceResponse</code> object with these key properties:</p> <ul> <li><code>contentDelta</code>: The new content received in this chunk</li> <li><code>content</code>: The accumulated content up to this point</li> <li><code>finishReason</code>: The reason why the response finished (empty until the final chunk)</li> <li><code>usage</code>: Token usage statistics</li> </ul> <pre><code>foreach ($stream as $partialResponse) {\n    // The new content in this chunk\n    echo \"New content: \" . $partialResponse-&gt;contentDelta . \"\\n\";\n\n    // The total content received so far\n    echo \"Total content so far: \" . $partialResponse-&gt;content() . \"\\n\";\n\n    // Check if this is the final chunk\n    if ($partialResponse-&gt;finishReason !== '') {\n        echo \"Response finished: \" . $partialResponse-&gt;finishReason . \"\\n\";\n    }\n}\n</code></pre>"},{"location":"packages/polyglot/streaming/overview/#retrieving-the-final-response","title":"Retrieving the Final Response","text":"<p>After processing the stream, you can get the complete response:</p> <pre><code>// Method 1: Using the original response object's get() method\n$completeText = $response-&gt;get();\n\n// Method 2: Getting the final state from the stream\n$finalResponse = $response-&gt;stream()-&gt;final();\n$completeText = $finalResponse-&gt;content();\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/debugging/","title":"Debugging Requests and Responses","text":"<p>Polyglot debug mode provides a simple way to enable HTTP-level debugging for LLM interactions. Debugging is essential for troubleshooting and optimizing your applications. It allows you to inspect the requests sent to the LLM and the responses received, helping you identify issues and improve performance.</p>"},{"location":"packages/polyglot/troubleshooting/debugging/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>Polyglot provides a simple way to enable HTTP debug mode:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Enable HTTP debug middleware when creating the inference object\n$inference = (new Inference())\n    -&gt;withHttpDebugPreset('on');\n\n// Make a request - debug output will show the request and response details\n$response = $inference-&gt;with(messages: 'What is the capital of France?')-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/debugging/#http-debug-events","title":"HTTP Debug Events","text":"<p>When HTTP debug mode is enabled, the HTTP middleware stack dispatches debug events that you can listen to with <code>onEvent()</code> or <code>wiretap()</code>.</p> <pre><code>&lt;?php\nuse Cognesy\\Http\\Events\\DebugRequestURLUsed;\nuse Cognesy\\Http\\Events\\DebugResponseBodyReceived;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = (new Inference())\n    -&gt;withHttpDebugPreset('url-only')\n    -&gt;onEvent(DebugRequestURLUsed::class, fn(DebugRequestURLUsed $e) =&gt; dump($e-&gt;toArray()))\n    -&gt;onEvent(DebugResponseBodyReceived::class, fn(DebugResponseBodyReceived $e) =&gt; dump($e-&gt;toArray()));\n\n$response = $inference-&gt;with(messages: 'What is the capital of France?')-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/debugging/#event-listeners","title":"Event Listeners","text":"<p>Use event listeners to trace the flow of requests and responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create an event dispatcher\n$events = new EventDispatcher();\n\n// Add listeners\n$events-&gt;listen(InferenceRequested::class, function (InferenceRequested $event) {\n    echo \"Request sent: \" . json_encode($event-&gt;request-&gt;toArray()) . \"\\n\";\n});\n\n$events-&gt;listen(InferenceResponseCreated::class, function (InferenceResponseCreated $event) {\n    echo \"Response received: \" . substr($event-&gt;inferenceResponse-&gt;content(), 0, 50) . \"...\\n\";\n    echo \"Token usage: \" . $event-&gt;inferenceResponse-&gt;usage()-&gt;total() . \"\\n\";\n});\n\n// Create an inference object with the event dispatcher\n$inference = new Inference(events: $events);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?'\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/debugging/#logging-to-files","title":"Logging to Files","text":"<p>For more persistent debugging, you can log to files:</p> <pre><code>&lt;?php\nuse Cognesy\\Events\\Dispatchers\\EventDispatcher;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceRequested;\nuse Cognesy\\Polyglot\\Inference\\Events\\InferenceResponseCreated;\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Create a function to log to file\nfunction logToFile(string $message, string $filename = 'llm_debug.log'): void {\n    $timestamp = date('Y-m-d H:i:s');\n    file_put_contents(\n        $filename,\n        \"[$timestamp] $message\" . PHP_EOL,\n        FILE_APPEND\n    );\n}\n\n// Create a custom event dispatcher\n$events = new EventDispatcher();\n\n// Listen for request events\n$events-&gt;listen(InferenceRequested::class, function (InferenceRequested $event) {\n    $request = $event-&gt;request;\n    logToFile(\"REQUEST: \" . json_encode($request-&gt;toArray()));\n});\n\n// Listen for response events\n$events-&gt;listen(InferenceResponseCreated::class, function (InferenceResponseCreated $event) {\n    $response = $event-&gt;inferenceResponse;\n    logToFile(\"RESPONSE: \" . json_encode($response-&gt;toArray()));\n});\n\n// Create an inference object with the custom event dispatcher\n$inference = new Inference(events: $events);\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'What is artificial intelligence?'\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/issues-authentication/","title":"Authentication","text":"<p>One of the most common issues when working with LLM APIs is authentication problems.</p>"},{"location":"packages/polyglot/troubleshooting/issues-authentication/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages containing terms like \"authentication failed,\" \"invalid API key,\" or \"unauthorized\"</li> <li>HTTP status codes 401 or 403</li> </ul>"},{"location":"packages/polyglot/troubleshooting/issues-authentication/#solutions","title":"Solutions","text":"<ol> <li> <p>Verify API Key: Ensure your API key is correctly set in your environment variables <pre><code>// Check if API key is set\nif (empty(getenv('OPENAI_API_KEY'))) {\necho \"API key is not set in environment variables\\n\";\n}\n</code></pre></p> </li> <li> <p>Check API Key Format: Some providers require specific formats for API keys <pre><code>// OpenAI keys typically start with 'sk-'\nif (!str_starts_with(getenv('OPENAI_API_KEY'), 'sk-')) {\necho \"OpenAI API key format is incorrect\\n\";\n}\n\n// Anthropic keys typically start with 'sk-ant-'\nif (!str_starts_with(getenv('ANTHROPIC_API_KEY'), 'sk-ant-')) {\necho \"Anthropic API key format is incorrect\\n\";\n}\n</code></pre></p> </li> <li> <p>Test Keys Directly: Use a simple script to test your API keys</p> </li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Polyglot\\Inference\\LLMProvider;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction testApiKey(string $preset): bool {\n    try {\n        $llm = (new LLMFactory)-&gt;fromPreset($preset);\n        $inference = new Inference($llm);\n        $inference-&gt;with(\n            messages: 'Test message',\n            options: ['max_tokens' =&gt; 5]\n        )-&gt;get();\n\n        echo \"Connection using '$connection' is working correctly\\n\";\n        return true;\n    } catch (HttpRequestException $e) {\n        echo \"Error with connection '$connection': \" . $e-&gt;getMessage() . \"\\n\";\n        return false;\n    }\n}\n\n// Test major providers\ntestApiKey('openai');\ntestApiKey('anthropic');\ntestApiKey('mistral');\n?&gt;\n</code></pre> <ol> <li>Environment Variables: Ensure your environment variables are being loaded correctly <pre><code>&lt;?php\n// If using dotenv\n$dotenv = Dotenv\\Dotenv::createImmutable(__DIR__);\n$dotenv-&gt;load();\n$dotenv-&gt;required(['OPENAI_API_KEY'])-&gt;notEmpty();\n?&gt;\n</code></pre></li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-configuration/","title":"Connection Configurations","text":""},{"location":"packages/polyglot/troubleshooting/issues-configuration/#symptoms","title":"Symptoms","text":"<ul> <li>Errors like \"connection timeout,\" \"failed to connect,\" or \"network error\"</li> <li>Long delays before errors appear</li> <li>Issues with specific providers (e.g., OpenAI, Anthropic, Mistral)</li> <li>Incorrect API keys or permissions</li> <li>Missing or incorrect configuration parameters</li> </ul>"},{"location":"packages/polyglot/troubleshooting/issues-configuration/#solutions","title":"Solutions","text":""},{"location":"packages/polyglot/troubleshooting/issues-configuration/#1-verify-api-keys","title":"1. Verify API Keys","text":"<p>Make sure your API keys are correct and have the necessary permissions:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction testApiKey(string $preset): bool {\n    try {\n        $llm = LLMProvider::using($preset);\n        $inference = new Inference($preset);\n        $response = $inference-&gt;with(\n            messages: 'Test message',\n            options: ['max_tokens' =&gt; 5]\n        )-&gt;get();\n\n        echo \"Connection preset '$preset' is working.\\n\";\n        return true;\n    } catch (HttpRequestException $e) {\n        echo \"Error with connection '$preset': \" . $e-&gt;getMessage() . \"\\n\";\n        return false;\n    }\n}\n\n// Test each connection\n$presets = ['openai', 'anthropic', 'mistral'];\nforeach ($presets as $preset) {\n    testApiKey($preset);\n}\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/issues-configuration/#2-enable-debug-mode","title":"2. Enable Debug Mode","text":"<p>Use debug mode to see the actual requests and responses:</p> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n// Enable debug mode\n$inference = new Inference()\n    -&gt;using('openai')\n    -&gt;withHttpDebugPreset('on');\n\n// Make a request\n$response = $inference-&gt;with(\n    messages: 'Test message with debug enabled'\n)-&gt;get();\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/issues-configuration/#3-check-provider-status","title":"3. Check Provider Status","text":"<p>Some issues might be related to the provider's service status. Check their status pages or documentation.</p>"},{"location":"packages/polyglot/troubleshooting/issues-configuration/#4-verify-configuration-parameters","title":"4. Verify Configuration Parameters","text":"<p>Ensure all required configuration parameters are present and correctly formatted:</p> <pre><code>&lt;?php\n\nfunction verifyConfig(string $preset): void {\n    try {\n        $provider = new ConfigProvider();\n        $config = LLMConfig::fromArray($provider-&gt;getConfig($preset));\n\n        echo \"Configuration for '$preset':\\n\";\n        echo \"API URL: {$config-&gt;apiUrl}\\n\";\n        echo \"Endpoint: {$config-&gt;endpoint}\\n\";\n        echo \"Default Model: {$config-&gt;model}\\n\";\n        echo \"Provider Type: {$config-&gt;providerType}\\n\";\n\n        // Check for empty values\n        if (empty($config-&gt;apiKey)) {\n            echo \"Warning: API key is empty\\n\";\n        }\n\n        if (empty($config-&gt;model)) {\n            echo \"Warning: Default model is not set\\n\";\n        }\n    } catch (\\Exception $e) {\n        echo \"Error loading configuration for '$preset': \" . $e-&gt;getMessage() . \"\\n\";\n    }\n}\n\n// Verify configurations\n$presets = ['openai', 'anthropic', 'mistral'];\nforeach ($presets as $preset) {\n    verifyConfig($preset);\n    echo \"\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/issues-connection/","title":"Connection Issues","text":"<p>Network connectivity problems can prevent successful API requests.</p>"},{"location":"packages/polyglot/troubleshooting/issues-connection/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages like \"connection timeout,\" \"failed to connect,\" or \"network error\"</li> <li>Long delays before errors appear</li> </ul>"},{"location":"packages/polyglot/troubleshooting/issues-connection/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Internet Connection: Ensure your server has a stable internet connection</p> </li> <li> <p>Verify API Endpoint: Make sure the API endpoint URL is correct <pre><code>// In your configuration file (config/llm.php)\n'apiUrl' =&gt; 'https://api.openai.com/v1', // Correct URL\n</code></pre></p> </li> <li> <p>Proxy Settings: If you're behind a proxy, configure it properly</p> </li> </ol> <pre><code>// Using custom HTTP client with proxy settings\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;\n\n$config = new HttpClientConfig(\n    requestTimeout: 30,\n    connectTimeout: 10,\n    additionalOptions: ['proxy' =&gt; 'http://proxy.example.com:8080']\n);\n\n$httpClient = new HttpClient('guzzle', $config);\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n</code></pre> <ol> <li> <p>Firewall Rules: Check if your firewall is blocking outgoing connections to API endpoints</p> </li> <li> <p>DNS Resolution: Ensure your DNS is resolving the API domains correctly</p> </li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-model-specific/","title":"Model-Specific Issues","text":"<p>When working with different LLM models, you may encounter issues that are specific to the model you're using, as different models have different capabilities and limitations. This section covers common model-specific issues and how to resolve them.</p>"},{"location":"packages/polyglot/troubleshooting/issues-model-specific/#symptoms","title":"Symptoms","text":"<ul> <li>Errors like \"model not found,\" \"parameter not supported,\" or \"context length exceeded\"</li> <li>Unexpected responses or performance from certain models</li> </ul>"},{"location":"packages/polyglot/troubleshooting/issues-model-specific/#solutions","title":"Solutions","text":"<ol> <li> <p>Check Model Availability: Ensure the model you're requesting is available from the provider <pre><code>// Check available models for each provider in their documentation\n// Example: For OpenAI 'gpt-4o-mini' is valid, but 'gpt5' is not\n</code></pre></p> </li> <li> <p>Context Length: Be aware of each model's maximum context length <pre><code>// In config/llm.php, check contextLength for each model\n// Example: OpenAI models have different context windows\n// - gpt-3.5-turbo: 16K tokens\n// - gpt-4-turbo: 128K tokens\n// - claude-3-opus: 200K tokens\n</code></pre></p> </li> <li> <p>Feature Support: Different models support different features <pre><code>// Some features may not work with all models\n// Example: Vision capabilities are only available in select models\n\n// Check for vision support before sending images\n$modelSupportsVision = in_array($model, [\n    'gpt-4-vision', 'gpt-4o', 'claude-3-opus', 'claude-3-sonnet'\n]);\n\nif (!$modelSupportsVision) {\n    echo \"Warning: The selected model doesn't support vision capabilities\\n\";\n}\n</code></pre></p> </li> <li> <p>Fallback Models: Implement fallbacks to other models when preferred models fail</p> </li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\nuse Cognesy\\Http\\Exceptions\\HttpRequestException;\n\nfunction withModelFallback(array $models, string $prompt): string {\n    $inference = new Inference();\n    $lastException = null;\n\n    foreach ($models as $model) {\n        try {\n            return $inference-&gt;with(\n                messages: $prompt,\n                model: $model\n            )-&gt;get();\n        } catch (HttpRequestException $e) {\n            $lastException = $e;\n            echo \"Model '$model' failed: \" . $e-&gt;getMessage() . \"\\n\";\n            echo \"Trying next model...\\n\";\n        }\n    }\n\n    throw new \\Exception(\"All models failed. Last error: \" .\n        ($lastException ? $lastException-&gt;getMessage() : \"Unknown error\"));\n}\n\n// Try advanced models first, then fall back to simpler ones\n$models = ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'];\n\ntry {\n    $response = withModelFallback($models, \"What is the capital of France?\");\n    echo \"Response: $response\\n\";\n} catch (\\Exception $e) {\n    echo \"Error: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre>"},{"location":"packages/polyglot/troubleshooting/issues-provider-specific/","title":"Provider-Specific Issues","text":"<p>Each LLM provider has unique quirks and issues. This section covers common provider-specific issues and how to resolve them.</p>"},{"location":"packages/polyglot/troubleshooting/issues-provider-specific/#openai","title":"OpenAI","text":"<ol> <li> <p>Organization IDs: Set the organization ID if using a shared account <pre><code>// In config/llm.php\n'metadata' =&gt; [\n    'organization' =&gt; 'org-your-organization-id',\n],\n</code></pre></p> </li> <li> <p>API Versions: Pay attention to API version changes <pre><code>// Updates to OpenAI API may require changes to your code\n// Monitor OpenAI's release notes for changes\n</code></pre></p> </li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-provider-specific/#anthropic","title":"Anthropic","text":"<ol> <li> <p>Message Format: Anthropic uses a different message format <pre><code>// Polyglot handles this automatically, but be aware when debugging\n</code></pre></p> </li> <li> <p>Tool Support: Tool support has specific requirements <pre><code>// When using tools with Anthropic, check their latest documentation\n// for supported features and limitations\n</code></pre></p> </li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-provider-specific/#mistral","title":"Mistral","text":"<ol> <li>Rate Limits: Mistral has strict rate limits on free tier <pre><code>// Implement more aggressive rate limiting for Mistral\n</code></pre></li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-provider-specific/#ollama","title":"Ollama","text":"<ol> <li> <p>Local Setup: Ensure Ollama is properly installed and running <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/version\n</code></pre></p> </li> <li> <p>Model Availability: Download models before using them <pre><code># Pull a model before using it\nollama pull llama2\n</code></pre></p> </li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-rate-limits/","title":"Rate Limits","text":"<p>Provider rate limits can cause request failures during high traffic periods.</p>"},{"location":"packages/polyglot/troubleshooting/issues-rate-limits/#symptoms","title":"Symptoms","text":"<ul> <li>Error messages containing \"rate limit exceeded,\" \"too many requests,\" or \"quota exceeded\"</li> <li>HTTP status code 429</li> </ul>"},{"location":"packages/polyglot/troubleshooting/issues-rate-limits/#solutions","title":"Solutions","text":"<ol> <li>Use built-in retry policy: Configure automatic retries with backoff/jitter</li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Polyglot\\Inference\\Inference;\n\n$inference = new Inference();\n\n$response = $inference-&gt;with(\n    messages: 'What is the capital of France?',\n    options: [\n        'retryPolicy' =&gt; [\n            'maxAttempts' =&gt; 4,\n            'baseDelayMs' =&gt; 250,\n            'maxDelayMs' =&gt; 8000,\n            'jitter' =&gt; 'full',\n            'retryOnStatus' =&gt; [429, 500, 502, 503, 504],\n        ],\n    ]\n)-&gt;get();\n\necho \"Response: $response\\n\";\n</code></pre> <ol> <li> <p>Request Throttling: Limit the rate of requests from your application <pre><code>&lt;?php\nclass RateLimiter {\n    private $lastRequestTime = 0;\n    private $requestsPerMinute;\n    private $minTimeBetweenRequests;\n\n    public function __construct(int $requestsPerMinute = 60) {\n        $this-&gt;requestsPerMinute = $requestsPerMinute;\n        $this-&gt;minTimeBetweenRequests = 60 / $requestsPerMinute;\n    }\n\n    public function waitIfNeeded(): void {\n        $currentTime = microtime(true);\n        $timeSinceLastRequest = $currentTime - $this-&gt;lastRequestTime;\n\n        if ($timeSinceLastRequest &lt; $this-&gt;minTimeBetweenRequests) {\n            $waitTime = $this-&gt;minTimeBetweenRequests - $timeSinceLastRequest;\n            usleep($waitTime * 1000000);\n        }\n\n        $this-&gt;lastRequestTime = microtime(true);\n    }\n}\n\n// Usage\n$limiter = new RateLimiter(30); // 30 requests per minute\n$inference = new Inference();\n\nfor ($i = 0; $i &lt; 10; $i++) {\n    $limiter-&gt;waitIfNeeded();\n    $response = $inference-&gt;with(\n        messages: \"This is request $i\"\n    )-&gt;toText();\n    echo \"Response $i: $response\\n\";\n}\n</code></pre></p> </li> <li> <p>Request Batching: Combine multiple requests into batches when possible</p> </li> </ol> <pre><code>&lt;?php\n// Instead of making many small requests\n$responses = [];\nforeach ($questions as $question) {\n    // This would hit rate limits quickly\n    $responses[] = $inference-&gt;with(messages: $question)-&gt;get();\n}\n\n// Better: Use a context-aware batch approach\n$batchedQuestions = \"Please answer the following questions:\\n\";\nforeach ($questions as $i =&gt; $question) {\n    $batchedQuestions .= ($i + 1) . \". $question\\n\";\n}\n\n$batchResponse = $inference-&gt;with(messages: $batchedQuestions)-&gt;get();\n// Then parse the batch response into individual answers\n</code></pre> <ol> <li>Upgrade API Plan: Consider upgrading to a higher tier with increased rate limits</li> </ol>"},{"location":"packages/polyglot/troubleshooting/issues-streaming/","title":"Streaming","text":"<p>Streaming responses can encounter specific problems.</p>"},{"location":"packages/polyglot/troubleshooting/issues-streaming/#symptoms","title":"Symptoms","text":"<ul> <li>Streams cutting off prematurely</li> <li>Errors during stream processing</li> <li>Partial or incomplete responses</li> </ul>"},{"location":"packages/polyglot/troubleshooting/issues-streaming/#solutions","title":"Solutions","text":"<ol> <li>Connection Timeouts: Increase timeout settings for streaming responses</li> </ol> <pre><code>&lt;?php\nuse Cognesy\\Http\\Config\\HttpClientConfig;use Cognesy\\Http\\HttpClient;\n\n// Create a custom HTTP client with longer timeouts\n$config = new HttpClientConfig(\n    requestTimeout: 180,  // 3 minutes for the entire request\n    connectTimeout: 10,   // 10 seconds to establish connection\n    idleTimeout: 60       // 60 seconds allowed between stream chunks\n);\n\n$httpClient = new HttpClient('guzzle', $config);\n$inference = new Inference();\n$inference-&gt;withHttpClient($httpClient);\n\n// Use streaming with the custom client\n$response = $inference-&gt;with(\n    messages: 'Write a long story about a space explorer.',\n    options: ['stream' =&gt; true]\n);\n\n$stream = $response-&gt;stream()-&gt;responses();\nforeach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n    flush();\n}\n</code></pre> <ol> <li> <p>Buffer Flushing: Ensure output buffers are properly flushed during streaming <pre><code>foreach ($stream as $partial) {\n    echo $partial-&gt;contentDelta;\n\n    // Flush output buffer to ensure content is sent immediately\n    if (ob_get_level() &gt; 0) {\n        ob_flush();\n    }\n    flush();\n}\n</code></pre></p> </li> <li> <p>Error Handling in Streams: Implement specific error handling for streams <pre><code>&lt;?php\ntry {\n    $response = $inference-&gt;with(\n        messages: 'Write a long story.',\n        options: ['stream' =&gt; true]\n    );\n\n    try {\n        $stream = $response-&gt;stream()-&gt;responses();\n        $content = '';\n\n        foreach ($stream as $partial) {\n            $content .= $partial-&gt;contentDelta;\n            echo $partial-&gt;contentDelta;\n            flush();\n        }\n    } catch (\\Exception $streamException) {\n        echo \"\\nStream error: \" . $streamException-&gt;getMessage() . \"\\n\";\n\n        // If we got a partial response before the error, use it\n        if (!empty($content)) {\n            echo \"Partial content received: \" . strlen($content) . \" characters\\n\";\n        }\n    }\n} catch (RequestException $e) {\n    echo \"Request failed: \" . $e-&gt;getMessage() . \"\\n\";\n}\n</code></pre></p> </li> <li> <p>Fallback to Non-streaming: Implement a fallback to non-streaming mode <pre><code>&lt;?php\nfunction getResponse(string $prompt, bool $preferStreaming = true): string {\n    $inference = new Inference();\n\n    try {\n        if ($preferStreaming) {\n            // Try streaming first\n            $response = $inference-&gt;with(\n                messages: $prompt,\n                options: ['stream' =&gt; true]\n            );\n\n            $content = '';\n            foreach ($response-&gt;stream()-&gt;responses() as $partial) {\n                $content .= $partial-&gt;contentDelta;\n                // Output can be done here if needed\n            }\n\n            return $content;\n        }\n    } catch (\\Exception $e) {\n        echo \"Streaming failed, falling back to non-streaming mode\\n\";\n    }\n\n    // Fallback to non-streaming\n    return $inference-&gt;with(messages: $prompt)-&gt;toText();\n}\n</code></pre></p> </li> </ol>"},{"location":"packages/polyglot/troubleshooting/overview/","title":"Overview of Troubleshooting","text":"<p>This chapter covers common issues you might encounter when working with Polyglot, along with best practices for effectively using LLMs in your applications.</p>"},{"location":"packages/polyglot/troubleshooting/overview/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li>Authentication Issues</li> <li>Connection Issues</li> <li>Rate Limiting</li> <li>Model-Specific Issues</li> <li>Streaming Issues</li> <li>Provider-Specific Issues</li> <li>Debugging and Logging</li> </ol>"},{"location":"release-notes/v0.12.0/","title":"v0.12.0","text":"<ul> <li>Redesigned directory structure to separate the essential code from the addons, aux tools, etc.</li> <li>Directory 'src' to hold Instructor structured outputs code</li> <li>New directory <code>src-llm</code> to hold the LLM connectivity code (required for Instructor)</li> <li>New directory <code>src-utils</code> to hold the utility classes (required for Instructor)</li> <li>Directory <code>src-setup</code> to hold the Instructor setup tool</li> <li>Directory <code>src-hub</code> to hold the CLI tool for executing examples and generating documentation</li> <li>Directory 'src-tell' to hold a simple tool for prompting LLMs from CLI</li> <li>New directory <code>src-addons</code> to hold the additional capabilities (optional)</li> <li>New directory <code>src-aux</code> to hold the auxiliary tools used, e.g. by examples (optional)</li> <li>New directory <code>src-experimental</code> to hold the not yet ready, experimental work (not distributed)</li> <li>Moved package-specific events to their respective directories</li> <li>Moved embeddings API support to 'src-llm'</li> <li>Added version sync script and Github release automation</li> </ul>"},{"location":"release-notes/v0.12.10/","title":"v0.12.10","text":"<ul> <li>Excluded examples, evals and some CLI tools (hub and tell) to minimize distribution size (they are still available in the source code repository)</li> </ul>"},{"location":"release-notes/v0.12.11/","title":"v0.12.11","text":"<ul> <li>Middleware support for HTTP client layer - unified across clients</li> <li>Request / response debugging is now unified across clients</li> <li>Response and stream buffering middleware to allow for multiple reads from LLM API responses across request lifecycle</li> <li>New tests added</li> </ul>"},{"location":"release-notes/v0.12.12/","title":"v0.12.12","text":"<ul> <li>Moved HTTP connectivity layer to a separate directory (src-http)</li> <li>Added HTTP connectivity layer docs</li> <li>Minor corrections in docs</li> </ul>"},{"location":"release-notes/v0.12.13/","title":"v0.12.13","text":"<ul> <li>Polyglot dev guide added</li> </ul>"},{"location":"release-notes/v0.12.2/","title":"v0.12.2","text":"<ul> <li>Corrected .gitignore to further limit the distribution size (excluded non-essential sources, e.g. /src-hub/ and /src-aux/)</li> <li>Renamed src-llm to src-polyglot</li> <li>Renamed Cognesy/LLM package to Cognesy/Polyglot</li> <li>Skipped obsolete tests for experimental Module code</li> </ul>"},{"location":"release-notes/v0.12.3/","title":"v0.12.3","text":"<ul> <li>Corrected names in composer.json (cognesy/llm &gt; cognesy/polyglot-llm)</li> </ul>"},{"location":"release-notes/v0.12.4/","title":"v0.12.4","text":"<ul> <li>Corrected composer.json and publish script to keep single repo model until everything is ready for the split</li> </ul>"},{"location":"release-notes/v0.12.5/","title":"v0.12.5","text":"<ul> <li>Added basic Polyglot docs, some moved from Instructor documentation</li> <li>Corrected mint.json to reflect new paths</li> <li>Corrected scrips and paths after file / dir location changes</li> <li>Renamed Cognesy\\Aux to Cognesy\\Auxiliary to avoid name conflicts</li> <li>Extracted class autoloading from examples and evals to separate files</li> </ul>"},{"location":"release-notes/v0.12.6/","title":"v0.12.6","text":"<ul> <li>Continued project files reorganization</li> <li>Created README.md, LICENSE.md, composer.json, .gitattributes for subprojects to be ready for future split</li> <li>Corrected composer.json, .gitignore and .gitattributes</li> <li>Docs directory structure cleanup to work with Mintlify</li> </ul>"},{"location":"release-notes/v0.12.7/","title":"v0.12.7","text":"<ul> <li>Moved Cognesy/Addons/Prompt to Cognesy/Utils/Template to clarify purpose and remove cyclic dependency between src-addons and src-utils</li> <li>Added Anthropic thinking traces support in LLM drivers (thinking data is now available directly in LLMResponse and LLMPartialResponse object properties)</li> </ul>"},{"location":"release-notes/v0.12.8/","title":"v0.12.8","text":"<ul> <li>composer.json cleanup</li> <li>MessageRole now supports OpenAI 'developer' role, other drivers recognize and properly convert it</li> <li>Removed missed dependency on experimental code in one of Structure traits</li> <li>Moved tests into subpackage-specific directories</li> <li>Added tests for EventDispatcher class</li> <li>Added tests for Message class</li> <li>Added tests for Messages class</li> </ul>"},{"location":"release-notes/v0.12.9/","title":"v0.12.9","text":"<ul> <li>Fixed composer.json</li> </ul>"},{"location":"release-notes/v0.13.0/","title":"v0.13.0","text":"<ul> <li>Moved source code of each component to the <code>packages</code> directory</li> <li>Added Github workflow to split the monorepo into multiple packages</li> <li>Corrected sync and publish scripts</li> <li>Templates use BasePath for better path handling</li> </ul>"},{"location":"release-notes/v0.14.0/","title":"v0.14.0","text":"<ul> <li>Added license files to subpackages</li> </ul>"},{"location":"release-notes/v0.14.1/","title":"v0.14.1","text":"<ul> <li>Better Mintlify docs structure and fixes in generation</li> </ul>"},{"location":"release-notes/v0.14.2/","title":"v0.14.2","text":"<ul> <li>(utils) Env class now uses BasePath for base path resolution</li> </ul>"},{"location":"release-notes/v0.14.3/","title":"v0.14.3","text":"<ul> <li>(all) Corrected dependencies in composer.json files for all packages</li> <li>(utils) Removed circular dependency on addons package (via Image class)</li> <li>(all) Tests moved under packages/*/tests directories</li> <li>(all) Added .gitattributes file to all packages</li> <li>(all) Corrected .gitignore files in all packages</li> <li>(main) Scripts added in ./bin to install &amp; update composer dependencies and run all tests</li> <li>(main) Corrected php.yml to use the new script ./bin/run-all-tests.sh</li> <li>(docs) Corrected docs references: old example viewer &amp; launcher script (./hub.sh) to new (./bin/instructor hub)</li> </ul>"},{"location":"release-notes/v0.14.4/","title":"v0.14.4","text":"<ul> <li>(docs) Package specific docs moved to subpackage docs/ directories</li> <li>(docs) Modified Mintlify group naming to avoid conflicts</li> <li>(docs) Modified docs building process to use subpackage docs</li> </ul>"},{"location":"release-notes/v0.14.5/","title":"v0.14.5","text":"<ul> <li>(all) Added missing license information to composer.json files</li> </ul>"},{"location":"release-notes/v0.14.6/","title":"v0.14.6","text":"<ul> <li>(addons) Moved Cognesy\\Addons\\Evals to a separate package under packages/evals and new   namespace Cognesy\\Evals</li> <li>(polyglot) Corrected warning on missing 'thinking' key in the non-reasoning responses</li> <li>(docs) Minor corrections in docs</li> </ul>"},{"location":"release-notes/v0.14.7/","title":"v0.14.7","text":"<ul> <li>(utils) Moved Cognesy\\Utils\\Template to a new, separate package cognesy/templates</li> <li>(utils) Moved Debug related files to cognesy/http-client package</li> <li>(utils) Moved chat template code (Script and related classes) to cognesy/templates</li> </ul>"},{"location":"release-notes/v0.15.0/","title":"v0.15.0","text":"<ul> <li>New packages extracted from instructor/utils and instructor/addons - evals and templates</li> </ul>"},{"location":"release-notes/v0.15.1/","title":"v0.15.1","text":"<ul> <li>Corrected dependencies after extraction of <code>evals</code> and <code>templates</code> packages</li> </ul>"},{"location":"release-notes/v0.15.2/","title":"v0.15.2","text":"<ul> <li>PHP8.4 compatibility fix across the codebase (warning: \"Implicitly marking parameter $parameters as nullable is deprecated, the explicit nullable type must be used instead\")</li> </ul>"},{"location":"release-notes/v0.16.0/","title":"v0.16.0","text":"<ul> <li>(http) Modified HTTP client layer to allow pluggable drivers</li> <li>(http) Minor additions to docs</li> <li>(http) Removed hard dependency of http config file on HttpClientType enum</li> <li>(all) Removed dependencies on HttpClientType enum</li> <li>(tests) Changes in tests to allow execution from both monorepo and individual packages</li> </ul>"},{"location":"release-notes/v0.17.0/","title":"v0.17.0","text":"<ul> <li>(polyglot) Renamed class <code>Mode</code> to <code>OutputMode</code></li> <li>(polyglot) Updates in driver mappings to catch up on the provider changes / improvements (e.g. support for JSON Schema / strict mode)</li> <li>(polyglot) Fixed token usage tracking for OpenAI/Azure and Gemini OpenAI-comptible drivers</li> <li>(http) Switching debug to true in config/debug.php turns on debugging globally</li> </ul>"},{"location":"release-notes/v0.17.1/","title":"v0.17.1","text":"<ul> <li>(polyglot) Corrected response format selection in Sambanova driver</li> </ul>"},{"location":"release-notes/v0.17.10/","title":"v0.17.10","text":"<ul> <li>(utils) <code>JsonSchema</code> class - simple API to build dynamic JSON schemas in Polyglot, lean alternative to <code>Structure</code> class (which is Instructor only)</li> <li>(docs) New and updated examples</li> </ul>"},{"location":"release-notes/v0.17.11/","title":"v0.17.11","text":"<ul> <li>(polyglot) Support for DSN string containing parameters of LLM provider connection - Inference, Embeddings classes and their configs</li> <li>(instructor) Support for DSN string containing parameters of LLM provider connection - Instructor class</li> <li>(utils) DSN class with DSN-like string parsing capabilities</li> <li>(docs) DSN examples for Inference and Instructor</li> <li>(instructor) Internal refactoring - moved some Instructor/Features/Core code to Instructor namespace</li> <li>(hub) Automated build of changelog section in docs</li> </ul>"},{"location":"release-notes/v0.17.12/","title":"v0.17.12","text":"<ul> <li>(hub) Moved Mintlify helpers to instructor-aux package to make it an integration available for other components</li> </ul>"},{"location":"release-notes/v0.17.3/","title":"v0.17.3","text":"<ul> <li>(build) Run package tests separately for each package after a new version release</li> </ul>"},{"location":"release-notes/v0.17.4/","title":"v0.17.4","text":"<ul> <li>(docs) Renamed <code>Mode</code> class references to <code>OutputMode</code> in docs and examples</li> <li>(docs) Slimmed down README.md - the removed sections are in the docs</li> <li>(docs) Minor corrections of obsolete or outdated code in docs and examples</li> </ul>"},{"location":"release-notes/v0.17.5/","title":"v0.17.5","text":"<ul> <li>(main) Added release script to distribute current versions of /bin/ins-setup, /bin/ins-hub, /bin/tell to subpackages</li> <li>(main) Release script now copies examples to hub subpackage, so they can be included in the distribution</li> <li>(main) Renamed /scripts/setup.php script to /bin/ins-setup</li> <li>(main) Renamed /scripts/hub.php script to /bin/ins-hub</li> <li>(main) Removed obsolete /bin/instructor script</li> <li>(all) Updated composer.json files in main and subpackages to include new package scripts</li> <li>(docs) Updated docs</li> </ul>"},{"location":"release-notes/v0.17.6/","title":"v0.17.6","text":"<ul> <li>(setup) Renamed bin/ins-setup to bin/instructor-setup for better clarity</li> <li>(hub) Renamed bin/ins-hub to bin/instructor-hub for better clarity</li> <li>(docs) Minor corrections related to setting default config path</li> </ul>"},{"location":"release-notes/v0.17.7/","title":"v0.17.7","text":"<ul> <li>Fixed script merge issues</li> </ul>"},{"location":"release-notes/v0.17.8/","title":"v0.17.8","text":"<ul> <li>(http) Replaced enums with strings in config files - config/http.conf</li> <li>(polyglot) Replaced enums with strings in config files - config/embeddings.php, config/llm.php</li> </ul>"},{"location":"release-notes/v0.17.9/","title":"v0.17.9","text":"<ul> <li>(polyglot) Refactoring - introduced driver factories for embeddings and inference</li> <li>(polyglot) Introduced provider specific driver classes (in addition to previous modular driver)</li> <li>(polyglot) Fixed issue with Deepseek reasoning model accepted message format (no support for successive user or assistant messages)</li> </ul>"},{"location":"release-notes/v0.8.0/","title":"v0.8.0","text":"<ul> <li>'Structured-to-structured' processing - provide objects or arrays to be processed with LLM, get object as a result</li> <li>Composite language programs with Module classes (inspired by DSPy)</li> <li><code>FunctionCall</code> helper class for extracting arguments for functions, methods or closures</li> <li>(experimental) Anthropic tool calls mode support</li> <li>(experimental) Cohere API support - MdJson only, other modes unstable</li> <li>(experimental) Gemini API client - MdJson &amp; sync only, streaming is unstable</li> <li>(internals) Simplified, cleaner API client code</li> <li>(internals) Consolidated message building logic to support formats required by different APIs</li> <li>(internals) Better control over complex chat message sequences with <code>Scripts</code> and <code>Sections</code></li> <li>(internals) Code cleanup and bug fixes</li> <li>Additions to docs and examples</li> </ul>"},{"location":"release-notes/v1.0.0-RC10/","title":"v1.0.0-RC10","text":"<ul> <li>(schema) Merged schema and schema-v6 code.</li> <li>(schema) Symfony version check and dynamic adapter selection (v6 for lowest, v7 for stable).</li> </ul>"},{"location":"release-notes/v1.0.0-RC11/","title":"v1.0.0-RC11","text":"<ul> <li>(schema) Corrections in Symfony version detection and adapter behavior.</li> </ul>"},{"location":"release-notes/v1.0.0-RC12/","title":"v1.0.0-RC12","text":"<ul> <li>(schema) Refactored schema handling, cleaned up the type handling code.</li> <li>(utils) Improvements in JsonSchema handling and integration with TypeDetails and schema handling.</li> </ul>"},{"location":"release-notes/v1.0.0-RC13/","title":"v1.0.0-RC13","text":"<ul> <li>(instructor) Improved JSON Schema handling with JsonSchemaType class</li> <li>(instructor) Improved date handling</li> <li>(schema) Support for parallel handling of PropertyInfo for Symfony v6 and v7 (auto-detected)</li> </ul>"},{"location":"release-notes/v1.0.0-RC14/","title":"v1.0.0-RC14","text":"<ul> <li>Fixed #43 - Call to undefined method Cognesy\\Http\\HttpClient::makeDefaultDriver()</li> </ul>"},{"location":"release-notes/v1.0.0-RC15/","title":"v1.0.0-RC15","text":"<ul> <li>(utils) Default config path is now taken from <code>INSTRUCTOR_CONFIG_PATHS</code> environment variable. <code>INSTRUCTOR_CONFIG_PATH</code> is still recognized for backward compatibility, but it is recommended to use <code>INSTRUCTOR_CONFIG_PATHS</code> instead. Default config location is now your project root's <code>config</code> directory, with fallback to the bundled config directory if not set.</li> </ul>"},{"location":"release-notes/v1.0.0-RC16/","title":"v1.0.0-RC16","text":"<ul> <li>(utils) Settings class now check not just for the existence of config dir, but also config files when resolving config path.</li> </ul>"},{"location":"release-notes/v1.0.0-RC17/","title":"v1.0.0-RC17","text":"<ul> <li>(utils) New, simplified Settings class with bugs fixed.</li> </ul>"},{"location":"release-notes/v1.0.0-RC18/","title":"v1.0.0-RC18","text":"<ul> <li>(all) Strict types checking is now enabled for all packages.</li> </ul>"},{"location":"release-notes/v1.0.0-RC19/","title":"v1.0.0-RC19","text":"<ul> <li>(utils) Split message and message list classes into separate package (instructor-messages)</li> </ul>"},{"location":"release-notes/v1.0.0-RC20/","title":"v1.0.0-RC20","text":"<ul> <li>(messages) Fixing: instructor/messages not split into separate repo</li> </ul>"},{"location":"release-notes/v1.0.0-RC21/","title":"v1.0.0-RC21","text":"<ul> <li>(messages) Fixing: instructor/messages not split into separate repo</li> </ul>"},{"location":"release-notes/v1.0.0-RC22/","title":"v1.0.0-RC22","text":"<ul> <li>(instructor) Moved structure class to a separate package (instructor-dynamic)</li> <li>(hub) Fix: error in summary generation (due to strict types)</li> </ul>"},{"location":"release-notes/v1.0.0-RC6/","title":"v1.0.0-RC6","text":"<ul> <li>Corrected dependencies in composer.json files.</li> <li>Corrected naming scheme for the release candidate (rc -&gt; RC) to make Packagist happy.</li> </ul>"},{"location":"release-notes/v1.0.0-RC7/","title":"v1.0.0-RC7","text":"<ul> <li>(composer/packagist) Corrected minimum-stability value to \"RC\" (case sensitive).</li> </ul>"},{"location":"release-notes/v1.0.0-RC8/","title":"v1.0.0-RC8","text":"<ul> <li>(packagist) Removed 'version' from composer.json files.</li> </ul>"},{"location":"release-notes/v1.0.0-RC9/","title":"v1.0.0-RC9","text":"<ul> <li>(utils) Corrected tests and removed Messages dependency on Template</li> <li>(scripts) Moved build scripts to <code>scripts/</code> directory</li> <li>(scripts) Corrected publish script to NOT force add version field to composer.json</li> </ul>"},{"location":"release-notes/v1.0.0-rc1/","title":"v1.0.0-rc1","text":"<ul> <li>(all) Multiple breaking changes - proceed with caution</li> <li>(instructor) the <code>Instructor</code> class is being replaced with <code>StructuredOutput</code> class; the old class will be kept for some time to allow for a smooth transition.</li> <li>(all) Common conventions for working with StructuredOutput, Inference, and Embeddings classes</li> <li>(examples) All examples have been updated to use the new <code>StructuredOutput</code> class and recommended create(), generate() methods</li> <li>(docs) Updated documentation to reflect the new <code>StructuredOutput</code> class and its usage</li> <li>(instructor) Extracted structured output config into a separate file config/structured.php (and removed from config/llm.php)</li> <li>(instructor) Added structured output config object</li> <li>(instructor) Cleaned up ChatTemplate class</li> <li>(instructor) Response and response streams are now cached after the first call, so multiple calls to <code>StructuredOutputResponse::response()</code> or <code>StructuredOutputResponse::stream()</code> do not cause re-processing (deserialization, validation, transformation) of the response value</li> <li>(instructor) StructuredOutput class offers fluent API for creating structured output requests</li> <li>(instructor) StructuredOutputResponse now offers getXxx() methods for getting the response value as a specific type (e.g. getString(), getInt(), getFloat(), getBool(), getArray(), getObject())</li> <li>(instructor) Removed <code>input</code> argument from StructuredOutput methods and StructuredOutputRequest class - use <code>messages</code> instead - (examples) Added StructuredOutput fluent API example</li> <li>(polyglot) Added fluent API calls to Inference class</li> <li>(polyglot) Added fluent API calls to Embeddings class</li> <li>(polyglot) Corrections in inference drivers, fixed defects in JSON/JSON Schema modes</li> <li>(polyglot) Fixed error in selection of embeddings driver</li> <li>(polyglot) Added <code>withDebug()</code> support to Embeddings class</li> <li>(polyglot) Added experimental support for HuggingFace inference API</li> <li>(all) Multiple changes, improvements and refactorings in the codebase</li> <li>(all) Updated docs and examples to reflect the latest changes</li> </ul>"},{"location":"release-notes/v1.0.0-rc2/","title":"v1.0.0-rc2","text":"<ul> <li>(polyglot) Cleaned up the code and interfaces - split responsibilities between inference vs HTTP layers</li> <li>(polyglot) Heavily refactored API to improve integration with HTTP layer</li> <li>(instructor) Object hydration via constructor parameters with support for parameter nullability and default values.</li> <li>(instructor) Object hydration via getters and setters (recognizes nullable parameters and default values).</li> <li>(instructor) Replaced deprecated PropertyInfo Type class with TypeInfo one.</li> <li>(instructor) Support for mixed property type.</li> <li>(schema) Introduced 2 versions of the schema package - Symfony 7 (default) and Symfony 6 (compatibility).</li> <li>(inference) Added <code>withHttpClientPreset()</code> to <code>Inference</code> and <code>StructuredOutput</code> facades</li> <li>(http) Configurable stream chunk size to optimize performance</li> <li>(http) Replaced debugging middleware with EventSourceMiddleware - generates events for HTTP requests, added 2 built-in listeners (PrintToConsole, DispatchHttpEvents)</li> </ul>"},{"location":"release-notes/v1.0.0-rc3/","title":"v1.0.0-rc3","text":"<ul> <li>(bin) Initial version of local split-packages.sh</li> </ul>"},{"location":"release-notes/v1.0.0-rc4/","title":"v1.0.0-rc4","text":"<ul> <li>(dependencies) Corrected schema-v6 version</li> </ul>"},{"location":"release-notes/v1.0.0-rc5/","title":"v1.0.0-rc5","text":"<ul> <li>(schema), (instructor) Corrected dependencies in composer.json</li> <li>Corrected version naming scheme.</li> </ul>"},{"location":"release-notes/v1.0.0/","title":"v1.0.0","text":"<ul> <li>(inference) Corrected CanHandleInference contract to enable future integration with HTTP pool mechanism</li> </ul>"},{"location":"release-notes/v1.1.0/","title":"v1.1.0","text":"<ul> <li>(messages) Added nicer API for creating message sequences (via <code>asUser()</code>, <code>asSystem()</code>, <code>asAssistant()</code>)</li> <li>(inference) <code>Inference::with()</code>, <code>Inference::withMessages()</code> now support <code>Message</code>, <code>Messages</code> objects directly</li> <li>(instructor) <code>StructuredOutput::with()</code>, <code>StructuredOutput::withMessages()</code> now support <code>Message</code>, <code>Messages</code> objects directly</li> </ul>"},{"location":"release-notes/v1.10.0/","title":"v1.10.0","text":""},{"location":"release-notes/v1.10.0/#1100-summary","title":"1.10.0 Summary","text":"<p>This is a release focused mostly on type safety &amp; static analysis. We have made comprehensive improvements to PHPDoc annotations, generic types, null-safety, and PHP 8.3+ compatibility.</p>"},{"location":"release-notes/v1.10.0/#features-improvements","title":"\u2728 Features &amp; Improvements","text":""},{"location":"release-notes/v1.10.0/#type-safety-enhancements-all-packages","title":"Type Safety Enhancements (All Packages)","text":"<ul> <li>Generic Type Support: Added comprehensive <code>@template</code>, <code>@param</code>, and <code>@return</code> PHPDoc annotations across collections, operators, and state processors</li> <li>PHP 8.3+ Compatibility: Added <code>#[\\Override]</code> attributes to 300+ method implementations for compile-time verification</li> <li>Strict Type Checking: Applied <code>strict_types=1</code> declarations and strict comparison operators throughout</li> <li>Callable Signatures: Enhanced PHPDoc with precise callable type hints (e.g., <code>callable(Section): bool</code>)</li> </ul>"},{"location":"release-notes/v1.10.0/#null-safety-improvements","title":"Null-Safety Improvements","text":"<ul> <li>schema: Fixed nullable type handling in <code>TypeDetails</code>, <code>SignatureField</code>, and reflection classes</li> <li>instructor: Better null handling in deserialization layer (<code>CustomObjectNormalizer</code>, <code>FlexibleDateDenormalizer</code>)</li> <li>messages: Fixed null pointer issues in <code>Messages::firstRole()</code>, <code>Messages::lastRole()</code>, and filter operations</li> <li>auxiliary: Added type guards in <code>Codebase::getMethodParams()</code> and <code>RawHtml::convertToAbsoluteUrls()</code></li> <li>dynamic: Explicit null initialization for <code>Field::$value</code> and <code>Field::$validator</code> properties</li> </ul>"},{"location":"release-notes/v1.10.0/#collection-api-standardization-utils","title":"Collection API Standardization (utils)","text":"<ul> <li>Renamed <code>ArrayList::get()</code> \u2192 <code>ArrayList::itemAt()</code> for interface consistency</li> <li>Added comprehensive generic type annotations to <code>ArrayList</code>, <code>ArrayMap</code>, <code>ArraySet</code></li> <li>Improved <code>TagMap</code> type safety with proper tag storage annotations</li> </ul>"},{"location":"release-notes/v1.10.0/#result-monad-enhancements-utils","title":"Result Monad Enhancements (utils)","text":"<p>Helper methods for batch operations:</p> <pre><code>Result::tryAll(callable ...$operations): Result  // Execute all, aggregate errors\nResult::tryUntil(callable ...$operations): Result  // Execute until first success\n</code></pre> <p>Improved error aggregation via <code>CompositeException</code> for better debugging.</p>"},{"location":"release-notes/v1.10.0/#state-processing-addons","title":"State Processing (addons)","text":"<ul> <li>Added <code>forceThrowOnFailure</code> parameter to <code>Chat</code> class for graceful error handling</li> <li>Enhanced generic type constraints in <code>StateProcessors</code>, <code>ContinuationCriteria</code>, and middleware chains</li> <li>Added <code>ChatFactory::defaultProcessors()</code> helper method</li> </ul>"},{"location":"release-notes/v1.10.0/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":""},{"location":"release-notes/v1.10.0/#file-operations","title":"File Operations","text":"<ul> <li>doctor: Fixed <code>glob()</code> returning <code>false</code> instead of empty arrays (multiple files)</li> <li>doctor: Fixed unhandled <code>false</code> returns from <code>file_get_contents()</code>, <code>realpath()</code>, and <code>preg_replace()</code></li> <li>instructor: Fixed file operation error handling with runtime exceptions</li> </ul>"},{"location":"release-notes/v1.10.0/#type-safety","title":"Type Safety","text":"<ul> <li>auxiliary: Fixed null safety in parameter extraction and DOM element handling</li> <li>dynamic: Fixed schema immutability violations in <code>Field::withName()</code> and <code>Field::withDescription()</code></li> <li>messages: Added runtime exceptions for empty collection operations instead of returning null</li> <li>schema: Improved nullable type handling with proper null coalescing and type guards</li> <li>pipeline: Better null handling in <code>StepTimingTag::startDateTime()</code></li> </ul>"},{"location":"release-notes/v1.10.0/#array-operations","title":"Array Operations","text":"<ul> <li>evals: Fixed array key conflicts in <code>MakeObservations</code> and <code>SelectObservations</code> using <code>array_values()</code></li> <li>schema: Added strict type comparisons to all <code>in_array()</code> calls</li> </ul>"},{"location":"release-notes/v1.10.1/","title":"v1.10.1","text":""},{"location":"release-notes/v1.10.1/#changes","title":"Changes","text":""},{"location":"release-notes/v1.10.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed tests failing due to ResponseFormat refactoring</li> </ul>"},{"location":"release-notes/v1.10.2/","title":"v1.10.2","text":""},{"location":"release-notes/v1.10.2/#changes","title":"Changes","text":""},{"location":"release-notes/v1.10.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed Symfony PropertyInfo adapter auto-detection failing with <code>--prefer-lowest</code> dependencies. Detection now properly distinguishes between Symfony 6 (uses <code>PropertyInfoExtractor::getTypes()</code>) and Symfony 7 (uses <code>PropertyInfoExtractor::getType()</code> with TypeInfo component) by checking for <code>Symfony\\Component\\TypeInfo\\Type</code> class existence.</li> </ul>"},{"location":"release-notes/v1.10.3/","title":"v1.10.3","text":""},{"location":"release-notes/v1.10.3/#changes","title":"Changes","text":""},{"location":"release-notes/v1.10.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Removed <code>#[\\Override]</code> attribute from <code>getSupportedTypes()</code> in <code>FlexibleDateDenormalizer</code>, <code>BackedEnumNormalizer</code>, and <code>CustomObjectNormalizer</code> - method only exists in Symfony 7+ interfaces, causing fatal errors with Symfony 6.4.</li> </ul>"},{"location":"release-notes/v1.11.0/","title":"Release Notes","text":""},{"location":"release-notes/v1.11.0/#executive-summary","title":"Executive Summary","text":"<p>This release represents a major architectural milestone for the instructor-php monorepo, introducing the new stream package for functional data processing and significantly refactoring the instructor package's streaming architecture. The changes focus on modularity, composability, and maintainability while improving type safety across all packages.</p> <ul> <li>New <code>stream</code> package: Complete functional stream processing library with 24+ sources, 40+ transducers, and 20+ reducers</li> <li>Major instructor refactoring: Streaming architecture completely redesigned for modularity and testability</li> <li>Better code organization: Logical namespace restructuring across multiple packages</li> <li>Enhanced utilities: New data structures (Deque, Buffer, RingBuffer) and error collection system</li> <li>Type safety improvements: Better PHPDoc annotations and flexible type hints</li> </ul>"},{"location":"release-notes/v1.11.0/#instructor-package","title":"<code>instructor</code> Package","text":"<ul> <li>Replaced <code>PartialsGenerator</code> with modular components:</li> <li>Use <code>GeneratePartialsFromJson</code> for JSON streaming mode</li> <li>Use <code>GeneratePartialsFromToolCalls</code> for tool-based streaming mode</li> <li>Method rename: <code>CanGeneratePartials::getPartialResponses()</code> \u2192 <code>makePartialResponses()</code></li> <li>Moved classes to new namespaces (aliases may be provided for backward compatibility):</li> <li><code>Core/ResponseModelFactory</code> \u2192 <code>Creation/ResponseModelFactory</code></li> <li><code>Core/StructuredOutput*</code> \u2192 <code>Creation/StructuredOutput*</code></li> <li><code>Core/SequenceableHandler</code> \u2192 <code>Streaming/SequenceGen/SequenceableEmitter</code></li> </ul>"},{"location":"release-notes/v1.11.0/#stream-package","title":"<code>stream</code> Package","text":"<p>A complete functional stream processing library implementing transducers: - 24+ Stream Sources: Array, CSV, JSON, JSONL, HTTP, Filesystem, Text processing - 40+ Transducers: Transform, filter, flatten, group, limit, combine, deduplicate - 20+ Reducers: Terminal operations for stats, selection, side effects - Tee Splitting: Process same source through multiple parallel pipelines - Result Monad Support: Functional error handling with dedicated transducers/reducers - Progressive Rendering: Iterator-based execution for memory-efficient streaming</p>"},{"location":"release-notes/v1.11.0/#utils-package","title":"<code>utils</code> Package","text":"<ul> <li>Deque and Circular Buffer data structures</li> <li>Immutable error collection system with serialization</li> <li>New Arrays operations for merging multiple arrays</li> </ul>"},{"location":"release-notes/v1.11.0/#polyglot-package","title":"<code>polyglot</code> Package","text":"<ul> <li><code>Inference/Creation/</code>: Factory and builder classes</li> <li><code>Inference/Streaming/</code>: Stream processing with new <code>ContentAccumulation</code> class</li> <li>Optimized <code>InferenceExecution::errors()</code> using <code>Arrays::mergeMany()</code></li> <li>Refactored <code>InferenceStream::makePartialResponses()</code> with extracted methods</li> </ul>"},{"location":"release-notes/v1.11.0/#http-client-package","title":"<code>http-client</code> Package","text":"<ul> <li>Simplified Symfony driver</li> <li>Added annotations to streaming methods</li> </ul>"},{"location":"release-notes/v1.11.0/#addons-package","title":"<code>addons</code> Package","text":"<p>Type Flexibility: - Relaxed <code>StepByStep</code> return types from <code>Generator&lt;mixed, TState, mixed, mixed&gt;</code> to <code>iterable&lt;TState&gt;</code> - Allows implementations to return any iterable type while maintaining type safety</p>"},{"location":"release-notes/v1.12.0/","title":"Release Notes - v1.12.0","text":""},{"location":"release-notes/v1.12.0/#executive-summary","title":"Executive Summary","text":"<p>The release includes substantial refactoring across core packages to improve separation of concerns, eliminate technical debt, and achieve better performance characteristics.</p>"},{"location":"release-notes/v1.12.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"release-notes/v1.12.0/#instructor-package","title":"instructor Package","text":"<ul> <li>Removed <code>CanExecuteStructuredOutput</code> interface - Replaced with dual-interface architecture: <code>CanStreamStructuredOutputUpdates</code> (stream-level iteration) and <code>CanHandleStructuredOutputAttempts</code> (attempt-level orchestration)</li> <li>Removed <code>IterativeToGeneratorAdapter</code> - All execution now flows through unified <code>AttemptIterator</code> with specialized stream iterators</li> <li>Request handler hierarchy replaced - <code>RequestHandler</code>, <code>SyncRequestHandler</code>, <code>PartialStreamingRequestHandler</code> removed in favor of stream iterator pattern</li> </ul>"},{"location":"release-notes/v1.12.0/#addons-package","title":"addons Package","text":"<ul> <li>Namespace reorganization - Shared components moved from <code>StepByStep</code> to domain-specific namespaces:</li> <li>Chat traits moved to <code>Chat/State</code> and <code>Chat/Step</code> namespaces</li> <li>Selector implementations moved to dedicated subdirectories</li> </ul>"},{"location":"release-notes/v1.12.0/#polyglot-package","title":"polyglot Package","text":"<ul> <li>Default HTTP client changed - Changed from <code>guzzle</code> to <code>curl</code> preset to make it easy to use Instructor out of the box</li> </ul>"},{"location":"release-notes/v1.12.0/#utils-package","title":"utils Package","text":"<ul> <li><code>ProgrammingLanguage</code> is now an enum - Changed from class to <code>enum ProgrammingLanguage : string</code>; direct instantiation patterns will break</li> </ul>"},{"location":"release-notes/v1.12.0/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.12.0/#native-curl-http-driver-http-client","title":"Native cURL HTTP Driver (http-client)","text":"<p>Zero-dependency HTTP functionality using PHP's built-in cURL: - CurlDriver: Full HTTP method support with custom headers, timeouts, SSL verification - Concurrent requests: <code>CurlPool</code> using <code>curl_multi</code> for parallel execution - Streaming support: <code>CurlStreamingHttpResponse</code> with real-time chunked delivery - Event integration: Full support for <code>HttpRequestSent</code>, <code>HttpResponseReceived</code>, <code>HttpRequestFailed</code> - Comprehensive tests: Integration tests covering GET/POST, headers, errors, timeouts, streaming</p>"},{"location":"release-notes/v1.12.0/#major-improvements","title":"Major Improvements","text":""},{"location":"release-notes/v1.12.0/#unified-streaming-architecture-instructor","title":"Unified Streaming Architecture (instructor)","text":"<p>Code refactoring achieving O(1) memory usage and consistent execution model: - Stream iterator pattern: Three specialized implementations (<code>SyncUpdateGenerator</code>, <code>PartialStreamingUpdateGenerator</code>, <code>StreamingUpdatesGenerator</code>) with unified contract - AttemptIterator orchestration: Single code path for sync and streaming with validation, retries, failure recovery - Transducer-based pipeline: Pure transformation stages (delta extraction, JSON assembly, deserialization, deduplication, sequence tracking) using immutable state - Rolling aggregation: Eliminated accumulation of all partial responses - Robust retry handling: <code>DefaultRetryPolicy</code> with flexible logic for validation failures, connection errors, rate limits - Enhanced state management: <code>StructuredOutputAttemptState</code> and <code>AttemptPhase</code> enum for explicit progress tracking</p>"},{"location":"release-notes/v1.12.0/#tooluse-refactoring-addons","title":"ToolUse Refactoring (addons)","text":"<p>Significant architectural improvements with better separation of concerns: - ReAct driver decomposition: Extracted into action classes (<code>MakeReActPrompt</code>, <code>MakeToolCalls</code>) and validation layer (<code>ReActValidator</code>) - Improved data structures: <code>DecisionWithDetails</code>, <code>ReActDecision</code> implementing <code>Decision</code> contract - Better error handling: Structured validation results with comprehensive decision validation - Formatter extraction: <code>ReActFormatter</code> utility for output formatting - Reorganized continuation: Moved to <code>ContinuationCriteria</code> subdirectory - Executor abstraction: <code>CanExecuteToolCalls</code> contract</p>"},{"location":"release-notes/v1.12.0/#chat-improvements-addons","title":"Chat Improvements (addons)","text":"<p>Enhanced code organization and reusability: - Extracted traits: <code>HandlesChatSteps</code>, <code>HandlesChatCompletion</code>, <code>HasChatCompletion</code> for state and step management - Reorganized selectors: Moved to dedicated namespaces (<code>RoundRobin</code>, <code>LLMSelector</code>) - Enhanced state management: Dedicated step handling</p>"},{"location":"release-notes/v1.12.0/#new-llm-provider-support","title":"New LLM Provider Support","text":""},{"location":"release-notes/v1.12.0/#inception-inference-provider-polyglot","title":"Inception Inference Provider (polyglot)","text":"<ul> <li>InceptionDriver: Complete inference driver supporting Inception API</li> </ul>"},{"location":"release-notes/v1.12.0/#bug-fixes-quality-improvements","title":"Bug Fixes &amp; Quality Improvements","text":""},{"location":"release-notes/v1.12.0/#stream-completion-handling-stream","title":"Stream Completion Handling (stream)","text":"<ul> <li>Fixed completion phase: Added <code>transduction-&gt;completed()</code> call to flush remaining values</li> <li>Stateful transformations: Properly finalize and emit accumulated data at stream end</li> <li>Data loss prevention: Ensures buffered data is not lost on stream termination</li> </ul>"},{"location":"release-notes/v1.12.0/#type-safety-improvements-utils","title":"Type Safety Improvements (utils)","text":"<ul> <li>PartialJsonParser: Enhanced type safety ensuring <code>parse()</code> always returns <code>array|object</code> as declared</li> <li>Metadata: Added <code>withMergedData(array $data)</code> method for batch updates</li> </ul>"},{"location":"release-notes/v1.12.0/#static-analysis-addons","title":"Static Analysis (addons)","text":"<ul> <li>Fixed Psalm/PHPStan issues: Resolved static analysis warnings throughout</li> <li>Improved transducer stability: Enhanced streaming driver reliability</li> <li>Better error handling: Comprehensive error handling in ReActDriver</li> </ul>"},{"location":"release-notes/v1.12.0/#migration-guide","title":"Migration Guide","text":""},{"location":"release-notes/v1.12.0/#for-instructor-users","title":"For <code>instructor</code> Users","text":"<ul> <li>No action required for public API users - changes are internal</li> <li>Custom implementations of <code>CanExecuteStructuredOutput</code> must migrate to new dual-interface architecture</li> <li>Streaming behavior is now more consistent with better memory efficiency</li> </ul>"},{"location":"release-notes/v1.12.0/#for-polyglot-users","title":"For <code>polyglot</code> Users","text":"<ul> <li>Verify HTTP behavior if relying on Guzzle-specific features</li> <li>Update configuration to explicitly specify <code>guzzle</code> preset if needed: <code>'preset' =&gt; 'guzzle'</code></li> </ul>"},{"location":"release-notes/v1.12.0/#for-utils-users","title":"For <code>utils</code> Users","text":"<ul> <li>Update <code>ProgrammingLanguage</code> usage from class to enum syntax</li> <li>Example: <code>ProgrammingLanguage::PHP</code> instead of <code>new ProgrammingLanguage('PHP')</code></li> <li>Consider SafeSandbox for any untrusted code execution needs</li> </ul>"},{"location":"release-notes/v1.13.0/","title":"Release Notes v1.13.0","text":""},{"location":"release-notes/v1.13.0/#major-package-enhancements","title":"\ud83d\ude80 Major Package Enhancements","text":"<p>Instructor Package (<code>packages/instructor/</code>)</p> <ul> <li>Streaming Response System: Complete rewrite with three implementation approaches:</li> <li>Decorated Pipeline with reducer pattern</li> <li>Generator-Based approach for memory efficiency</li> <li>Modular Pipeline with transducers</li> <li>Enhanced Event System: 30+ new event types covering the full request/response lifecycle</li> <li>Improved Structured Output: Better validation, retry handling, and error recovery</li> <li>Response Iterators: Advanced partial response generation and aggregation</li> <li>Tool Call Handling: Enhanced support for streaming tool call processing</li> </ul> <p>Polyglot Package (<code>packages/polyglot/</code>)</p> <ul> <li>New Inference Drivers: Added Amazon Bedrock OpenAI-compatible integration</li> <li>Enhanced Usage Tracking: Simplified overflow handling and better token management</li> <li>Improved Response Adapters: Better provider-specific response handling</li> <li>Streaming Enhancements: Improved event stream processing</li> </ul> <p>New Logging Framework (<code>packages/logging/</code>)</p> <ul> <li>Comprehensive event logging system for Instructor operations</li> <li>Laravel and Symfony framework integrations</li> <li>Configurable enrichers, filters, and formatters</li> <li>PSR-3 logger compatibility</li> <li>Event hierarchy filtering and log level management</li> </ul>"},{"location":"release-notes/v1.13.0/#infrastructure-improvements","title":"\ud83d\udee0\ufe0f Infrastructure Improvements","text":"<p>Utils Package (<code>packages/utils/</code>)</p> <ul> <li>Sandbox Security: New sandbox drivers (Bubblewrap, Firejail, Podman)</li> <li>Enhanced JSON Processing: Improved JSON utilities with better error handling</li> <li>Execution Policies: Configurable security policies for command execution</li> </ul> <p>Stream Package (<code>packages/stream/</code>)</p> <ul> <li>Enhanced transformation stream capabilities</li> <li>Better stream processing utilities</li> </ul> <p>Refactored HTTP Client Package (<code>packages/http-client/</code>)</p> <ul> <li>Complete HTTP client abstraction layer with multiple driver support</li> <li>Drivers: Guzzle, Symfony, Laravel, cURL, Mock</li> <li>Advanced middleware system for request/response processing</li> <li>Pool/concurrent request handling capabilities</li> <li>Event-driven architecture with Server-Side Events support</li> <li>Record/replay functionality for testing</li> <li>Comprehensive streaming support</li> </ul> <p>Hub Package (<code>packages/hub/</code>)</p> <ul> <li>Enhanced command interface with better type safety</li> <li>Improved error handling and output formatting</li> <li>Better execution tracking and status management</li> <li>Enhanced example repository functionality</li> </ul>"},{"location":"release-notes/v1.13.0/#quality-compatibility-fixes","title":"\ud83d\udc1b Quality &amp; Compatibility Fixes","text":"<ul> <li>Static Analysis Compliance: Full PHPStan level 8 and Psalm level 1 compliance across all packages</li> <li>PHP 8.x Compatibility: Proper <code>#[Override]</code> attribute usage where appropriate</li> <li>CI/CD Improvements: Better test coverage and build pipeline stability</li> <li>Type Safety: Enhanced type hints and return type declarations throughout</li> </ul>"},{"location":"release-notes/v1.13.0/#breaking-changes","title":"\ud83d\udd27 Breaking Changes","text":"<ul> <li>Changes in HTTP client APIs</li> <li>Some Instructor streaming APIs have been redesigned for better performance</li> </ul>"},{"location":"release-notes/v1.13.0/#migration-notes","title":"\ud83d\udccb Migration Notes","text":"<ol> <li>HTTP Client: Update API calls to match new interfaces</li> <li>Streaming: Review custom streaming implementations - new APIs may provide better performance</li> <li>Logging: Configure new logging framework if advanced logging features are needed</li> </ol>"},{"location":"release-notes/v1.13.0/#key-benefits","title":"\ud83c\udfaf Key Benefits","text":"<ul> <li>Better Performance: Optimized streaming and response processing</li> <li>Enhanced Developer Experience: Improved type safety and IDE support</li> <li>Greater Modularity: Separated concerns with dedicated packages</li> <li>Production Ready: Comprehensive logging and monitoring capabilities</li> <li>Security Enhanced: Sandbox execution for secure command processing</li> </ul>"},{"location":"release-notes/v1.14.0/","title":"Release Notes v1.14.0","text":""},{"location":"release-notes/v1.14.0/#new-package-instructor-laravel","title":"New Package: instructor-laravel","text":"<p>Laravel Integration Package (<code>packages/laravel/</code>)</p> <p>A dedicated package providing first-class Laravel integration for Instructor PHP:</p>"},{"location":"release-notes/v1.14.0/#facades","title":"Facades","text":"<p>Four main facades for interacting with LLMs and code agents:</p> <ul> <li><code>StructuredOutput::</code> - Extract structured data from text with validation and retries</li> <li><code>Inference::</code> - Raw LLM inference without structured output constraints</li> <li><code>Embeddings::</code> - Generate text embeddings for vector operations</li> <li><code>AgentCtrl::</code> - Invoke CLI-based code agents (Claude Code, Codex, OpenCode)</li> </ul> <pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\StructuredOutput;\n\n$person = StructuredOutput::with(\n    messages: 'John Smith is 30 years old',\n    responseModel: PersonData::class,\n)-&gt;get();\n</code></pre>"},{"location":"release-notes/v1.14.0/#testing-fakes","title":"Testing Fakes","text":"<p>Laravel-style testing doubles with <code>::fake()</code> pattern:</p> <pre><code>$fake = StructuredOutput::fake([\n    PersonData::class =&gt; new PersonData(name: 'John', age: 30),\n]);\n\n// Your code runs...\n\n$fake-&gt;assertExtracted(PersonData::class);\n$fake-&gt;assertExtractedTimes(PersonData::class, 1);\n$fake-&gt;assertUsedPreset('anthropic');\n</code></pre> <p>Available fakes: - <code>StructuredOutput::fake()</code> - Mock structured output responses - <code>Inference::fake()</code> - Mock inference responses with pattern matching - <code>Embeddings::fake()</code> - Mock embedding vectors - <code>AgentCtrl::fake()</code> - Mock code agent executions</p>"},{"location":"release-notes/v1.14.0/#laravel-http-client-integration","title":"Laravel HTTP Client Integration","text":"<p>Uses Laravel's <code>Http::</code> client internally, enabling: - Full compatibility with <code>Http::fake()</code> in tests - Request/response logging through Laravel's HTTP middleware - Automatic retry handling via Laravel's HTTP client features</p>"},{"location":"release-notes/v1.14.0/#event-bridge","title":"Event Bridge","text":"<p>Instructor events are automatically dispatched to Laravel's event system:</p> <pre><code>// In EventServiceProvider\nprotected $listen = [\n    \\Cognesy\\Events\\Event::class =&gt; [\n        MyInstructorEventListener::class,\n    ],\n];\n</code></pre>"},{"location":"release-notes/v1.14.0/#artisan-commands","title":"Artisan Commands","text":"<ul> <li><code>php artisan instructor:install</code> - Publish configuration and setup</li> <li><code>php artisan instructor:test</code> - Test your API configuration</li> <li><code>php artisan make:response-model</code> - Generate response model classes</li> </ul>"},{"location":"release-notes/v1.14.0/#configuration","title":"Configuration","text":"<p>Laravel-style configuration with environment variables:</p> <pre><code>// config/instructor.php\nreturn [\n    'default' =&gt; env('INSTRUCTOR_CONNECTION', 'openai'),\n    'connections' =&gt; [\n        'openai' =&gt; [\n            'driver' =&gt; 'openai',\n            'api_key' =&gt; env('OPENAI_API_KEY'),\n            'model' =&gt; env('OPENAI_MODEL', 'gpt-4o-mini'),\n        ],\n        'anthropic' =&gt; [\n            'driver' =&gt; 'anthropic',\n            'api_key' =&gt; env('ANTHROPIC_API_KEY'),\n            'model' =&gt; env('ANTHROPIC_MODEL', 'claude-sonnet-4-20250514'),\n        ],\n    ],\n];\n</code></pre>"},{"location":"release-notes/v1.14.0/#new-feature-agentctrl-facade","title":"New Feature: AgentCtrl Facade","text":"<p>Unified API for Controlling Agents</p> <p>The <code>AgentCtrl</code> facade provides access to CLI-based code agents that can execute code, modify files, and perform complex tasks:</p>"},{"location":"release-notes/v1.14.0/#supported-agents","title":"Supported Agents","text":"Agent Description Claude Code Anthropic's Claude agent with code execution Codex OpenAI's Codex agent OpenCode Multi-model code agent with provider flexibility"},{"location":"release-notes/v1.14.0/#usage","title":"Usage","text":"<pre><code>use Cognesy\\Instructor\\Laravel\\Facades\\AgentCtrl;\n\n// Execute with Claude Code\n$response = AgentCtrl::claudeCode()\n    -&gt;withModel('claude-opus-4-5')\n    -&gt;inDirectory(base_path())\n    -&gt;withTimeout(300)\n    -&gt;execute('Generate a Laravel migration for users table');\n\nif ($response-&gt;isSuccess()) {\n    echo $response-&gt;text();\n    echo \"Cost: $\" . $response-&gt;cost;\n}\n</code></pre>"},{"location":"release-notes/v1.14.0/#streaming-support","title":"Streaming Support","text":"<pre><code>$response = AgentCtrl::claudeCode()\n    -&gt;onText(fn($text) =&gt; echo $text)\n    -&gt;onToolUse(fn($tool, $input, $output) =&gt; logger(\"Tool: $tool\"))\n    -&gt;executeStreaming('Refactor the User model');\n</code></pre>"},{"location":"release-notes/v1.14.0/#session-management","title":"Session Management","text":"<pre><code>// Start a session\n$response = AgentCtrl::claudeCode()-&gt;execute('Start refactoring...');\n$sessionId = $response-&gt;sessionId;\n\n// Resume later\n$response = AgentCtrl::claudeCode()\n    -&gt;resumeSession($sessionId)\n    -&gt;execute('Continue with the next file');\n</code></pre>"},{"location":"release-notes/v1.14.0/#testing","title":"Testing","text":"<pre><code>$fake = AgentCtrl::fake([\n    'Generated migration successfully',\n    'Created test file',\n]);\n\n// Your code runs...\n\n$fake-&gt;assertExecuted();\n$fake-&gt;assertUsedClaudeCode();\n$fake-&gt;assertExecutedWith('migration');\n</code></pre>"},{"location":"release-notes/v1.14.0/#sandbox-drivers","title":"Sandbox Drivers","text":"<p>Control agent execution isolation:</p> Driver Description <code>host</code> Direct execution (development) <code>docker</code> Docker container isolation <code>podman</code> Rootless container isolation <code>firejail</code> Linux sandbox <code>bubblewrap</code> Minimal sandbox (CI/CD)"},{"location":"release-notes/v1.14.0/#installation","title":"Installation","text":"<pre><code>composer require cognesy/instructor-laravel\n</code></pre> <p>Add to your <code>.env</code>:</p> <pre><code>OPENAI_API_KEY=your-api-key\n</code></pre> <p>The package auto-discovers and registers itself with Laravel.</p>"},{"location":"release-notes/v1.14.0/#migration-notes","title":"Migration Notes","text":""},{"location":"release-notes/v1.14.0/#for-existing-instructor-users","title":"For Existing Instructor Users","text":"<p>The Laravel package is additive - no changes required to existing code. You can: 1. Continue using Instructor directly via dependency injection 2. Adopt facades gradually where convenient 3. Use <code>::fake()</code> pattern for new tests</p>"},{"location":"release-notes/v1.14.0/#recommended-adoption-path","title":"Recommended Adoption Path","text":"<ol> <li>Install the package</li> <li>Replace <code>Http::fake()</code> based tests with <code>StructuredOutput::fake()</code></li> <li>Adopt facades in new code for cleaner syntax</li> <li>Migrate existing DI usage to facades where appropriate</li> </ol>"},{"location":"release-notes/v1.14.0/#key-benefits","title":"Key Benefits","text":"<ul> <li>Laravel Integration: Facades, fakes, and configuration follow Laravel conventions</li> <li>Testing: <code>::fake()</code> pattern with comprehensive assertions</li> <li>Zero Configuration: Works out of the box with sensible defaults</li> <li>Event Integration: Full visibility into Instructor operations via Laravel events</li> <li>Code Agents: Access powerful CLI agents directly from your Laravel application</li> </ul>"},{"location":"release-notes/v1.15.0/","title":"Release Notes v1.15.0","text":""},{"location":"release-notes/v1.15.0/#new-package-agent-ctrl","title":"New Package: agent-ctrl","text":"<p>Agent Control Package (<code>packages/agent-ctrl/</code>)</p> <p>Extracted standalone package for agent control and orchestration, previously part of <code>packages/auxiliary</code>.</p>"},{"location":"release-notes/v1.15.0/#key-features","title":"Key Features","text":"<ul> <li>Agent request/response handling with unified interface</li> <li>Event-driven architecture for logging and monitoring</li> <li>Independent versioning and distribution</li> <li>Sandbox driver support (Docker, Podman, Firejail, Bubblewrap)</li> </ul>"},{"location":"release-notes/v1.15.0/#breaking-changes","title":"Breaking Changes","text":"<p>If you were using agent control features from the <code>auxiliary</code> package, update your imports to use the new <code>agent-ctrl</code> package:</p> <pre><code>// Before\nuse Cognesy\\Auxiliary\\Agents\\...;\n\n// After\nuse Cognesy\\AgentCtrl\\...;\n</code></pre>"},{"location":"release-notes/v1.16.0/","title":"Release Notes - v1.16.0","text":""},{"location":"release-notes/v1.16.0/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.16.0/#outputformat-api","title":"OutputFormat API","text":"<p>Control output format independently from schema:</p> <pre><code>// Get raw arrays instead of objects\n$data = StructuredOutput::with(...)\n    -&gt;intoArray()\n    -&gt;get();\n\n// Deserialize to different class than schema\n$user = StructuredOutput::with(...)\n    -&gt;intoInstanceOf(User::class)\n    -&gt;get();\n\n// Self-deserializing objects\n$obj = StructuredOutput::with(...)\n    -&gt;intoObject($customObject)\n    -&gt;get();\n</code></pre>"},{"location":"release-notes/v1.16.0/#pluggable-extraction","title":"Pluggable Extraction","text":"<p>Custom JSON extraction strategies:</p> <pre><code>use Cognesy\\Instructor\\Extraction\\Contracts\\CanExtractResponse;\nuse Cognesy\\Instructor\\Extraction\\Data\\ExtractionInput;\nuse Cognesy\\Instructor\\Extraction\\Exceptions\\ExtractionException;\n\nclass CustomExtractor implements CanExtractResponse {\n    public function extract(ExtractionInput $input): array {\n        // custom extraction logic\n        // throw ExtractionException on failure\n    }\n\n    public function name(): string {\n        return 'custom';\n    }\n}\n\nStructuredOutput::with(...)\n    -&gt;withExtractors(\n        new CustomExtractor(),\n        new ResilientJsonExtractor(),\n    )\n    -&gt;get();\n</code></pre> <p>Built-in extractors: - <code>DirectJsonExtractor</code> - Parse as-is - <code>ResilientJsonExtractor</code> - Handle malformed JSON - <code>MarkdownBlockExtractor</code> - Extract from code blocks - <code>BracketMatchingExtractor</code> - Find first <code>{</code> to last <code>}</code> - <code>SmartBraceExtractor</code> - Handle escaped quotes</p>"},{"location":"release-notes/v1.16.0/#extraction-events","title":"Extraction Events","text":"<p>Track extraction lifecycle: - <code>ExtractionStarted</code> - <code>ExtractionStrategyAttempted</code> - <code>ExtractionStrategySucceeded</code> - <code>ExtractionStrategyFailed</code> - <code>ExtractionCompleted</code> - <code>ExtractionFailed</code></p>"},{"location":"release-notes/v1.16.0/#manual-schema-support","title":"Manual Schema Support","text":"<p>JSON schemas no longer require <code>x-php-class</code>:</p> <pre><code>$schema = [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'name' =&gt; ['type' =&gt; 'string'],\n    ],\n];\n\n// Returns raw array when no class specified\n$data = StructuredOutput::with(\n    messages: 'John',\n    responseModel: $schema\n)-&gt;get();\n</code></pre>"},{"location":"release-notes/v1.16.0/#response-caching","title":"Response Caching","text":"<pre><code>use Cognesy\\Polyglot\\Inference\\Enums\\ResponseCachePolicy;\n\n// Cache responses in memory\n$response = Inference::with(...)\n    -&gt;withCachePolicy(ResponseCachePolicy::Memory)\n    -&gt;get();\n</code></pre>"},{"location":"release-notes/v1.16.0/#improvements","title":"Improvements","text":"<ul> <li>Simplified processing pipeline</li> <li>Removed unused partial validations</li> </ul>"},{"location":"release-notes/v1.16.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed <code>withInput()</code> to handle objects correctly</li> <li>Fixed <code>HostSandbox</code> to run in <code>baseDir</code> (fixes CLI agents)</li> <li>Fixed environment inheritance for agent execution</li> <li>Fixed exit code propagation in examples</li> <li>Resolved all PHPStan errors</li> </ul>"},{"location":"release-notes/v1.16.0/#new-package-metrics","title":"New Package: metrics","text":"<p>Event-driven metrics collection system.</p> <p>Metric types: - Counter - incremental counts - Gauge - point-in-time values - Histogram - value distributions - Timer - duration measurements</p> <p>Usage:</p> <pre><code>use Cognesy\\Metrics\\Collectors\\MetricsCollector;\nuse Cognesy\\Metrics\\Metrics;\n\n// Create custom collector\nclass StreamMetricsCollector extends MetricsCollector {\n    protected function listeners(): array {\n        return [\n            StreamFirstChunkReceived::class =&gt; $this-&gt;onFirstChunk(...),\n            InferenceCompleted::class =&gt; $this-&gt;onCompleted(...),\n        ];\n    }\n\n    public function onFirstChunk(StreamFirstChunkReceived $event): void {\n        $this-&gt;timer('llm.stream.ttfc_ms', $event-&gt;timeToFirstChunkMs, [\n            'model' =&gt; $event-&gt;model,\n        ]);\n    }\n\n    public function onCompleted(InferenceCompleted $event): void {\n        $this-&gt;gauge('llm.output_tokens', (float) $event-&gt;usage-&gt;output());\n    }\n}\n\n// Register collector and exporter\n$metrics = new Metrics($events);\n$metrics\n    -&gt;collect(new StreamMetricsCollector())\n    -&gt;exportTo(new CallbackExporter(fn($metrics) =&gt; dump($metrics)));\n\n// Metrics auto-collected from events\n$metrics-&gt;export();\n</code></pre>"},{"location":"release-notes/v1.17.0/","title":"Release Notes - v1.17.0","text":""},{"location":"release-notes/v1.17.0/#major-changes","title":"Major Changes","text":""},{"location":"release-notes/v1.17.0/#capability-based-agent-architecture","title":"Capability-Based Agent Architecture","text":"<p>Overhaul of the Agent system with a modular, capability-based design:</p> <pre><code>use Cognesy\\Addons\\AgentBuilder\\AgentBuilder;\nuse Cognesy\\Addons\\AgentBuilder\\Capabilities\\Bash\\UseBash;\nuse Cognesy\\Addons\\AgentBuilder\\Capabilities\\File\\UseFileTools;\nuse Cognesy\\Addons\\AgentBuilder\\Capabilities\\Tasks\\UseTaskPlanning;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseFileTools())\n    -&gt;withCapability(new UseTaskPlanning())\n    -&gt;build();\n</code></pre> <p>New Agent Capabilities: - <code>UseBash</code> - Execute shell commands with configurable policies - <code>UseFileTools</code> - Read, write, edit, search, and list files - <code>UseSkills</code> - Manage reusable skill libraries - <code>UseTaskPlanning</code> - Multi-step task management with TodoWriteTool - <code>UseMetadataTools</code> - Agent scratchpad for inter-tool data sharing - <code>UseStructuredOutputs</code> - LLM-powered structured data extraction - <code>UseSubagents</code> - Recursive agent spawning with AgentRegistry - <code>UseSelfCritique</code> - Self-evaluation of agent work</p>"},{"location":"release-notes/v1.17.0/#http-client-resilience-framework","title":"HTTP Client Resilience Framework","text":"<p>New middleware for robust HTTP communication:</p> <pre><code>use Cognesy\\HttpClient\\HttpClientBuilder;\nuse Cognesy\\HttpClient\\Middleware\\Policies\\RetryPolicy;\nuse Cognesy\\HttpClient\\Middleware\\Policies\\CircuitBreakerPolicy;\n\n$client = HttpClientBuilder::make()\n    -&gt;withRetryPolicy(new RetryPolicy(\n        maxAttempts: 3,\n        baseDelayMs: 250,\n        maxDelayMs: 8000,\n        jitter: 'full',\n    ))\n    -&gt;withCircuitBreakerPolicy(new CircuitBreakerPolicy(\n        failureThreshold: 5,\n        openWindowSeconds: 30,\n    ))\n    -&gt;withIdempotencyMiddleware()\n    -&gt;build();\n</code></pre> <p>Features: - Exponential backoff with configurable jitter strategies - Respects <code>Retry-After</code> headers - Circuit breaker pattern (closed \u2192 open \u2192 half-open \u2192 closed) - Per-host circuit tracking - Idempotency keys for safe retries</p>"},{"location":"release-notes/v1.17.0/#inference-retry","title":"Inference Retry","text":"<p>Smart retry logic for LLM inference requests:</p> <pre><code>use Cognesy\\Polyglot\\Inference\\Inference;\n\n$response = Inference::with($messages)\n    -&gt;withRetryPolicy([\n        'maxAttempts' =&gt; 3,\n        'baseDelayMs' =&gt; 500,\n        'lengthRecovery' =&gt; 'continue', // or 'increase_max_tokens'\n    ])\n    -&gt;get();\n</code></pre> <p>Features: - Automatic retry on rate limits and transient errors - Length limit recovery (retry with \"Continue.\" prompt or increased max_tokens) - Provider-specific error classification with typed exceptions</p>"},{"location":"release-notes/v1.17.0/#driver-capabilities-system","title":"Driver Capabilities System","text":"<p>Explicit capability declarations for all LLM providers:</p> <pre><code>$driver = $inference-&gt;getDriver();\n$capabilities = $driver-&gt;capabilities('gpt-4');\n\nif ($capabilities-&gt;supportsJsonSchema) {\n    // Use native JSON schema mode\n}\nif ($capabilities-&gt;supportsToolCalling) {\n    // Use function calling\n}\n</code></pre> <p>All 14+ drivers now declare support for: - Output modes (JSON Schema, Tools, MdJson, Text) - Streaming - Tool/function calling - Native JSON schema mode</p>"},{"location":"release-notes/v1.17.0/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.17.0/#agent-registry-specifications","title":"Agent Registry &amp; Specifications","text":"<p>Declarative agent definitions with registry-based discovery:</p> <pre><code>use Cognesy\\Addons\\AgentTemplate\\Registry\\AgentRegistry;use Cognesy\\Addons\\AgentTemplate\\Spec\\AgentSpec;\n\n$registry = new AgentRegistry();\n$registry-&gt;register('researcher', AgentSpec::from([\n    'name' =&gt; 'researcher',\n    'description' =&gt; 'Research agent',\n    'capabilities' =&gt; [UseFileTools::class, UseSkills::class],\n]));\n\n$agent = $registry-&gt;make('researcher');\n</code></pre>"},{"location":"release-notes/v1.17.0/#agent-state-serialization","title":"Agent State Serialization","text":"<p>Efficient state persistence for distributed/resumable agents:</p> <pre><code>use Cognesy\\Addons\\Agent\\Serialization\\SlimAgentStateSerializer;\n\n$serializer = new SlimAgentStateSerializer();\n$serialized = $serializer-&gt;serialize($agent-&gt;state());\n\n// Later...\n$state = $serializer-&gt;deserialize($serialized);\n</code></pre>"},{"location":"release-notes/v1.17.0/#deterministic-agent-testing","title":"Deterministic Agent Testing","text":"<p>Test agents without LLM calls:</p> <pre><code>use Cognesy\\Addons\\Agent\\Drivers\\Testing\\DeterministicAgentDriver;\nuse Cognesy\\Addons\\Agent\\Drivers\\Testing\\ScenarioStep;\n\n$driver = new DeterministicAgentDriver([\n    ScenarioStep::toolCall('search', ['query' =&gt; 'test']),\n    ScenarioStep::response('Search completed'),\n]);\n\n$agent = AgentBuilder::base()\n    -&gt;withDriver($driver)\n    -&gt;build();\n</code></pre>"},{"location":"release-notes/v1.17.0/#real-time-agent-event-broadcasting","title":"Real-time Agent Event Broadcasting","text":"<p>Broadcast agent events via Laravel Reverb/Pusher:</p> <pre><code>use Cognesy\\Addons\\Agent\\Broadcasting\\AgentEventEnvelopeAdapter;\n\n$adapter = new AgentEventEnvelopeAdapter($events);\n$adapter-&gt;broadcast($agentId);\n</code></pre>"},{"location":"release-notes/v1.17.0/#message-collection-classes","title":"Message Collection Classes","text":"<p>New collection abstractions for messages:</p> <pre><code>use Cognesy\\Messages\\ContentParts;\nuse Cognesy\\Messages\\MessageList;\n\n// ContentParts collection\n$parts = new ContentParts([$textPart, $imagePart]);\n$filtered = $parts-&gt;filter(fn($p) =&gt; $p-&gt;isText());\n\n// MessageList collection\n$messages = new MessageList([$msg1, $msg2]);\n$reversed = $messages-&gt;reversed();\n</code></pre>"},{"location":"release-notes/v1.17.0/#centralized-input-handling","title":"Centralized Input Handling","text":"<p>Unified factories for content and message creation:</p> <pre><code>use Cognesy\\Messages\\Support\\ContentInput;\nuse Cognesy\\Messages\\Support\\MessageInput;\n\n// Normalize any input to Content\n$content = ContentInput::fromAny($stringOrArrayOrContent);\n\n// Create Message from various inputs\n$message = MessageInput::fromAny($input, MessageRole::User);\n</code></pre>"},{"location":"release-notes/v1.17.0/#provider-error-classification","title":"Provider Error Classification","text":"<p>Typed exceptions for better error handling:</p> <pre><code>use Cognesy\\Polyglot\\Inference\\Exceptions\\ProviderRateLimitException;\nuse Cognesy\\Polyglot\\Inference\\Exceptions\\ProviderQuotaExceededException;\n\ntry {\n    $response = $inference-&gt;get();\n} catch (ProviderRateLimitException $e) {\n    // Retriable - wait and retry\n} catch (ProviderQuotaExceededException $e) {\n    // Non-retriable - notify user\n}\n</code></pre>"},{"location":"release-notes/v1.17.0/#mocksandbox-for-testing","title":"MockSandbox for Testing","text":"<p>Mock command execution in tests:</p> <pre><code>use Cognesy\\Utils\\Sandbox\\MockSandbox;\n\n$sandbox = new MockSandbox();\n$sandbox-&gt;queueResponse('ls', ['stdout' =&gt; 'file1.txt\\nfile2.txt']);\n\n$result = $sandbox-&gt;execute('ls');\n</code></pre>"},{"location":"release-notes/v1.17.0/#improvements","title":"Improvements","text":""},{"location":"release-notes/v1.17.0/#agent-system","title":"Agent System","text":"<ul> <li>Sophisticated continuation logic with priority-based resolution</li> <li>Enhanced error handling with granular <code>ErrorPolicy</code> configuration</li> <li>State processors integrated into capability installation</li> <li>Tools can access agent state via <code>CanAccessAgentState</code> contract</li> <li>Improved <code>ToolCallingDriver</code> with better parallelization</li> </ul>"},{"location":"release-notes/v1.17.0/#messages-package","title":"Messages Package","text":"<ul> <li><code>Content</code>, <code>Messages</code>, <code>Sections</code>, <code>Section</code> now implement <code>Countable</code> and <code>IteratorAggregate</code></li> <li><code>MessageStore</code> uses standard <code>Metadata</code> class instead of custom parameters</li> <li>Cleaner API with <code>partsList()</code> and <code>messageList()</code> methods</li> </ul>"},{"location":"release-notes/v1.17.0/#polyglot-package","title":"Polyglot Package","text":"<ul> <li>Response adapters simplified (tool calls handled separately from content)</li> <li>Better handling of empty content parts</li> <li><code>AnthropicBodyFormat</code> updated for new message structure</li> </ul>"},{"location":"release-notes/v1.17.0/#evals-package","title":"Evals Package","text":"<ul> <li>Driver capability filtering for test cases</li> <li>Dependency injection support for test mocking</li> </ul>"},{"location":"release-notes/v1.17.0/#instructor-package","title":"Instructor Package","text":"<ul> <li>Simplified <code>MessageStore</code> operations using new merge/cleanup methods</li> <li>Enhanced array return handling respects <code>defaultToStdClass()</code> config</li> </ul>"},{"location":"release-notes/v1.17.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"release-notes/v1.17.0/#agent-namespace-reorganization","title":"Agent Namespace Reorganization","text":"<p>Core classes moved from <code>Agent/</code> to <code>Agent/Core/</code>: - <code>Agent/Data/AgentState</code> \u2192 <code>Agent/Core/Data/AgentState</code> - <code>Agent/Collections/</code> \u2192 <code>Agent/Core/Collections/</code> - <code>Agent/Contracts/</code> \u2192 <code>Agent/Core/Contracts/</code></p>"},{"location":"release-notes/v1.17.0/#agent-construction","title":"Agent Construction","text":"<p>Old <code>AgentFactory</code> removed. Use <code>AgentBuilder</code>: <pre><code>// Before\n$agent = AgentFactory::create($config);\n\n// After\n$agent = AgentBuilder::base()\n    -&gt;withCapability(...)\n    -&gt;build();\n</code></pre></p>"},{"location":"release-notes/v1.17.0/#messages-api-deprecations","title":"Messages API Deprecations","text":"<ul> <li><code>Content::parts()</code> \u2192 use <code>Content::partsList()</code></li> <li><code>Messages::head()</code> \u2192 use <code>Messages::headList()</code></li> <li><code>Messages::tail()</code> \u2192 use <code>Messages::tailList()</code></li> <li><code>Messages::all()</code> \u2192 use <code>Messages::messageList()</code></li> </ul>"},{"location":"release-notes/v1.17.0/#messagestore-parameters","title":"MessageStore Parameters","text":"<p><code>MessageStoreParameters</code> class removed. Use <code>Metadata</code> instead: <pre><code>// Before\n$store = new MessageStore($sections, new MessageStoreParameters($data));\n\n// After\n$store = new MessageStore($sections, new Metadata($data));\n</code></pre></p>"},{"location":"release-notes/v1.17.0/#driver-capabilities-interface","title":"Driver Capabilities Interface","text":"<p>Drivers must implement new <code>capabilities()</code> method: <pre><code>public function capabilities(?string $model = null): DriverCapabilities;\n</code></pre></p>"},{"location":"release-notes/v1.17.0/#provider-exceptions","title":"Provider Exceptions","text":"<p>HTTP errors now throw typed <code>ProviderException</code> subclasses instead of generic exceptions.</p>"},{"location":"release-notes/v1.17.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed variadic parameter type handling in <code>StructureFactory</code> for mixed/null types</li> <li>Fixed PHP 8.5 compatibility issues with reflection API</li> <li>Fixed Gemini response adapter content concatenation</li> <li>Fixed contentParts() returning collection instead of array in templates</li> <li>Updated Symfony Console API calls (<code>add()</code> \u2192 <code>addCommand()</code>)</li> </ul>"},{"location":"release-notes/v1.18.0/","title":"Release Notes - v1.18.0","text":""},{"location":"release-notes/v1.18.0/#major-changes","title":"Major Changes","text":""},{"location":"release-notes/v1.18.0/#capability-based-agent-architecture","title":"Capability-Based Agent Architecture","text":"<p>Overhaul of the Agent system with a modular, capability-based design:</p> <pre><code>use Cognesy\\Addons\\AgentBuilder\\AgentBuilder;\nuse Cognesy\\Addons\\AgentBuilder\\Capabilities\\Bash\\UseBash;\nuse Cognesy\\Addons\\AgentBuilder\\Capabilities\\File\\UseFileTools;\nuse Cognesy\\Addons\\AgentBuilder\\Capabilities\\Tasks\\UseTaskPlanning;\n\n$agent = AgentBuilder::base()\n    -&gt;withCapability(new UseBash())\n    -&gt;withCapability(new UseFileTools())\n    -&gt;withCapability(new UseTaskPlanning())\n    -&gt;build();\n</code></pre> <p>New Agent Capabilities: - <code>UseBash</code> - Execute shell commands with configurable policies - <code>UseFileTools</code> - Read, write, edit, search, and list files - <code>UseSkills</code> - Manage reusable skill libraries - <code>UseTaskPlanning</code> - Multi-step task management with TodoWriteTool - <code>UseMetadataTools</code> - Agent scratchpad for inter-tool data sharing - <code>UseStructuredOutputs</code> - LLM-powered structured data extraction - <code>UseSubagents</code> - Recursive agent spawning with AgentRegistry - <code>UseSelfCritique</code> - Self-evaluation of agent work</p>"},{"location":"release-notes/v1.18.0/#http-client-resilience-framework","title":"HTTP Client Resilience Framework","text":"<p>New middleware for robust HTTP communication:</p> <pre><code>use Cognesy\\HttpClient\\HttpClientBuilder;\nuse Cognesy\\HttpClient\\Middleware\\Policies\\RetryPolicy;\nuse Cognesy\\HttpClient\\Middleware\\Policies\\CircuitBreakerPolicy;\n\n$client = HttpClientBuilder::make()\n    -&gt;withRetryPolicy(new RetryPolicy(\n        maxAttempts: 3,\n        baseDelayMs: 250,\n        maxDelayMs: 8000,\n        jitter: 'full',\n    ))\n    -&gt;withCircuitBreakerPolicy(new CircuitBreakerPolicy(\n        failureThreshold: 5,\n        openWindowSeconds: 30,\n    ))\n    -&gt;withIdempotencyMiddleware()\n    -&gt;build();\n</code></pre> <p>Features: - Exponential backoff with configurable jitter strategies - Respects <code>Retry-After</code> headers - Circuit breaker pattern (closed \u2192 open \u2192 half-open \u2192 closed) - Per-host circuit tracking - Idempotency keys for safe retries</p>"},{"location":"release-notes/v1.18.0/#inference-retry","title":"Inference Retry","text":"<p>Smart retry logic for LLM inference requests:</p> <pre><code>use Cognesy\\Polyglot\\Inference\\Inference;\n\n$response = Inference::with($messages)\n    -&gt;withRetryPolicy([\n        'maxAttempts' =&gt; 3,\n        'baseDelayMs' =&gt; 500,\n        'lengthRecovery' =&gt; 'continue', // or 'increase_max_tokens'\n    ])\n    -&gt;get();\n</code></pre> <p>Features: - Automatic retry on rate limits and transient errors - Length limit recovery (retry with \"Continue.\" prompt or increased max_tokens) - Provider-specific error classification with typed exceptions</p>"},{"location":"release-notes/v1.18.0/#driver-capabilities-system","title":"Driver Capabilities System","text":"<p>Explicit capability declarations for all LLM providers:</p> <pre><code>$driver = $inference-&gt;getDriver();\n$capabilities = $driver-&gt;capabilities('gpt-4');\n\nif ($capabilities-&gt;supportsJsonSchema) {\n    // Use native JSON schema mode\n}\nif ($capabilities-&gt;supportsToolCalling) {\n    // Use function calling\n}\n</code></pre> <p>All 14+ drivers now declare support for: - Output modes (JSON Schema, Tools, MdJson, Text) - Streaming - Tool/function calling - Native JSON schema mode</p>"},{"location":"release-notes/v1.18.0/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.18.0/#agent-registry-specifications","title":"Agent Registry &amp; Specifications","text":"<p>Declarative agent definitions with registry-based discovery:</p> <pre><code>use Cognesy\\Addons\\AgentTemplate\\Registry\\AgentRegistry;use Cognesy\\Addons\\AgentTemplate\\Spec\\AgentSpec;\n\n$registry = new AgentRegistry();\n$registry-&gt;register('researcher', AgentSpec::from([\n    'name' =&gt; 'researcher',\n    'description' =&gt; 'Research agent',\n    'capabilities' =&gt; [UseFileTools::class, UseSkills::class],\n]));\n\n$agent = $registry-&gt;make('researcher');\n</code></pre>"},{"location":"release-notes/v1.18.0/#agent-state-serialization","title":"Agent State Serialization","text":"<p>Efficient state persistence for distributed/resumable agents:</p> <pre><code>use Cognesy\\Addons\\Agent\\Serialization\\SlimAgentStateSerializer;\n\n$serializer = new SlimAgentStateSerializer();\n$serialized = $serializer-&gt;serialize($agent-&gt;state());\n\n// Later...\n$state = $serializer-&gt;deserialize($serialized);\n</code></pre>"},{"location":"release-notes/v1.18.0/#deterministic-agent-testing","title":"Deterministic Agent Testing","text":"<p>Test agents without LLM calls:</p> <pre><code>use Cognesy\\Addons\\Agent\\Drivers\\Testing\\DeterministicAgentDriver;\nuse Cognesy\\Addons\\Agent\\Drivers\\Testing\\ScenarioStep;\n\n$driver = new DeterministicAgentDriver([\n    ScenarioStep::toolCall('search', ['query' =&gt; 'test']),\n    ScenarioStep::response('Search completed'),\n]);\n\n$agent = AgentBuilder::base()\n    -&gt;withDriver($driver)\n    -&gt;build();\n</code></pre>"},{"location":"release-notes/v1.18.0/#real-time-agent-event-broadcasting","title":"Real-time Agent Event Broadcasting","text":"<p>Broadcast agent events via Laravel Reverb/Pusher:</p> <pre><code>use Cognesy\\Addons\\Agent\\Broadcasting\\AgentEventEnvelopeAdapter;\n\n$adapter = new AgentEventEnvelopeAdapter($events);\n$adapter-&gt;broadcast($agentId);\n</code></pre>"},{"location":"release-notes/v1.18.0/#message-collection-classes","title":"Message Collection Classes","text":"<p>New collection abstractions for messages:</p> <pre><code>use Cognesy\\Messages\\ContentParts;\nuse Cognesy\\Messages\\MessageList;\n\n// ContentParts collection\n$parts = new ContentParts([$textPart, $imagePart]);\n$filtered = $parts-&gt;filter(fn($p) =&gt; $p-&gt;isText());\n\n// MessageList collection\n$messages = new MessageList([$msg1, $msg2]);\n$reversed = $messages-&gt;reversed();\n</code></pre>"},{"location":"release-notes/v1.18.0/#centralized-input-handling","title":"Centralized Input Handling","text":"<p>Unified factories for content and message creation:</p> <pre><code>use Cognesy\\Messages\\Support\\ContentInput;\nuse Cognesy\\Messages\\Support\\MessageInput;\n\n// Normalize any input to Content\n$content = ContentInput::fromAny($stringOrArrayOrContent);\n\n// Create Message from various inputs\n$message = MessageInput::fromAny($input, MessageRole::User);\n</code></pre>"},{"location":"release-notes/v1.18.0/#provider-error-classification","title":"Provider Error Classification","text":"<p>Typed exceptions for better error handling:</p> <pre><code>use Cognesy\\Polyglot\\Inference\\Exceptions\\ProviderRateLimitException;\nuse Cognesy\\Polyglot\\Inference\\Exceptions\\ProviderQuotaExceededException;\n\ntry {\n    $response = $inference-&gt;get();\n} catch (ProviderRateLimitException $e) {\n    // Retriable - wait and retry\n} catch (ProviderQuotaExceededException $e) {\n    // Non-retriable - notify user\n}\n</code></pre>"},{"location":"release-notes/v1.18.0/#mocksandbox-for-testing","title":"MockSandbox for Testing","text":"<p>Mock command execution in tests:</p> <pre><code>use Cognesy\\Utils\\Sandbox\\MockSandbox;\n\n$sandbox = new MockSandbox();\n$sandbox-&gt;queueResponse('ls', ['stdout' =&gt; 'file1.txt\\nfile2.txt']);\n\n$result = $sandbox-&gt;execute('ls');\n</code></pre>"},{"location":"release-notes/v1.18.0/#improvements","title":"Improvements","text":""},{"location":"release-notes/v1.18.0/#agent-system","title":"Agent System","text":"<ul> <li>Sophisticated continuation logic with priority-based resolution</li> <li>Enhanced error handling with granular <code>ErrorPolicy</code> configuration</li> <li>State processors integrated into capability installation</li> <li>Tools can access agent state via <code>CanAccessAgentState</code> contract</li> <li>Improved <code>ToolCallingDriver</code> with better parallelization</li> </ul>"},{"location":"release-notes/v1.18.0/#messages-package","title":"Messages Package","text":"<ul> <li><code>Content</code>, <code>Messages</code>, <code>Sections</code>, <code>Section</code> now implement <code>Countable</code> and <code>IteratorAggregate</code></li> <li><code>MessageStore</code> uses standard <code>Metadata</code> class instead of custom parameters</li> <li>Cleaner API with <code>partsList()</code> and <code>messageList()</code> methods</li> </ul>"},{"location":"release-notes/v1.18.0/#polyglot-package","title":"Polyglot Package","text":"<ul> <li>Response adapters simplified (tool calls handled separately from content)</li> <li>Better handling of empty content parts</li> <li><code>AnthropicBodyFormat</code> updated for new message structure</li> </ul>"},{"location":"release-notes/v1.18.0/#evals-package","title":"Evals Package","text":"<ul> <li>Driver capability filtering for test cases</li> <li>Dependency injection support for test mocking</li> </ul>"},{"location":"release-notes/v1.18.0/#instructor-package","title":"Instructor Package","text":"<ul> <li>Simplified <code>MessageStore</code> operations using new merge/cleanup methods</li> <li>Enhanced array return handling respects <code>defaultToStdClass()</code> config</li> </ul>"},{"location":"release-notes/v1.18.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"release-notes/v1.18.0/#agent-namespace-reorganization","title":"Agent Namespace Reorganization","text":"<p>Core classes moved from <code>Agent/</code> to <code>Agent/Core/</code>: - <code>Agent/Data/AgentState</code> \u2192 <code>Agent/Core/Data/AgentState</code> - <code>Agent/Collections/</code> \u2192 <code>Agent/Core/Collections/</code> - <code>Agent/Contracts/</code> \u2192 <code>Agent/Core/Contracts/</code></p>"},{"location":"release-notes/v1.18.0/#agent-construction","title":"Agent Construction","text":"<p>Old <code>AgentFactory</code> removed. Use <code>AgentBuilder</code>: <pre><code>// Before\n$agent = AgentFactory::create($config);\n\n// After\n$agent = AgentBuilder::base()\n    -&gt;withCapability(...)\n    -&gt;build();\n</code></pre></p>"},{"location":"release-notes/v1.18.0/#messages-api-deprecations","title":"Messages API Deprecations","text":"<ul> <li><code>Content::parts()</code> \u2192 use <code>Content::partsList()</code></li> <li><code>Messages::head()</code> \u2192 use <code>Messages::headList()</code></li> <li><code>Messages::tail()</code> \u2192 use <code>Messages::tailList()</code></li> <li><code>Messages::all()</code> \u2192 use <code>Messages::messageList()</code></li> </ul>"},{"location":"release-notes/v1.18.0/#messagestore-parameters","title":"MessageStore Parameters","text":"<p><code>MessageStoreParameters</code> class removed. Use <code>Metadata</code> instead: <pre><code>// Before\n$store = new MessageStore($sections, new MessageStoreParameters($data));\n\n// After\n$store = new MessageStore($sections, new Metadata($data));\n</code></pre></p>"},{"location":"release-notes/v1.18.0/#driver-capabilities-interface","title":"Driver Capabilities Interface","text":"<p>Drivers must implement new <code>capabilities()</code> method: <pre><code>public function capabilities(?string $model = null): DriverCapabilities;\n</code></pre></p>"},{"location":"release-notes/v1.18.0/#provider-exceptions","title":"Provider Exceptions","text":"<p>HTTP errors now throw typed <code>ProviderException</code> subclasses instead of generic exceptions.</p>"},{"location":"release-notes/v1.18.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed variadic parameter type handling in <code>StructureFactory</code> for mixed/null types</li> <li>Fixed PHP 8.5 compatibility issues with reflection API</li> <li>Fixed Gemini response adapter content concatenation</li> <li>Fixed contentParts() returning collection instead of array in templates</li> <li>Updated Symfony Console API calls (<code>add()</code> \u2192 <code>addCommand()</code>)</li> </ul>"},{"location":"release-notes/v1.18.1/","title":"Release Notes - v1.18.1","text":""},{"location":"release-notes/v1.18.1/#bug-fixes","title":"Bug Fixes","text":""},{"location":"release-notes/v1.18.1/#agent-event-status-fix","title":"Agent Event Status Fix","text":"<ul> <li><code>AgentFinished</code> event now correctly reports <code>status: completed</code> (or <code>failed</code>) instead of <code>in_progress</code> when agent terminates</li> <li>The status is determined based on the <code>StopReason</code> from continuation criteria evaluation</li> </ul>"},{"location":"release-notes/v1.18.1/#duplicate-event-fix","title":"Duplicate Event Fix","text":"<ul> <li>Fixed duplicate <code>ContinuationEvaluated</code> events being emitted during agent execution</li> <li>Removed redundant <code>hasNextStep()</code> check from <code>StepByStep::nextStep()</code></li> </ul>"},{"location":"release-notes/v1.18.1/#agentbuilder-maxretries-fix","title":"AgentBuilder maxRetries Fix","text":"<ul> <li><code>AgentBuilder::withMaxRetries()</code> now actually works - the value is passed to <code>ToolCallingDriver</code> via inference retry policy</li> <li>Previously the property was set but never used</li> </ul>"},{"location":"release-notes/v1.18.1/#abstractagent-event-handler-fix","title":"AbstractAgent Event Handler Fix","text":"<ul> <li>Fixed <code>AbstractAgent::build()</code> returning stale agent instance without the configured event handler</li> <li>Event handlers and listeners set via <code>withEventHandler()</code>, <code>wiretap()</code>, <code>onEvent()</code> now correctly propagate to the built agent</li> </ul>"},{"location":"release-notes/v1.18.1/#static-analysis-improvements","title":"Static Analysis Improvements","text":""},{"location":"release-notes/v1.18.1/#phpstan-level-8-compliance","title":"PHPStan Level 8 Compliance","text":"<ul> <li>Added callable signatures to <code>ContentParts</code> and <code>MessageList</code> methods (<code>map</code>, <code>reduce</code>, <code>filter</code>)</li> <li>Added proper Closure signatures to <code>MockTool</code> and <code>IdempotencyMiddleware</code></li> <li>Added <code>@param class-string</code> annotations to <code>SchemaDefinition</code></li> <li>Fixed array type annotations in <code>Task</code>, <code>TaskList</code>, <code>TodoReminderProcessor</code></li> <li>Fixed <code>ReActDriver::withCachedContext()</code> parameter passing</li> <li>Fixed <code>DeterministicAgentDriver</code> array key type issues</li> <li>Removed redundant int casts in retry policy classes</li> <li>Added <code>Agent::isStateChanged()</code> override to prevent spurious iterator yields</li> </ul>"},{"location":"release-notes/v1.18.1/#psalm-compliance","title":"Psalm Compliance","text":"<ul> <li>Added missing <code>#[\\Override]</code> attributes across codebase</li> <li>Added template parameters to <code>IteratorAggregate</code> implementations</li> <li>Fixed namespace issues in <code>ResearchSubagentTool</code></li> <li>Added <code>ExecutionPolicy::default()</code> static method</li> </ul>"},{"location":"release-notes/v1.18.2/","title":"Release Notes - v1.18.2","text":""},{"location":"release-notes/v1.18.2/#highlights","title":"Highlights","text":"<ul> <li>Continuation-safe agent state persistence with new serialization helpers.</li> <li>MessageStore serialization is now round-trip safe and section-aware.</li> <li>Metadata preservation fixes for slim agent state snapshots.</li> </ul>"},{"location":"release-notes/v1.18.2/#behavior-changes","title":"Behavior Changes","text":""},{"location":"release-notes/v1.18.2/#messagestore-toarray-structure","title":"MessageStore toArray Structure","text":"<ul> <li><code>MessageStore::toArray()</code> now returns a structured payload with <code>sections</code> and <code>parameters</code> instead of a flat messages list.</li> <li>Use <code>MessageStore::toFlatArray()</code> to obtain the previous flat messages array format.</li> </ul>"},{"location":"release-notes/v1.18.2/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.18.2/#agent-continuation-apis","title":"Agent Continuation APIs","text":"<ul> <li>Added <code>AgentState::withUserMessage()</code> for multi-turn continuation.</li> <li>Added <code>AgentState::forContinuation()</code> to reset execution state (steps, cache, usage, timing) while preserving conversation history.</li> </ul>"},{"location":"release-notes/v1.18.2/#continuation-serializer","title":"Continuation Serializer","text":"<ul> <li>Added <code>ContinuationAgentStateSerializer</code> with <code>ContinuationSerializationConfig</code> to persist multi-turn state with retention limits.</li> <li>Supports truncation, tool-result omission, and tool-arg redaction while preserving <code>_metadata</code>.</li> </ul>"},{"location":"release-notes/v1.18.2/#bug-fixes","title":"Bug Fixes","text":""},{"location":"release-notes/v1.18.2/#metadata-preservation","title":"Metadata Preservation","text":"<ul> <li><code>SlimAgentStateSerializer</code> now emits <code>_metadata</code> (with <code>metadata</code> alias support) so tool calls/results survive round-trips.</li> <li><code>MessageInput::fromArray()</code> now accepts <code>metadata</code> as a fallback alias for <code>_metadata</code>.</li> </ul>"},{"location":"release-notes/v1.18.2/#serialization-round-trip","title":"Serialization Round-Trip","text":"<ul> <li>MessageStore serialization now round-trips sections, messages, and parameters without data loss.</li> </ul>"},{"location":"release-notes/v1.18.2/#tests","title":"Tests","text":"<ul> <li>Added round-trip tests for MessageStore and continuation serialization.</li> <li>Added AgentState continuation tests covering reset semantics and message appending.</li> </ul>"},{"location":"release-notes/v1.18.3/","title":"Release Notes - v1.18.3","text":""},{"location":"release-notes/v1.18.3/#highlights","title":"Highlights","text":"<ul> <li>Tool discovery registry with metadata/full-spec layering for progressive tool discovery.</li> <li>New <code>tools</code> tool for list/help/search of available tools without loading full specs.</li> <li>Capability to wire tool registries into agents for dynamic tool catalogs.</li> </ul>"},{"location":"release-notes/v1.18.3/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.18.3/#tool-registry","title":"Tool Registry","text":"<ul> <li>Added <code>ToolRegistryInterface</code> and <code>ToolRegistry</code> for registering tool instances or factories.</li> <li>Added <code>ToolPolicy</code> filtering for allow/deny tool lists.</li> <li>Added metadata and full-spec resolution helpers to support progressive discovery.</li> </ul>"},{"location":"release-notes/v1.18.3/#tools-tool","title":"Tools Tool","text":"<ul> <li>Added <code>ToolsTool</code> for <code>list</code>, <code>help</code>, and <code>search</code> actions (plus CLI-like <code>command</code> parsing).</li> <li>Supports locale-aware metadata/full spec rendering via registry.</li> </ul>"},{"location":"release-notes/v1.18.3/#agent-capability","title":"Agent Capability","text":"<ul> <li>Added <code>UseToolRegistry</code> capability for agent blueprints to expose registries as tools.</li> </ul>"},{"location":"release-notes/v1.18.3/#tests","title":"Tests","text":"<ul> <li>Added registry and tools discovery tests for metadata/full spec, search, and help flows.</li> </ul>"},{"location":"release-notes/v1.18.4/","title":"Release Notes - v1.18.4","text":""},{"location":"release-notes/v1.18.4/#highlights","title":"Highlights","text":"<ul> <li>YAML agent definitions with strict parsing, validation, and version checks.</li> <li>Blueprint and capability registries for mapping definitions to implementations.</li> <li>Definition loading and auto-discovery from files, directories, and <code>.claude/agents</code>.</li> </ul>"},{"location":"release-notes/v1.18.4/#new-features","title":"New Features","text":""},{"location":"release-notes/v1.18.4/#agent-definitions","title":"Agent Definitions","text":"<ul> <li>Added <code>AgentDefinition</code> DTO with LLM, execution, tool allow/deny, capabilities, and metadata.</li> <li>Added <code>AgentDefinitionParser</code> with strict key validation and version enforcement.</li> <li>Added <code>AgentDefinitionLoader</code> for loading definitions from files/directories (recursive supported).</li> <li>Added <code>AgentDefinitionRegistry</code> with auto-discovery and error collection.</li> </ul>"},{"location":"release-notes/v1.18.4/#blueprint-and-capability-registries","title":"Blueprint and Capability Registries","text":"<ul> <li>Added <code>AgentBlueprintRegistry</code> for alias-to-class resolution.</li> <li>Added <code>AgentDefinitionFactory</code> to build agents from definitions via blueprints.</li> <li>Added <code>AgentCapabilityRegistry</code> for instance/factory resolution.</li> </ul>"},{"location":"release-notes/v1.18.4/#tests","title":"Tests","text":"<ul> <li>Added unit tests for definition parsing/loading/registry and blueprint/capability registries.</li> </ul>"},{"location":"release-notes/v1.19.0/","title":"Release Notes - v1.19.0","text":""},{"location":"release-notes/v1.19.0/#highlights","title":"Highlights","text":"<ul> <li>Cleaned inference attempt lifecycle and event ownership in Polyglot.</li> <li>Fixed serialization and caching edge cases across inference requests/partials.</li> <li>Structured output handling is more robust with safer validation and immutable models.</li> </ul>"},{"location":"release-notes/v1.19.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li><code>InferenceExecution::withNewResponse()</code> renamed to <code>withSuccessfulAttempt()</code>.</li> <li><code>InferenceExecution::withFailedResponse()</code> renamed to <code>withFailedAttempt()</code>.</li> <li><code>InferenceExecution::withFailedFinalizedResponse()</code> removed.</li> <li><code>InferenceAttempt::withFailedResponse()</code> removed; create attempts via <code>startAttempt()</code> / <code>InferenceAttempt::started()</code>.</li> <li><code>Usage::accumulate()</code> removed; use <code>withAccumulated()</code> for immutable usage totals.</li> <li><code>ValidationResult::invalid()</code> now accepts <code>ValidationError|ValidationError[]</code> only.</li> <li><code>ResponseModel::setPropertyValues()</code> removed; use <code>withPropertyValues()</code>.</li> </ul>"},{"location":"release-notes/v1.19.0/#fixes-and-improvements","title":"Fixes and Improvements","text":""},{"location":"release-notes/v1.19.0/#polyglot","title":"Polyglot","text":"<ul> <li><code>InferenceRequest::toArray()</code> now serializes <code>messages</code> and <code>response_format</code> as arrays.</li> <li><code>InferenceRequest::hasMessages()</code> and <code>withCacheApplied()</code> use explicit emptiness checks.</li> <li><code>PartialInferenceResponse::toArray()</code> now serializes <code>response_data</code>, with clearer tool accumulation.</li> <li><code>InferenceStream</code> no longer double-dispatches completion events.</li> <li><code>HandlesRequestBuilder::withToolChoice()</code> accepts <code>string|array</code>.</li> </ul>"},{"location":"release-notes/v1.19.0/#instructor","title":"Instructor","text":"<ul> <li><code>StructuredOutputStream</code> reuses its generator and emits <code>StructuredOutputStarted</code> once.</li> <li>Response validation captures thrown exceptions as <code>ValidationResult::invalid()</code>.</li> <li>Tool-call JSON encoding failures return <code>Result</code> errors instead of throwing.</li> <li><code>ResponseModel::toArray()</code> guards non-object instances; fluent methods return <code>static</code>.</li> </ul>"},{"location":"release-notes/v1.19.0/#addons","title":"Addons","text":"<ul> <li>Continuation stop reasons are explicit on evaluations instead of inferred from class names.</li> <li>AgentState adds <code>recordStep()</code> and <code>failWith()</code> helpers, plus explicit execution hook interfaces for timing.</li> </ul>"},{"location":"release-notes/v1.19.0/#tests","title":"Tests","text":"<ul> <li>Added coverage for attempt IDs, streaming completion events, serialization, validation failures, and ResponseModel immutability.</li> </ul>"},{"location":"release-notes/v1.2.0/","title":"v1.2.0","text":"<p>\ud83d\udd27 Centralized Monorepo Management: - packages.json - New centralized package configuration - Scripts modernized: - load-packages.sh - Loads centralized config - generate-split-matrix.sh - Generates GitHub Actions matrix - update-split-yml.sh - Updates split.yml automatically - sync-ver.sh &amp; publish-ver.sh - Now use centralized config</p> <p>\u2699\ufe0f GitHub Actions: - split.yml - Now triggers on main branch pushes + tags (was tags only) - Matrix generation - Auto-generated from packages.json</p> <p>\ud83d\udce6 New Package: - instructor-doctor - Added to monorepo</p> <p>\ud83d\udcda Documentation: - Doc generation - Split from hub to separate system - Codeblocks - Many new HTTP examples added - CONTENTS.md &amp; CONTRIBUTOR_GUIDE.md - Updated for new workflows</p> <p>Key Impact: Eliminates manual package list maintenance across scripts and GitHub Actions.</p>"},{"location":"release-notes/v1.20.0/","title":"v1.20.0","text":""},{"location":"release-notes/v1.20.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"release-notes/v1.20.0/#renames","title":"Renames","text":"<ul> <li><code>ReverbAgentEventAdapter</code> \u2192 <code>AgentEventEnvelopeAdapter</code></li> <li><code>ToolRegistryContract</code> \u2192 <code>ToolRegistryInterface</code></li> <li><code>DeterministicDriver</code> \u2192 <code>DeterministicAgentDriver</code></li> </ul>"},{"location":"release-notes/v1.20.0/#agent","title":"Agent","text":"<ul> <li><code>AgentState</code> now implements <code>CanMarkExecutionStarted</code>, <code>CanMarkStepStarted</code>, <code>CanTrackExecutionTime</code></li> <li>Added <code>AgentState::recordStep()</code>, <code>AgentState::failWith()</code>, <code>AgentState::withAddedExecutionTime()</code></li> <li><code>Agent::applyStep()</code> and <code>Agent::handleError()</code> delegate to <code>AgentState</code> methods</li> </ul>"},{"location":"release-notes/v1.20.0/#stepbystep-continuation","title":"StepByStep / Continuation","text":"<ul> <li>New <code>CanProvideStopReason</code> interface for criteria to provide explicit stop reasons</li> <li>All continuation criteria implement <code>CanProvideStopReason</code>:</li> <li><code>StepsLimit</code> \u2192 <code>StopReason::StepsLimitReached</code></li> <li><code>TokenUsageLimit</code> \u2192 <code>StopReason::TokenLimitReached</code></li> <li><code>ExecutionTimeLimit</code>, <code>CumulativeExecutionTimeLimit</code> \u2192 <code>StopReason::TimeLimitReached</code></li> <li><code>ErrorPolicyCriterion</code> \u2192 <code>StopReason::ErrorForbade</code></li> <li><code>FinishReasonCheck</code> \u2192 <code>StopReason::FinishReasonReceived</code></li> <li><code>ErrorPresenceCheck</code>, <code>RetryLimit</code> \u2192 <code>StopReason::GuardForbade</code></li> <li><code>ContinuationEvaluation</code> includes <code>stopReason</code> field</li> <li>Removed <code>ContinuationCriteria::inferStopReason()</code> (stop reasons now come from criteria directly)</li> <li>New state contracts: <code>CanMarkExecutionStarted</code>, <code>CanMarkStepStarted</code>, <code>CanTrackExecutionTime</code></li> </ul>"},{"location":"release-notes/v1.20.0/#command-builders-agent-ctrl","title":"Command Builders (agent-ctrl)","text":"<p><code>ClaudeCommandBuilder</code>, <code>CodexCommandBuilder</code>, <code>OpenCodeCommandBuilder</code>: - <code>stdbuf -o0</code> prefix now conditional (checks availability) - Skipped on Windows - Skipped when <code>stdbuf</code> not found on PATH (fixes macOS) - Override via <code>COGNESY_STDBUF</code> env var: <code>0</code> = disable, <code>1</code> = force</p>"},{"location":"release-notes/v1.20.0/#polyglot-inference","title":"Polyglot / Inference","text":"<ul> <li><code>InferenceExecution::usage()</code>: fixed double-counting of current attempt</li> <li><code>InferenceResponse</code>: added <code>&lt;think&gt;...&lt;/think&gt;</code> tag parsing for reasoning content fallback</li> <li>New <code>ReasoningContentSplit</code> data class</li> <li><code>AnthropicBodyFormat</code>: cache marking applies only to last message in sequence (was all messages)</li> <li><code>DeepseekResponseAdapter</code>: supports <code>reasoning</code>, <code>analysis</code> fields as alternatives to <code>reasoning_content</code></li> <li><code>Inference::with()</code>: parameters now nullable (pass <code>null</code> to skip, was required empty values)</li> <li><code>PendingInference::asJson()</code>, <code>asJsonData()</code>: use proper JSON extraction with output mode</li> </ul>"},{"location":"release-notes/v1.20.0/#http-client","title":"HTTP Client","text":"<ul> <li><code>CurlHandle::close()</code>: removed <code>curl_close()</code> call (no-op since PHP 8.0, deprecated in PHP 8.5)</li> </ul>"},{"location":"release-notes/v1.20.0/#instructor","title":"Instructor","text":"<ul> <li><code>StructuredOutputStream</code>: fixed execution reference update during streaming to capture accumulated usage</li> </ul>"},{"location":"release-notes/v1.21.0/","title":"v1.21.0","text":""},{"location":"release-notes/v1.21.0/#highlights","title":"Highlights","text":"<ul> <li>StepResult pattern across Agent, Chat, ToolUse, and Collaboration with immutable steps and explicit continuation outcomes.</li> <li>Continuation evaluation is unified around <code>ContinuationEvaluation</code> and <code>ContinuationOutcome</code>, with clearer stop reasons.</li> <li>Retry policy handling is explicit for Inference and Embeddings (no more <code>retryPolicy</code> inside options).</li> <li>Hub status persistence now tolerates malformed UTF-8 output instead of failing to write status data.</li> </ul>"},{"location":"release-notes/v1.21.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Continuation interfaces <code>CanDecideToContinue</code>, <code>CanExplainContinuation</code>, and <code>CanProvideStopReason</code> were removed. Custom criteria must implement <code>CanEvaluateContinuation</code> and return <code>ContinuationEvaluation</code>.</li> <li>Error handling classes moved from <code>Cognesy\\Addons\\StepByStep\\Continuation</code> to <code>Cognesy\\Addons\\StepByStep\\ErrorHandling</code>.</li> <li><code>retryPolicy</code> is no longer accepted in <code>options</code> or <code>LLMConfig</code> options. Use explicit retry policy objects.</li> <li>Continuation outcomes are no longer stored on step objects. Use <code>StepResult</code> or state accessors instead.</li> </ul>"},{"location":"release-notes/v1.21.0/#agent-chat-tooluse-collaboration","title":"Agent, Chat, ToolUse, Collaboration","text":"<ul> <li>States now store <code>StepResult</code> collections and serialize them.</li> <li><code>canContinue()</code> reads from the last step result; mismatches between steps and step results now raise a logic error.</li> <li>Message compilation supports <code>summary</code> and <code>buffer</code> sections for inference context.</li> <li>Token usage is accumulated before continuation evaluation so usage limits are accurate.</li> </ul>"},{"location":"release-notes/v1.21.0/#stepbystep-continuation","title":"StepByStep / Continuation","text":"<ul> <li><code>ContinuationCriteria</code> composes <code>CanEvaluateContinuation</code> criteria and exposes <code>evaluateAll()</code> with aggregated outcomes.</li> <li>Criteria classes return richer <code>ContinuationEvaluation</code> objects with explicit decisions and stop reasons.</li> <li>New outcome helpers provide derived decision, resolver, and stop reason from evaluations.</li> </ul>"},{"location":"release-notes/v1.21.0/#polyglot-inference-and-embeddings","title":"Polyglot / Inference and Embeddings","text":"<ul> <li><code>InferenceRequest</code> and <code>EmbeddingsRequest</code> now carry retry policies explicitly.</li> <li><code>InferenceRequestBuilder</code> and request builder traits expose <code>withRetryPolicy()</code>.</li> <li><code>PendingInference</code> and <code>PendingEmbeddings</code> pull retry policies from requests rather than <code>options</code>.</li> </ul>"},{"location":"release-notes/v1.21.0/#hub","title":"Hub","text":"<ul> <li>Status JSON encoding now substitutes invalid UTF-8 bytes to avoid failures when saving example output.</li> </ul>"},{"location":"release-notes/v1.21.0/#migration-from-v1200","title":"Migration from v1.20.0","text":"<ul> <li>Update custom continuation criteria to implement <code>CanEvaluateContinuation</code> and return <code>ContinuationEvaluation</code> (replace <code>CanDecideToContinue</code>, <code>CanExplainContinuation</code>, <code>CanProvideStopReason</code>).</li> <li>Update imports to the new error handling namespace: <code>Cognesy\\Addons\\StepByStep\\ErrorHandling\\*</code>.</li> <li>Remove <code>retryPolicy</code> from LLM or request options. Use <code>withRetryPolicy()</code> on inference/embeddings builders or requests.</li> <li>Replace any step-level continuation outcome access with <code>state-&gt;continuationOutcome()</code> or <code>state-&gt;lastStepResult()</code>.</li> </ul>"},{"location":"release-notes/v1.22.0/","title":"v1.22.0","text":""},{"location":"release-notes/v1.22.0/#highlights","title":"Highlights","text":"<ul> <li>Agent builder and capability namespaces are reorganized into <code>Cognesy\\Addons\\AgentBuilder\\*</code>, with agent templates in <code>Cognesy\\Addons\\AgentTemplate\\*</code>.</li> <li>Structured extraction is reworked with <code>ExtractionInput</code>, <code>ResponseContent</code>, and streaming-friendly <code>ExtractingBuffer</code> + <code>PartialJsonExtractor</code>.</li> <li>Hub example discovery now supports configurable sources and grouping via YAML config files.</li> </ul>"},{"location":"release-notes/v1.22.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Agent builder, capabilities, and tool registry classes moved from <code>Cognesy\\Addons\\Agent\\*</code> to <code>Cognesy\\Addons\\AgentBuilder\\*</code>.</li> <li>Agent template definitions, registries, and blueprints moved to <code>Cognesy\\Addons\\AgentTemplate\\*</code>; <code>AgentContract</code> is now <code>AgentInterface</code> and <code>fromConfig()</code> returns an <code>AgentInterface</code> (no <code>Result</code> wrapper).</li> <li>Task planning capability (<code>UseTaskPlanning</code>, <code>Todo*</code> classes) and <code>LlmQueryTool</code> were removed.</li> <li>Instructor extraction contracts changed: <code>CanExtractResponse::extract()</code> now accepts <code>ExtractionInput</code> and returns an array, throwing on failure. <code>CanExtractContent</code>, <code>CanParseContent</code>, <code>DataFormat</code>, <code>JsonParser</code>, and <code>ExtractingJsonBuffer</code> were removed.</li> </ul>"},{"location":"release-notes/v1.22.0/#addons-agents","title":"Addons / Agents","text":"<ul> <li>Agent builder moved to <code>Cognesy\\Addons\\AgentBuilder\\AgentBuilder</code> with capabilities under <code>Cognesy\\Addons\\AgentBuilder\\Capabilities\\*</code>.</li> <li>New <code>SubagentProvider</code>/<code>SubagentDefinition</code> contracts and <code>EmptySubagentProvider</code>; <code>AgentTemplate\\Registry\\AgentRegistry</code> implements <code>SubagentProvider</code>.</li> <li>Agent template registry and definition flow now live under <code>Cognesy\\Addons\\AgentTemplate\\*</code> with explicit blueprint exceptions.</li> </ul>"},{"location":"release-notes/v1.22.0/#instructor","title":"Instructor","text":"<ul> <li><code>ResponseExtractor</code> now consumes <code>ExtractionInput</code> and uses <code>ResponseContent</code> for tool-mode extraction.</li> <li>Streaming extraction uses <code>ExtractingBuffer</code> and <code>PartialJsonExtractor</code> for more resilient partial JSON handling.</li> </ul>"},{"location":"release-notes/v1.22.0/#hub","title":"Hub","text":"<ul> <li>Example sources can be configured via <code>config/examples.yaml</code> (multiple source roots supported).</li> <li>Example grouping and ordering can be configured via <code>config/examples-groups.yaml</code> (subgroup include/exclude rules).</li> </ul>"},{"location":"release-notes/v1.22.0/#migration-from-v1210","title":"Migration from v1.21.0","text":"<ul> <li>Update imports to <code>Cognesy\\Addons\\AgentBuilder\\*</code> (including <code>AgentBuilder</code>, all <code>Capabilities</code>, and <code>Capabilities\\Tools</code>).</li> <li>Move agent templates and registries to <code>Cognesy\\Addons\\AgentTemplate\\*</code> and update <code>AgentContract</code> implementations to <code>AgentBuilder\\Contracts\\AgentInterface</code> with <code>fromConfig()</code> returning an <code>AgentInterface</code>.</li> <li>Remove task planning usage (<code>UseTaskPlanning</code>, <code>Todo*</code>) and <code>LlmQueryTool</code> references; replace with custom tools or direct inference calls.</li> <li>Update custom extractors to <code>CanExtractResponse::extract(ExtractionInput $input): array</code> and throw <code>ExtractionException</code> (or any <code>Throwable</code>); call with <code>ExtractionInput::fromResponse(...)</code> or <code>ExtractionInput::fromContent(...)</code>.</li> <li>Replace any direct usage of <code>ExtractingJsonBuffer</code>/<code>JsonParser</code>/<code>DataFormat</code> with <code>ExtractingBuffer</code> and the new extractor chain.</li> </ul>"},{"location":"release-notes/v1.3.0/","title":"v1.3.0","text":""},{"location":"release-notes/v1.3.0/#new-pipeline-package","title":"New Pipeline Package","text":"<ul> <li>Pipeline processing - New <code>packages/pipeline/</code> with pipeline components</li> <li>State and Result Aware Transformations - Apply transformations while maintaining computation integrity</li> <li>Conditional Processing - Execute steps based on runtime conditions</li> <li>Tap Operations - Inspect and modify data without affecting main flow</li> <li>ProcessingState Container - New immutable state wrapper with <code>Result&lt;T&gt;</code> monad and metadata via TagMap</li> <li>PipelineBuilder &amp; PendingExecution - Lazy evaluation with fluent builder pattern for pipeline construction</li> <li>Enhanced Operator System - Comprehensive set of operators: Call, ConditionalCall, Tap, Skip, Fail, and observability operators</li> <li>Advanced Error Handling - Sophisticated error strategies with <code>ErrorTag</code> and <code>CompositeException</code> support</li> </ul>"},{"location":"release-notes/v1.3.0/#utils-package-enhancements","title":"Utils Package Enhancements","text":"<ul> <li>Result Improvements - Extended Result type with better error handling and composition support</li> <li>Composite Exceptions - Better error aggregation with <code>CompositeException</code></li> <li>FrontMatter Parser - parsing front matter in documents uses Symfony YAML (replaced <code>webuni/front-matter</code>)</li> <li>Clock and Duration - time management components (<code>ClockInterface</code>, <code>SystemClock</code>, <code>VirtualClock</code>, <code>Duration</code>)</li> </ul>"},{"location":"release-notes/v1.3.0/#core-library-improvements","title":"Core Library Improvements","text":"<ul> <li>Response Generation - Enhanced <code>ResponseGenerator</code> with better partial response validation</li> <li>Partials Generator - Improved <code>PartialsGenerator</code> with unified naming and optimized processing</li> <li>Request Materialization - Streamlined request handling and validation flows</li> </ul>"},{"location":"release-notes/v1.3.0/#doc-generation-refactoring-in-progress","title":"Doc generation Refactoring (in progress)","text":"<ul> <li>Streamlined Architecture - Cleaner separation of concerns for documentation generation</li> <li>Enhanced Documentation Config - Better configuration management for docs generation</li> <li>Archived Legacy Components - Moved old doc generation components to archived folder</li> </ul>"},{"location":"release-notes/v1.3.0/#repository-structure","title":"Repository Structure","text":"<ul> <li>Script Modernization - Renamed <code>create-package.php</code> to <code>make-package</code> script</li> <li>Data Directory - Moved empty package templates to <code>data/empty-new/</code></li> </ul>"},{"location":"release-notes/v1.3.0/#codebase-cleanup","title":"Codebase Cleanup","text":"<ul> <li>Repository Organization - Better package structure with centralized configuration in packages.json</li> <li>Improved Gitignore - Enhanced ignore patterns across all packages</li> </ul> <p>Full Changelog: v1.2.0...v1.3.0</p>"},{"location":"release-notes/v1.4.0/","title":"v1.4.0","text":""},{"location":"release-notes/v1.4.0/#core-changes","title":"Core Changes","text":"<ul> <li>Migration to <code>Pipeline</code> - Library code migrated to use new <code>Pipeline</code> replacing legacy <code>RawChain</code> and <code>ResultChain</code></li> <li>Response Generation - Enhanced <code>ResponseGenerator</code> with improved error handling using <code>CanCarryState</code> interface</li> <li>Partial Response Validation - Updated validation flow to use new state interface</li> <li>Type Safety Improvements - Better type annotations and interface compliance across pipeline components</li> </ul>"},{"location":"release-notes/v1.4.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>JsonSchema factory methods - Parameter order changed in factory methods:</li> <li>Old: <code>JsonSchema::string($name, $nullable, $description)</code></li> <li>New: <code>JsonSchema::string($name, $description, $title, $nullable)</code></li> <li>Pipeline interfaces - Introduction of <code>CanCarryState</code> interface may affect custom pipeline processors (update type hints from <code>ProcessingState</code> to <code>CanCarryState</code>)</li> </ul>"},{"location":"release-notes/v1.4.0/#jsonschema-package","title":"JsonSchema Package","text":"<ul> <li>Fixed parameter ordering - Corrected <code>JsonSchema</code> factory method parameter order for consistent API across <code>string()</code>, <code>integer()</code>, <code>number()</code>, <code>boolean()</code>, <code>enum()</code>, <code>array()</code>, and <code>collection()</code> methods</li> <li>Enhanced nullable handling - Fixed bug where nullable property was incorrectly set when description was passed as second parameter</li> </ul>"},{"location":"release-notes/v1.4.0/#pipeline-package","title":"Pipeline Package","text":"<ul> <li>CanCarryState Interface - Introduced new <code>CanCarryState</code> interface for improved state management abstraction</li> <li>ProcessingState Optimizations - Slimmed down <code>ProcessingState</code> class for cleaner API</li> <li>Enhanced Error Handling - Improved error extraction and processing with better type safety</li> </ul>"},{"location":"release-notes/v1.4.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>JsonSchema nullable property - Fixed critical bug where <code>nullable: true</code> was incorrectly set in JSON schema output when description was passed as nullable parameter</li> <li>Parameter type coercion - Resolved issue where string descriptions were being cast to boolean for nullable property</li> </ul>"},{"location":"release-notes/v1.4.0/#documentation","title":"Documentation","text":"<ul> <li>Pipeline Documentation - New <code>CHEATSHEET.md</code> and <code>OVERVIEW.md</code> with current API patterns</li> </ul> <p>Full Changelog: v1.3.0...v1.4.0</p>"},{"location":"release-notes/v1.4.1/","title":"v1.4.1","text":""},{"location":"release-notes/v1.4.1/#documentation-system","title":"Documentation System","text":"<ul> <li>Dual Documentation Support - Separate Mintlify and MkDocs pipelines with independent command structure</li> <li>MkDocs Integration - Complete MkDocs documentation generation system with Material theme</li> <li>MkDocs Documentation Generator - New <code>MkDocsDocumentation</code> class for static site generation with YAML preprocessing</li> <li>Command Separation - Split documentation generation into separate commands:</li> <li><code>gen:mkdocs</code> - Complete MkDocs generation with initialization</li> <li><code>gen:mintlify</code> - Complete Mintlify generation with initialization  </li> <li><code>gen:packages</code> - Package docs only (no file clearing)</li> <li><code>gen:examples</code> - Example docs only (no file clearing)</li> <li>GitHub Pages Deployment - Automated MkDocs deployment via GitHub Actions to <code>docs-site/</code> output directory</li> </ul>"},{"location":"release-notes/v1.4.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Command Interference - Fixed Mintlify commands deleting each other's files by removing <code>initializeBaseFiles()</code> from individual commands</li> <li>Image Path Resolution - Fixed absolute image paths in MkDocs to use relative paths for GitHub Pages compatibility</li> </ul> <p>Full Changelog: v1.4.0...v1.4.1</p>"},{"location":"release-notes/v1.4.2/","title":"v1.4.2","text":""},{"location":"release-notes/v1.4.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Infinite Loop in LLMProvider - Fixed recursive call in <code>LLMProvider::using()</code> method that caused infinite loops when creating LLM providers</li> <li>Config Resolution Circular Dependency - Fixed circular dependency issues in <code>ConfigResolver</code> when handling <code>ConfigResolver</code> instances as providers</li> <li>Deferred Logic Error - Corrected inverted logic in <code>Deferred::isSet()</code> method that was causing incorrect state reporting</li> <li>OpenAI Token Parameter - Updated OpenAI driver to use <code>max_completion_tokens</code> instead of deprecated <code>max_tokens</code> for compatibility with newer models like o3-mini (thanks to @gewa24)</li> <li>Dynamic Structure Deserialization - Fixed bug where <code>Field::structure</code> was not deserializable when nested in structures used as collection item types (thanks to @gewa24)</li> </ul>"},{"location":"release-notes/v1.4.2/#core-changes","title":"Core Changes","text":"<ul> <li>Utils Package Reorganization - Moved core data structures to organized subdirectories:</li> <li><code>CachedMap</code> \u2192 <code>Data/CachedMap</code></li> <li><code>DataMap</code> \u2192 <code>Data/DataMap</code></li> <li><code>FrontMatter</code> \u2192 <code>Markdown/FrontMatter</code></li> <li>Context System - Added new <code>Context</code> and <code>Layer</code> classes to utils package for improved context management</li> <li>ImmutableDataMap - New immutable data structure for safer data handling</li> </ul>"},{"location":"release-notes/v1.4.2/#development-tools","title":"Development Tools","text":"<ul> <li>Code Validation Enhancement - Added comprehensive code block validation system in doctor package:</li> <li><code>ValidateCodeBlocks</code> command for automated validation</li> <li><code>ValidationService</code> with event-driven metrics collection</li> <li>Enhanced validation events and result tracking</li> <li>Cleanup - Removed unused clock-related classes and obsolete test files</li> </ul>"},{"location":"release-notes/v1.4.2/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Utils Package Structure - File locations changed for <code>CachedMap</code>, <code>DataMap</code>, and <code>FrontMatter</code> classes. Update import statements:   <pre><code>// Old\nuse Cognesy\\Utils\\Data\\CachedMap;\nuse Cognesy\\Utils\\DataMap;\nuse Cognesy\\Utils\\FrontMatter;\n\n// New\nuse Cognesy\\Utils\\Data\\CachedMap;\nuse Cognesy\\Utils\\Data\\DataMap;\nuse Cognesy\\Utils\\Markdown\\FrontMatter;\n</code></pre></li> </ul> <p>Full Changelog: v1.4.1...v1.4.2</p>"},{"location":"release-notes/v1.5.0/","title":"v1.5.0","text":""},{"location":"release-notes/v1.5.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Provider Architecture - LLMProvider and EmbeddingsProvider no longer create drivers directly. Custom implementations using these classes must be updated to work with the new configuration-only pattern</li> <li>Interface Requirements - Components relying on driver creation methods in providers need to implement new typed interfaces (<code>HasExplicitInferenceDriver</code>, <code>HasExplicitEmbeddingsDriver</code>)</li> </ul>"},{"location":"release-notes/v1.5.0/#architecture-refactoring","title":"Architecture Refactoring","text":"<ul> <li>Configuration Provider Pattern - Complete transformation of LLMProvider and EmbeddingsProvider into pure configuration resolvers, removing driver creation responsibilities for cleaner separation of concerns</li> <li>HTTP Client Initialization - Enhanced HTTP client handling with fallback creation in HandlesInvocation trait for improved reliability</li> <li>Type-Safe Interfaces - Added new typed interfaces: <code>CanResolveLLMConfig</code>, <code>CanResolveEmbeddingsConfig</code>, <code>HasExplicitInferenceDriver</code>, <code>HasExplicitEmbeddingsDriver</code> for better type safety</li> </ul>"},{"location":"release-notes/v1.5.0/#core-improvements","title":"Core Improvements","text":"<ul> <li>Immutable Data Structures - Enhanced message handling with immutable data structures for better thread safety and predictability</li> <li>Configuration Resolution - Improved configuration provider resolution in EmbeddingsProvider with proper events and error handling</li> <li>PHP 8.4 Support - Better support for PHP 8.4 with updated dependencies</li> </ul>"},{"location":"release-notes/v1.5.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>LLMProviderConfigTest - Fixed test configuration issues for improved reliability</li> <li>Import Statements - Corrected import statements and ensured consistent use of qualified class names</li> <li>Property Assignment - Fixed property assignment typo in HandlesFluentMethods trait</li> <li>Config Resolver Events - Added proper config resolution events and error handling throughout the system</li> </ul>"},{"location":"release-notes/v1.5.0/#development-tools-documentation","title":"Development Tools &amp; Documentation","text":"<ul> <li>Documentation Cleanup - Removed extensive outdated cookbook documentation, examples, and legacy configuration files for cleaner codebase maintenance</li> <li>Documentation Restructure - Reorganized package documentation files with consistent naming conventions (<code>OVERVIEW.md</code> \u2192 <code>CHEATSHEET.md</code>, <code>INTERNALS.md</code>)</li> <li>Configuration Files - Cleaned up deprecated <code>httpClientPreset</code> parameters from configuration presets</li> </ul> <p>Full Changelog: v1.4.2...v1.5.0</p>"},{"location":"release-notes/v1.6.0/","title":"v1.6.0","text":""},{"location":"release-notes/v1.6.0/#major-features","title":"Major Features","text":"<ul> <li>Chat Architecture - Multi-participant chat mechanism with conversation coordination, and automatic summaries</li> <li>ReAct Tool Use - New ReAct (Reasoning and Acting) driver for advanced tool-based reasoning patterns alongside existing tool calling capabilities</li> <li>Script System Refactoring - Script functionality moved from templates to dedicated messages package with enhanced rendering capabilities</li> </ul>"},{"location":"release-notes/v1.6.0/#chat-system","title":"Chat System","text":"<ul> <li>Multi-Participant Chats - Support for multiple LLM participants, human participants, and tool-equipped agents in conversation flows</li> <li>Chat Coordination - Built-in coordinators including LLM-based, tool-based, and round-robin participant selection</li> <li>Conversation Summaries - Automatic chat summarization with configurable buffering and context management</li> <li>Chat State Management - Comprehensive state tracking with steps, outcomes, and continuation criteria</li> <li>Chat Events - Complete event system for monitoring chat lifecycle, participant selection, and tool usage</li> </ul>"},{"location":"release-notes/v1.6.0/#tool-use-enhancements","title":"Tool Use Enhancements","text":"<ul> <li>ReAct Driver - New reasoning and acting pattern for more sophisticated tool use with decision tracking</li> <li>Tool Calling Refactoring - Enhanced tool calling driver with improved error handling and execution flow</li> <li>Tool Execution Formatting - Better formatting and display of tool execution results</li> <li>Multiple Driver Support - Clean separation between ReAct and tool calling approaches</li> </ul>"},{"location":"release-notes/v1.6.0/#architecture-improvements","title":"Architecture Improvements","text":"<ul> <li>Messages Package - Script system relocated from templates to messages package for better separation of concerns</li> <li>Rendering System - New arrow-pipe messages renderer and role-based rendering capabilities</li> <li>Time Management - Added clock interfaces for better testability and time-dependent operations</li> <li>Continuation Criteria - Enhanced continuation logic with token limits, execution time limits, and step limits</li> </ul>"},{"location":"release-notes/v1.6.0/#core-enhancements","title":"Core Enhancements","text":"<ul> <li>Structured Output - Improved deserialization and streaming capabilities for structured data</li> <li>HTTP Client - Enhanced HTTP client handling with better error management</li> <li>Documentation Testing - Major Doctest architecture refactoring with improved code block extraction and validation</li> <li>Cohere Integration - Fixed max_tokens parameter handling for Cohere API</li> </ul>"},{"location":"release-notes/v1.6.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Property Info Adapter - Enhanced compatibility with PropertyInfo v7 for better type introspection</li> <li>Test Infrastructure - Added comprehensive fake inference drivers and mock HTTP testing capabilities</li> <li>Memory Management - Improved memory handling in streaming and sequence processing</li> </ul>"},{"location":"release-notes/v1.6.0/#testing-quality","title":"Testing &amp; Quality","text":"<ul> <li>Test Coverage - New test suites for Chat, ToolUse, and Doctest functionality</li> <li>Mock Infrastructure - Enhanced testing infrastructure with fake drivers and HTTP mocking</li> <li>Static Analysis - Updated PHPStan and Psalm configurations for better code quality</li> </ul>"},{"location":"release-notes/v1.6.0/#documentation","title":"Documentation","text":"<ul> <li>Package Documentation - Updated CHEATSHEET.md files across packages with comprehensive usage examples</li> <li>Chat Documentation - New dedicated documentation for chat and tool use patterns</li> <li>Architecture Guides - Enhanced OVERVIEW.md files explaining package internals and design decisions</li> </ul> <p>Full Changelog: v1.5.0...v1.6.0</p>"},{"location":"release-notes/v1.7.0/","title":"v1.7.0","text":""},{"location":"release-notes/v1.7.0/#major-changes","title":"Major Changes","text":""},{"location":"release-notes/v1.7.0/#chat-module-refactoring","title":"Chat Module Refactoring","text":"<ul> <li>Refactoring of the Chat system - cleaner, simpler code</li> <li>Improved architecture supporting before, after, and before-and-after processing patterns</li> </ul>"},{"location":"release-notes/v1.7.0/#messagestore-improvements","title":"MessageStore Improvements","text":"<ul> <li>Migrated from Script to MessageStore for better clarity of purpose</li> <li>Replaced array-based section handling with dedicated Sections collection</li> <li>Enhanced message handling consistency across the system</li> </ul>"},{"location":"release-notes/v1.7.0/#http-client-enhancements","title":"HTTP Client Enhancements","text":"<ul> <li>Improved exception handling hierarchy with specific error types</li> <li>Better error categorization: client errors, server errors, network issues, timeouts</li> </ul>"},{"location":"release-notes/v1.7.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Chat API: <code>Chat::default()</code> method removed, use <code>ChatFactory::default()</code> instead</li> <li>Chat State Processing: Middleware pattern replaces simple processor loop</li> <li>Script Module: Replaced with MessageStore - update imports and usage patterns</li> <li>HTTP Exceptions: New exception hierarchy may require catch block updates</li> </ul>"},{"location":"release-notes/v1.7.0/#improvements","title":"Improvements","text":"<ul> <li>Enhanced type safety across Chat components</li> <li>Better separation of concerns in processing pipelines  </li> <li>More consistent API patterns across modules</li> <li>Improved test coverage and reliability</li> </ul>"},{"location":"release-notes/v1.8.0/","title":"v1.8.0","text":""},{"location":"release-notes/v1.8.0/#major-changes","title":"Major Changes","text":""},{"location":"release-notes/v1.8.0/#immutable-tooluse-system","title":"Immutable ToolUse System","text":"<ul> <li><code>ToolUseState</code> and <code>ToolUseStep</code> are now immutable readonly classes</li> <li>State transitions create new instances instead of modifying existing ones</li> <li>Functional composition replaces method chaining</li> </ul>"},{"location":"release-notes/v1.8.0/#enhanced-chat-events","title":"Enhanced Chat Events","text":"<ul> <li>Added <code>ChatResponseRequested</code> event for tracking participant responses</li> <li>Improved event dispatching with dedicated methods per event type</li> <li>Better participant selection and state update event handling</li> </ul>"},{"location":"release-notes/v1.8.0/#improved-message-compilation","title":"Improved Message Compilation","text":"<ul> <li>Chat participants now handle their own message compilation</li> <li><code>CanCompileMessages</code> interface used consistently across participants</li> <li>Default compiler (<code>AllSections</code>) applied where not specified</li> </ul>"},{"location":"release-notes/v1.8.0/#internal-improvements","title":"Internal Improvements","text":""},{"location":"release-notes/v1.8.0/#processing-pipeline","title":"Processing Pipeline","text":"<ul> <li><code>StepProcessors</code> rewritten using middleware pattern</li> <li>State processors use functional composition for better predictability</li> <li>Continuation criteria refactored as immutable collections</li> </ul>"},{"location":"release-notes/v1.8.0/#code-organization","title":"Code Organization","text":"<ul> <li>Event emission extracted to private methods for better maintainability (selected locations so far)</li> <li>Consistent error handling across participant implementations</li> <li>Reduced coupling between chat state and compilation logic</li> </ul>"},{"location":"release-notes/v1.8.0/#performance","title":"Performance","text":"<ul> <li>Immutable structures reduce side effects and improve memory safety</li> <li>Functional approach enables better optimization opportunities</li> <li>Cleaner separation of concerns reduces object graph complexity</li> </ul>"},{"location":"release-notes/v1.8.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Removed: <code>ToolUseOptions</code> class - configuration now handled directly in <code>ToolUse</code> constructor</li> <li>Changed: <code>CanUseTools::useTools()</code> signature now requires <code>Tools</code> parameter</li> <li>Removed: Observer interfaces (<code>ToolUseObserver</code>, <code>ToolsObserver</code>) - replaced with event system</li> <li><code>CanProcessToolStep::processStep()</code> \u2192 <code>CanProcessToolState::process()</code></li> <li><code>ToolUse</code> methods (<code>withOptions()</code>, <code>withTools()</code>, <code>withMessages()</code>) removed</li> <li><code>ChatState::compiledMessages()</code> removed - compilation moved to participants</li> </ul>"},{"location":"release-notes/v1.8.1/","title":"v1.8.1","text":""},{"location":"release-notes/v1.8.1/#bug-fixes","title":"Bug Fixes","text":""},{"location":"release-notes/v1.8.1/#test-suite-corrections","title":"Test Suite Corrections","text":"<ul> <li>Fixed test failures caused by removal of <code>Messages::middle()</code> method in v1.8.0</li> <li>Updated test assertions to use alternative message access patterns</li> <li>Restored test suite compatibility with new immutable message handling</li> </ul>"},{"location":"release-notes/v1.8.1/#notes","title":"Notes","text":"<p>This is a patch release addressing test suite issues introduced in v1.8.0. No functional changes to the library itself.</p>"},{"location":"release-notes/v1.9.0/","title":"v1.9.0","text":""},{"location":"release-notes/v1.9.0/#190-summary","title":"1.9.0 Summary","text":"<ul> <li>Unified execution model across Instructor and Polyglot</li> <li>Streaming and usage accounting made consistent and test\u2011covered</li> <li>Cleaner APIs and collections, clearer events</li> </ul>"},{"location":"release-notes/v1.9.0/#instructor","title":"Instructor","text":"<ul> <li>Execution/Attempts: <code>StructuredOutputExecution</code> + <code>StructuredOutputAttempt</code> track full lifecycle (final + partials, errors).</li> <li>Streaming: sequence updates yield immutable snapshots; final event payload standardized to <code>value</code> (+ <code>cached</code>).</li> <li>Usage: execution = finalized attempts + current partial tokens (until finalization). Attempt list usage counts only finalized.</li> <li>API cleanup: removed unused <code>$messages</code> from <code>withCurrentAttempt/withFailedAttempt/withSuccessfulAttempt</code> (retries come from execution history).</li> <li>Collections: <code>StructuredOutputAttemptList::of</code> variadic fix; <code>fromArray</code> supports wrapped and flat input.</li> </ul>"},{"location":"release-notes/v1.9.0/#polyglot","title":"Polyglot","text":"<ul> <li>Execution/Attempts: <code>InferenceExecution</code> + <code>InferenceAttempt</code> with immutable transitions and full state.</li> <li>Usage: attempt usage = finalized response + partial tokens; AttemptList = only finalized; Execution = finalized + current partials.</li> <li>Collections: variadic fixes (<code>InferenceAttemptList</code>, <code>InferenceResponseList</code>); <code>fromArray</code> uses <code>ArrayList::fromArray</code>.</li> <li>Correctness: <code>InferenceAttempt::hasErrors/isFailed</code> safety.</li> </ul>"},{"location":"release-notes/v1.9.0/#embeddings","title":"Embeddings","text":"<ul> <li>Facade: declared resolver property; removed unused field; avoid duplicate \u201crequested\u201d event (driver emits once).</li> <li>Shortcuts: <code>first()</code> returns <code>?Vector</code>. Cosine similarity guarded for zero vectors.</li> </ul>"},{"location":"release-notes/v1.9.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Instructor <code>StructuredOutputExecution</code>: removed <code>$messages</code> params from attempt helpers.</li> <li>Instructor <code>RequestHandler</code>: <code>responseFor</code> \u2192 <code>executionResultFor</code>; <code>streamResponseFor</code> \u2192 <code>streamUpdatesFor</code>.</li> <li>Usage semantics tightened to avoid double counting (see above).</li> <li>Polyglot driver/collection variadic behavior corrected.</li> </ul>"},{"location":"release-notes/v1.9.1/","title":"v1.9.1","text":""},{"location":"release-notes/v1.9.1/#changes","title":"Changes","text":""},{"location":"release-notes/v1.9.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Prevent null dereference when finalizing streaming responses with no current attempt</li> <li>packages/polyglot/src/Inference/Data/InferenceExecution.php<ul> <li>Null-safe access to <code>currentAttempt</code> in <code>withFinalizedPartialResponse()</code> and <code>withFailedAttempt()</code></li> </ul> </li> <li> <p>packages/polyglot/src/Inference/Data/InferenceAttempt.php</p> <ul> <li>Null-safe <code>withFinalizedPartialResponse()</code> using empty <code>PartialInferenceResponseList</code> when needed</li> </ul> </li> <li> <p>Console display stability in evals</p> </li> <li>packages/evals/src/Console/Display.php<ul> <li>Replaced invalid <code>ImmutableDataMap::except()</code> usage with safe filtering of <code>toArray()</code> output</li> <li>Fixed constant name <code>COLOR::BOLD</code> \u2192 <code>Color::BOLD</code></li> </ul> </li> </ul>"},{"location":"release-notes/v1.9.1/#behaviorcompatibility","title":"Behavior/Compatibility","text":"<ul> <li>Groq driver request body cleanup</li> <li>packages/polyglot/src/Inference/Drivers/Groq/GroqBodyFormat.php<ul> <li>Removed redundant <code>max_tokens</code> \u2192 <code>max_completion_tokens</code> conversion (already handled by parent)</li> </ul> </li> </ul>"},{"location":"release-notes/v1.9.1/#immutability-usage-accounting","title":"Immutability &amp; Usage Accounting","text":"<ul> <li>Immutable accumulation for usage data to avoid inadvertent mutation/double-counting</li> <li>packages/polyglot/src/Inference/InferenceResponseFactory.php<ul> <li>Use <code>withAccumulated(...)</code> when combining partial usage</li> </ul> </li> <li>packages/addons/src/StepByStep/State/Traits/HandlesUsage.php<ul> <li>Replace clone+mutate with <code>withAccumulated(...)</code></li> </ul> </li> <li>packages/evals/src/Traits/Experiment/HandlesAccess.php<ul> <li>Compute experiment usage on demand by accumulating execution usages immutably</li> </ul> </li> <li>packages/evals/src/Experiment.php<ul> <li>Removed cached <code>$usage</code> and post-run accumulator; observations/summary rely on computed usage</li> </ul> </li> </ul>"},{"location":"release-notes/v1.9.1/#notes","title":"Notes","text":"<ul> <li>This release focuses on correctness and immutability:</li> <li>Eliminates potential fatals in streaming finalization and console display</li> <li>Standardizes usage accumulation to immutable patterns across modules</li> </ul>"},{"location":"release-notes/versions/","title":"Overview","text":""},{"location":"release-notes/versions/#versioning","title":"Versioning","text":""},{"location":"release-notes/versions/#versioning-rules","title":"Versioning Rules","text":"<p>Starting from version 1.0.0 Instructor follows semantic versioning (SemVer) with version numbers in the format x.y.z (e.g., 1.2.3, where x is the major version, y is the minor version, and z is the patch version).</p> <p>Use these rules to plan updates for the Instructor library:</p> <ul> <li>Major version (x): Incremented for significant changes, such as extensive refactoring of the core library or breaking changes to public APIs that are incompatible with previous versions. Major version updates may not be fully incompatible, but compatibility depends on the specific changes. Always consult the upgrade guide for the corresponding major version to understand the impact.</li> <li>Minor version (y): Incremented for backward-compatible additions, such as new features or components, or for breaking changes to a limited subset of public APIs (e.g., modifying or removing specific APIs). While minor versions aim to maintain compatibility, breaking changes in these releases may affect some use cases. Refer to the upgrade guide for details.</li> <li>Patch version (z): Incremented for backward-compatible bug fixes, security patches, or minor enhancements that do not affect existing functionality. In rare cases, a patch release may include breaking changes to fix a completely unusable feature, but these changes are not treated as minor version updates since the affected functionality was already broken. New features or components introduced in patch releases are designed to be backward-compatible and should not impact existing code.</li> </ul>"},{"location":"release-notes/versions/#upgrading-instructor","title":"Upgrading Instructor","text":"<p>When upgrading the Instructor library, follow these guidelines:</p> <ul> <li>Major (x) or Minor (y) Upgrades: Review the upgrade guide for the specific version in the documentation. These upgrades may include breaking changes or new features that require code adjustments.</li> <li>Patch (z) Upgrades: These are backward-compatible and can typically be applied by running composer update instructor-php in your project\u2019s root directory to update the dependency. No additional changes are usually required.</li> </ul> <p>We recommend upgrading all Instructor components together to ensure a consistent development experience, rather than updating individual components separately.</p>"},{"location":"snippets/snippet-intro/","title":"Snippet intro","text":"<p>One of the core principles of software development is DRY (Don't Repeat Yourself). This is a principle that apply to documentation as well. If you find yourself repeating the same content in multiple places, you should consider creating a custom snippet to keep your content in sync.</p>"},{"location":"snippets/snippet-intro/#setting-up","title":"Setting up","text":"<p>The first step to data processing with LLMs is setting up your editing environments.</p> <p>          Setup your API keys in .env file to access LLM API provider               Run examples to see how Instructor in action      </p>"},{"location":"snippets/snippet-intro/#using-instructor","title":"Using Instructor","text":"<p>Learn how to use Instructor to process your data with LLMs.</p> <p>          Understand basic concepts behind Instructor               Learn how to use Instructor in your projects               Find out the ways to define response data models               Use validation to automatically retry for incorrect LLM responses      </p>"}]}